<?xml version="1.0" encoding="utf-8" ?>
<body>
<Informacion xmlns='http://www.w3.org/1999/xhtml' >

<style type='text/css'>
	
@charset 'utf-8';

table{
	border: 1px black solid;
	border-radius: 5px;
	min-width: 400px;
	font-family: Helvetica,Arial;
	margin-top: 10px;
	margin-right: 40px;
	margin-bottom: 10px;
	margin-left: 40px;
	} 

table td{
	padding:7px;
    text-align:justify;
	}

.tablas_corpus { 
	font-family: Arial, Helvetica, sans-serif;
	line-height:25px;
	float:left;
	}

.tablas_corpus table{
	font-family: 'Lucida Sans Unicode', 'Lucida Grande', Sans-Serif;
	font-size: 14px;
	text-align: left;
	border-top-style: none;
	border-right-style: none;
	border-bottom-style: none;
	border-left-style: none;
	}

.tablas_corpus th {
	font-size: 13px;
	font-weight: normal;
	padding: 8px;
	background: #b9c9fe;
	color: #039;
	border: 1px solid #FFFFFF;
	}

#campos{
    text-align:center;
    font-weight: bold;
    color: #4646C1;
	}

.tablas_corpus td {
	padding: 8px;
	background: #e8edff;
	color: #4E4E4E;
	border: 1px solid #fff;
	}

.tablas_corpus tr:hover td { 
	background: #d0dafd; 
 	}

h1 {
	color: #666666;
 	font-family: Helvetica Neue, Arial, Helvetica, sans-serif;
	letter-spacing: -1px;
	text-decoration: none; 
	text-shadow: 2px 2px #fff, 0 0 #0e0e0e, 3px 4px 2px #e3e3e3; 
	text-transform: none; 
	word-spacing: -2px;
	}

.titulo{
	width: 1000px;
	height: 50px;
	margin-left: auto;
	margin-right: auto;
	background-color: #BDDDF2;
	text-align: center;
	vertical-align: middle;
	line-height: 50px;
	border-radius: 26px 26px 26px 26px;
	}

resultados{
	float: right;
	padding-right: 100px;
    font-weight: bold;
	margin-top: 40px;
	margin-bottom: 10px;
	}

</style>
<div class='titulo'> <center><h1> INAOE CORPUS </h1></center> </div>
<resultados> Resultados = 66</resultados>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Metodología de Medición y Evaluación de la Usabilidad en
Sitios Web Educativos</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La gran cantidad de información disponible en Internet, la complejidad y heterogeneidad de la
misma, junto con las distintas características de los usuarios que acceden a dicha información, así
como la gran competencia existente en la red, hacen crítico que el desarrollo de aplicaciones
basadas en Web, y de manera particular los sitios Web educativos deban contemplar no solo
aspectos de funcionalidad sino también de usabilidad, tomando en cuenta que estas aplicaciones no
solo deben tener como objetivo poner a disposición de los usuarios información exacta, sino orientar
adecuadamente al usuario en su búsqueda, adecuando los caminos más apropiados para cada
usuario de acuerdo con sus objetivos, sus conocimientos previos o sus necesidades y perfiles.
La usabilidad en este contexto, se enmarca en una pregunta sobre si una aplicación Web educativa
es lo suficientemente buena para satisfacer las necesidades y requerimientos del usuario. Es decir,
la usabilidad corresponde a una variable en el marco de la aceptación práctica de un sitio Web
educativo. Así, para que un sitio Web pueda ser utilizado para alcanzar alguna tarea, tiene que
cumplir con criterios de utilización (es decir referido a la funcionalidad: puede hacer lo que se
necesita) y usabilidad (cuán bien los usuarios pueden usar esa funcionalidad del sitio Web). La
usabilidad considera todos los aspectos de un sitio Web educativo con el que el usuario puede
interactuar y sus principales criterios de evaluación (aprendizaje, operabilidad, comunicación
contenido, etc.). Por ejemplo, el sitio debe cumplir una funcionalidad de presentar determinado tipo
de contenido, pero este contenido debe ser presentado de una manera atractiva y sencilla al usuario
y que además el aprendizaje de su uso y navegación sea intuitiva y fácil, para llevar a cabo un
proceso de aprendizaje rápido y eficaz.
Muchos son los autores que han investigado y propuesto recomendaciones (guías, listas de
comprobación, principios, etc.) para el desarrollo de aplicaciones usables ([NIE90a], [NIE94a],
[SHN98], [BEV93] entre otros), y muy pocos sobre la evaluación de la usabilidad de sitios Web
educativos [GON03], [CAR02]. Sin embargo, basta hacer una exploración en la Web para
comprobar que esto aún sigue siendo un problema. Si tenemos en cuenta que existe una diversidad
de dominios de aplicación basadas en Web, comerciales, ocio, entretenimiento, educación, etc.; una
mala usabilidad en sitios comerciales, de ocio o incluso de noticias puede significar simplemente que
el usuario busque otro lugar. Sin embargo, en el caso de sitios de carácter educativo el problema se
agudiza ya que una mala usabilidad puede repercutir fuertemente en los objetivos de aprendizaje.
Es decir, los requisitos de usabilidad de un sitio Web comercial de venta de herramientas de
carpintería orientado a aquellos profesionales del área o personas aficionadas a la carpintería, cuyo
interés esta centrado en encontrar un producto de carpintería determinado al mejor precio, será
diferente de los requisitos de usabilidad del sitio Web de algún curso de una universidad o institución
educativa, en principio por que las motivaciones de los usuarios son distintas, el perfil de ellos
también lo es, así como las metas a alcanzar por cada uno. Así preparar el proceso de evaluación
de usabilidad de un sitio comercial no será el mismo que el de un sitio educativo aunque ambos
buscan cubrir un servicio, lograr la satisfacción del usuario y en algunos casos ambos pretendan
lograr dividendos económicos. Los sitios Web educativos tienen un añadido fundamental, y es la
transmisión y adquisición del conocimiento. Por ende un factor importante de evaluación es el
aspecto educativo y su presentación de contenidos.
La consideración de la usabilidad como un factor importante en el desarrollo de aplicaciones Web ha
ido en aumento en los últimos años. Basándose en el fundamento teórico de la usabilidad de
software tradicional, y en los principios establecidos por los estándares internacionales y en la experiencia adquirida por el diseño y desarrollo de aplicaciones Web, se han desarrollado
metodologías y guías para el desarrollo de aplicaciones Web con criterios de usabilidad. Estas
permiten realizar un proceso de evaluación de usabilidad, pero limitados a aspectos específicos de
la misma, y enfocados a la evaluación de sitios comerciales, que aunque señalan su extensión a
sitios educativos, los criterios considerados no se ajustan a los requisitos de usabilidad de estas
aplicaciones.
En la actualidad existen diferentes métodos que pueden ser usados durante una evaluación de
usabilidad. Dependiendo del propósito de medición, del tipo de medida a obtener, de la etapa del
ciclo de vida, etc., unos u otros son usados para asegurar referencias que mejoren la usabilidad o
establezcan si esta es lo “suficientemente buena. Desafortunadamente, no hay un acuerdo respecto
a la denominación, uso y aplicación de los métodos existentes en el desarrollo de aplicaciones Web.
Así los diferentes autores los denominan de acuerdo a sus preferencias y juicio o experiencia
(Kirakowski: Indagación, [KIR95]), (Nielsen: las Heurísticas, Nielsen [NIE94], [NIE90c]), (Preece:
Evaluación de expertos [PRE93]), (Scriven: Formativa/Sumativa [SCR67]). Los métodos de
evaluación de usabilidad por un lado tienen fortalezas y debilidades, y por otro están enfocados a
evaluar determinados aspectos o requisitos de usabilidad, por lo que es recomendable combinarlos
en una evaluación para complementar unos con otros en cuanto a sus fortalezas y lograr cubrir un
mayor número de aspectos de evaluación. La selección y combinación de los métodos de
evaluación dependerá de restricciones financieras y de tiempo, de las fases en el ciclo de desarrollo
y de la naturaleza del sistema bajo desarrollo (Bevan, [BEV94]), (Nielsen, [NIE94b]).
En la aplicación de los métodos de evaluación de usabilidad, los evaluadores hacen uso de un
conjunto de técnicas diseñadas para la recolección de datos de usabilidad, y definidas en términos
del método que soportan, su aplicabilidad en las diferentes etapas de desarrollo, costos y número de
participantes necesarios para la obtención de la información. (Focus Group [NIE97c], Think Aloud
[BOR00], cuestionarios, entre otros). Es importante establecer que al igual que con los métodos es
necesario seleccionar las técnicas más adecuadas en función a sus características y fortalezas, así
como combinarlos adecuadamente para la obtención de una mayor cantidad de información que sea
segura y confiable.
Hasta ahora, se han desarrollado muchas herramientas de soporte para la aplicación de
determinados métodos de evaluación de usabilidad, y aunque estas están basadas en un mismo
método y utilizan técnicas comunes en la colección de datos, no se ha logrado establecer un
acuerdo respecto a los criterios de evaluación ni entre ellos, ni respecto a los estándares
establecidos. En su mayoría han sido desarrolladas para dar soporte a la evaluación de una
aplicación particular en un contexto muy específico, principalmente aplicaciones comerciales.
A la actualidad se han desarrollado algunas metodologías para evaluar la usabilidad de aplicaciones
basadas en Web, en su mayoría orientadas principalmente a sitios comerciales. Ante esta situación
creemos necesario un modelo de evaluación de usabilidad orientada específicamente a las
aplicaciones educativas basadas en Web, que contemple los objetivos propios de este dominio, y
aspectos como las necesidades de la audiencia destino y el perfil del usuario final.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Es una metodología que soporta la evaluación remota de sitios Web [PAT99]. Consiste en el análisis
asistido del registro de eventos de usuario basado en el modelo de tarea. Este enfoque combina dos
técnicas que usualmente son aplicadas separadamente:
* Pruebas empíricas.
* Evaluación basada en modelo.
La metodología está orientada a proporcionar a los evaluadores información para identificar partes
problemáticas de la interfaz de usuario y posibles sugerencias para mejorarlo. Estos resultados
relacionan las tareas propuestas para que ejecute el usuario, las páginas Web y su mutua relación,
permitiendo analizar datos relacionados a las interacciones del usuario y compararlos al modelo de
tarea correspondiente al diseño del sitio Web. Esta metodología hace uso de la herramienta
REMUSINE.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este capítulo se presenta un resumen del proceso seguido en el desarrollo de esta tesis, de los
resultados más destacables obtenidos, de las ventajas del sistema diseñado y finalmente se
describen los trabajos en curso y las líneas de investigación futuras.
12.1 Proceso seguido en el desarrollo de la tesis
Necesidad de evaluación de la usabilidad para sitios Web educativos
Este trabajo de tesis se inicia con la presentación de los problemas respecto a la evaluación de la
usabilidad en la Web, encontrándose que no existe una estandarización respecto al qué, cómo y
cuándo realizarla, sino que se han desarrollado y/o utilizado métodos de manera aislada y con
criterios específicos para evaluar un producto particular. En principio se pudo determinar que si bien
existen algunas metodologías desarrolladas para la evaluación de usabilidad éstas están orientadas
a las aplicaciones comerciales en la Web, por lo que al ser aplicadas a entornos educativos no
permiten evaluar todos los aspectos que el desarrollo de este tipo de aplicaciones considera, como
son por ejemplo los contenidos educativos, las facilidades de comunicación o el método de trabajo.
Por otro lado, y considerando que el contenido educativo es diseñado para una audiencia
determinada, y que en la actualidad se ha incrementado considerablemente este tipo de sitios, lo
que conlleva al incremento en la diversidad de usuarios que participan en ellos (cada uno con
diferentes características y expectativas), el conocer el perfil de esa audiencia en el proceso de
evaluación se convierte en un aspecto crítico.
Estudio de la evaluación de la usabilidad
Respecto a la evaluación de la usabilidad, los estudios mostraron que cada investigador o grupo de
investigadores implementa, de acuerdo a sus habilidades, sus propios mecanismos de evaluación
(Véase capítulo 2), de tal forma que se pueden encontrar métodos de evaluación de lo más diverso,
que aunque están orientados al mismo dominio, los elementos de evaluación considerados difieren
sustancialmente. Es decir, no existe un consenso respecto a que se debe medir. Además, no existe
aún uniformidad en la denominación de los procesos (métodos, técnicas, modelo, etc.) pudiendo
encontrarse a diferentes autores refiriéndose de modo distinto a un mismo proceso. Por ello, fue
necesario conocer las características de un proceso de evaluación en usabilidad, para poder
diferenciar y ubicar con claridad los métodos, técnicas y herramientas (capítulos 2 y 3), así como
identificar qué y cómo evalúan y cuando son aplicables de manera que resulten eficientes y de bajo
costo. De este estudio se puede concluir que los métodos de evaluación están enfocados a evaluar
aspectos específicos de usabilidad, los cuales necesitan de ciertas técnicas para poder obtener un
conjunto de datos válidos para la evaluación. La herramienta por su parte supone la adaptación de
la técnica en función del objetivo de evaluación perseguido por el método. El trabajo realizado nos
llevó a concluir que los métodos, técnicas y herramientas deberían ser utilizados de una manera
ordenada y sistemática de acuerdo a los propósitos de evaluación perseguidos. Por ello parecía
conveniente revisar las metodologías de evaluación de la usabilidad de aplicaciones Web existentes
a fin de evaluar sus limitaciones y fortalezas (desarrollado en el capítulo 4). Estas metodologías
están enfocadas bien en evaluar la eficiencia del usuario o de la aplicación en el logro de la tarea
propuesta o bien en evaluar algunos aspectos específicos de la usabilidad de sitios de comercio
electrónico.
Necesidad de una metodología
De lo anterior se deduce que poco se ha hecho para integrar métodos, técnicas y herramientas en
un proceso coherente, metódico y sistemático que permita obtener datos válidos y confiables de la
evaluación de la usabilidad. Además, teniendo en cuenta que la investigación realizada no sólo
perseguía entender el proceso de evaluación de la usabilidad Web sino principalmente la evaluación
de la usabilidad en los sitios Web educativos, se ha podido comprobar la inexistencia de
metodologías de evaluación diseñadas para este dominio, si existen, sin embargo, algunas
metodologías desarrolladas para aplicaciones comerciales, ambientes virtuales colaborativos u otro
tipo de aplicación muy concreta. Por todo esto, se propuso la adopción de una metodología de evaluación para este tipo de sitios (capítulo 5), que incluya la combinación de los métodos y las
técnicas más adecuadas y económicas y que abarquen un mayor número de aspectos a evaluar.
Para ello se considero necesario abarcar la perspectiva del usuario y la del experto en la evaluación
para lograr una mayor eficiencia y confiabilidad en el proceso de evaluación de acuerdo a los
objetivos perseguidos. Un análisis de los mismos (realizado en el capítulo 6) nos llevó a combinar
los métodos de indagación, con los métodos de inspección, utilizando las técnicas de cuestionario y
lista de verificación respectivamente por ser las que mejor se ajustan a los objetivos perseguidos
para lograr una evaluación de usabilidad basada en los datos proporcionados por el evaluador
(usuario / experto).
Evaluación del experto
El modelo de evaluación de experto (Capítulo 7) está basado en el uso varios métodos de
inspección, debido a que algunos estudios empíricos han demostrado que el porcentaje de
problemas detectados por cada experto utilizando sólo un método de inspección es bajo. Por ello en
nuestro enfoque proponemos combinar el uso de estos métodos, para lograr por un lado, una mayor
perspectiva de evaluación enfocando tanto problemas de la interfaz como de diseño y contenido
educativo, y por otro lado, para complementar la evaluación basada en el usuario. Por esta razón se
ha considerado una lista de verificación para cada método incluido. Además fue necesario definir el
perfil del experto que participaría en el proceso, el número de ellos, y los requisitos de información
necesarios para realizar la evaluación. Los criterios de valoración de estas listas de verificación, sin
embargo, pueden variar en función de la naturaleza de los métodos.
Con el objetivo de que la evaluación de experto permita la detección directa de problemas de
usabilidad, así como señalar su urgencia de mejora fue necesario considerar la inclusión de un
modelo basado en los aspectos de severidad, importancia y consistencia que permitía calcular el
impacto que cada problema detectado causa en la usabilidad del sitio y que determina la prioridad
de mejora del mismo.
Evaluación de usuario
Para poder definir un modelo de evaluación de usuario (véase capítulo 8) fue necesario conocer las
características de este dominio de aplicación, y la de sus usuarios, a fin de determinar los requisitos
de usabilidad a aplicar en un proceso de evaluación. Esto nos condujo a establecer que las
necesidades de usabilidad de un sitio podían variar sustancialmente en función a la edad del
usuario, por lo cual se decidió que era necesario definir una lista de evaluación adecuada a cada
nivel de audiencia definida por la edad. Del mismo modo, los estudios permitieron determinar que la
experiencia del usuario es un factor crítico en el uso de este tipo de aplicaciones y, por consiguiente,
en la manera como entienden y evalúan la usabilidad de él. Por ello, se utilizó un modelo que
permite determinar el perfil del usuario que evalúa un sitio educativo, con el fin de tamizar los
resultados obtenidos.
Asimismo, respecto a la lista de parámetros de medición de la usabilidad utilizados en la evaluación
de usuario, fue necesario estudiar y adaptar algunas técnicas para:
a) Establecer el conjunto de parámetros más apropiados a cada audiencia
b) La clasificación de estos parámetros, en función a la necesidad de su presencia para lograr el
nivel de usabilidad deseado y,
c) La asignación de pesos relativos para cada parámetro establecido, de acuerdo a la
importancia de este en la usabilidad del sitio.
Finalmente fue necesario definir un modelo de puntuación de la evaluación de la usabilidad. Este
modelo fue diseñado basándose en un árbol de requisitos, donde la puntuación obtenida parte de
las hojas (atributos) hasta llegar a la raíz (criterios), es decir, a partir de la valoración de los
parámetros directamente medibles (atributos) se utilizaron funciones adecuadas que permitieran por
agregación obtener la puntuación de los parámetros de niveles superiores.
Necesidad de herramientas para la evaluación
La metodología propuesta puede aplicarse de manera manual. Sin embargo, por la naturaleza del
tipo de aplicación a evaluar y el hecho de emplear dos métodos de evaluación que persiguen la
obtención de datos (los cuales deben ser procesados y comparados), así como la necesidad de un
análisis de los resultados de nivel y perfil, parece necesario disponer de herramientas de soporte a
la metodología propuesta, tal como se describe en el capítulo 10. En este sentido se han diseñado
dos herramientas para cubrir la evaluación del usuario y la evaluación del experto dando soporte al proceso de evaluación y al tratamiento de los datos para la obtención de resultados. Además las
herramientas diseñadas posibilitan una mayor participación en los evaluadores remotos.
Construcción de prototipos
Con el objetivo de facilitar la realización de la evaluación, la recolección de datos y reducir el tiempo
de su procesamiento, se implementaron dos prototipos Web, uno para dar soporte a la evaluación
del usuario y otro para dar soporte a la evaluación del experto (capítulo 11).
Finalmente, para probar la eficiencia en la obtención de resultados del modelo de evaluación
propuesto, se aplicó este proceso a la evaluación de un sitio educativo, con contenido pedagógico,
como es educar.org. Con él se ha realizado la experiencia de evaluación de usuario y la evaluación
de experto aplicando las técnicas de recolección y tratamiento de datos establecidos en el capítulo
9, con el fin de verificar los resultados obtenidos, comprobando la completitud del modelo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE OVIEDO
Departamento de Informática
TESIS DOCTORAL
Metodología de Medición y Evaluación de la Usabilidad en
Sitios Web Educativos
Presentada por
María Elena Alva Obeso
Para la obtención del título de Doctora en Informática
Dirigida por
Doctor D. Juan Manuel Cueva Lovelle
Doctora Dña. Ana Belén Martínez Prieto
Oviedo, Marzo de 2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>UN SERVIDOR HÍBRIDO ADAPTATIVO PARA LA DIFUSIÓN DE INFORMACIÓN EN ENTORNOS DE COMUNICACIÓN ASIMÉTRICOS CON RESTRICCIONES TEMPORALES.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Los principales objetivos de la presente tesis son la
presentación, estudio, análisis y evaluación cuantitativa de un
modelo de servidor de difusión híbrido adaptativo que se comporte
de manera eficiente en entornos de comunicación asimétricos con
restricciones temporales y perfiles dinámicos de acceso a la
información.
Nuestra propuesta se basa en el aprovechamiento máximo del
ancho de banda disponible con el objetivo de minimizar el número
de plazos perdidos y en consecuencia satisfacer el mayor número de
peticiones de información.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con la creciente popularidad de aplicaciones para usuarios de
dispositivos móviles, la transmisión utilizando difusión ha
demostrado ser el modo más eficiente para diseminación de
información a un gran número de usuarios con un alto grado de
concordancia en sus demandas de información.
La necesidad de diseñar servidores de información eficientes y
escalables adecuados a estos entornos es patente. En este trabajo
de investigación hemos presentado, analizado y evaluado un nuevo
modelo de servidor de información para entornos de comunicación
asimétricos con restricciones temporales donde la población de
usuarios presenta perfiles de acceso a la información dinámicos
debido a la movilidad.
El modelo AHB presentado y analizado en la presente tesis
utiliza difusión con modo híbrido de transmisión, esto es,
transmisión periódica y bajo demanda, permitiendo a los usuarios
demandar información con requisitos temporales.
Con el objetivo de maximizar el número de usuarios
satisfechos, nuestra aproximación al problema trata de obtener los
algoritmos de planificación que utilicen con máxima eficacia el
ancho de banda disponible, combinando factores como frecuencias
de acceso a la información, plazos temporales y consumo de ancho
de banda.
Los resultados obtenidos mediante simulación ponen de
manifiesto que utilizando un programa periódico que se adapte a las
necesidades reales de los usuarios, es decir, transmitiendo
periódicamente aquellos elementos con alta frecuencia de acceso y
bajo consumo de ancho de banda, nuestro servidor de información
es claramente superior en número de usuarios satisfechos y
requisitos computacionales tanto a modelos híbridos no adaptativos
como a modelos basados en pull.
Es especialmente relevante el hecho de que nuestro modelo
tiene en cuenta que el ancho de banda en el canal de subida es
limitado y que las frecuencias de acceso a los datos son cambiantes
en un escenario realista (especialmente si los usuarios tienen
movilidad).
El efecto de la limitación del ancho de banda en el canal de
subida es muy importante. Nuestro modelo evita que un tráfico
excesivo sature el canal de subida, pudiendo soportar una mayor
carga efectiva que el resto de modelos evaluados.
El trabajo futuro incluye extender la funcionalidad de muestro
modelo para incorporar mecanismos que permitan la difusión de
información multimedia con calidades de servicio garantizadas.
También es interesante estudiar la posibilidad de incorporar
técnicas de aprendizaje al servidor para variar algunos parámetros
que hemos considerado fijos (BW_Limit, Cooling_Factor,
Expected_Sample_Size), en función de la distribución de acceso a
los datos, con el objetivo de mejorar las prestaciones.
Por último, otra línea de trabajo sería la modificación del
algoritmo de planificación para considerar diferentes clases de
usuarios, con diferentes prioridades en el acceso a la información,
así como diferentes tipos de plazos (rígidos y no rígidos).</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD COMPLUTENSE DE MADRID
FACULTAD DE INFORMÁTICA
Departamento de Arquitectura de Computadores y Automática
UN SERVIDOR HÍBRIDO ADAPTATIVO PARA LA
DIFUSIÓN DE INFORMACIÓN EN ENTORNOS DE
COMUNICACIÓN ASIMÉTRICOS CON
RESTRICCIONES TEMPORALES.
MEMORIA PARA OPTAR AL GRADO DE DOCTOR
PRESENTADA POR
Jesús Fernández Conde
Bajo la dirección del doctor
Daniel Mozos Muñoz
Madrid, 2011
ISBN: 978-84-694-2881-8 * © Jesús Fernández Conde, 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>REDES NEURONALES ALFA-BETA SIN PESOS: TEORÍA Y FACTIBILIDAD DE IMPLEMENTACIÓN</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Crear, diseñar e implementar un nuevo modelo de redes neuronales sin pesos basado en
los algoritmos conocidos de WNN, específicamente de ADAM, y en los operadores Alfa y
Beta, los cuales son tomados de los modelos de memorias asociativas Alfa-Beta. El nuevo
modelo de redes neuronales Alfa-Beta sin pesos se ha denominado CAINN (Computing
Artificial Intelligence Neural Network).</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La reflexión previa da pie al surgimiento de la hipótesis de este trabajo de tesis: es
posible crear, diseñar e implementar redes neuronales sin pesos, cuyos algoritmos de
aprendizaje y recuperación de patrones estén basados en las operaciones binarias Alfa y
Beta, lo cual redunde en la obtención de un nuevo modelo de WNN que conjugue las
ventajas de ambos campos: las WNN y los modelos asociativos Alfa-Beta. El nuevo
modelo de redes neuronales Alfa-Beta sin pesos exhibirá un rendimiento competitivo con
los modelos actuales de redes neuronales, con pesos o sin pesos, y de memorias asociativas.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En la sección anterior se ha evidenciado la importancia de las WNN y las ventajas que
ofrecen estos modelos sobre las RNA con pesos, y se han fundamentado estos hechos con
referencias bibliográficas correspondientes a trabajos de investigación científica de punta realizada por equipos internacionales. Por otro lado, se ha enfatizado la existencia, la importancia y la aplicación de los modelos asociativos Alfa-Beta, cuyas características más importantes son similares a las que exhiben las WNN, en cuanto eficacia, eficiencia y facilidad de implementación.
Ciertamente algunos connotados autores en el área de las redes neuronales sin pesos
han creado modelos relacionados con memorias asociativas [6, 8, 19, 30, 56, 79]; otros, han incursionado en los modelos asociativos Alfa-Beta [19-24, 70], o bien, han realizado
implementaciones en hardware de algunos modelos conocidos de redes neuronales sin
pesos [25-29, 41, 60, 67, 72, 74-77, 91]. Sin embargo, hasta noviembre de 2007, no se han
reportado en la literatura trabajos científicos donde se relacionen específicamente las redes neuronales sin pesos con los modelos de memorias asociativas Alfa-Beta.
El presente trabajo de tesis queda justificado porque incluye, de manera central, la
creación y diseño de un nuevo modelo de redes neuronales sin pesos, basado en los
operadores Alfa y Beta, los cuales son tomados de los modelos de memorias asociativas
Alfa-Beta. Por otro lado, se establecen las condiciones necesarias y suficientes de
equivalencia de las redes neuronales sin pesos y los circuitos booleanos, y se presenta un
estudio completo de factibilidad de implementación en hardware del nuevo modelo, el cual
exhibe un alto desempeño, derivado del hecho de que es un hecho ya probado que la
densidad aritmética de los modelos de memorias asociativas Alfa y Beta es menor que la
otros modelos relevantes de memorias asociativas, como las morfológicas [18].</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>1. Se definen, como una aportación original de esta tesis, las tres nuevas operaciones
 que sirven de base para el diseño e implementación del producto
principal: el modelo CAINN.
2. Se presenta el algoritmo de la nueva red neuronal Alfa-Beta sin pesos denominado
CAINN (Computing Artificial Intelligent Neural Network), en ambas fases:
aprendizaje y recuperación de patrones.
3. Se realiza el diseño e implementación en FPGAs de las operaciones Alfa, Beta, Alfa
Generalizada, Sigma-Alfa y Sigma-Beta. Estos diseños sirven de elementos
constitutivos de la arquitectura hardware para CAINN.
4. Se establecen formalmente las condiciones necesarias y suficientes de equivalencia
entre las redes neuronales sin pesos y los circuitos booleanos, y un estudio de la
factibilidad de implementación de CAINN.
5. La Red Neuronal Alfa-Beta sin pesos permite recuperar de forma correcta el
conjunto fundamental completo siempre que se presente a la entrada de cada
memoria patrones sin alteraciones.
6. El desempeño mostrado por el modelo propuesto, al realizarse los experimentos y
estudios comparativos con las bases de datos Iris Plants y CMC, es claramente
superior al alcanzado por el modelo ADAM y otros modelos.
7. Se ha empleado la lógica reconfigurable para la integración en hardware de las
operaciones Alfa y Beta del modelo CAINN.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITECNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
REDES NEURONALES ALFA-BETA SIN PESOS: TEORÍA Y
FACTIBILIDAD DE IMPLEMENTACIÓN
T E S I S
QUE PARA OBTENER EL GRADO DE
DOCTOR EN CIENCIAS DE LA COMPUTACIÓN
PRESENTA:
AMADEO JOSÉ ARGÜELLES CRUZ
DIRECTORES DE TESIS:
DR. CORNELIO YÁÑEZ MÁRQUEZ
DR. MIGUEL LINDIG BOS
MÉXICO, D.F. DICIEMBRE DE 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Diseño de un protocolo de autenticidad para redes de sensores</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Debido a que las redes de sensores inalambricas son un campo en pleno desarrollo, que la operacion de servicios de autenticidad en sus comunicaciones es un problema que no ha sido resuelto de forma definitiva y que la naturaleza de operacion de dichas redes presenta requerimientos específicos de escalabilidad y flexibilidad para el diseño de cualquier control de seguridad, se llego a determinar lo siguiente.
El problema a resolver es la busqueda de un mecanismo que permita el
uso de operaciones de criptografía asimetrica en un ambiente de redes de
sensores, para operar de forma eficiente un protocolo de autenticacion que
permita explotar las características de flexibilidad y escalabilidad en dicho
ambiente.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar un mecanismo de seguridad que permita garantizar la autenticidad entre nodos de una red inalámbrica de sensores, bajo las limitantes asociadas a esta tecnología.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La idea de considerar la necesidad de seguridad de redes ad hoc no es nueva, inclusive en redes de sensores hay ataques específicos ante los cuales es necesario utilizar mecanismos de proteccion para evitar ataques de suplantacion y negacion de servicios, entre otros posibles; sin embargo no todas las tecnicas utilizadas en soluciones a estos problemas en redes ad hoc, no son del todo aplicables en una WSN.
Con el fin de establecer claramente los riesgos a que se expone la informacion en una WSN en un ataque, conviene retomar una definición de la seguridad de la informacion, la cual consiste en otorgarle a la informacion cuatro propiedades basicas, como se indica en [50]:
Confidencialidad, Autenticidad, Integridad,Disponibilidad.
Con base en estas cuatro propiedades, los escenarios de riesgo en una WSN, y sobre todo, considerando el impacto de un ataque sobre la operacion de la red, se considera que el problema de la suplantacion de nodos es uno de los mas severos dada la mecanica de operacion de la red. A fin de ilustrar esta situacion, en las siguientes líneas se plantea un ataque hipotetico.
Una red de WSN con topología ad hoc esta formada por varios nodos, entre los cuales existe una relacion de confianza mutua, por lo que cada uno puede establecer enlaces con cualquier otro nodo que este dentro de su area de cobertura. Estos enlaces permiten a la WSN cumplir sus dos funciones basicas: recoleccion y transporte de datos y órdenes de control de la red. En las condiciones mas críticas, no se tiene forma alguna de establecer
autenticidad entre los nodos de la red.
Para esta red, la ejecucion de un ataque de suplantacion es una situacion no muy ajena a la realidad. Este ataque puede llevarse a cabo por lo menos de las siguientes dos formas:
Con la captura de un nodo y la manipulacion del mismo.
Con la incorporacion de un dispositivo que se haga pasar por un nodo de la red autentico.
La realizacion del primer ataque es muy simple, ya que como se ha dicho anteriormente, uno de los problemas del dispositivo en sí es la facilidad de ser capturado, ya que la mayoría de las veces los sensores no cuentan con mecanismos de proteccion física alguna, ademas de estar expuestos a fallas, e interrupcion en su servicio.
Para el caso del segundo ataque, si bien se requiere la construccion de un
sensor compatible con la WSN, esto no parece ser muy complicado una vez que se tiene la posibilidad de capturar uno o mas nodos originales y recuperar informacion del mismo que permita completar la construccion.
Dada la dinámica de operacion de las WSN, y las posibilidades de ocurrencia de un ataque de suplantacion, es conveniente evaluar que tan graves podrían ser los daños en caso de la realizacion del mismo para así tener una mejor perspectiva del enfoque del problema que adopta el presente estudio.
Bajo la suposicion que un tercer elemento ha logrado suplantar un nodo autentico en la red, los daños generados a partir de tal accion pueden agruparse en dos casos generales:
1. Una parte de la informacion recolectada por la WSN tendra que hacer un salto a traves del nodo comprometido, esto pone en riesgo la disponibilidad, la integridad y la confidencialidad de dicha informacion, ya que ésta podría perderse, alterarse o en un caso extremo, ser expuestos a terceros no autorizados.
2. De igual forma, las órdenes de control que viajan por la red pueden ser interceptadas y alteradas al momento de pasar por el nodo comprometido, acarreando un mal funcionamiento de la WSN, llegando en algunos casos incluso a poner en riesgo la integridad de elementos circundantes al sensor o del sensor mismo.
La ocurrencia de cualquiera de estos casos anteriores sería en menoscabo de la operacion de la WSN, lo que repercutiría en consecuencias acordes a la importancia de la mision que desempeñe dicha red. Esto deja sobre la mesa la necesidad de contar con mecanismos de autenticidad que operen en las WSNs.
En terminos generales, el problema de establecimiento de identidad tiene solucion mediante un proceso de autenticacion entre pares. En las redes de comunicaciones y en los sistemas de informacion, este proceso comunmente involucra el uso de mecanismos criptograficos, ya sean de clave secreta osimetricos, o bien de clave publica o asimetricos.
La autenticidad en una WSN debe ser una propiedad inherente de la propia red, ya que como se ha establecido en la seccion anterior, un ataque originado a raíz de la falta de ésta puede llegar a comprometer el funcionamiento de la red completa. Sin embargo, a diferencia de otros entornos, las WSN tienen como un marco de operacion muy reducido en recursos, por lo que la seleccion de los mecanismos de autenticidad a utilizar debe ser acorde a dichas limitantes.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se presento un panorama general del campo de las redes de sensores, haciendo hincapíe en las características del hardware, así como de la dinamica de organizacion de las WSN y de los sistemas operativos, y herramientas de desarrollo disponibles en este ambiente.
Se presentaron las principales estructuras algebraicas utilizadas en la criptografía, entre las que destacan los campos finitos binarios, y sus extensiones construidas como campos de polinomios, y que permiten un procesamiento eficiente en sistemas informaticos.
Se identificaron los principales requerimientos de diseño para un protocolo de autenticacion aplicable a redes de sensores, entre los que destacan la funcionalidad, la fiabilidad, la seguridad, la eficiencia y la escalabilidad, los cuales son de especial interes para el diseño de un protocolo criptografico en ambientes de red con recursos limitados.
Se definio una arquitectura de la solucion, la cual se baso en dividir las tareas de autenticacion a desarrollar por el protocolo en dos fases: la autenticacion de nodos y la autenticacion de mensajes. La finalidad de dicha separacion fue explotar al maximo los beneficios de la criptografía simetrica y asimetrica en cada caso.
Se describio la construccion del protocolo de autenticacion usando mecanismos
criptograficos basados en criptografía basada en identidad, lo que permitio la utilizacion de cifrado asimetrico para la generacion del desafío sin la necesidad de autenticar las llaves publicas.
En la construccion del protocolo de autenticacion se utilizo el proceso de autenticacion de nodos para efectuar un acuerdo de llave simetrica entre dos nodos. Dicha llave es la utilizada para generar los codigos de autenticacion de mensaje para autenticar los mensajes intercambiados.
Se describio un analisis del protocolo de autenticacion propuesto, considerando
los aspectos de funcionalidad, fiabilidad, seguridad, eficiencia y escalabilidad, destacando sus fortalezas y limitaciones.
Tambien se describieron un conjunto de pruebas experimentales realizadas
sobre varias implementaciones de componentes diseñados bajo TinyOS para realizar la tarea de autenticacion de mensajes. Del resultado de dichas pruebas, de obtuvieron mediciones de consumo de energía asociadas a la ejecucion del componente.
Se hizo una comparativa de los costos de ejecucion del componente para la autenticacion de mensajes, con resultados reportados en la literatura referentes al calculo de emparejamientos, los cuales son el principal componente usado en la tarea de autenticacion de nodos. De esta comparativa se desprendio un analisis que propone la observacion del gasto de energía de un nodo sensor que representa la ejecucion del protocolo, el cual arrojo como resultado que el uso de emparejamientos es viable dentro de una red de sensores,siempre y cuando no sea muy frecuente su invocacion.
Aunque el uso de emparejamientos es un componente que aporta un alto costo a la operacion del protocolo de autenticacion, dicho efecto puede ser distribuido entre el costo de las comunicaciones autenticadas a lo largo de la vida util de un enlace formado entre dos nodos sensores, para lo cual se preferiran aquellos enlaces y situaciones donde la vida de dichos enlaces sea la mayor posible.
A manera de principal conclusion del trabajo se puede establecer que el uso de criptografía basada en identidad es una opcion viable dentro de entornos limitados en recursos, como son las redes de sensores, siempre y cuando su uso de centre en tareas específicas y a su vez, éste se complemente con el uso de otras primitivas criptograficas de muy bajo costo computacional, tales como las funciones hash o el cifrado simetrico.
De este trabajo tambien se desprenden líneas interesantes de trabajo, algunas de las cuales ya se ha abordado por otras personas en la literatura, pero que aun se tiene mucho trabajo por desarrollar. Una de estas líneas es el diseño de implementaciones criptograficas altamente eficientes para ambientes limitados, tanto en sus versiones de criptografía simetrica, como asimetrica y basada en identidad. Por ejemplo, la implementacion eficiente de emparejamientos.
Otra línea que se destaca es el estudio de la aplicacion de tecnicas
de co-diseño de implementaciones criptograficas hardware-software, para su aplicacion en ambientes limitados, en busca de maximizar el aprovechamiento de componentes de hardware ya existentes, y lograr componentes de software mas eficientes.
Finalmente, este estudio deja abierto el interes para trabajar en el desarrollo
de sistemas verdes o amigables con el entorno, toda vez que el uso indiscriminado de energía en muchos de los sistemas existentes hoy en día tiene una componente altamente nociva para el ambiente.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Escuela Superior de Ingenier'ía Mecánica y Eléctrica
Unidad Culhuacan
Sección de Estudios de Posgrado e Investigación
Diseño de un protocolo de autenticidad para
redes de sensores
Tesis que presenta el
M. en C. Moisés Salinas Rosales
Para obtener el grado de
Doctorado en Comunicaciones y Electrónica
Bajo la dirección de:
Dr. Gonzalo Isaac Duchén Sánchez
Dr. Juan Carlos Sánchez Garc'ía
Ciudad de México, D.F., Diciembre de 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Afinación Estática Global de Redes Complejas y Control Dinámico Local de la Función Tiempo de Vida en el Problema de Direccionamiento de Consultas Semánticas"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Formular una función de control adaptativo f que determine de manera local el valor óptimo del parámetro Tiempo de Vida (TTL) de un algoritmo de búsqueda basado en colonia de hormigas
aplicado en la solución de SQRP.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Para dar solución al problema de SQRP se propone modelar un sistema de consultas P2P el cual
haga uso de una función de ajuste de control adaptativo para el tiempo de vida del proceso de
consulta. Con este fin, el desarrollo de este trabajo sustenta la siguiente hipótesis:
Es posible resolver de manera eficiente SQRP ajustando los parámetros de un algoritmo
de búsqueda basado en colonia de hormigas, mediante técnicas de control adaptativo
que utilicen información local derivada de: la configuración inicial, el desempeño de
consultas anteriores y las condiciones topológicas heterogéneas.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Este trabajo se presenta un enfoque innovador en la solución del problema del ajuste de
parámetros a través de estrategias de control adaptativas, en particular al aplicarlas al parámetro tiempo de vida (TTL) del algoritmos de colonias de hormigas AdaNAS.
De acuerdo con la literatura especializada, se han propuesto algunas estrategias para el ajuste de parámetros. Estos han dividido el problema de ajuste de parámetros en estrategias: de afinación (global) y de control (local). Encontrándose que se identificaron pocos trabajos que aborden el problema de ajuste de parámetros bajo las condiciones aquí presentadas. Específicamente la estrategia de ajuste de control adaptivo propuesta muestra que es factible resolver de manera eficiente el problema de direccionamiento de consultas semánticas (SQRP) en la Internet, modelado como un sistema complejo, controlando adaptivamente el parámetro TTL del algoritmo AdaNAS, mediante el uso de información local derivada de:
* La configuración inicial,
* El desempeño de consultas anteriores,
* Las condiciones topológicas heterogéneas.
El algoritmo adaptativo AdaNAS al ser comparado en el algoritmo NAS con estrategias de ajuste global obtuvo una mejora de: para la eficiencia promedio la mejora va desde un 43.7% a un 54.5% éxitos por salto. Para el promedio de los éxitos por consulta obtuvo una mejora del 43.8% en la cantidad de éxitos, y para la cantidad de saltos obtuvo una mejora del 21.2% saltos por consulta.
Concluyéndose que la versión adaptativa de AdaNAS alcanza mejores resultados que la versión con ajuste global NAS. Dentro de las versiones desarrolladas con ajuste global como son SemAnt, RandomWalk y NAS, al ser comparadas, se demostró que el algoritmo NAS es el que obtuvo un mayor eficiencia promedio, esto fue reportado en la publicación " A Self-Adaptive Ant Colony System for Semantic Query Routing Problem in P2P Networks".</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN CIENCIA
APLICADA Y TECNOLOGÍA AVANZADA,
UNIDAD ALTAMIRA
"Afinación Estática Global de Redes Complejas y
Control Dinámico Local de la Función Tiempo de
Vida en el Problema de Direccionamiento de
Consultas Semánticas"
T E S I S
QUE PARA OBTENER EL GRADO DE
DOCTOR EN TECNOLOGÍA AVANZADA
P R E S E N T A:
CLAUDIA GUADALUPE GÓMEZ SANTILLÁN
DIRECTORES DE TESIS:
DRA. LAURA CRUZ REYES
DR. EUSTORGIO MEZA CONDE
2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Desarrollo de algoritmos conceptuales restringidos basados en semillas</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de este trabajo es resolver el problema de clasificación no
supervisada conceptual restringida basada en semillas, dura y difusa, cuando se tienen 
objetos descritos por variables cualitativas y cuantitativas mezcladas o ausencia de 
información en los datos, para lo cual se propone crear una familia de algoritmos
conceptuales restringidos basados en semillas, duros y difusos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es posible crear una familia de algoritmos conceptuales restringidos basados en semillas, duros y difusos, que permitan superar las limitante de los algoritmos actuales?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Es importante abordar el problema del agrupamiento conceptual ya que en muchas 
ciencias el especialista está interesado en encontrar no sólo los agrupamientos en que se 
clasifican los objetos sino además las propiedades que caracterizan a estos agrupamientos. 
Sin embargo, las herramientas que se han diseñado hasta el momento tienen algunas 
limitantes al ser aplicados en problemas donde se tienen objetos descritos por variables 
cualitativas y cuantitativas mezcladas. Además, no permiten trabajar con ausencia de 
información en las descripciones de los objetos. 
En muchas ocasiones los conceptos obtenidos por los algoritmos no son fácilmente 
interpretables, no cubren a todos los objetos del agrupamiento o cubren objetos que 
pertenecen a otros agrupamientos. 
Una restricción fuerte de estos algoritmos es que los conceptos deben ser disjuntos, es 
decir, los objetos sólo pueden pertenecer a una clase. Sin embargo, en muchos problemas
reales no sucede así, ya que los objetos pueden pertenecer a más de una clase y en este caso
los conceptos deberían ser no disjuntos. 
También es importante desarrollar algoritmos conceptuales difusos ya que en 
problemas reales el especialista puede estar interesado en conocer en qué grado pertenece 
un objeto a cada uno de los agrupamientos, más que saber si un objeto pertenece o no a un 
agrupamiento.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología a seguir para alcanzar los objetivos planteados en esta propuesta es la 
siguiente: 
* Recopilación y estudio crítico de los trabajos relacionados con el área de investigación. 
* Análisis crítico de los algoritmos de clasificación no supervisada restringida para 
obtener las características principales de cada uno de ellos. 
* Comparación entre estos algoritmos. 
* Análisis crítico de las herramientas de agrupamiento conceptual para obtener las 
características principales de cada una de ellas. 
* Comparación entre dichas herramientas. 
* Desarrollo de nuevos algoritmos. 
* Conjuntar características de diferentes algoritmos de clasificación no supervisada 
restringida y de algoritmos conceptuales restringidos para diseñar nuevos 
algoritmos. 
* Implementar los algoritmos obtenidos de la conjunción de características. 
* Realizar pruebas experimentales y analizar los resultados obtenidos. 
* Proponer mejoras a los algoritmos actuales. 
* Implementar las mejoras propuestas. 
* Realizar pruebas experimentales y analizar los resultados obtenidos. 
* Proponer estrategias diferentes para generar los conceptos. 
* Implementar las estrategias propuestas. 
* Realizar pruebas experimentales y analizar los resultados obtenidos. 
* Buscar las características principales y/o comunes de los algoritmos propuestos con el 
fin de crear una familia de algoritmos conceptuales restringidos basados en semillas, y 
de este modo tener un marco de desarrollo para este tipo de algoritmos. 
* Redacción de artículos para congresos y revistas.
* Redacción de la tesis doctoral.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO NACIONAL DE ASTROFÍSICA, 
ÓPTICA Y ELECTRÓNICA 
 COORDINACIÓN DE CIENCIAS COMPUTACIONALES
Desarrollo de algoritmos 
conceptuales restringidos basados 
en semillas 
Propuesta de Tesis Doctoral 
Irene Olaya Ayaquica Martínez 
Asesores: 
Dr. José Francisco Martínez Trinidad 
Dr. Jesús Ariel Carrasco Ochoa 
Tonantzintla, Pue., Agosto de 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Búsqueda de respuestas en fuentes documentales multilingües</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un método para la búsqueda de respuestas en situación multilingüe, donde la
pregunta es expresada en un lenguaje, y la búsqueda se realiza en una o varias colecciones
de documentos escritos en varios lenguajes incluyendo el de la pregunta, partiendo de
sistemas de BR monolingües.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿se mejorará la precisión de la tarea de BR con respecto a una búsqueda monolingüe?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿podremos obtener una traducción que mejore el comportamiento del sistema de BR multilingüe?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Cómo aprovechar la redundancia y/o complementariedad en las listas de respuestas provenientes de fuentes multilingües, para mejorar la precisión del sistema de BR multilingüe?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La investigación propuesta se llevará a cabo de acuerdo a la siguiente metodología:
¿Realizar un estudio de las colecciones de documentos y preguntas de referencia
multilingües usadas en las evaluaciones del CLEF.
v Determinar la redundancia y complementariedad de las colecciones multilingües 
para un subconjunto de preguntas de estudio.
¿Determinar el conjunto de los sistemas MT más apropiados para la tarea de BR.
v Traducir las preguntas de referencia a los 5 idiomas propuestos, tanto de forma
manual (por un experto), como de manera automática por el conjunto de los
sistemas MT.
v Con este conjunto de preguntas, alimentar al sistema de BR, y realizar la
búsqueda en cada una de las colecciones de forma monolingüe.
v Analizar cada una de las listas de respuestas que se obtengan, con la finalidad de 
realizar las siguientes observaciones:
o La redundancia existente en las colecciones
o La precisión obtenida 
¿Diseñar el método para la elección de la mejor traducción
v Experimentar la traducción automática en ambos sentidos junto con una función
de similitud.
o Estudiar métricas como la función de similitud propuesta en [25] o la
métrica Bleu [26]
v Experimentar el uso de modelos del lenguaje para la elección de la traducción
más adecuada. 
o Experimentar con corpus de distintas características. 
v Aplicar aprendizaje automático para elegir la mejor traducción, basándonos en
características de la pregunta como: la longitud, sus adverbios interrogativos,
existencia o no de nombres propios, etc.
v Experimentar con la reformulación de las traducciones:
o Manipulando las entidades nombradas.
o Uniendo los n-gramas más pertinentes de cada traducción, según un
modelo de lenguaje.
¿Construir el método para la integración de las listas de respuestas 
v Experimentar al traducir las respuestas a un mismo idioma
v Emplear truncamiento sobre las respuestas calculando su frecuencia en todas las 
listas.
v Hacer pruebas usando métodos de los sistemas CLIR, antes mencionados, para
el problema de la fusión de información.
o Estudiar también acerca del uso de aprendizaje automático para la
integración de listas de respuestas [27].
v Utilizar la Web para validar la selección de la respuesta</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Coordinación de Ciencias Computacionales
Laboratorio de Tecnologías del Lenguaje
Búsqueda de respuestas en fuentes
 documentales multilingües
Propuesta de Tesis Doctoral
que presenta:
M. en C. Rita Marina Aceves Pérez
Directores
Dr. Luis Villaseñor Pineda,
Dr. Manuel Montes y Gómez</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>IDENTIFICACION AUTOMATICA DEL LENGUAJE HABLADO SIN RECONOCIMIENTO FONÉTICO DE LA SEÑAL DE VOZ</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un método para la identificación del lenguaje hablado sin recurrir a la
representación fonética de la señal de voz, con un nuevo procesamiento acústico que obtenga
las características directamente de la señal acústica.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Se podrá hacer la identificación del lenguaje hablado utilizando sólo las características
acústicas de la onda de señal de voz?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Cuál es el conjunto de características acústicas más adecuadas para la identificación del
lenguaje hablado?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Se puede hacer la identificación automática del lenguaje hablado en tiempos comparables a 
los empleados por el ser humano?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Podrá un método o combinación de métodos de clasificación mejorar los resultados
actuales en la identificación del lenguaje hablado?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En base al diagrama de componentes básicos para la identificación del lenguaje hablado sin
representación fonética visto en la sección 1.3.2, realizaremos nuestro trabajo en dos pasos 
principales: el primero dedicado al procesamiento acústico de la señal de voz y el segundo al
proceso de aprendizaje.
5.1 Procesamiento acústico de la señal de voz.
Al eliminar el paso de la representación fonética de la señal de voz, el peso recae en el
procesamiento acústico, por ello, realizaremos los siguientes pasos:
1. Observar las diferencias que se obtienen de utilizar procesos acústicos basados en
características de articulación, como producimos el habla, contra los basados en la 
percepción de cómo escuchamos. Existen varios algoritmos para cada uno de ellos, pero
utilizaremos los que actualmente han obtenido mejores resultados:
		*	Para la obtención de características de articulación, utilizaremos el de coeficientes de
predicción lineal LPC (Linear Predictive Coefficients).
		*	Para la obtención de características perceptúales, utilizaremos el de coeficientes
cepstrales de frecuencia Mel MFCC (Mel Frecuency Cepstral Coefficients).
2. Instrumentar un algoritmo para la identificación y eliminación de silencios en el habla, a partir 
de las observaciones del experimento 2 (véase sección XX).
3. Observar el resultado que se obtiene de utilizar las bandas criticas, que son frecuencias bajas 
en la onda acústica, con los coeficientes cepstrales Mel, en la identificación de los lenguajes
hablados, ya que el ritmo y por consecuente la prosodia se encuentran en esas frecuencias.
		*	Instrumentar un algoritmo para enfatizar las bajas frecuencias. 
		*	Aplicar filtros de Mel y después obtener los coeficientes cepstrales.
4. Desarrollar un método para la identificación de los intervalos vocálicos y consonánticos.
		*	Observar la sonoridad en la señal de voz, como base para la discriminación de las 
clases rítmicas.
5. Obtener nuevas características para la discriminación de los lenguajes en base a la
desviación estándar de la duración de los intervalos vocálicos y consonánticos. 
6. Observar los resultados con estas nuevas características con el corpus OGI_TS.
		*	En primera instancia con un conjunto de idiomas rítmicamente lejanos.
		*	Posteriormente con un conjunto de idiomas rítmicamente semejantes.
5.2 Procesamiento de aprendizaje.
Realizar un estudio para definir cual es la representación y algoritmo o algoritmos de
aprendizaje, que nos ayude con la tarea de identificación del lenguaje hablado, teniendo como conocimiento previo (entrada) las características acústicas producidas en el primer paso. Para ello
será necesario:
		*	Realizar experimentos con diferentes clasificadores, de acuerdo a diferentes
representaciones: estocásticos, máquinas de vectores de soporte y redes neuronales.
		*	Observar el uso de análisis de componentes principales (PCA) y ganancia de e información, 
para reducir la dimensionalidad de los vectores de características acústicas.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>IDENTIFICACION AUTOMATICA DEL LENGUAJE HABLADO SIN RECONOCIMIENTO 
FONÉTICO DE LA SEÑAL DE VOZ
PROPUESTA DE TESIS DOCTORAL
Realizado por:
Ana Lilia Reyes Herrera
Director:
Dr. Luis Villaseñor Pineda
Instituto Nacional de Astrofísica Óptica y Electrónica,
Luis E. Erro 1, Tonantzintla, Puebla, 72840, México
Junio 2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Protocolo para la Generacion y Distribucion de Llaves para  Comunicacion Multimedia  Multipartita sobre Redes Celulares</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En una red celular, diseñar un protocolo para la generación y distribución de llaves, 
para una comunicacion multimedia multipartita segura, requiere consideraciones adicionales que para una red fija. Tales consideraciones estan en función de las características de la multimedia, de las caracter'ísticas para una comunicacion en grupo segura, de 
la arquitectura de la red celular y del modo de comunicacion a considerar. Por ello, 
abordamos el problema desde cuatro puntos de vista. Primero, consideramos las caracteíísticas necesarias a garantizar para una comunicacion en grupo segura. Segundo, 
se toman en cuenta los problemas que surgen debido a las características de la arquitectura de una red celular. Tercero, se consideran aspectos relacionados al uso de una
comunicacion asíncrona. Cuarto, consideramos las características de la multimedia.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>diseñar y desarrollar un protocolo eficiente para la generación y distribución de 
llaves en una comunicacion multimedia multipartita segura en redes celulares.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cual es el impacto, en la generación y distribución de llaves, de utilizar una
agrupacion dinámica de entidades para comunicación multimedia multipartita en redes celulares?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Como diseñar protocolos para la generación y distribución de llaves en redes celulares que mantengan la asincron'ía del sistema durante el manejo de la
movilidad?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Como disminuir el overhead por mensajes de rekeying enviados en los canales
de comunicacion inalámbricos causados por la movilidad?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿De que forma podemos diseñar protocolos para la generación y distribución de
llaves, donde el costo computacional y el costo de almacenamiento, presenten un
crecimiento abajo del lineal con respecto al numero de dispositivos móviles?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. diseñar un modelo organizacional para la generación y distribución de llaves en
una comunicacion multipartita segura en redes celulares. Este modelo estará for- 
mado por un conjunto de dispositivos moviles, un conjunto de estaciones base 
y un controlador central. Servira de base en el diseño de las arquitecturas en los puntos 2 y 5.
2. diseñar una arquitectura estática para la generación y distribución de llaves par- 
tiendo del modelo organizacional. En esta arquitectura los grupos de estaciones
base son fijos. El agrupamiento y la estructuracion son realizados de maneraápriori. Este paso lo hemos dividido en tres partes:
diseñar una arquitectura estática considerando estaciones base fiables. 
diseñar una arquitectura estática considerando estaciones base semi-fiables. 
diseñar una arquitectura estática considerando estaciones base no fiables. 
3. diseñar protocolos para la generación y distribución de llaves para la arquitectura 
estatica. Hemos dividido este paso en dos fases: 
FASE 1: diseñar los mecanismos para la distribución de las llaves. En esta 
fase se diseñarán tres mecanismos: 
* Mecanismo para el manejo del handoff
* Mecanismo para la entrada de un nuevo participante (JOIN)
* Mecanismo para la salida de un participante (LEAVE)
FASE 2: diseñar los mecanismos para la generación de las llaves. 
4. diseñar un mecanismo para la construcción del agrupamiento de estaciones base 
considerando las caracter'ísticas de movilidad de los diferentes tipos de usuarios.
Este paso lo dividimos en dos fases:
FASE 1: Realizar el agrupamiento mediante el uso de mecanismos para la
construccion de lista de vecinos. Se considerarán todas las estaciones base 
potenciales de servicio.
FASE 2: Realizar el agrupamiento mediante el uso de mecanismos predictivos. Se consideraran las estaciones base potenciales de servicio con base 
en probabilidades de trayectoria.
5. diseñar una arquitectura dinámica para la generación y distribución de llaves 
partiendo de la arquitectura estatica. El agrupamiento de estaciones base debe ser 
dinamico. Este paso lo hemos dividido en dos fases: 
FASE 1: diseñar el mecanismo de agrupamiento dinámico. Se debe consid- 
erar las siguientes etapas:
* diseñar el mecanismo para la configuración de un nuevo grupo de esta- 
ciones base.
* diseñar el mecanismo para la actualización de un grupo de estaciones 
base al ingresar una nueva estacion base al grupo. 
* diseñar el mecanismo para la actualización de un grupo de estaciones 
base al eliminar una estacion base del grupo. 
* diseñar el mecanismo para la eliminación de un grupo de estaciones 
base existente.
FASE 2: Realizar una extension de la arquitectura estática para desarrol- 
lar una arquitectura dinamica considerando estaciones base fiables, semi'
fiables y no fiables.
6. Realizar una extension de los protocolos de la arquitectura estática para desar- 
rollar protocolos para la generacion y distribución de llaves para la arquitectura 
dinamica. 
7. diseñar un mecanismo a orden causal parcial para la sincronización entre men- 
sajes de llaves y mensajes de datos.
8. Verificar formalmente el protocolo para la generacion y distribución de llaves, 
a fin de probar el cumplimiento de propiedades que garanticen su correcto funcionamiento (correctness).
Safety
Liveness
9. Emular la implementacion del protocolo para la generación y distribución de 
llaves en una comunicacion multimedia multipartita utilizando un simulador de 
red inalambrica (NS-2). La finalidad es probar el rendimiento del prótocolo en
aplicaciones multimedia.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Protocolo para la Generacion y 
Distribucion de Llaves para 
Comunicacion Multimedia 
Multipartita sobre Redes
Celulares
por
Jorge Estudillo Ramírez
Propuesta sometida como requisito parcial para
obtener el grado de
CANDIDATO A DOCTORADO
en el
Instituto Nacional de Astrofísica, Optica y 
Electronica 
Septiembre 2008
Tonantzintla, Puebla
Supervisada por:
Dr. Saul E. Pomares-Hernández, INAOE 
Dr. Gustavo Rodríguez-Gomez, INAOE '
c INAOE 2008</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Aprendizaje por refuerzo mediante transferencia de conocimiento cualitativo</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El aprendizaje por refuerzo se ha aplicado para resolver problemas en
varias disciplinas, incluyendo aplicaciones recientes, como la robótica. Los
retos actuales principales para aprendizaje por refuerzo son:
La exploración en el ambiente real consume mucho tiempo.
El conocimiento obtenido se limita a resolver solamente la tarea apren-
dida.
Los dominios probados en el estado del arte son de dinámicas simples.
El dilema exploración-explotación es algo que ha ocupado a los investi-
gadores en el área de aprendizaje por refuerzo. Las técnicas se han inclinado
por algoritmos que encuentren un correcto balance entre la exploración y
la explotación durante el aprendizaje, y con ello hallar una buena política
en un tiempo reducido. U na solución alternativa, que permite reutilizar el
conocimiento entre tareas similares y así reducir el tiempo de aprendizaje en
la tarea objetivo, es la transferencia de conocimiento.
En a~nos recientes se han hecho avances en algoritmos para transferir di-
ferentes tipos de conocimiento entre tareas de aprendizaje por refuerzo, sin
embargo, la mayoría de los trabajos se ha centrado en tareas con una natu-
raleza discreta.
El trabajo que aquí se propone se enfoca en la transferencia de conoci-
miento entre problemas con dinámicas complejas y con espacios de estado y acciones continuos, usando una representación cualitativa compacta y gene-
ralizada, adquirida durante el aprendizaje de una primera tarea relacionada
y aplicada como guía para el aprendizaje de la tarea objetivo.
Son muy pocos los trabajos que han hecho transferencia en tareas de
estados continuos [Taylor et al., 2008], [Lazaric et al., 2008] y nulos en tareas
con acciones continuas. De tal situación se hace visible la importancia de este
trabajo, cuyo objetivo es transferir información cualitativa entre tareas con
espacios de estado y acción continuos.
La ventaja de una abstracción cualitativa sobre otros enfoques es la gene-
ralidad en la representación de acciones y estados así como la representación
compacta del conocimiento sobre la tarea. El razonamiento cualitativo y abs-
tracciones cualitativas han sido empleados para representar funciones cuali-
tativas en procesos de control ([Mugan, 2010], [Suc, 2003]).También se han
usado para representar reglas durante el aprendizaje por refuerzo ([Mugan
and Kuipers, 2008], [Mugan, 2010]), pero hasta el momento no se han desarro-
llado trabajos sobre transferencia de información cualitativa en aprendizaje
por refuerzo, lo cual es el objetivo que persigue este trabajo.
En una tarea donde el agente se arriesga al hacer exploración, es deseable
tener un desempe~no inicial al ejecutarse los primeros episodios del aprendi-
zaje y correr menos riesgo de realizar acciones peligrosas. En aprendizaje por
refuerzo la métrica que mejor re
eja tal comportamiento es jumpstart1
. En
la literatura revisada no se encontraron trabajos en los cuáles se centraran
los esfuerzos en generar un alto valor de jumpstart, excepto por el trabajo de
Lazaric ([Lazaric et al., 2008]). El trabajo que aquí se propone se enfoca en
generar un valor de jumpstart suficientemente alto para guíar la exploración
y reducir el riesgo de explorar acciones peligrosas, evitando la transferencia
negativa2
.
Todos los trabajos de transferencia para aprendizaje por refuerzo que
se han encontrado en la literatura, han sido probados por sus autores en
simulaciones. Uno de los objetivos del trabajo propuesto es hacer validaciones
de transferencia de conocimiento en un problema de robótica complejo, con
alta dimensionalidad, dinámica no-lineal y estocasticidad, ese será en volar
de manera autónoma un helicóptero.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo principal del trabajo propuesto es desarrollar un método para
transferir conocimiento cualitativo entre tareas de aprendizaje por refuerzo
con espacios de estado y acción continuos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Qué conocimiento cualitativo se puede usar para mejorar el aprendizaje en la tarea objetivo?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Cuál es la comparación de la transferencia de conocimiento cualitativo
con la transferencia de otro tipo de información (V,Q,..) en aprendizaje
por refuerzo?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Es posible incrementar el valor de la métrica jumpstart con el conocimiento cualitativo en la tarea objetivo?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Cuáles son las métricas que se pueden superar comparando el enfoque
cualitativo contra los métodos de transferencia existentes?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para alcanzar los objetivos de investigación planteados, la metodología a
seguir contempla el desarrollo de varios módulos, que interconectados definen
la estructura de transferencia de conocimiento cualitativo para aprendizaje
por refuerzo.
La metodología a seguir comprende los siguientes puntos:
Definición de arquitectura general de transferencia de conocimiento en-
tre tarea origen y tarea destino.
Identificación de conocimiento cualitativo relevante para mejorar el
aprendizaje por refuerzo.
Definición de algoritmos a usarse como base para el aprendizaje por
refuerzo en tareas de espacios y acciones continuos.
Definición de métrica para cuantificar la relación entre tarea origen y
objetivo y con ello determinar cómo usar el conocimiento cualitativo.
Evaluaciones y pruebas.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En aprendizaje por refuerzo hay progresos algorítmicos significativos para
reducir el tiempo de aprendizaje en una tarea. Para el aprovechamiento de la
experiencia adquirida en una tarea inicial (origen) para aprender más rápido
una tarea relacionada (objetivo) un enfoque nuevo son los algoritmos de
transferencia de aprendizaje. En la literatura, normalmente el mecanismo de
transferencia es independiente del parecido que tienen la tareas.
En este documento se ha presentado la propuesta de trabajo doctoral
que propone el uso de un método nuevo de transferencia de conocimiento
cualitativo para tareas de aprendizaje por refuerzo. A diferencia de otras
abstracciones, es una representación generalizada y compacta de aspectos
continuos de la dinámica de la tarea origen. Se pretende que este conocimiento
se ajuste según la similitud entre la tarea origen y la tarea objetivo y que al
final sirva de guía durante el aprendizaje de la tarea objetivo, reduciendo el
espacio de exploración y agilizando el aprendizaje.
Se han presentado resultados preliminares sobre la prueba de concepto de
transferencia de conocimiento cualitativo en torno a un problema con dinámica compleja consistente en el balanceo de péndulo invertido. Los resultados
muestran que la transferencia cualitativa disminuye el tiempo de aprendizaje.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Aprendizaje por refuerzo mediante
transferencia de conocimiento cualitativo
Propuesta doctoral
Presenta:
Esteban Omar García Rodríguez
Asesores:
Dr. Eduardo Morales Manzanares
Dr. Enrique Muñoz de Cote Flores Luna
30 Agosto, 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"CLASIFICADORES RÁPIDOS BASADOS EN EL VECINO MÁS SIMILAR PARA DATOS MEZCLADOS"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El algoritmo de clasificación supervisada, basado en la búsqueda del vecino más cercano (NN) 
[COVER67], ha sido ampliamente utilizado debido a su simplicidad y capacidad para resolver 
problemas complejos de Reconocimiento de Patrones. Este algoritmo está basado en un 
conjunto de objetos de entrenamiento T, previamente clasificado. 
T = {Zi | 1 &lt;&#61; i &lt;&#61;N}			(1) 

Donde N es el número de objetos. El objeto Zi
está descrito por d atributos en un espacio R^d. 

Zi = [x1(Zi), x2(Zi),..., xd(Zi)] 		(2) 

Dado un nuevo objeto Q a clasificar, el objetivo del clasificador es encontrar el objeto ONN en el 
conjunto T que minimice la función de distancia Dist con el objeto Q. 

O[MN] &#61; minDist [Z&#61;T](Zi ,Q) 		 (3) 

Para encontrar ONN se requiere una comparación exhaustiva entre Q y cada objeto del conjunto 
T. Una vez encontrado el vecino más cercano, se asigna la clase de este objeto a Q. 
 Una generalización de este método es el algoritmo de k vecinos más cercanos (k-NN). En 
este caso, la búsqueda se realiza de la misma forma que en NN, salvo que en lugar de encontrar 
un sólo objeto, se buscan los k más cercanos. Finalmente, la clase del objeto Q se elige por 
votación de los k objetos más cercanos. Un diagrama del algoritmo NN se muestra en la figura 
1.1.
El algoritmo de clasificación basado en la búsqueda del vecino más cercano se ha aplicado 
en otros problemas aparte de la clasificación, tal es el caso de: estimación de densidad 
[CHEN89], compresión de datos [GERSHO91], recuperación de información 
[DEERWESTER90], reconocimiento de caracteres [STELIOS93], cuantización de vectores 
[CHEN97]. 
En las aplicaciones antes mencionadas, dicho algoritmo de clasificación ha presentado un 
buen desempeño. Sin embargo, comparar un nuevo objeto Q contra todos los elementos del 
conjunto T implica un alto costo en tiempo, cuando T es muy grande. Por esta razón en los 
últimos años se han propuesto métodos para hacer más rápida la búsqueda, la mayoría de los 
cuales omiten comparaciones con base en propiedades métricas de la función de distancia.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>La búsqueda rápida del vecino más cercano ha sido una rama de investigación activa en los 
últimos años, en la cual se han explotado ampliamente las propiedades métricas de una 
distancia para reducir el número de comparaciones entre objetos. 
En la sección anterior, se introdujo a grandes rasgos una división de los métodos de 
clasificación rápida basada en el vecino más cercano propuestos hasta el momento. Incluso, se 
puede notar, que en varios trabajos se combinan estos métodos para obtener un algoritmo más 
robusto de clasificación rápida, lo cual se realiza de manera exacta o aproximada. Sin embargo, 
se presenta la limitante de que estos métodos presuponen que los objetos están descritos por 
atributos numéricos o Booleanos y la mayoría basa su funcionamiento en el cumplimiento de 
propiedades métricas de la función de distancia. 
Por lo expuesto hasta el momento, surge la necesidad de considerar objetos descritos por 
atributos numéricos, no numéricos o ambos (mezclados) incluso con ausencia de información. 
En tales problemas, para comparar los objetos se utilizan funciones de similaridad. Por esta 
razón, es importante desarrollar estrategias aplicables a problemas con medidas de 
comparación que no necesariamente cumplan las propiedades métricas. 
Con base en lo antes mencionado, podemos plantear el objetivo general de este trabajo, 
como: 
"Proponer algoritmos de clasificación rápida basados en la búsqueda del vecino más 
similar (MSN) de manera exacta o aproximada y que funcionen para datos mezclados cuando 
no se tiene una distancia. 
De manera general, se busca un método aplicable a cualquier tipo de datos, el cual sea 
independiente de la medida de comparación. Incluso, sin importar si dicha medida cumple o no 
propiedades métricas.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En esta sección se describe la metodología para lograr cada objetivo específico propuesto en la 
sección anterior. Para lo cual, se propone realizar las siguientes actividades: 
1. Analizar de manera crítica el trabajo relacionado existente referente a la clasificación 
rápida basada en el algoritmo del vecino más cercano, para conocer sus limitantes. 
2. Debido a que el objetivo es crear métodos aplicables a datos mezclados con ausencia de 
información, se debe analizar algunas funciones de similaridad existentes. Esto se debe a 
que es necesario analizar qué propiedades cumplen algunas funciones de similaridad 
usadas en la literatura, con las cuales pudiera ser posible desarrollar un método exacto. 
3. Como parte de dicha metodología, se propone diseñar métodos basados en estructuras de 
árboles, de tal manera que se pueda trabajar con datos mezclados e independientemente 
de propiedades métricas de la distancia. Para lo cual, se realizarán las siguientes 
actividades: 
a. Análisis crítico de los métodos basados en estructuras de árboles. En este punto, es 
importante analizar los métodos existentes para conocer sus ventajas y limitantes. 
De tal manera, que se pueda desarrollar un método más robusto en el que se 
mejoren precisamente las limitantes identificadas. 
b. Análisis de algoritmos de agrupamiento. Debido a que se propone abordar un 
enfoque basado en estructuras de árboles, es necesario hacer un análisis de los 
algoritmos de agrupamiento, con los cuales sea posible crear la estructura de árbol 
mediante el conjunto de entrenamiento. Sin embargo, los algoritmos de 
agrupamiento que han sido utilizados en los métodos de clasificación rápida 
basados en árboles, son algoritmos aplicables a objetos descritos por atributos numéricos. Por esta razón, es importante hacer una revisión en la literatura de los 
algoritmos de agrupamiento más recientes que permitan trabajar con datos 
mezclados, con los cuales sea posible crear una estructura de árbol. 
c. Extender algunos de los métodos existentes. Se propone evaluar si es posible 
adaptar o extender alguno de los métodos existentes para la búsqueda del vecino 
más cercano, de manera que permitan trabajar con datos mezclados. 
d. Diseñar nuevos métodos de clasificación aproximada, con los cuales no se pierda 
mucha calidad de clasificación en el proceso y en los cuales, el número de 
comparaciones entre objetos se reduzca notablemente. 
e. Implementación de métodos adaptados y/o propuestos. 
f. Realización de pruebas y comparación de resultados. 
g. Análisis de los resultados obtenidos y retroalimentación. 
4. Se planea diseñar métodos basados en el algoritmo de Aproximación-Eliminación, los 
cuales sean aplicables a datos mezclados. Para lo cual, se realizarán las siguientes 
actividades: 
a. Análisis crítico de los métodos basados en el algoritmo de AproximaciónEliminación. 
b. Extender algunos de los métodos existentes. 
c. Diseñar nuevos métodos de clasificación aproximada, con los cuales no se pierda 
mucha calidad de clasificación en el proceso y en los cuales, el número de 
comparaciones entre objetos se reduzca notablemente. 
d. Implementación de métodos adaptados y/o propuestos. 
e. Realización de pruebas y comparación de resultados. 
f. Análisis de los resultados obtenidos y retroalimentación. 
5. Diseñar un nuevo método exacto, basado en las propiedades de alguna función de 
similaridad. A partir del estudio de las propiedades de la función de similaridad, 
seleccionar alguna que permita definir propiedades que ayuden a reducir comparaciones 
entre objetos, sin perder calidad de clasificación en el proceso. 
a. Diseño e implementación del método propuesto. 
b. Realización de pruebas y comparación de resultados. 
c. Análisis de los resultados obtenidos y retroalimentación.
6. El análisis del estado del arte, la escritura de artículos y la redacción de tesis se proyectan 
como actividades a realizar durante el desarrollo de este trabajo. A continuación se 
muestra el calendario de actividades.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En aplicaciones reales, es frecuente encontrar objetos descritos por atributos numéricos y no 
numéricos (datos mezclados). Por lo tanto, es importante utilizar métodos adecuados que nos 
permitan trabajar con tales características. 
Como parte de los resultados preliminares, se propone un método rápido de clasificación 
aproximada del vecino más similar aplicable a datos mezclados, basado en estructura de árbol. 
Con el objetivo de comparar dicho método, dos algoritmos de clasificación fueron adaptados 
para trabajar con datos mezclados (clasificadores propuestos por Fukunaga y Moreno-Seco). 
Con base en los experimentos realizados, es posible concluir que los métodos propuestos en este 
trabajo reducen de manera considerable el número de comparaciones entre objetos, perdiendo 
cierta calidad en el proceso en comparación con el clasificador NN exhaustivo y el clasificador 
modificado para trabajar con funciones de disimilaridad, propuesto por Fukunaga. Sin 
embargo, en comparación con el clasificador propuesto por Moreno-Seco los métodos 
propuestos (clasificadores MSN basados en búsqueda local y en búsqueda global) realizan 
menos comparaciones y la calidad de clasificación obtenida en el proceso es similar. 
Con los resultados preliminares obtenidos, se muestra que es posible diseñar nuevos 
métodos para acelerar el proceso de clasificación basado en el MSN, lo cual nos indica que es 
factible alcanzar los objetivos planteados en el presente trabajo de investigación.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO NACIONAL DE ASTROFÍSICA, 
ÓPTICA Y ELECTRÓNICA 
Tema de propuesta doctoral: 
"CLASIFICADORES RÁPIDOS BASADOS EN EL VECINO MÁS 
SIMILAR PARA DATOS MEZCLADOS 
Por: 
Selene Hernández Rodríguez 
Trabajo dirigido por: 
Dr. Jesús Ariel Carrasco Ochoa Dr. José Francisco Martínez Trinidad</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Una Representación Vectorial para Contenido  de Textos en Tratamiento de Información</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La efectividad de una representación de documentos, está directamente relacionada a la 
exactitud con la cual el conjunto de términos seleccionados representa el contenido de un 
documento y a cuán bien se puede contrastar el contenido de dicho documento con otro. Es decir, 
dados dos documentos D1 y D2 y sus representaciones R1 y R2, respectivamente, si R1 es igual a R2
esto significa que el contenido de R1 es igual al contenido de R2 a cierto nivel de abstracción. 
En cuanto a la representación de contenido, la bolsa de palabras es insuficiente, ya que 
palabras aisladas son poco específicas para efectuar una discriminación adecuada entre documentos 
[23, 26]. Un método más apropiado, como se ha mencionado, sería identificar grupos de palabras 
con significado que denoten conceptos o relaciones, este será el trabajo de la presente investigación, 
la utilidad de la representación resultante será comprobada en RI, por tal motivo, a continuación se 
presenta una descripción de dicha tarea.
La recuperación de información (RI) se interesa en seleccionar, de grandes volúmenes de 
texto, aquellos documentos que contienen información relevante de acuerdo a las necesidades de 
información expresadas por un usuario. 
La RI incluye dos actividades principales: indexar y buscar. La primera se refiere a 
representar el contenido de los documentos y la solicitud de información que emite el usuario. La 
segunda, a la forma de examinar la representación de los documentos con respecto a la petición de 
información, para proporcionar como respuesta los que resulten de mayor relevancia. 
La riqueza inherente al lenguaje, y la diversidad en la manera de expresar las necesidades 
de información por parte de los usuarios, ocasionan que las operaciones de indexar y de buscar 
nunca recuperen información de manera perfecta, como puede ser el caso de los manejadores de 
bases de datos. Por lo tanto, ha sido necesario establecer métodos cuantitativos (funciones de 
relevancia) para evaluar de forma aproximada la similitud entre la consulta y los documentos, y 
entonces determinar cuáles recuperar. 
Un modelo de RI se define con la representación que se da a los documentos y las consultas 
(objetivo de esta investigación), y la función de relevancia que se emplea para compararlos. 
Ahora bien, dada una solicitud de información, por ejemplo: 
"Estoy interesado en mecanismos de comunicación entre procesos disjuntos, posiblemente, 
pero no exclusivamente en ambientes distribuidos. Preferiría ver descripciones de mecanismos
completos con o sin implementación en lugar de trabajos teóricos del problema. Llamada a 
procedimientos remotos y paso de mensajes, son ejemplos de mi interés" [10]. 
Podemos observar que existen palabras como descripciones, mecanismos, implementación
y problema, que deberían estar presentes en los documentos recuperados pero manteniendo la 
relación expresada, ya que se buscan "descripciones de mecanismos de comunicación entre 
procesos". Si empleamos de manera aislada las palabras mencionadas, algunas serán ignoradas por 
su generalidad al crear el índice y si se incluyen en el índice conducirán a recuperar gran cantidad 
de documentos irrelevantes. 
Cuando se recupera un grupo de documentos se encontrará que algunos de ellos no son 
relevantes y que algunos relevantes no se han recuperado. El éxito de la recuperación se establece 
mediante dos métricas: la precisión, que es la razón de los documentos relevantes recuperados entre 
el total de documentos recuperados y el recuerdo, la razón del número de documentos relevantes 
recuperados entre el total de documentos relevantes existentes en la colección. 
En el área de recuperación de información se continúa experimentando con nuevos modelos 
y por ende nuevas representaciones de documentos, para intentar cubrir con mayor precisión las 
necesidades de información de los usuarios. 
Como se ha especificado, nos centraremos en RI, donde si se desea obtener mejoras 
substanciales debe empezarse a pensar más allá de la aproximación de bolsa de palabras, con 
precisiones típicamente por debajo del 50% hay mucho margen para mejorar [27]. En la siguiente 
sección se presentan trabajos que describen modelos clásicos y nuevos de recuperación de 
información para concluir con métodos que explotan la obtención de relaciones, empleando técnicas 
de procesamiento de lenguaje natural, para realizar recuperación de información.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Establecer una representación de contenido de textos para la cual se localicen, extraigan y 
expresen relaciones entre términos para mejorar la expresividad en tareas de tratamiento de 
información, con respecto a la representación vectorial de textos tradicional. </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cuál será el impacto en tareas de tratamiento de información, si se consideran relaciones 
entre términos, que permitan asociarlos y emplear dichas asociaciones como unidades para 
evaluar la similitud entre documentos?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En esta sección se describe la metodología para lograr cada objetivo específico propuesto en la 
sección anterior. Para lo cual, se propone realizar las siguientes actividades: 
1. Analizar de manera crítica el trabajo relacionado existente referente a la clasificación 
rápida basada en el algoritmo del vecino más cercano, para conocer sus limitantes. 
2. Debido a que el objetivo es crear métodos aplicables a datos mezclados con ausencia de 
información, se debe analizar algunas funciones de similaridad existentes. Esto se debe a 
que es necesario analizar qué propiedades cumplen algunas funciones de similaridad 
usadas en la literatura, con las cuales pudiera ser posible desarrollar un método exacto. 
3. Como parte de dicha metodología, se propone diseñar métodos basados en estructuras de 
árboles, de tal manera que se pueda trabajar con datos mezclados e independientemente 
de propiedades métricas de la distancia. Para lo cual, se realizarán las siguientes 
actividades: 
a. Análisis crítico de los métodos basados en estructuras de árboles. En este punto, es 
importante analizar los métodos existentes para conocer sus ventajas y limitantes. 
De tal manera, que se pueda desarrollar un método más robusto en el que se 
mejoren precisamente las limitantes identificadas. 
b. Análisis de algoritmos de agrupamiento. Debido a que se propone abordar un 
enfoque basado en estructuras de árboles, es necesario hacer un análisis de los 
algoritmos de agrupamiento, con los cuales sea posible crear la estructura de árbol 
mediante el conjunto de entrenamiento. Sin embargo, los algoritmos de 
agrupamiento que han sido utilizados en los métodos de clasificación rápida 
basados en árboles, son algoritmos aplicables a objetos descritos por atributos numéricos. Por esta razón, es importante hacer una revisión en la literatura de los 
algoritmos de agrupamiento más recientes que permitan trabajar con datos 
mezclados, con los cuales sea posible crear una estructura de árbol. 
c. Extender algunos de los métodos existentes. Se propone evaluar si es posible 
adaptar o extender alguno de los métodos existentes para la búsqueda del vecino 
más cercano, de manera que permitan trabajar con datos mezclados. 
d. Diseñar nuevos métodos de clasificación aproximada, con los cuales no se pierda 
mucha calidad de clasificación en el proceso y en los cuales, el número de 
comparaciones entre objetos se reduzca notablemente. 
e. Implementación de métodos adaptados y/o propuestos. 
f. Realización de pruebas y comparación de resultados. 
g. Análisis de los resultados obtenidos y retroalimentación. 
4. Se planea diseñar métodos basados en el algoritmo de Aproximación-Eliminación, los 
cuales sean aplicables a datos mezclados. Para lo cual, se realizarán las siguientes 
actividades: 
a. Análisis crítico de los métodos basados en el algoritmo de AproximaciónEliminación. 
b. Extender algunos de los métodos existentes. 
c. Diseñar nuevos métodos de clasificación aproximada, con los cuales no se pierda 
mucha calidad de clasificación en el proceso y en los cuales, el número de 
comparaciones entre objetos se reduzca notablemente. 
d. Implementación de métodos adaptados y/o propuestos. 
e. Realización de pruebas y comparación de resultados. 
f. Análisis de los resultados obtenidos y retroalimentación. 
5. Diseñar un nuevo método exacto, basado en las propiedades de alguna función de 
similaridad. A partir del estudio de las propiedades de la función de similaridad, 
seleccionar alguna que permita definir propiedades que ayuden a reducir comparaciones 
entre objetos, sin perder calidad de clasificación en el proceso. 
a. Diseño e implementación del método propuesto. 
b. Realización de pruebas y comparación de resultados. 
c. Análisis de los resultados obtenidos y retroalimentación.
6. El análisis del estado del arte, la escritura de artículos y la redacción de tesis se proyectan 
como actividades a realizar durante el desarrollo de este trabajo. A continuación se 
muestra el calendario de actividades.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los experimentos realizados dan un indicio de que la representación propuesta para la 
codificación de relaciones tiene la posibilidad de contribuir a mejorar la precisión, permitiendo 
representar documentos y consultas de manera más detallada. 
La representación propuesta da auspicios de ser benéfica en otras tareas de tratamiento de 
información. Por ejemplo en búsqueda de respuesta, puede contribuir a una adecuada selección de 
la respuesta y en el agrupamiento, a asociar documentos con atributos más específicos 
Concluimos que la precisión no empeora con el modelo propuesto y que si se enriquece dicho 
modelo agregando nuevas relaciones como entidades nombradas, sujeto-verbo, verbo-objeto, 
sustantivo-adjetivo, verbo-adverbio, o relaciones que reflejen conocimiento de un dominio 
particular, podrá alcanzarse una mejora mayor.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Una Representación Vectorial para Contenido 
 de Textos en Tratamiento de Información
 Maya Carrillo Ruiz, Aurelio López López
Reporte Técnico No. CCC-08-004 
25 de Abril de 2008
© 2008 
Coordinación de Ciencias Computacionales 
INAOE 
Luis Enrique Erro 1 
Sta. Ma. Tonantzintla, 
72840, Puebla, México.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ONTOLOGÍAS PARA SERVICIOS WEB SEMÁNTICOS DE INFORMACIÓN DE TRÁFICO: DESCRIPCIÓN Y HERRAMIENTAS DE EXPLOTACIÓN</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Debido al vertiginoso ritmo de aparición y posicionamiento de las tecnologías, tanto en 
el nivel de las comunicaciones como en el de tráfico, se han cometido diversos errores 
en cuanto a la generación de los contenidos publicados en la Web, y la forma en la que 
son transmitidos a los usuarios. Estos errores han constituido duplicidad de esfuerzos al 
desarrollar diversas aplicaciones entre diferentes administradores, sin una 
estandarización como base y generalmente asociada con el desarrollo de contenidos 
basados en lenguajes de marcado como HTML. Esto ha dificultado procesos deseables 
como el compartir la información al tratar con vocabularios no comunes entre los 
diferentes elementos involucrados, y que al final dan como resultado la percepción por 
parte del usuario de información diversa y difusa. 
A continuación se describen cada uno de los problemas que bajo mi perspectiva, 
caracterizan la problemática de la difusión de información de tráfico vía Web: 
Problema 1: Ausencia de vocabularios comunes 
 En el desarrollo de las distintas aplicaciones de tráfico es característico la 
ausencia de vocabularios comunes que puedan ser elegidos como núcleo y punto de 
partida para desarrolladores de contenidos. Por ejemplo, en el caso de las aplicaciones 
sobre gestión de incidencias de tráfico, no existen tales vocabularios comunes que sean 
asociados a los descriptores o metadatos3
 de la información que deben de desarrollar las 
aplicaciones, con lo que los desarrolladores pueden llegar a describir la información 
(por ejemplo de los vehículos implicados en un accidente) de diferente forma, inclusive 
entre desarrolladores de una misma empresa de desarrollo y más aún entre diferentes 
administraciones y países. 
Problema 2: Múltiples fuentes y formatos 
 Existe una distribución inherente de los datos e información repartidos entre 
fuentes muy diversas como bases de datos y portales web entre otras. Incluso los 
documentos públicos aportados por las diferentes entidades, aparecen en formatos tan diversos como puedan ser HTML, XML, WML, VoiceXML etc. y a su vez dicha 
información debe ser tratada por diferentes usuarios como puedan ser las propias 
administraciones de tráfico o por los conductores o viajeros. En el dominio de los ITS el 
tratamiento de la información y las consecuencias de este tratamiento, como la toma de 
decisiones o la simple consulta de este tipo de información han sido hasta la fecha 
altamente dependientes del conocimiento humano. 
Problema 3: Gran volumen de información distribuido 
 La difusión de información de tráfico abarca un gran abanico de posibilidades 
que hacen que el tratamiento de la información se deba abordar de manera eficaz. El 
usuario se ve obligado a tratar con un gran volumen de información que aparece a su 
vez distribuida por los diferentes portales web de administraciones o entidades privadas 
que la ofrecen. A su vez la aparición de múltiples dispositivos de usuario final ha dado 
lugar a que desde el punto de vista del usuario haya una gran dificultad para encontrar 
estos lugares y tratar con las diferentes representaciones y accesos a la información. 
Problema 4: Imposibilidad de almacenaje de toda la información 
 Por otra parte, un usuario puede necesitar información de diferente naturaleza, y 
por lo tanto el almacenaje de toda esta información en un solo sitio web no es viable a 
nivel de costes de almacenamiento ni incluso a nivel de mantenimiento. 
Problema 5: Carencia de descripciones semánticas y de servicios web 
 Tal y como se describe en el problema 1, los vocabularios o lenguajes que 
describen conceptos y estructuras de datos relacionados con tráfico suelen ser Ad Hoc 
para cada aplicación siendo además descripciones de tipo sintáctico, carentes de 
semántica. Hasta donde llegó la búsqueda y revisión bibliográfica de esta tesis, no se 
encontraron ontologías o vocabularios con valor semántico que aporten significado a los 
conceptos y sus relaciones. Tampoco se conoce de la existencia de Servicios Web (SW) 
de información de tráfico. 
Problema 6: Solamente es posible obtener conocimiento explícito 
 Este problema es derivado del anterior. El tratamiento de la información no 
permite realizar inferencias sobre ella de manera que pueda ser obtenida como 
resultado, información que a priori no estuviera explícitamente detallada.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo principal de esta tesis es proponer un modelo de integración basado en Web 
Semántica para el desarrollo de SW de información de tráfico, a partir del manejo de 
distintas fuentes de información (por ejemplo, diferentes administraciones u 
organismos) como recursos Web. Se tratará de establecer una descripción de los 
servicios aportados, así como hacer posible la interacción con dichos servicios de forma 
que el usuario satisfaga sus requerimientos y pueda interactuar con dichos recursos 
(previo tratamiento de la información) de manera relativamente sencilla mediante el uso 
de interfaces adaptables al servicio y sin preocuparse de donde pudiera encontrar ésta.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Qué vocabularios comunes pueden ser definidos para representar información 
de tráfico en lo relacionado con accidentes, medidas, localizaciones, tipos de 
vehículos etc.?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Qué principios, tecnologías y herramientas de representación del conocimiento 
y de definición de ontologías son las más apropiadas para abordar este 
problema?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Es posible obtener un marco de referencia que ayude en la descripción y 
posterior uso de información distribuida a través de páginas web tradicionales, 
para convertir éstas en SWS?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Qué mecanismos pueden utilizarse para permitir que cualquier usuario acceda 
de forma transparente o a través de interacciones sencillas a la información 
deseada por él, expresada según sus requerimientos?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1 Planteamiento del problema
Planificación
Revisión del estado del arte
Experimentos iniciales
Obtención de resultados iniciales
Contraste con Trabajos Otros Grupos
Experimentos y Análisis
Escritura</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>A la vista del trabajo realizado en esta tesis, se ha podido constatar que la Web 
Semántica está todavía en un proceso de inmadurez, y que por lo tanto queda aún un 
largo camino por recorrer. Necesita de aplicaciones reales e industriales que muestren 
todo su potencial y no simplemente ejemplos donde se llegue a lo más a usar RDF o 
cualquier lenguaje basado en él, sin tener en cuenta el resto de capas que conforman su 
arquitectura. La conjunción de esfuerzos y resultados por parte de las diferentes 
comunidades completarán el proyecto. El uso de tecnologías complementarías más 
consolidadas como los SW permitirán cubrir parte de los objetivos. Debido a esta 
consolidación y al futuro que se prevé para este tipo de tecnologías, sobre todo en el 
área B2B, se puede diagnosticar que la integración de la WS y los SW suponen un 
pequeño escalón en el proceso de alcanzar la visión inicial de WS y el cual puede ser 
cubierto sin esperar la completa realización de ésta. 
 La aplicación de los SWS (producto de esta integración) en los ITS, me ha 
permitido ver con claridad las ventajas de su uso en cuanto a la consecución de los 
objetivos iniciales marcados, pero también las grandes dificultades que conlleva hacer 
uso de tecnologías emergentes no consolidadas. El principal escollo a salvar ha sido la 
constante evolución de las tecnologías, el gran abanico de posibilidades para la 
ejecución de cada uno de los objetivos parciales y sobretodo la falta de sensibilización 
que a fecha de hoy, poseen los distintos organismos relacionados con el tráfico vial, y 
ajenos a la gran revolución que está sufriendo la Web. Sin embargo, gracias a la 
interacción del mundo académico con el de las administraciones públicas, auguro que 
en un futuro próximo la aplicación de técnicas como las tratadas en esta tesis serán 
adoptadas en los ITS, al igual que ocurrió con tecnologías predecesoras como XML.
En lo referente al dominio de aplicación de información de tráfico ofrecida al 
viajero (TIS), el trabajo expuesto en esta tesis, supone una iniciativa totalmente 
novedosa en cuanto a su aplicación, puesto que las líneas actuales no contemplan el uso 
de la semántica como alternativa en la distribución o publicación de la información. 
Actualmente los proveedores de servicios de tráfico así como los CIT, han considerado 
el uso de lenguajes de marcado XML para sus dominios específicos así como 
tecnologías de servicios tradicionales, sin embargo, las ventajas del uso de una 
semántica común en los vocabularios permitiría no solo la obtención de los mismos 
resultados sino un incremento en cuanto a eficiencia por las razones expuestas a lo largo 
de la memoria. 
 Las diferentes pruebas de funcionalidad realizadas al prototipo desarrollado han 
permitido verificar que el objetivo general marcado ha sido satisfecho. Principalmente, 
se ha constatado como ha sido posible la realización del ciclo completo desde la 
petición de búsqueda por parte de un cliente hasta la invocación y ejecución del servicio 
encontrado y la posterior entrega de información al usuario de la aplicación, gracias al 
seguimiento de los mensajes entre los agentes involucrados. Mediante estas pruebas ha 
sido posible comprobar la integración de todos los elementos propuestos en esta tesis, 
especialmente las ontologías creadas, así como el algoritmo de emparejamiento. 
 El algoritmo presentado, supone un extensión y mejora de anteriores propuestas 
enmarcadas en la misma línea de investigación, ya que tras el análisis de los resultados 
obtenidos de las pruebas realizadas se puede concluir que en determinadas ocasiones 
este algoritmo propuesto ha conseguido obtener servicios que más se ajustaban por sus 
características a los requerimientos de un determinado cliente, que los obtenidos por 
otras aproximaciones ampliamente referenciadas, aunque a veces esto haya repercutido 
en un incremento en el coste computacional. 
 Se ha podido comprobar que el hecho de hacer uso en un principio de ontologías 
en castellano no supuso un agravio en cuanto el desarrollo del prototipo y además se 
pudo comprobar el mapeo ontológico entre ontologías de diferentes idiomas. 
 En está investigación se ha constatado la gran inmadurez de estándares y de 
plataformas tecnológicas en cuanto a los SW, así como la ausencia de un metodología 
común y normas preestablecidas para el diseño de servicios. No obstante, los SW 
crearán nuevas oportunidades de negocio, aportando como beneficios el poder 
compartir datos, aplicaciones y procesos entre proveedores y clientes. Es de resaltar la 
importancia de los SW en la comunicación de las aplicaciones y los Sistemas de 
Información sin importar el lenguaje en el que fueron desarrollados o la plataforma en la 
que se ejecuten. 
 La integración de ontologías de tráfico vial ha contribuido no solo a la 
consecución de los objetivos que inicialmente fueron tomados, sino a la certificación de 
que el uso de la semántica puede ser aplicado en cualquier entorno o dominio.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSITAT DE VALÉNCIA 
DEPARTAMENT D' INFORMÁTICA 
ONTOLOGÍAS PARA SERVICIOS WEB
SEMÁNTICOS DE INFORMACIÓN DE TRÁFICO:
DESCRIPCIÓN Y HERRAMIENTAS DE
EXPLOTACIÓN
TESIS DOCTORAL 
Presentada por: 
D. José Javier Samper Zapater 
Dirigida por: 
Dr. Juan José Martínez Durá 
Dr. Eduardo Carrillo Zambrano 
Valencia, 2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Minería de texto empleando la semejanza entre estructuras semánticas</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La minería de texto es el área de investigación más reciente del procesamiento de
textos. Ella se define como el proceso de descubrimiento de patrones interesantes
en una colección de textos. Estos patrones no deben de existir explícitamente en
ningún texto de la colección, y deben de surgir de relacionar el contenido de varios
de ellos (Hearst, 1999; Kodratoff, 1999).
El proceso de minería de texto consiste de dos etapas principales: una etapa de
preprocesamiento y una etapa de descubrimiento (Tan, 1999). En la primera etapa, 
los textos se transforman a algún tipo de representación estructurada o semiestructurada que facilite su posterior análisis, mientras que en la segunda etapa las representaciones intermedias se analizan con el objetivo de descubrir en ellas algunos patrones interesantes o nuevos conocimiento.
Entonces, dependiendo del tipo de métodos aplicados en la etapa de preprocesamiento son el tipo de representaciones intermedias construidas, y en función de
dicha representación son el tipo de métodos usados en la etapa de descubrimiento, y 
en consecuencia, el tipo de patrones descubiertos.
La figura 1.1 muestra las principales estrategias usadas en los actuales sistemas
de minería de texto. De acuerdo con esta figura, la mayoría de los actuales de minería de texto limitan sus resultados a un nivel temático o de entidad, y por lo tanto
imposibilitan el descubrimiento de cosas más detalladas como:
		*	Consensos, que por ejemplo respondan a preguntas como: * ¿Cuál es la opinión
mayoritaria de los mexicanos sobre el gobierno de Fox?
		*	Tendencias, que indiquen por ejemplo si han existido variaciones en la postura de 
Fox con respecto a la educación.
		*	Desviaciones, que identifiquen por ejemplo opiniones "raras con respecto al desempeño de la selección mexicana de fútbol.
Una idea para mejorar la expresividad y la diversidad de los descubrimientos de
los sistemas de minería de textos consiste en usar una representación del contenido
de los textos más completa que las representaciones usadas actualmente. Siguiendo 
este enfoque, en esta investigación se propone usar los grafos conceptuales (Sowa,
1984; 1999) como representación del contenido de los textos. 
Los grafos conceptuales se seleccionaron de entre otras representaciones (por
ejemplo, árboles sintácticos, predicados lógicos y KL-ONE) por las siguientes razones:
1. Los grafos conceptuales permiten representar adecuadamente en términos de
expresividad y eficiencia notacional la información en lenguaje natural.1
2. Los grafos conceptuales disponen de mecanismos formales que facilitan su manipulación, transformación y análisis (ver sección 3.2).
3. Los grafos conceptuales se usan en otras tareas afines a la minería de texto, por
ejemplo en la recuperación de información y en el agrupamiento de textos.
Ahora bien, realizar la minería de texto usando los grafos conceptuales como representación del contenido de los textos involucra dos problemas diferentes. En primer lugar, la transformación de los textos en grafos conceptuales, y en segundo lugar, el análisis automático de un conjunto de grafos conceptuales.
La transformación de los textos en grafos conceptuales es un problema complejo 
vinculado con su análisis sintáctico y semántico (Sowa and Way, 1986; Sowa 1991). 
Debido a ello, todos los métodos que existen para realizar dicha transformación se
enfocan en un dominio único y restringido. Algunos ejemplos son los siguientes:
		*	Partes de artículos científicos a grafos conceptuales (Myaeng and Khoo, 1994;
López-López, 1995; Montes-y-Gómez, 1998; Montes-y-Gómez et al., 1999e).
		*	Partes de expedientes médicos a grafos conceptuales (Baud et al., 1992; Rassinoux et al., 1994).
		*	Partes de casos legales a grafos conceptuales (Boucier and Rajman, 1994).
		*	Partes de manuales de referencia a grafos conceptuales (Petermann, 1996).
Por su parte, el análisis automático de un conjunto de grafos conceptuales orientado al descubrimiento de patrones descriptivos nuevos conocimientos es un problema poco estudiado (Mineau and Godin, 1995; Godin et al., 1995; Bournaud and
Ganascia, 1996; Bournaud and Ganascia, 1997; Míller, 1997). Así pues, este trabajo 
de investigación se enfoca en el análisis de un conjunto de grafos conceptuales que 
representan el contenido de un conjunto de textos, y en el descubrimiento de varios 
tipos de patrones interesantes, por ejemplo: agrupamientos, asociaciones y desviaciones.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Nosotros asumimos, al igual que muchos otros, que disponer de más y mejor información del contenido de los textos permitirá descubrir más y mejores conocimientos a partir de ellos. Con base en esta suposición, el objetivo general de esta investigación es diseñar un nuevo método de minería de texto apto para emplear los grafos 
conceptuales como representación del contenido de los textos, y a su vez, capaz de
trasladar los descubrimientos del nivel temático a un nivel de mayor detalle un nivel 
más descriptivo</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La mayoría de los métodos actuales de minería de texto utilizan representaciones 
sencillas del contenido de los textos, por ejemplo listas de palabras clave o tablas de 
datos. Estas representaciones son relativamente fáciles de construir a partir de los
textos, pero impiden representar varios detalles de su contenido. Como consecuencia, los resultados de estos sistemas, es decir, los patrones que con ellos se descubren, son poco descriptivos y de nivel temático.
Una idea generalizada para mejorar la expresividad de los resultados de los mé-
todos de minería de texto consiste en emplear representaciones de los textos más
complejas que las palabras clave, es decir, representaciones que consideren más tipos 
de elementos textuales. Siguiendo este idea, en este trabajo propusimos un método
para hacer minería de texto a nivel detalle. Este método tiene la capacidad de usar 
grafos conceptuales para representar el contenido de los textos, y el potencial para
trasladar los resultados, es decir, los patrones descubiertos, del actual nivel temático 
a un nivel mucho más descriptivo.
Algunas contribuciones importantes de esta investigación son las siguientes:
		*	Se planteó, por primera vez, el uso una "representación semántica, en especifico 
grafos conceptuales, en las tareas de minería de texto.1
		*	Se demostró que el uso de los grafos conceptuales, y en general de las representaciones semánticas, en la minería de texto es factible, pero sobre todo, beneficioso 
para mejorar el nivel descriptivo de resultados.
		*	Se diseñó una nueva aproximación para realizar minería de texto. Para ello se
adaptaron algunos métodos de comparación y agrupamiento de grafos conceptuales para las tareas propias de minería de texto; y se diseñaron nuevas estrategias 
para descubrir asociaciones y detectar desviaciones en un conjunto de grafos
conceptuales.
Así pues, esta investigación contribuyó al estado del arte de diversas áreas del
conocimiento, entre las que destacan la minería de texto y la teoría de grafos conceptuales.
A continuación se describen las principales contribuciones específicas, trabajos
futuros, y limitaciones del método de minería de texto propuesto en este trabajo de
investigación.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN 
COMPUTACIÓN
LABORATORIO DE LENGUAJE NATURAL Y 
PROCESAMIENTO DE TEXTO
Minería de texto empleando 
la semejanza entre 
estructuras semánticas
TESIS
QUE PARA OBTENER EL GRADO DE 
DOCTOR EN CIENCIAS DE LA COMPUTACIÓN
P R E S E N T A
Manuel Montes y Gómez
Director de tesis: Dr. Alexander Gelbukh
 Codirector: Dr. Aurelio López López
México, D.F., 2002.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Desambiguación de los sentidos de las palabras en español usando textos paralelos</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El objetivo de la desambiguación de los sentidos de las palabras es determinar el
significado correcto, o el sentido de una palabra dada en el contexto. Este se consi-
dera como uno de los problemas más difíciles en el nivel léxico del procesamiento del
lenguaje natural [2], [3] y es fundamental en aplicaciones de traducción automática,
recuperación de información, etc.
Las dificultades provienen de algunos orígenes, incluyendo la falta de medios para
formalizar las propiedades o parámetros del contexto que caracterizan el uso de una pa-
labra ambigua en un sentido en particular y la falta de un inventario de sentidos usuales.
Entre los enfoques que existen para WSD, el de aprendizaje supervisado es el más
exitoso hasta la fecha [4]. En este se emplea un corpus en el que cada ocurrencia de una
palabra ambigua w ha sido manualmente anotada con el sentido correcto, de acuer-
do con la existencia de sentidos en un diccionario. De esta forma, este corpus sirve
como material de entrenamiento para el algoritmo de aprendizaje, cuyo resultado es
un modelo automáticamente aprendido y que puede ser usado para atribuir el sentido.
Los corpus empleados en la tarea de clasificación supervisada, requieren una gran
cantidad ejemplos para cada sentido de la palabra. Este requisito hace al enfoque su-
pervisado impracticable al tratar de desambiguar todas las palabras en los textos. Este
problema ha sido llamado: obstáculo de adquisición de conocimientos [5].
Ha sido reconocido que la mejor manera adquirir etiquetas de sentidos para las pa-
labras en un corpus es a mano [6]. Sin embargo, la formulación manual de inventarios
de sentidos ha estado basada en la intuición de los lexicógrafos y se ha visto afectada de
problemas que incluyen el alto costo, la asignación arbitraria del significado a palabras
y el emparejamiento equivocado a dominios de aplicación [7].
Debido a los anteriores inconvenientes, los acercamientos no supervisados han re-
cibido mucha atención en los últimos años, pues tienen el potencial para superar el
cuello de botella de adquisición de sentidos. Esto lo han conseguido con la introducción
del sentido de la palabra directamente del corpus. Sin embargo, lo que parece a simple
vista una ventaja, constituye también su principal desventaja: la desambiguación se
lleva a cabo partiendo de un conjunto no muy bien definido de sentidos, lo que conlle-
va en muchas ocasiones a la obtención de resultados pobres durante el proceso de WSD.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un método que permita desambiguar los sentidos de las palabras para el
idioma español y el resto de las lenguas involucradas en corpus paralelos multilingues,
alineados a nivel de palabra; explotando así las diferencias polisémicas que prevalecen
entre los idiomas.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Un recurso que se hace cada vez más disponible en los medios electrónicos compar-
tidos a través de Internet son los textos paralelos. Estos poseen el mismo contenido 
semántico, pero expresado en lenguajes diferentes [12] y reciben el nombre de corpus
paralelos bilingues. Actualmente existe un creciente interés en la alineación de dichos
corpus, que consiste en el establecimiento de correspondencias entre los elementos de
un texto y sus traducciones en el otro [13]. La alineación puede ser realizada tomando
con base a alguno de los elementos estructurales: párrafos, oraciones o palabras.
Como hipótesis de la investigación se planteó la posibilidad de utilizar los textos
paralelos como fuente potencial en el proceso de etiquetado para la desambiguación de
los sentidos de las palabras del idioma español; dado que en un corpus paralelo alineado
por palabras, las diferentes traducciones en una lengua de meta sirven de etiquetas de
sentido de una palabra ambigua en la lengua de origen. Es decir, las distinciones en las
traducciones proveen una correlación práctica para la distinción de sentidos.
Para ello, se partió de los siguientes supuestos:
Los corpus muy grandes, hasta de texto plano no preparado, son difíciles de
obtener para un lenguaje, género y dominio específico.
Información adicional disponible para un corpus de entrenamiento hace que la
curva de aprendizaje se haga más rápida, y por lo tanto, con corpus de un tamaño
no muy grande se logran mejores resultados.
Es más barato encontrar un corpus paralelo alineado (traducido) para un len-
guaje, género y dominio específico, que etiquetar un corpus del tamaño adecuado
como se hace para el WSD supervisado tradicional.
La traducción paralela proporciona información adicional sobre el corpus, útil
para el aprendizaje de WSD.
Esta información adicional es suficiente para que el balance de costo/calidad para
un método basado en el uso de tal traducción sea mejor que para los métodos
tradicionales supervisados y no supervisados.
La información aprendida de un corpus con traducción (paralelo) será útil para
desambiguar nuevos textos (monolingues), de tamaño corto.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El uso tan frecuente de las computadoras en la actualidad ha conllevado al desa-
rrollo de múltiples aplicaciones tecnológicas, cuya base es el lenguaje natural. Aunado
a esto, el surgimiento de Internet y con ello la gran reserva de recursos de información
digitales, incita el desarrollo de algoritmos de búsqueda eficientes, que ponen a nuestra
disposición todo un mundo de lenguaje. Con esta plataforma es común el uso de apli-
caciones importantes de la linguística computacional, como: la traducción automática,
el manejo y gestión de documentos, la generación de texto y resúmenes, las bibliote-
cas digitales, las interfaces en lenguaje natural, etc.; cuya efectividad depende en gran
medida de la eficacia de los algoritmos que han sido propuestos y que en todas estas
tareas consideran la desambiguación de los sentidos de las palabras.
La investigación y mejora de algoritmos de WSD conduce, por consiguiente, a la
mejora en las aplicaciones antes mencionadas, lo que justifica el área de investigación
seleccionada para esta tesis.
Muchos métodos han sido propuestos para llevar a cabo la desambiguación y entre
ellos los supervisados han sido de los más sobresalientes, con la presente dificultad de
que requieren datos etiquetados de sentidos, adquiridos en la gran mayoría de las oca-
siones, de forma manual.
La disponibilidad de los conjuntos adecuados de datos etiquetados es por tanto un
factor significativo y crucial para el entrenamiento de algoritmos basados en aprendi-
zaje. Por desgracia, estos recursos son difíciles de obtener y de crear, pues requieren
que cada una de las palabras haya sido etiquetada con su sentido. Este procedimiento
resulta muy costoso y por ello solamente algunos artículos de vocabulario terminan
siendo etiquetados.
Mientras el cuello de botella léxico parece estar resuelto para el idioma inglés, no
existe un amplio rango de recursos linguísticos para el procesamiento del lenguaje na-
tural en otros idiomas [11]. El recurso de mayor cobertura de palabras es Princeton
WordNet, un lexicón de concordancia semántica para el idioma inglés1
, que agrupa los
sinónimos en conjuntos denominados synsets y proporciona a las palabras su defini-
ción. El propósito de éste es producir una combinación de diccionario y tesauro que sea
intuitivamente utilizable y respalde el análisis de texto automático y las aplicaciones
de inteligencia artificial. Sin embargo, su utilidad como recurso de entrenamiento y
evaluación para anotadores de sentidos supervisados está actualmente algo limitada
por su metodología secuencial. Esta notación obliga a los intérpretes a familiarizarse
con los inventarios de sentidos de cada palabra [2].
A pesar de sus inconvenientes, este recurso representa una contribución muy im-
portante al campo, proporcionando el primer conjunto de datos balanceados de larga
escala para el estudio de las propiedades de polisemia en el idioma inglés.
Desafortunadamente, no existe para el idioma español un recurso léxico de tal mag-
nitud con el cual se pueda proveer de sentido a las palabras, aunque se han están
llevando a cabo esfuerzos en esta dirección. Por lo tanto, la búsqueda de recursos dis-
ponibles que permitan asociar etiquetas de sentidos y más aún, esto lo realice de forma
automática, es una tarea intermedia indispensable en el procesamiento de lenguaje
natural</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para desarrollar el trabajo propuesto se siguió un conjunto de pasos que para ase-
gurar el cumplimiento de cada uno de los objetivos presentados. A continuación se
enumeran las necesidades superadas en el desarrollo de la investigación:
1. Recopilación bibliográfica y análisis detallado de los acercamientos de desambi-
guación existentes.
2. Caracterización de las familias de lenguajes y su relación con el lenguaje español.
3. Selección del idioma(s) que se empleará como lenguaje meta en los textos para-
lelos.
4. Búsqueda de corpus paralelos con el par de lenguas: español / meta(s).
5. Comparación y aplicación de diversas herramientas de alineación a nivel de pa-
labras sobre el corpus elegido.
6. Análisis de diccionarios monolingues y bilingues disponibles.
7. Diseño de un algoritmo para la adquisición de etiquetas de sentidos extraídas de
la alineación resultante.
8. Definición e implementación computacional de los procedimientos requeridos para
la desambiguación de los sentidos de las palabras.
9. Aplicación de los procedimientos implementados sobre el corpus de prueba.
10. Incorporación de un módulo de estadísticas al sistema.
11. Selección de conjunto de pruebas, siguiendo criterios estudiados para la realiza-
ción de pruebas y refinamiento del método definido.
12. Desarrollo de artículos relacionados con los resultados obtenidos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Muchas de las aplicaciones relacionadas con procesamiento de lenguaje natural re-
quieren previa desambiguación para conseguir resultados satisfactorios en las labores
que desempeñan. Es por ello que la diferenciación de los sentidos ha sido considerada,
desde hace muchos años, objeto de estudio como tarea independiente.
Desde sus inicios, se han utilizado diversas aproximaciones para la diferenciación de
sentidos en palabras polisémicas. El método de desambiguación propuesto combina el
enfoque supervisado y no supervisado, empleando textos paralelos alineados a nivel de
palabra como corpus de entrada. En los ensayos realizados se usaron textos en español
como idioma origen y traducciones de los mismos al italiano e inglés.
Como diccionario léxico multilingue se usó MultiWordNet, aprovechando la ventaja
que su concepción ofrece: las redes de palabras que lo conforman se encuentran alinea-
das.
El sistema implementado está dividido en tres módulos principales: (1) lematiza-
ción, (2) alineación y (3) desambiguación. En la primera fase se obtienen los lemas de
la palabra polisémica y de todas las formas que aparecen en el contexto del texto meta,
pues es posible establecer relaciones con los lemas obtenidos de la aplicación de reglas
morfológicas sobre las entradas. Durante esta etapa, no se utiliza heurística sintáctica
alguna para resolver homonimia morfológica. En la fase de alineación se toma ventaja
de la correspondencia que existe entre las wordnets de MWN y se incluye un análisis
de pesos por dominio semántico o medidas de similitud, bajo el supuesto de que los
textos se encuentran alineados a nivel de sentencias. Como resultado de este módulo
se obtienen grupos de alineación que constituyen la entrada en el proceso de desambi-
guación. Finalmente, cada palabra polisémica en el texto origen es etiquetada con el
sentido determinado por el desambiguador, basándose en intersecciones de synstes y
categorías semánticas.
Para la evaluación de los resultados obtenidos se efectuaron experimentos en dos
direcciones. La primera estuvo enfocada en el proceso de alineación de palabras no funcionales, para la cual se emplearon fragmentos de la novela Don Quijote de la Mancha,
sin etiquetado ni preprocesamiento previo. Los resultados fueron comparados con un
gold standard, producido por anotadores humanos, a través de tres medidas de evalua-
ción: precisión, recall y F-measure.
Para contar con un corpus de prueba estándar que incluyera anotación manual de
sentidos, en el segundo acercamiento de los experimentos se hizo uso de traducción
artificial para obtener los textos meta. Se partió del supuesto que aún las traducciones
conseguidas mediante un traductor automático proporcionarían un alto porcentaje de
lexicalizaciones que permitiesen descartar sentidos de las palabras polisémicas en el
origen. Para ello se empleó el traductor comercial LEC Power Translator 11 sobre el
corpus de muestra léxica del español en SENSEVAL-3.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto POLITÉCNICO Nacional 
Centro de Investigacion en Computación
Desambiguación de los sentidos de las palabras en
español usando textos paralelos
Que para obtener el grado de:
Doctorado en Ciencias de la Computación
Presenta:
Grettel Barceló Alonso
Directores:
Dr. Alexandre Felixovich Guelboukh Kahn
Dr. Grigori Sidorov
México, D.F. Junio 2010</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Reconocimiento de Emociones a Partir de Voz Basado en un Modelo Emocional Continuo</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El área de reconocimiento automático de emociones ha sido un área de investigación muy activa en 
los últimos años, no obstante aun no hay una solución clara para este problema. Diversos 
inconvenientes han influido en la construcción de una solución apropiada. Por un lado, un factor 
que afecta el desempeño de los reconocedores de emociones en contextos reales es la dificultad de 
generar bases de datos con emociones espontáneas. Generalmente se ha trabajado con bases de 
datos actuadas las cuales proporcionan "Retratos de emociones" representando expresiones 
prototípicas e intensas que facilitan la búsqueda de correlación acústica y la subsecuente 
clasificación automática, sin embargo, no se han tenido buenos resultados al trasladar el 
conocimiento extraído de estas bases de datos a contextos reales (Steidl, 2009). 
Las características de las bases de datos actuadas son: 
1. Los "Retratos de emociones" representando expresiones prototípicas e intensas facilitan la 
búsqueda de correlación acústica y la subsecuente clasificación automática. 
2. Las grabaciones en estudio con alta calidad eliminan problemas en el procesamiento de la 
señal. Por ejemplo ruido o reverberación. 
3. Se puede garantizar una cantidad balanceada de ejemplos por clase. 
En contraparte, las bases de datos con emociones espontáneas muestran elocuciones con contenido 
emocional no perteneciente a una sola clase, sino que son una mezcla de emociones. En otros casos, 
existen elocuciones con una carga emocional muy ligera, cercana a un estado emocional neutro. 
Además, las bases de datos con emociones espontáneas suelen grabarse en ambientes ruidosos 
como conversaciones telefónicas o programas de televisión lo que conlleva la inclusión de ruido. 
Finalmente, por la naturaleza misma del problema no es posible asegurar una cantidad balanceada 
de ejemplos por clase. 
Otro reto a resolver es la identificación de un conjunto de características acústicas que permitan 
reconocer emociones en el habla espontánea. El trabajo hecho a la fecha se ha centrado 
principalmente en características relacionadas con aspectos prosódicos. Sin embargo, se ha 
descubierto que entre más nos acercamos a un escenario realista, menos fiable es la prosodia como 
un indicador del estado emocional del hablante (Batliner, et al., 2003), por lo tanto es necesario 
encontrar características que complementen la información que proporciona el aspecto prosódico de 
la voz. 
Finalmente, hasta la fecha la mayoría de los trabajos han utilizado modelos emocionales discretos, 
donde las emociones a reconocer están claramente identificadas en el corpus de entrenamiento. Bajo 
este enfoque no existe una valoración de la emoción sino la búsqueda de una o varias reglas que 
permitan la discriminación de las emociones en cuestión. De esta forma, es prácticamente necesario 
repetir todo el trabajo si se desea agregar una nueva emoción o se desea trabajar con otro corpus. 
Además los modelos discretos no parecen ser los más adecuados para trabajar con emociones 
espontáneas ya que no representan apropiadamente el traslape de emociones en el habla. 
A pesar de que los avances en el área han sido importantes, se ha comprobado que en contextos 
realistas aún falta mucho por hacer. Por lo tanto, es necesario proponer y explorar otros enfoques 
que permitan llegar a un buen desempeño del reconocimiento de emociones en aplicaciones del 
mundo real. Para ello, en esta tesis se propone trabajar con características diversificadas que 
expandan el uso de características acústicas y lingïsticas, además de emplear un modelo continuo que nos permita apegarnos más a la realidad. Nuestro trabajo estará basado en un modelo continuo 
tridimensional cuyas primitivas son Valencia, Activación y Dominación. Se emplearán 
principalmente características acústicas, que incluyen características prosódicas, de calidad de voz, 
y espectrales. Se explorará el uso de características lingïsticas como complemento o soporte a las 
características acústicas. Además se explorará el uso tanto de procesamiento estático como 
dinámico de características.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un método para el reconocimiento de emociones espontáneas basado en un modelo 
emocional continuo a partir de la información acústica extraída de la señal de voz, alcanzando 
un desempeño similar o mejor que los reconocedores de emociones actuales basados en 
modelos discretos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Qué características acústicas son útiles para reconocer emociones en el habla 
independientemente del dominio de aplicación?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Cuáles de esas características son más útiles para un modelo emocional continuo?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿De qué forma podemos emplear un modelo emocional continuo en él diseño de un método 
de reconocimiento de emociones aplicable a emociones espontáneas?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Cómo se pueden aprovechar las ventajas de los modelos emocionales continuos para 
estimar con una buena precisión la carga emocional en la voz?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿El uso de modelos continuos mejorará el reconocimiento de emociones con respecto al uso 
de modelos discretos? ¿Cuánto puede mejorar el reconocimiento de emociones en voz 
usando modelos continuos con respecto al uso de modelos discretos? </Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Las tareas listadas abajo no se realizaran en el estricto orden en el que aparecen. Existen tareas que 
se pueden realizar en paralelo como lo muestra el calendario de actividades mostrado en la sección 
8. La metodología se divide en tres componentes. En el Componente 1 se estudian las 
características que se usarán. En el Componente 2 se desarrolla el método propuesto para el 
reconocimiento de emociones basado en un modelo emocional continuo. Dicho método se ilustra en 
las Figuras 3, 4 y 5. En el Componente 3 se evalúan los resultados de los dos componentes 
previos. 
Componente 1 
1. Identificar grupos de características acústicas usadas hasta el momento mediante la 
revisión del estado del arte. 
a. Hacer una recopilación de características extraídas de la señal de voz que hayan 
sido propuestas en los trabajos en esta área publicados hasta el momento. 
b. Al mismo tiempo hacer una relación de los métodos de clasificación empleados con 
cada conjunto de características. 
c. Hacer una lista de bases de datos utilizadas en dichos trabajos poniendo especial 
atención en bases de datos de emociones espontáneas para tratar de obtenerlas. 
2. Estudiar métricas de calidad de voz y articulación usadas en medicina y comprobar la 
viabilidad de aplicación: 
a. Realizar un estudio sobre estándares y metodologías de medición de calidad y otros 
aspectos en la de voz en áreas médicas. 
b. Estudiar la viabilidad de extraer las medidas subjetivas usadas en estos estándares 
médicos de manera automática. 
c. Adoptar características acústicas para la clasificación de emociones basadas en este 
estudio. 
3. Estudiar métodos basados en información lingïstica 
a. Hacer un estudio sobre trabajos enfocados a extraer emociones a partir de texto. 
b. Estudiar la viabilidad de aplicar las técnicas y características propuestas en estos 
trabajos sobre las bases de datos que se tengan disponibles para experimentar. 
c. Calcular el aporte de la información lingïstica a la clasificación de emociones. 
d. Estudiar la manera de fusionar la información lingïstica con la acústica para 
mejorar la clasificación 
4. Proponer grupos de características representativas, analizar sus propiedades y 
experimentar en diferentes contextos de aplicación: 
a. Agrupar características de acuerdo a alguna taxonomía como puede ser la 
presentada en la sección 2.2.1 de tal manera que al momento de probar se haga de 
una manera más ordenada o metodológica y se pueda explicar a qué grupo o 
clasificación de características pertenecen las características utilizadas en cierto 
experimento. 
b. Hacer un estudio de la importancia de características individuales y de grupos de 
características basado en la aplicación de técnicas de selección de atributos y/o de 
reducción de dimensionalidad 
c. Crear un sistema basado en scripts que extraiga estas características 
automáticamente para probarlas con diferentes bases de datos. 
d. Conseguir las bases de datos de emociones espontáneas y actuadas para comparar. 
e. Experimentar con las características en diferentes bases de datos. 
f. Experimentar con diferentes clasificadores de acuerdo a los enfoques de 
procesamiento de características estático y dinámico. Probar Hidden Markov 
Models y Gaussian Mixture Models. 
g. Probar diferentes formas de fusionar la información obtenida con el procesamiento 
estático y dinámico. 
h. Explorar nuevas características basándose en los hallazgos de pasos previos. 
Componente 2 
5. Crear un modelo que asocie medidas objetivas extraídas de la señal de audio con 
primitivas emocionales usadas en evaluaciones perceptivas: 
a. Hacer un estudio del estado del arte para definir qué modelo emocional continuo 
será el más conveniente para usar en esta tesis. Se definirá la dimensionalidad y las 
primitivas usadas. 
b. Proponer un método para descubrir la relación existente entre las características 
estudiadas en pasos previos con las primitivas emocionales del modelo continuo 
propuesto. 
c. Experimentar con técnicas difusas y probabilísticas para determinar dicha relación. 
d. Diseñar un esquema de mapeo entre las características acústicas y primitivas 
emocionales en un modelo continuo a partir del estudio de esa relación. 
e. Probar otras técnicas de computación suave para mejorar el desempeño de los 
modelos difusos. Entre otras técnicas podrían emplearse Algoritmos Genéticos. 
f. Experimenta usando un clasificador para cada primitiva. 
g. Diseñar un método de reconocimiento de patrones adecuado a los hallazgos de 
pasos anteriores 
6. Generar un mapeo entre coordenadas en un modelo continuo y emociones básicas: 18 
a. Hacer una revisión del estado del arte para determinar según diferentes autores que 
niveles de primitivas emocionales se esperan para las emociones básicas siendo 
analizadas de acuerdo a la base de datos con la que se esté probando. 
b. Diseñar un método de transformación de los valores esperados de las primitivas 
emocionales, según el paso anterior, hacia una categoría emocional específica. Este 
método no dependerá de un conjunto fijo de emociones sino que aceptará diferentes 
emociones dependiendo de la aplicación o de la base de datos con la que se esté 
probando. 
c. Probar ANFIS (sistema adaptativo de inferencia neuro difuso) para diseñar un 
método que genera automáticamente reglas que relacionen primitivas emocionales 
con emociones básicas a partir de un entrenamiento basado en una base de datos de 
emociones básicas 
7. Crear un esquema que fusione la información obtenida con cada grupo de 
características 
a. Implementar un esquema de filtrado y pesado de características dependiendo de la 
información disponible y de las características de la base de datos. 
b. Incorporar procesamiento dinámico y estático de características, fusionando las 
predicciones hechas por cada tipo de procesamiento
Componente 3 
8. Evaluar el sistema de acuerdo a los lineamientos de HUMAINE Association: 
a. Hacer una evaluación bajo los estándares de HUMAINE Association (Schuller, et 
al., 2009). 
9. Evaluar el desempeño en otros contextos: 
a. Llevar a cabo evaluaciones sobre diferentes bases de datos tanto de emociones 
reales, como actuadas con el fin de evaluar el alcance del sistema. 
b. Hacer una evaluación subjetiva con personas no especializadas o no entrenadas.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este documento se propone un método de clasificación de emociones a partir de la voz basado en 
un modelo emocional continuo, que incluye el estudio de características apropiadas para esta tarea y 
un esquema para reconocer emociones automáticamente. Este método intenta eliminar las limitantes 
de los métodos basados en emociones básicas actuales. Dentro de los avances conseguidos hasta el 
momento se ha hecho un estudio con más de 350 características incluyendo información acústica y 
lingïstica. Se ha planteado un método de reconocimiento de emociones en dos fases. La primera 
fase, de entrenamiento, incluye dos sub-métodos, el Método Particular de Clasificación de 
Emociones Básicas, que sirve para generar un Sistema de Inferencia el cual mapea emociones 
básicas en un espacio tridimensional continuo, y el Método General de Estimación de Primitivas, 
que sirve para estimar primitivas emocionales a partir de una señal de voz mediante modelos 
entrenados con SVM. La segunda fase es la de aplicación y está compuesta por los modelos 
entrenados por los sub-métodos de la fase de entrenamiento. 
Se han realizado experimentos para validar la viabilidad de lo propuesto en esta tesis. Mediante 
estos experimentos nos hemos comparado contra los resultados del estado del arte clasificando 
emociones básicas y prediciendo primitivas emocionales obteniendo resultados alentadores. Con 
base en los avances logrados hasta el momento y en los resultados obtenidos en nuestros 
experimentos concluimos que se pueden lograr los objetivos planteados según la metodología y el 
plan de trabajo propuestos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Reconocimiento de Emociones a Partir de 
Voz Basado en un Modelo Emocional 
Continuo
Humberto Pérez Espinosa, Carlos Alberto Reyes García 
Reporte Técnico No. CCC-10-005 
4 de mayo de 2010
Año 2010 
Coordinación de Ciencias Computacionales 
INAOE 
Luis Enrique Erro 1 
Sta. Ma. Tonantzintla, 
72840, Puebla, México.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>LA INTEGRACIÓN DE LAS TECNOLOGÍAS DE LA INFORMACIÓN Y DE LA COMUNICACIÓN EN EL ÁREA DE LA EDUCACIÓN FÍSICA DE SECUNDARIA: ANÁLISIS SOBRE EL USO, NIVEL DE  CONOCIMIENTOS Y ACTITUDES HACIA LAS TIC Y DE SUS POSIBLES APLICACIONES EDUCATIVAS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Determinar el nivel de uso y de conocimientossobre las TIC del profesorado de
EF de Secundaria y establecerlas necesidades de incorporación, posibles
aplicaciones y losrequerimientos necesarios para la utilización beneficiosa de las
NNTT en el área de la EF de Secundaria.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿El profesional de la Educación Física utiliza las TIC para organizar y gestionarsu actividad docente?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Se utilizan lasNNTT durante las clases de Educación Física de Secundaria?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Con qué limitaciones o problemas se encuentran los profesores al introducir las TIC en la asignatura de
Educación Física de Secundaria?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿De qué manera podría utilizar el profesorado de Educación Física de Secundaria las Nuevas Tecnologías
paramejorarsu labor docente?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Qué utilización habrían de tenerlas TIC como herramienta de aprendizaje en el área de la EF?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>* 1.1. El profesorado de Educación Física utiliza frecuentemente las TIC en su
vida diaria.
* 1.2. El profesorado utiliza las TIC preferentemente en casa y no en el
centro.
Hipótesis 1. En relación con el uso general de las TIC por parte del profesorado
* 2.1. El profesorado de Educación Física no emplea las TIC en sus clases.
* 2.2. El profesorado piensa que la utilización de las TIC optimiza los
procesos de enseñanzaaprendizaje del alumnado.
Hipótesis 2. En relación con la utilización de las TIC en los procesos de
enseñanzaaprendizaje de la EF
* 3.1. La mayoría del profesorado utiliza las TIC como recurso para la gestión
y organización de la asignatura y como herramienta para el diseño de
clases y de materiales de aula.
* 3.2. El nivel de formación y de conocimientos sí que es un factor favorable
hacia la utilización de las TIC como herramienta de trabajo.
* 3.3. La existencia de espacios específicos con recursos TIC para el
profesorado es un factor que favorece claramente el uso de estas
herramientas.
* 3.4. El profesorado opina que el tratamiento de las TIC en el área debe ser
como tema transversal, y que los contenidos referidos al bloque de
capacidades condicionales y los deportes son los que mejor se adaptan a
la aplicación educativa del ordenador dentro de la Educación Física.
Hipótesis 4. En relación con la formación en TIC del profesorado
* 4.1. El nivel de formación y de conocimientos del profesorado no es un
factor decisivo para utilizar las TIC en el aula, pero sí lo es una actitud
positiva hacia las mismas.
* 4.2. El profesorado piensa que los profesionales de la Educación Física no
están preparados para introducir con garantías de éxito las NNTT.
* 4.3. El profesorado en activo cuenta con una formación básica o media en
TIC.
* 4.4. La oferta formativa de cursos relacionados con las TIC es insuficiente y
en su mayoría no son útiles para los profesionales de la Educación Física.
Hipótesis 5. En relación con el conocimiento y valoración de las posibilidades
educativas de las TIC en la Educación Física
* 5.1. El profesorado piensa que es compatible el uso de las TIC con la
Educación Física de Secundaria.
* 5.2. El profesorado piensa que su falta de formación en TIC y la naturaleza
motriz y procedimental del área son los principales problemas para el uso
didáctico de las TIC en el área.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>10.2. Conclusiones respecto a los objetivos de la investigación 
A partir del análisis de los resultados obtenidos a través de los diversos 
instrumentos de investigación y de los puntos fuertes y débiles vistos 
anteriormente, podemos extraer las siguientes conclusiones respecto a los 
objetivos que nos habíamos propuesto al inicio de esta investigación: 
10.2.1. Objetivos específicos 
* Podemos afirmar que la utilización de las TIC en la vida diaria del 
profesorado de EF se ha generalizado y que este colectivo docente está 
familiarizado con el uso de las NNTT. A este aspecto contribuye, 
favorablemente, el hecho de que gran parte de los docentes disponen de 
ordenador personal y de conexión a internet en su domicilio lo cual le 
permite el uso de las NNTT fuera del centro educativo para sus tareas 
personales y profesionales. Respecto al uso profesional de las TIC, la 
mayoría del profesorado de EF contempla la utilización de las nuevas 
herramientas, únicamente, como apoyo o complemento de la asignatura. 
Este empleo supone utilizar las TIC para tareas relacionadas con la 
organización y la gestión de la asignatura (diseño y preparación de 
clases, control de faltas, registro de notas y evaluación de los alumnos, 
etc.) y para la comunicación de los contenidos teóricos de las sesiones. 
Estos resultados podrían estar relacionados con la orientación de la 
formación digital del profesorado, más enfocada hacia el dominio 
instrumental y técnico de las NNTT que a la aplicación pedagógica de 
las nuevas herramientas. Este tipo de formación instrumental podría 
generar en los docentes sentimientos de inseguridad e incluso rechazo a 
la hora de aplicar las TIC con su alumnado debido a su escasez de 
competencias didácticas con las NNTT. En este sentido, hemos encontrado una relación entre la formación tecnológica del docente y el 
uso de las TIC en el centro escolar y, más concretamente, en 
actividades de enseñanza-aprendizaje, lo cual nos viene a apoyar en 
esta idea. Por otra parte, hay que destacar entre los factores 
significativos de la integración educativa de la tecnológica las actitudes 
que muestran los docentes de EF hacia las nuevas herramientas 
educativas. En relación a este aspecto, en los centros educativos donde 
la predisposición del docente a utilizar las TIC era más favorable había 
también un mayor empleo de los nuevos instrumentos fuera y dentro del 
aula. 
* La ocupación de los recursos tecnológicos en las actividades de 
enseñanza-aprendizaje del área es poco significativa y casi siempre está 
en función de las iniciativas personales o de factores meteorológicos 
(lluvia, viento, etc.). Por un lado, la falta de formación, y por otro, los 
temores que despiertan en los docentes el uso didáctico de las TIC, 
conducen a que la introducción tecnológica no sea contemplada en las 
programaciones didácticas de estos profesionales y su uso didáctico sea 
más bien producto de la confluencia de diversas circunstancias que 
nada tienen que ver con un proceso de programación y de reflexión 
sobre las posibilidades educativas de las TIC en la asignatura. 
* Al respecto, parecen surgir reticencias en el uso educativo de las TIC 
cuando los docentes afirman no sentirse suficientemente capacitados 
digitalmente. Recordemos que una gran mayoría del profesorado 
reconoció en el estudio no sentirse preparado para introducir estas 
nuevas herramientas en su labor docente con el alumnado. Esta 
autopercepción del profesorado también provoca que la integración de 
las NNTT en las clases no se vea acompañada del cambio en el rol del 
docente y éste no desempeñe las nuevas funciones que se demandan al docente del siglo XXI (guía y seleccionador de contenidos digitales de 
calidad). Parece ser que los cambios metodológicos relacionados con 
una nueva forma de enseñar y donde el alumno ha de adquirir un papel 
activo en la busqueda de la información, suelen requerir más tiempo ya 
que a los docentes les cuesta cambiar sus métodos y la forma en que 
imparten las clases. 
* Cuando el docente de EF decide emplear las TIC con su alumnado, las 
principales finalidades hacia las que se orienta este uso son la búsqueda 
en internet de información relacionada con la EF y la elaboración de 
trabajos teóricos de la asignatura. Ninguna de las actividades 
mencionadas se relacionan con la nueva metodología y el nuevo 
paradigma educativo que impulsan las NNTT: trabajo en equipo, 
intercambio y construcción colaborativa de conocimiento, orientación y 
guía del alumno hacia la información de calidad en Internet. En relación 
a este hecho, parece que a los docentes de EF les cuesta cambiar sus 
funciones docentes tradicionales y siguen siendo ellos los que 
transmiten, directamente, la información teórica y conceptual al alumno. 
En este sentido, no parece que los nuevos instrumentos se empleen 
para enseñar los contenidos del área de una forma diferente ni para 
potenciar el papel activo y crítico que debe desempeñar el estudiante 
ante la abundante información virtual de las redes informáticas. 
Podríamos concluir este apartado afirmando que el profesorado del área 
parece que aún no dispone de guías prácticas ni preparación suficiente 
para incorporar las NNTT desde una perspectiva de aprendizaje con el 
alumnado. 
* El profesorado de EF posee un nivel básico y medio en el manejo de las 
nuevas herramientas educativas. Esto supone, entre otras 
competencias, que este colectivo docente está capacitado para utilizar sin dificultad las diferentes aplicaciones ofimáticas (Office y OpenOffice) 
y los servicios básicos que le ofrece Internet (correo electrónico, 
búsqueda y consulta de información virtual). Sin embargo, este dato 
también nos indica una cierta dificultad del docente a la hora de integrar 
las NNTT en los procesos de enseñanza-aprendizaje con el alumnado 
pues carece de las competencias pedagógicas necesarias para el uso 
didáctico de las TIC con sus estudiantes. Parece que estos docentes, al 
no contar con el nivel de conocimiento suficiente que les permita utilizar 
los medios digitales con el alumnado, prefieren emplear las TIC, 
exclusivamente, para tareas relacionadas con la gestión y la 
organización de la asignatura. Además, una gran parte de los docentes 
del área desconocen todavía la utilidad educativa de recursos digitales 
básicos como las webquest, cazas del tesoro o ludos, aunque sí le 
resultan conocidos otros materiales tecnológicos como los blogs, la 
pizarra digital, el programa Clic, o la PDA. 
* Podemos afirmar que la formación tecnológica del profesorado de EF 
está, básicamente, orientada hacia aspectos técnicos e instrumentales 
(alfabetización digital) en el uso de las TIC. En este sentido, la oferta de 
cursos TIC dirigida a este colectivo docente está enfocada a 
proporcionarle las destrezas básicas para el uso de las nuevas 
herramientas ofimáticas, el empleo de internet y del correo electrónico, 
pero muy pocos se ocupan de la aplicación didáctica de los instrumentos 
tecnológicos en la asignatura de la EF. De este modo, la capacitación 
digital que reciben el docente del área no le permite adquirir las 
competencias necesarias para llevar a cabo la integración de los 
instrumentos tecnológicos en su práctica docente y tan sólo le facilita 
realizar tareas profesionales relacionadas con la gestión y organización 
de la asignatura. Esta escasa formación respecto al empleo de las NNTT 
en los procesos de enseñanza-aprendizaje, constituye uno de los elementos clave para entender las dificultades que se encuentra el 
docente a la hora de incorporar las TIC en la enseñanza de los 
contenidos de la EF de Secundaria. 
* Por otra parte, hemos detectado una escasa participación del 
profesorado en cursos relacionados con la formación en el uso de las 
NNTT. Los docentes señalaron como principales factores de su escasa 
participación en actividades de formación tecnológica la falta de tiempo y 
la poca aplicabilidad práctica de los conocimientos tecnológicos que se 
imparten en este tipo de cursos formativos. 
* Por último, se han encontrado pocas actividades virtuales basadas en el 
intercambio de materiales y de experiencias didácticas entre el 
profesorado del área, lo cual dificultaría su autoformación a través de los 
Entornos Virtuales de Aprendizaje (EVA), aspecto que constituye uno de 
los puntos fuertes que ofrece Internet para el desarrollo profesional del 
colectivo docente. En este sentido, parece que los docentes del área 
dedican poco tiempo en compartir sus ideas y experiencias didácticas a 
través de las redes virtuales. Esta situación provoca, además, que los 
profesores noveles no encuentren suficientes guías prácticas ni 
materiales digitales que le ayuden a dar sus primeros pasos en la 
docencia de la EF de Secundaria. 
* Podemos afirmar que este colectivo profesional posee, como ciudadano 
o ciudadana, una conciencia muy positiva hacia los efectos y las 
posibilidades educativas de las TIC. Los docentes de EF no se oponen a 
la integración educativa de las NNTT, más bien al contrario, ya que en 
general estos docentes muestran una visión muy positiva hacia las 
posibilidades pedagógicas de las TIC. Esta actitud positiva del 
profesorado tiene que ver con el convencimiento de la utilidad y la 
capacidad de innovación y de motivación que poseen las NNTT con el Estos profesionales reconocen y aceptan, mayoritariamente, el 
compromiso que deben asumir con las TIC y que la mejor forma de 
hacerlo es a través de su integración en el área como contenido 
transversal. Además, creen que con el diseño de las herramientas 
tecnológicas adecuadas sería compatible el uso de las NNTT con la 
enseñanza de la EF de Secundaria y se mejorarían los aprendizajes del 
alumnado. Sin embargo, se constató que esta predisposición positiva del 
docente de EF hacia el uso de las TIC no se traducía después en un 
mayor uso de las nuevas herramientas con el alumnado del área. Esto 
nos lleva de nuevo a pensar que estos docentes son partidarios de 
emplear las TIC tan sólo para tareas personales y para la gestión de la 
asignatura pues aún no posee las directrices ni las capacidades 
suficientes para llevar a cabo la integración tecnológica con los 
estudiantes. 
* Por último, podríamos pensar que la percepción por parte del docente de 
un alumnado mucho más capacitado que él en el manejo de las TIC le 
genera un sentimiento de inseguridad que le lleva, finalmente, a 
decidirse por no emplear las NNTT desde una perspectiva de 
aprendizaje con su alumnado. 
* Entre los principales factores que dificultan hoy en día la introducción de 
las TIC en la asignatura de la EF de Secundaria, podríamos citar el 
escaso equipamiento tecnológico de los Departamentos de EF, el 
carácter motriz y procedimental del área, el sedentarismo del alumno 
cuando hace uso de las TIC y, por último, una insuficiente formación 
tecnológica del profesorado de EF que le impide integrar las TIC en los 
procesos de enseñanza-aprendizaje del área. A estas limitaciones señaladas podríamos sumar el escaso tiempo lectivo que posee la 
materia de EF en el actual currículo de Secundaria, lo cual limita la 
utilización de las TIC en las sesiones de EF por el temor del profesorado 
a sustituir las horas de compromiso motor por el uso del ordenador. Este 
escaso tiempo de práctica física es un problema que preocupa a los 
 profesores y que les limita a la hora de decidir emplear las nuevas 
herramientas en las sesiones con el alumnado. 
* En cuanto al equipamiento informático, éste no suele estar bien ubicado 
para el docente de la Educación Física. Este colectivo profesional ve 
limitado el uso de las TIC debido a que su trabajo se desarrolla, 
normalmente, en el patio y en espacios abiertos donde se encuentra con 
múltiples limitaciones estructurales (falta de conexiones eléctricas y de 
acceso a internet alámbrico e inalámbrico-, reflejos en las pantallas de 
los dispositivos electrónicos, etc.). En la mayoría de los centros 
educativos la conectividad (puntos de acceso a la red Internet) no llega a 
los gimnasios lo cual dificulta al profesional docente la utilización de la 
red Internet desde su lugar de trabajo. Además, se suelen producir 
problemas de horarios para acceder al aula de informática a lo que se 
añade el tiempo necesario para desplazar al grupo de alumnos desde el 
gimnasio hasta otra aula diferente para utilizar el equipamiento 
tecnológico. 
* Otro aspecto a tener en cuenta es que la disponibilidad en Internet de 
materiales y de recursos dirigidos a la EF de Secundaria resulta algo 
escaso si lo comparamos con otras áreas del currículo. Además, no 
siempre los docentes del área conocen estos recursos y materiales 
digitales aplicables en su labor docente. También se echan en falta 
orientaciones y guías didácticas que ayuden al profesorado de EF a dar 
sus primeros pasos con las TIC educativas. 
* En relación a las necesidades de formación inicial del profesorado de 
EF, el nuevo plan de estudio de las titulaciones de grado y postgrado en 
Ciencias del Deporte habría de garantizar una formación básica en TIC a 
través de la incorporación de una asignatura relacionada con la 
aplicación educativa de las TIC, a ser posible de carácter obligatorio, y 
de la oferta de un Máster dirigido a la formación en tres ámbitos: 
plataformas de educación a distancia (eLearning), materiales educativos 
multimedia, y programas de innovación educativa basados en la 
utilización de las TIC. Este Máster en TIC podría ser desarrollado de 
modo interuniversitario y ofertado en distintos formatos o modalidades 
educativas (semipresencial y/o a distancia). 
* Pero además, la incorporación de las TIC en los estudios superiores del 
docente de EF no debería limitarse a un único curso o a una única 
asignatura de carrera, sino que las NNTT también habrían de integrarse 
de forma transversal en la metodología de las diversas materias 
universitarias. Para ello, sería recomendable que las diferentes 
asignaturas de los estudios de EF trabajaran conjunta e 
interdisciplinariamente las TIC y que los formadores universitarios 
incorporaran los nuevos medios en sus métodos didácticos, ya que esta 
integración tecnológica facilitaría al futuro profesor del área aplicar estas 
nuevas herramientas desde una perspectiva de aprendizaje con su 
alumnado. 
* Por otra parte, es imprescindible que esta formación en TIC, 
desarrollada en la universidad, esté conectada con las herramientas 
tecnológicas que, cotidianamente, emplean los docentes de EF en los 
IES. Este último objetivo se conseguiría si desde la universidad se 
adoptaran medidas paralelas a las diferentes estrategias que pone en 
funcionamiento la administración educativa en los centros de enseñanza 
Secundaria. Finalmente, también sería importante que se crearan redes 
de intercambio y de colaboración entre los estudiantes de EF de manera 
que éstos pudieran continuar este contacto una vez acabados sus 
estudios superiores. Otra medida dirigida a mejorar la formación inicial 
del profesorado de EF sería la creación de bancos virtuales de buenas 
prácticas TIC a cargo de docentes de EF que se encuentran en activo. 
Esta medida ayudaría a los profesores de EF noveles a dar sus primeros 
pasos con las TIC. 
* En cuanto a los requerimientos necesarios en los procesos de formación 
permanente, es imprescindible que el diseño de la oferta formativa en 
TIC esté dirigido a la aplicación didáctica de los conocimientos 
tecnológicos. Estos cursos habrían de priorizar la estrategia pedagógica 
por encima de la preparación intensiva o monográfica sobre un 
determinado recurso tecnológico de manera que ayudaran al 
profesorado a integrar las TIC dentro de su práctica docente. 
* Los cursos de formación TIC para docentes de EF habrían de ser 
impartidos no por profesores generalistas y expertos en TIC, sino por 
profesionales de la EF de Secundaria que hubieran utilizado con éxito 
las nuevas herramientas en sus clases. En este sentido, sería 
fundamental que este profesional formador, aparte de conocer y dominar 
las TIC, también conozca y trabaje en el ámbito docente de la EF de 
Secundaria. Asimismo, sería importante que los cursos de formación 
tecnológica presentaran, claramente, las ventajas educativas que ofrece 
el uso de las TIC en la docencia de la EF y lograr inculcar entre los 
asistentes un espíritu investigador que les llevara a formarse en TIC de 
una manera autónoma a través del uso de internet y de los nuevos EVA. 
* En este sentido, sería preciso disponer de un sistema de redes virtuales 
de aprendizaje (redes sociales, bancos de recursos, foros, blogs, wikis, 
repositorios de contenidos, portales, etc.), con cobertura nacional e 
internacional, que favoreciera el intercambio de experiencias y 
materiales didácticos, la formación entre iguales, la transmisión de 
buenas prácticas y el trabajo cooperativo del profesorado. De este modo, la organización de los profesores del área en comunidades 
virtuales de aprendizaje facilitaría la creación de una nueva cultura de 
formación permanente en red donde los espacios virtuales 2.0 se 
convertirían en un lugar de encuentro para los docentes del área y le 
permitirían construir nuevo conocimiento profesional de forma 
colaborativa. Estas acciones formativas, desarrolladas en las nuevas 
redes virtuales, posibilitarían la implicación activa de los docentes en su 
propio proceso formativo, evitando el sentimiento de soledad que surge 
durante el proceso de introducción de las TIC en la labor profesional. A 
través de esta vía formativa el docente podría contactar, fácilmente, con 
otros profesionales del área, agruparse con compañeros de profesión 
según sus intereses y motivaciones, y tener acceso a multitud de ideas y 
de materiales que, posteriormente, podría aplicar a su actividad docente. 
* Para un mejor aprovechamiento de los instrumentos tecnológicos en los 
centros educativos, sería fundamental que antes de dotar al centro 
educativo con los nuevos recursos, se realizara la formación del 
profesorado en la utilización de estos nuevos medios, y que esta 
formación se realizara dentro del horario lectivo del docente. Estos 
cursos formativos en TIC podrían ser ofertados a través de Planes de 
Formación en Centros, una modalidad formativa que podría mejorar la 
baja participación en actividades formativas en TIC detectada en el 
estudio y ayudar a incrementar la implicación del profesorado en su 
formación continua. Esta modalidad formativa haría participes del diseño 
de su capacitación tecnológica a los profesores del área de manera que 
éstos verían recogidas sus demandas formativas, orientadas a la 
búsqueda de soluciones concretas dentro del contexto de su centro 
educativo. En el caso de aquellos centros educativos que cuentan con 
un reducido número de profesores de EF, la demanda de formación 
tecnológica podría diseñarse a través de la coordinación del PFC del 
centro con el desarrollado en otros IES cercanos.
organizarse en grupos y seminarios de profesores que le permitiera 
investigar las nuevas posibilidades de las TIC en su área y compartir los 
nuevos materiales y usos didácticos de las NNTT descubiertos con la 
comunidad educativa. 
* Como hemos visto, el escaso tiempo lectivo de la materia de la EF de 
Secundaria y el espíritu motriz que caracteriza al área hacen difícil la 
introducción de las herramientas tecnológicas con el alumnado. Quizá el 
registro de datos del alumnado (asistencia, actitudes, procedimientos 
motrices, etc.) durante las sesiones de clase se vería facilitado, 
significativamente, si el profesorado dispusiera de la tecnología 
multitáctil adecuada para el trabajo de pie y en espacios abiertos (PDA, 
HTC, iPAD, Tablet-TC, etc.). Como viene siendo habitual, las NNTT se 
podrían emplear durante las sesiones para tareas relacionadas con el 
registro audiovisual de las ejecuciones motrices (vídeo y fotografía) y 
para la presentación de contenidos que faciliten la comprensión por 
parte del alumnado de los objetivos y actividades de las sesiones 
prácticas. Partiendo pues de la base de que el compromiso motor del 
alumnado es la esencia de la asignatura de EF, las TIC representarían 
una buena herramienta que permitiría al profesional de la EF disponer 
de espacios virtuales donde el alumnado podría trabajar los contenidos 
de la asignatura fuera del horario lectivo. Esta integración tecnológica se 
podría enfocar a través de la aplicación de la metodología blendedlearning (aprendizaje mixto), que posibilitaría complementar las clases 
presenciales de carácter obligatorio con la enseñanza virtual de los 
contenidos de la asignatura. En este sentido, el aprendizaje y la reflexión 
sobre los contenidos teóricos del área a través de las TIC, enfocado 
como trabajo extraescolar de la asignatura y realizado por el estudiante 
de forma individual o exploratoria en grupo, conduciría a que el docente 
del área no tuviera que emplear sesiones dirigidas, exclusivamente, a transmitir los contenidos teóricos del área. Así, el aprendizaje de la parte 
conceptual de la asignatura a través de trabajos de exploración 
desarrollados en la Red (webquest, cazas del tesoro, proyectos 
telemáticos, etc.), en los nuevos espacios virtuales 2.0 (blogs, wikis, 
foros, alojamiento de presentaciones y archivos, pagina web de la 
asignatura, etc.), o a través de las plataformas de manejo de contenidos 
(Moodle, BSCW, etc.), es una de las posibilidades que mejor aceptación 
despierta entre los docentes de la EF ya que esta opción les permitiría 
obtener más tiempo de compromiso motor con el alumnado. Además, 
las plataformas de formación virtual (Moddle, BSCW) y los programas 
informáticos de generación de actividades (Jclic, LIM, Hot Potatoes, etc.) 
permitirían al profesor evaluar conceptualmente al alumno por Internet 
pues estos recursos digitales ofrecen la opción de diseñar, fácilmente, 
cuestionarios y pruebas de examen referidas a los contenidos teóricos 
de la asignatura. 
* Por otra parte, la utilización del espacio virtual 2.0 (blogs, wikis, 
repositorios de contenidos, etc.) para impartir los contenidos teóricos de 
la asignatura y para visualizar los trabajos del alumnado supondría un 
ahorro en cuanto al coste de adquisición y de reproducción que supone 
el libro de texto de los trabajos entregados en formato papel. Además, el 
libro digital de EF en forma de apuntes virtuales alojados en Internet 
facilitaría que el alumnado tuviera acceso permanente a los contenidos 
teóricos de la asignatura, de manera que podría trabajar los conceptos 
del área desde cualquier lugar y en cualquier momento. 
* No obstante, como hemos señalado anteriormente, la simple 
incorporación de los nuevos instrumentos no garantiza una mejora en la 
enseñanza de la EF. Para que la innovación tecnológica en el área 
suponga una mejora educativa ésta ha de ir siempre acompañada de un 
cambio en el rol tradicional del docente (de transmisor a orientador del 
aprendizaje) y de un proceso de reflexión y de planificación de la 
programación didáctica del profesional del área. Una posibilidad a la 
hora de incorporar las TIC en los procesos de aprendizaje del área sería contemplar el uso de las nuevas herramientas de una forma transversal 
a todas las unidades didácticas recogidas en la programación didáctica. 
En este sentido, las diferentes unidades didácticas que contiene la 
programación podrían contener un recurso telemático (webquest, caza 
del tesoro, proyecto telemático colaborativo, actividad de intercambio de 
información con otro centro educativo, reflexiones de los alumnos en un 
foro o un blog acerca del trabajo desarrollado en las sesiones, etc.) 
sobre el que el alumnado del área podría trabajar, bien de forma 
individual o en grupo, fuera del centro educativo. Para que el estudiante 
fuera familiarizándose poco a poco con esta nueva forma de aprender 
los contenidos de la asignatura, el docente podría introducir, en los 
cursos iniciales de la ESO, un recurso telemático por trimestre e ir 
aumentando su presencia, progresivamente, en los últimos cursos de 
esta etapa educativa.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Carlos Ferreres Franco 
LA INTEGRACIÓN DE LAS TECNOLOGÍAS DE LA INFORMACIÓN 
Y DE LA COMUNICACIÓN EN EL ÁREA DE LA EDUCACIÓN FÍSICA 
DE SECUNDARIA: ANÁLISIS SOBRE EL USO, NIVEL DE 
CONOCIMIENTOS Y ACTITUDES HACIA LAS TIC Y DE SUS 
POSIBLES APLICACIONES EDUCATIVAS 
TESIS DOCTORAL 
Dirigida por el Dr. Saturnino Gimeno Martín 
Departamento de Pedagogía</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Construcción de "campus virtuales en Argentina"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Esta tesis parte de considerar que las tecnologías en general y las TIC en 
particular no son neutrales ni autónomas. 
En tal sentido, una de las premisas clave de las que se parte tiene que ver con 
la idea de que una pieza de tecnología adquiere su significado dentro de grupos 
sociales más amplios. Es decir, no se desarrolla bajo su propia lógica técnica 
inmanente. Las tecnologías adquieren significados en el mundo social y estos 
significados dan forma y constriñen su desarrollo. A menudo en las etapas iniciales de 
su producción, se conciben diferentes significados de una tecnología, algunos 
enfrentados entre sí.
La concepción "instrumentalista de la tecnología, es decir, aquella que 
considera que las tecnologías son simples herramientas o artefactos construidos para 
una diversidad de tareas, colabora a consolidar la percepción de la misma como algo 
"neutral. El problema mayor de ese planteo es que considera que la tecnología es 
independiente de cualquier sistema político o social y así cualquier tecnología puede 
ser transferida de un espacio social a otro sin mayores consecuencias. 
En esta tesis se considera que tales cosmovisiones son reduccionistas y 
esconden el potencial peligro que acarrea ignorar las redes de intereses sociales, 
económicos y políticos de aquellos que diseñan, desarrollan, financian y controlan la 
tecnología (González García, López Cerezo y Luján López, 2000).
El desarrollo de la informática agudizó la percepción de la centralidad de lo 
tecnológico en los procesos de cambio social. Al mismo tiempo, pareciera que tales 
procesos demandan, promueven o motivan cambios tecnológicos. La importancia de lo 
tecnológico ha alcanzado una relativa centralidad entre investigadores provenientes de 
tradiciones disciplinares diversas, no obstante, la pregunta por el "cómo se articulan tales cambios junto a otros político institucionales, organizacionales, económicos, 
cognitivos, simbólicos y territoriales, es crucial y aún permanece abierta. 
Si bien pareciera haber cada vez más esfuerzos por sistematizar teorías en 
torno a distintas problemáticas de las TIC, los términos, las formulaciones y hasta las 
descripciones se revelan como insuficientes frente a las dimensiones de un fenómeno 
que no termina de "estabilizarse. En efecto, apenas una configuración de sentido 
comienza a consolidarse se produce algún cambio en la tecnología que obliga a 
revisar lo que hasta hace poco se pensaba. Para Castells (2002) el "paradigma de la 
tecnología de la información no evoluciona hacia su cierre como sistema, sino hacia 
su apertura como una red multifacética. Es poderoso e imponente en su materialidad, 
pero adaptable y abierto en su desarrollo histórico. Sus cualidades decisivas son su 
carácter integrador, la complejidad, la interconexión y la generación de redes.
La incertidumbre surge en este escenario caracterizado por la complejidad de 
un entorno cada vez más abierto. Así, resulta central contar con un adecuado corpus 
teórico para el estudio específico de las TIC. Por supuesto, en esta finalidad hay 
desafíos complejos ya que por un lado, se debe tener en consideración la "flexibilidad 
que imponen los cambios tecnológicos y, por otro lado, debe ser posible enmarcar a 
los distintos campos del desarrollo de las TIC. En un escenario marcado por la 
inestabilidad y velocidad del cambio junto a la obsolescencia de teorías, es importante 
contribuir en la consolidación de conocimientos así como en la descripción de 
experiencias, a fin de ir constituyendo un corpus que permita aproximar a la 
comprensión de los fenómenos en su singularidad. En tal sentido, a medida que las 
TIC se "difunden en la vida cotidiana y se internalizan y "naturalizan determinados 
usos tecnológicos, se desconoce cuán complejo es el funcionamiento interno de la 
tecnología lo que contribuye a crear una "percepción mágica sobre las TIC. Esta idea 
"fantástica se refuerza con otra que se encuentra arraigada en algunos estudios 
orientados a analizar los "impactos o los "efectos que tienen las "nuevas tecnologías 
sobre la sociedad. Estas tesis "deterministas tienden a naturalizar la aparente 
neutralidad y autonomía de la tecnología.
En relación a la informática, se suele describir su desarrollo como un proceso 
autónomo que sigue una lógica propia al margen de cualquier dinámica social. Se 
piensa así que la tecnología sigue su propio curso al margen de la intervención 
humana. La tesis de la "tecnología autónoma patrocina una relación unidireccional 
entre tecnología y sociedad. Se considera que los desarrollos tecnológicos influyen 
significativamente en el orden social, mientras que la tecnología se muestra, por el 
contrario, impermeable a la influencia de factores sociales. La influencia de la 
tecnología en el ámbito social se produce, pues, desde el exterior.
Emparentadas con estas ideas se encuentran otras que sostienen que el 
cambio social está determinado por el cambio tecnológico. Estos supuestos 
consideran que la base técnica de una sociedad es la condición fundamental que 
afecta a todos los modos de existencia social y los cambios tecnológicos son la fuente 
más importante de cambios sociales.
En esta tesis, por el contrario, se sostiene que la tecnología es "moldeada 
socialmente. El problema radica en que los enfoques "tecnocráticos ignoran la 
complejidad de los procesos sociales. Por consiguiente, "abrir la caja negra de las TIC 
se vuelve crucial a fin de evitar recaer en el viejo dualismo de tecnología y sociedad. 
Para ello, analizar cómo las opciones sociales se cristalizan y encastran, es decir, se 
"instituyen dentro de la tecnología, es fundamental. El desafío, por tanto, deviene 
comprender cómo las tecnologías en sí mismas son socialmente construidas.
Describir los procesos de adopción y cambio de tecnologías a través de 
conceptualizaciones dinámicas que den lugar a un análisis en términos de "relaciones 
y "procesos requirió de un abordaje analítico conceptual que se generó mediante un 
procedimiento de triangulación teórica. En dicha triangulación se combinaron 
conceptos acuñados en distintos "campos disciplinares: los estudios sociales de la 
tecnología, los estudios sobre la educación superior y las universidades y los estudios 
territoriales. 
En relación a la educación superior y a la universidad en particular, en la 
denominada "sociedad de la información o "del conocimiento se modifica la misión 
institucional de las universidades y también se amplían sus alcances territoriales por 
medio de la generación de propuestas "virtuales de enseñanza que, en algunos 
casos, trascienden sus límites institucionales. En tal sentido, los "campus virtuales, 
dependiendo de las distintas tramas de significados que lo co construyen, acrecientan
las posibilidades "expansivas de la universidad ramificando, en algunos casos más 
que otros, el área de influencia geográfica de las instituciones universitarias. 
Así, en esta tesis se considera que un "campus virtual no es una entidad 
"desterritorializada sino que está física y "virtualmente situado en un soporte dado y 
sus elementos componentes circulan mediante redes. El contexto local / regional fue 
uno de los elementos que influyó en la localización de las universidades nacionales 
argentinas. Así, en algunos casos, se privilegió la relación con el medio socio 
productivo de la región de pertenencia de dichas instituciones o las relaciones 
establecidas con distintos organismos vinculados al contexto local donde se
encuentran ubicadas. De esta manera, la dimensión territorial es un elemento 
importante para el análisis del objeto de estudio de esta obra y que no siempre, como veremos en el capítulo uno, se ha incorporado al análisis de los procesos de 
construcción social de la tecnología. 
En Argentina en particular y en América Latina en general, un enfoque que 
combine distintas disciplinas se vuelve necesario para comprender las características 
de la tecnología local y las posibilidades de desarrollo que éstas pueden brindar a la 
región. Las condiciones en las que se producen y utilizan tecnologías en nuestros 
países presentan una dinámica muy distinta a la de los países centrales. Esta 
situación demanda un análisis que tenga en cuenta distintas dimensiones cognitivas, 
simbólicas, político institucionales, económicas, organizacionales, territoriales y 
"artefactuales y consecuentemente, genere un pensamiento propio para la región, 
adecuado a las condiciones sociotécnicas locales.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objeto de estudio de esta tesis son las tecnologías y en particular las 
Tecnologías de la Información y la Comunicación y los procesos de incorporación en 
las universidades argentinas. Así se seleccionaron como unidades de análisis los 
"campus virtuales desarrollados por las universidades nacionales del país y sus 
imbricaciones territoriales.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿de qué manera los procesos sociales influyen en el 
contenido mismo de la tecnología?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Qué características asumen los "campus virtuales de las universidades nacionales 
del país?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Cuándo surgen y con qué propósitos? ¿Qué actores intervienen en los 
momentos fundacionales?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Qué tipo de tecnología utilizan las universidades nacionales argentinas? ¿Cómo se 
selecciona la tecnología?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Cuál es el significado que los actores le atribuyen a la 
tecnología empleada?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Esta tesis está orientada por el siguiente juego de hipótesis de carácter 
general:
En primer lugar, se parte de considerar que la historia de la tecnología y en 
particular de las llamadas Tecnologías de la Información y la Comunicación, deriva 
de las sofisticadas "innovaciones desarrolladas en los países "centrales pero también 
de los distintos usos alternativos y apropiaciones creativas de todo tipo de técnicas y 
artefactos que tienen lugar en los países "periféricos.
Así, en segundo lugar, si se tiene en cuenta un contexto que, desde mediados 
del siglo XX, pareciera caracterizarse por una creciente valorización del conocimiento 
científico como insumo económico y productivo - a tal punto que algunos trabajos 
conceptualizan al conocimiento como una Ã¢âËœmateria prima„* o un Ã¢âËœnuevo factor de 
producción„* junto a los cambios derivados, entre otros, de los avances en la 
automatización, la microelectrónica, la informática y las biotecnologías al ámbito 
productivo - una vía de entrada para dar cuenta de éste fenómeno es preguntarse por 
los desarrollos tecnológicos, en especial los que atañen a las TIC y al software en particular, que se producen en universidades argentinas. No obstante, dar cuenta sólo 
de la creación de software que se produce no es suficiente sino se tienen en cuenta 
los usos alternativos y apropiaciones creativas que se realizan a uno existente. 
En tercer lugar, se parte de sostener que los procesos de incorporación de 
TIC que conducen a la construcción de "campus virtuales no se dan nunca de modo 
homogéneo y dependen en gran parte de la disponibilidad de recursos -financieros, 
tecnológicos, "humanos, simbólicos, cognitivos, entre otros-, las características de la 
institución, así como también de las distintas estrategias desplegadas por los actores 
involucrados.
Asimismo, en cuarto lugar, se sostiene que la construcción de "campus 
virtuales no es un acontecimiento aislado ya que refleja un estado determinado de 
conocimiento, una cierta disponibilidad de aptitudes para definir un problema técnico y 
resolverlo y una red de actores que además de desarrollar sus experiencias de forma 
acumulativa, aprenden al desarrollarlas. En tal sentido se sostiene que las tecnologías 
son "moldeadas socialmente. La interactividad inherente a las TIC es un rasgo que 
debe considerarse para comprender los procesos de su incorporación. La morfología 
de la red parece estar adaptada para una complejidad de interacción creciente y para 
pautas de desarrollo impredecibles que surgen del poder creativo de esa interacción.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Esta tesis espera realizar distintas contribuciones. En primer lugar, 
incorporando determinadas herramientas conceptuales desarrolladas en el campo de 
los estudios sociales de la ciencia y la tecnología que no han sido mayormente 
aplicadas al estudio particular de las Tecnologías de la Información y la Comunicación. 
Añadir aquellos conceptos presentes en el estudio de las tecnologías en general 
permitiría comprender lo particular de los procesos de incorporación de TIC y de la 
construcción de "campus virtuales en universidades del país. Junto con ello, 
proporcionar nuevos andamiajes teóricos vinculados con las características 
inherentes a las TIC. Este propósito se presenta como inédito ya que pocos trabajos 
en el ámbito de las ciencias sociales han analizado las dinámicas de desarrollo 
tecnológico que posibilitan la construcción de entornos "virtuales que se gestan en las 
universidades desde un enfoque que se encuadre en el propuesto por el campo de los 
estudios sociales de la ciencia y la tecnología. 
En ese sentido se aspira a establecer una ruptura con aquellas formas 
"cristalizadas de determinismo tecnológico que consideran que la creciente "difusión 
y proliferación de las Tecnologías de la Información y la Comunicación producen 
"efectos en la "sociedad y se limitan a una mirada meramente instrumental dejando 
de lado aquellas que hacen hincapié en la tecnología como construcción social, 
entendiendo que no hay una relación sociedad-tecnología, como si se tratara de dos 
entidades separadas y que la distinción misma entre social y técnico debe 
considerarse como el resultado de un proceso de "co-construcción.
Por otra parte, esta investigación espera contribuir al campo de los estudios 
de la educación superior y los estudios sobre universidad en el cual aparece 
como vacante la problemática vinculada a la educación universitaria "virtual. La 
utilización de la tecnología para la enseñanza y la investigación no es únicamente una 
cuestión técnica, puesto que replantea cuestiones fundamentales no sólo sobre los 
métodos de trabajo sino sobre las metas y los propósitos que persiguen las 
instituciones universitarias. Así, el estudio de los "campus virtuales de las 
universidades nacionales argentinas podría contribuir a replantear la dinámica de 
formulación de políticas y toma de decisiones al interior de la universidad. 
Por último se espera contribuir a los estudios que relacionan los procesos 
de "innovación tecnológica y los territorios indagando en la nueva dimensión 
espacio-temporal que generan las TIC y en las condiciones territoriales, económicas, 
sociales, organizacionales y simbólicas, entre otras, que inciden sobre la 
implementación de los "campus virtuales"</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad Nacional de Quilmes
Doctorado con mención en Ciencias Sociales y Humanas
Tesis de doctorado
TECNOLOGIAS DE INFORMACION Y COMUNICACION, 
UNIVERSIDAD Y TERRITORIO
Construcción de "campus virtuales en Argentina
Luciana Mónica GUIDO
Directora: Ester SCHIAVO
Co Directora: Elsa LAURELLI</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"ESTUDIO EN AULAS DE INNOVACIÓN PEDAGÓGICA Y  DESARROLLO DE CAPACIDADES TIC"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Actualmente nos encontramos en la denominada Sociedad Red, que de acuerdo a 
Castells (2000), es una sociedad que se generó de la revolución tecnológica de la 
información y el florecimiento de las redes sociales, creando una nueva estructura social 
dominante con una nueva economía informacional/global y una nueva cultura de la 
virtualidad/real. Este nuevo tipo de sociedad se inició aproximadamente en los años 60 
del siglo pasado, con la incorporación de una serie de nuevas tecnologías y medios que 
tuvieron como finalidad como señala MacLuhan (1964) de extender el cuerpo y la 
mente. La Sociedad Red ha traído consigo una serie de transformaciones tanto en los 
aspectos económicos, políticos, sociales, culturales, comunicacionales, tecnológicos, 
psicológicos y también educativos. Castells (2006), establece que estamos en los inicios 
de la Sociedad Red y que a futuro se avizora una serie de transformaciones en diversos 
campos incluidos en el educativo. 
Un aspecto importante de la Sociedad Red es que no todos los países se vienen 
incorporando de una manera homogénea, sino que existen grandes diferencias en 
diversos aspectos tal como refiere (Barbero, 2005, p.14) "la brecha digital en realidad es 
una brecha social. Mientras que en los países desarrollados cerca del 80% de hogares 
tienen acceso a Internet en los hogares de Lima de acuerdo al reporte del Instituto 
Nacional de Estadística e Informática del Perú (2008) el 26,4% disponen de una 
computadora y el 14,7% tienen instalado acceso a Internet en el hogar. 
Un fenómeno muy importante que se debe tomar en cuenta en el campo educativo, es 
que actualmente de acuerdo a APOYO (2008) el acceso a las nuevas TIC en este caso al 
Internet, especialmente por la generación joven entre los 12 y 17 años de edad es en el 
95% en forma permanente, la cual se da principalmente en las cabinas públicas de 
Internet. Aparte del acceso en las cabinas públicas, también se inició con el Proyecto 
Huascarán, el acceso a Internet en las instituciones educativas. Con esto se evidencia el 
mayor acceso de los jóvenes a la computadora y el Internet, incluso en las instituciones 
educativas. 
En este marco, en el Perú desde el año 2002 se viene implementando paulatinamente la 
integración de las TIC en el sistema educativo de la educación básica, habiéndose 
iniciado con el Proyecto Huascarán y desde el año 2007 a través de la Dirección de 
Tecnologías Educativas del Ministerio de Educación. La visión de la integración de las TIC en el sistema educativo peruano es crear entornos de aprendizaje con mejor calidad 
y mayores oportunidades educativas, en el marco de una política intercultural y 
bilingüe, mediante la generación de un proceso sostenido de la aplicación de tecnologías 
de información y comunicación en todos los niveles y procesos del sistema educativo. 
Los estudiantes que están inmersos en la integración de las TIC, estudian semanalmente 
en las Aulas de Innovación Pedagógica en promedio 4 horas pedagógicas, siendo al mes 
aproximadamente 16 horas y durante el año escolar 144 horas pedagógicas. 
Así, los estudiantes de educación básica vienen teniendo un acceso e interacción muy 
importante de forma regular y planificada a las nuevas TIC, es decir a la computadora e 
Internet. Esto implica que los estudiantes al estar en contacto con las nuevas TIC, 
vienen teniendo efectos tanto CON la tecnología y efectos DE la tecnología. El efecto 
CON la tecnología está referido a un mejor desempeño académico en el desarrollo de 
las asignaturas que cursan regularmente como son matemática, comunicación, ciencias 
sociales, entre otras y el efecto DE la tecnología comprende los residuos cognitivos que 
se van generando y que se concretizan en "nuevas capacidades tecnológicas, a las 
cuales las denominamos las capacidades TIC.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Determinar si la aplicación del estudio en las Aulas de Innovación Pedagógica mejora el 
desarrollo de capacidades TIC en los estudiantes de educación secundaria de una red 
educativa del distrito de San Juan de Lurigancho de Lima.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cómo influye el estudio en las Aulas de Innovación Pedagógica en el desarrollo de 
capacidades TIC en los estudiantes de educación secundaria de una red educativa 
del distrito de San Juan de Lurigancho de Lima?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La aplicación del estudio en las Aulas de Innovación Pedagógica mejora el desarrollo de 
capacidades en tecnologías de la información y comunicación en los estudiantes de 
educación secundaria de una red educativa del distrito de San Juan de Lurigancho de 
Lima.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Actualmente nos encontramos en un proceso de constantes cambios y transformaciones 
que obedecen a una serie de factores y entre ellas a la incorporación vertiginosa de 
nuevas tecnologías de la información y la comunicación en diversos campos, incluidos 
en el aspecto educativo. De acuerdo a la UNESCO (2005) los sistemas educativos de 
todo el mundo se enfrentan actualmente al desafío de utilizar las nuevas tecnologías de 
la información y la comunicación TIC para proveer a sus estudiantes con las 
herramientas y conocimientos necesarios para el siglo XXI. En el año 2005, el Informe 
Mundial sobre la Educación de la UNESCO, "El Imperativo de la Calidad, enfatizó en 
la importancia de los métodos de aprendizaje y en la utilización de materiales 
educativos, infraestructura y acceso a las TIC, como un importante desafío en el campo 
educativo. 
Las tecnologías de la información y la comunicación TIC son un factor de vital 
importancia en la transformación de diversos campos de la sociedad. En el campo 
educativo las TIC tienen el potencial de transformar la naturaleza de la educación en 
cuanto a dónde y cómo se produce el proceso de enseñanza aprendizaje, así como de 
introducir cambios en los roles de los profesores y los estudiantes, y en las diferentes 
acciones que se realiza en el proceso educativo, incluido en temas de gestión 
institucional. 
En este nuevo panorama se enfatiza la importancia de desarrollar nuevas competencias, 
capacidades, habilidades y uso de herramientas. De acuerdo a Valzacchi (2003) los estudiantes deben cultivar las siguientes destrezas que según los estándares de la 
International Society for Technology in Education son necesarios para desenvolverse 
en el siglo XXI. Estas son: manejarse con soltura en el empleo de la tecnología; 
comunicar información e ideas usando una gran variedad de medios y formatos; 
acceder, intercambiar, compilar, organizar, analizar y sintetizar información; saber 
encontrar información adicional; saber evaluar la información y sus fuentes; construir, 
producir y publicar modelos, contenidos y otros trabajos creativos; colaborar y cooperar 
en grupos de trabajo e interactuar con otros en forma apropiada y ética. 
En el país se inició el proceso de integración de las TIC en el sistema educativo público 
estatal a través del Proyecto Huascarán y actualmente a través de la Dirección General 
de Tecnologías Educativas del Ministerio de Educación, con la finalidad que las TIC de 
acuerdo al (Ministerio de Educación, 2007, p.60) "mejoren la calidad de la educación 
secundaria para que los estudiantes alcancen una formación integral que comprenda la 
consecución de logros de aprendizaje y una sólida formación en valores. 
Esta investigación también se justifica teniendo en consideración que de acuerdo a 
(Guiloff, 2007, p.11) "no existe suficiente información sobre el compromiso e 
interacción tecnológica relacionada con las actividades de aprendizaje formal de los 
estudiantes y de acuerdo a (Condie, 2007, p.75) "a la fecha hay muchos estudios de la 
relación entre TIC y la educación, sin embargo varían de acuerdo a las regiones donde 
se han realizado, con estudiantes de ciertas características y disciplinas; siendo 
necesario hacer investigaciones locales específicas. 
Asimismo este tema es de especial importancia y de actualidad, y de acuerdo a las 
diversas organizaciones, instituciones e investigadores que vienen trabajando sobre las 
TIC y la educación, han convocado a realizar investigaciones a nivel micro con la 
finalidad de contribuir a la generación de conocimiento científico. 
El trabajo también se justifica puesto que permitirá entregar a las autoridades que 
vienen implementando la integración de las TIC a través de la Dirección General de 
Tecnologías Educativas del Ministerio de Educación, resultados sobre el desarrollo de 
capacidades TIC en los estudiantes.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>1. Los estudiantes que participaron en la investigación tienen una media de edad de 15 
años, proceden de instituciones educativas públicas del distrito de San Juan de 
Lurigancho de la ciudad de Lima, el 46% es de sexo masculino y el 54% de sexo 
femenino, el 5% está repitiendo de grado, el 57% se dedica exclusivamente a los 
estudios y el 13% se dedica también a trabajar. El 75% de los estudiantes acceden 
principalmente a la computadora en una cabina pública, el 82% de los estudiantes 
usan principalmente el Internet en una cabina pública el 82%. Aprendieron a usar la 
computadora y el Internet principalmente a través de sus amigos y por ellos mismos. 
Las actividades que con mayor frecuencia hacen con Internet es comunicarse, jugar 
y buscar información. 
2. El estudio en las aulas de innovación pedagógica permitió un mayor desarrollo de la 
capacidad de adquisición de la información en el grupo experimental. De los 14 
indicadores se encontró diferencias estadísticamente significativas en 9 indicadores, 
así como a nivel global. Navegar por Internet en ambos grupos no muestra 
diferencias, sin embargo se encontró diferencias a favor del grupo experimental en 
el uso de la página web del Proyecto Huascarán, el ingreso a otras web educativas, 
realizar búsquedas avanzadas y en otros idiomas a través de varios buscadores. 
Asimismo distinguen la información científica de la información común, almacenan 
la información obtenida y elaboran documentos sobre sus tareas escolares con la 
información que obtienen. 
3. El estudio en las aulas de innovación pedagógica permitió un mayor desarrollo de la 
capacidad de trabajo en equipo en el grupo experimental. De los 14 indicadores se 
encontró diferencias estadísticamente significativas en 09 indicadores, así como a 
nivel global. En ambos grupos no se encontró diferencias en la posesión de una 
cuenta de correo electrónico ni en la posesión o uso del Chat. Sin embargo se 
encontró diferencias a favor del grupo experimental en lo referido a escribir y enviar 
correos electrónicos para comunicarse con sus compañeros, enviando archivos 
adjuntos y teniendo una lista de sus compañeros. Asimismo se encontró diferencias 
favorables en el uso del foro para fines educativos, la creación de un weblog y la 
publicación de sus productos en la enciclopedia virtual wikipedia y la participación 
en proyectos colaborativos escolares. 
4. El estudio en las aulas de innovación pedagógica permitió un mayor desarrollo de la 
capacidad de estrategias de aprendizaje en el grupo experimental. De los 14 
indicadores se encontró diferencias estadísticamente significativas en 09 
indicadores, así como a nivel global. Si bien es cierto que en ambos grupos no  existen diferencias en el uso de Word y Excel, sí se encontró diferencias en el uso 
del Power Point, los mapas conceptuales, los mapas mentales y las bases de datos. 
Asimismo se encontró que en el grupo experimental hay un mayor uso para bajar 
libros de las bibliotecas digitales, utilizar diccionarios electrónicos, hacer 
resúmenes, reelaborar textos y participar en proyectos colaborativos. En tal sentido 
el uso de las TIC tiene un alto impacto para el desarrollo de acciones netamente 
educativas. 
5. Los estudiantes que interactúan con las nuevas TIC, en este caso con las 
computadoras e Internet tienen como producto de esa interacción resultados de 
aprendizaje CON la tecnología y DE la tecnología. Aprenden CON la tecnología los 
cursos de la currícula escolar y aprenden DE la tecnología, ciertas capacidades 
tecnológicas como son la adquisición de información, el trabajo en equipo y la 
ejecución de estrategias de aprendizaje tecnológicas. 
6. Las tecnologías desde un enfoque tecnocrático son vistas como herramientas en el 
sentido más instrumentalista del término, desde un enfoque postecnocrático, la 
posibilidad de concebir que las tecnologías nos modifican cuando las utilizamos y 
de esta manera pensar en una concepción relacional dialéctica entre tecnologías y 
sujetos. Así las tecnologías son productos sociales que tienen como finalidad ser 
canales o rutas de transmisión del conocimiento, del pensamiento y de la cognición. 
La cognición no es un proceso aislado que se da solo en el cerebro de la persona, 
sino la cognición con las TIC es el "cerebro-más, es decir es el cerebro más la 
computadora y es el cerebro más el Internet. Visto así los medios tecnológicos son 
extensiones de nuestro sistema nervioso central. Las TIC entonces demandan una 
atención importante en el contexto actual, es decir en la Sociedad Red.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD NACIONAL MAYOR DE SAN MARCOS 
(Universidad del Perú, Decana de América) 
FACULTAD DE EDUCACIÓN 
UNIDAD DE POST GRADO 
"ESTUDIO EN AULAS DE INNOVACIÓN PEDAGÓGICA Y 
DESARROLLO DE CAPACIDADES TIC 
EL CASO DE UNA RED EDUCATIVA DE SAN JUAN DE 
LURIGANCHO DE LIMA 
Tesis presentada por: 
Mg. Raúl Choque Larrauri 
Para optar el Grado Académico de Doctor en Educación
Lima - Perú 
2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Diseño de un protocolo para votaciones electrónicas basado en firmas a ciegas definidas sobre emparejamientos bilineales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los avances en la tecnología y las técnicas criptográficas han hecho posible considerar a
las votaciones electrónicas como una alternativa factible para las elecciones tradicionales.
Los medios de comunicación como el Internet y los dispositivos electrónicos como las
computadoras personales, los teléfonos celulares, tarjetas inteligentes, etc., facilitan la
captura, transmisión y recepción del voto, permitiendo así un sistema de elecciones más
cómodo para los votantes y más eficiente para las autoridades electorales.
Sin embargo, la eficiencia y la comodidad que ofrecen los esquemas de votación
electrónica abren la puerta a distintos problemas de los ya existentes en las votaciones
tradicionales. Por ejemplo, la identificación del votante no es tan simple en las votaciones
electrónicas como lo es en las convencionales, si se considera que el votante debe ser
identificado antes de votar y a su vez su identidad debe ser anónima al momento de
emitir el voto.
Otro problema es la auditoria de la elección. En las votaciones tradicionales, el resguardo de las boletas electorales impresas ofrece una garantía de escrutinio. Si existieran
dudas o desconfianza en los resultados finales, bastaría con realizar un recuento de votos
para verificar la validez del resultado final. De manera electrónica, el voto es capturado y
transmitido hasta la urna electoral electrónica para ser contabilizado al final del periodo
de votación, sin alguna comprobación contundente que asegure al votante que su voto
fue correctamente recibido y contabilizado.
Lo que indican los problemas mencionados es que, considerando que Internet es un
canal inseguro de comunicación, son necesarios los esquemas de seguridad para votaciones electrónicas que permitan ofrecer la seguridad y la privacidad requeridas en todo
proceso electoral. Sin embargo, no siempre es obvia la forma de alcanzar tales características a un precio razonable, debido al hecho de que cuando un proceso electoral se
lleva a cabo, los mecanismos que aseguran tanto la seguridad como la privacidad pueden
ser demasiado costosos para los administradores (entidades electorales) por un lado, e inconvenientes para los usuarios (votantes) por el otro. En este sentido, un sistema de
votación electrónica no puede ser completamente automatizado. No obstante, mediante
el uso de herramientas criptográficas, es posible satisfacer los requisitos de seguridad
más importantes asociados a los esquemas de votación electrónica.
En esta tesis estamos interesados en los sistemas de votación electrónica los cuales
utilizan Internet para transmitir los votos. Informalmente, este tipo de sistemas se puede
definir como sigue.
Definición 1.1.1. Un sistema electrónico de votación por Internet es un sistema de
elección que genera boletas electorales electrónicas, que permite a los votantes emitir
su voto desde un dispositivo electrónico y transmitirlo por Internet hacia la urna electoral también electrónica donde será depositado y contabilizado al término de la jornada
electoral.
De manera general, un sistema de votación electrónica por Internet debe cubrir todos los
requisitos funcionales del proceso electoral, así como los servicios de seguridad necesarios
para protegerse de potenciales ataques provenientes de la red. Algunos de los requisitos
esenciales son los siguientes.
. Autenticación: sólo los votantes incluidos en el padrón electoral serán autorizados
para emitir su voto.
. Anonimato y no coerción: nadie debe ser capaz de determinar el valor del voto ni
de vincularlo con el votante.
. Unicidad: ningún votante debe votar más de una sola vez.
. Integridad: los votos no pueden ser modificados, olvidados o borrados sin ser detectado.
. Verificación y auditoría: debe ser posible verificar que al final del proceso electoral,
todos los votos fueron contados correctamente, demostrando así la honorabilidad
del sistema.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>desarrollar un esquema seguro de votación electrónica el cual satisface los principales requerimientos de seguridad, utilizando como
principal herramienta criptográfica una firma a ciegas basada en emparejamientos bilineales.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para alcanzar el objetivo de esta tesis, se analiza el modelo de capas de los esquemas
de votación electrónica basados en firmas a ciegas y para nuestro caso en particular
utilizando emparejamientos bilineales.
Como se muestra en la Figura 1.1, de abajo hacia arriba, en la primera capa se
tiene la aritmética de campos finitos, que incluye las operaciones básicas como la suma,
multiplicación, inversión y la exponenciación modular.
4La capa siguiente corresponde a la curvas elípticas, donde las operaciones de suma,
doblado y bisección de puntos son requeridas para el cáculo de la multiplicación escalar,
que para este trabajo es la operación principal en esta capa.
La capa tres corresponde a los emparejamientos bilineales, donde se encuentran las
funciones de emparejamiento de Weil y Tate, y versiones posteriores de éste último tales
como ate, R-ate y ate óptimo entre otros.
La capa de firmas contiene tanto las firmas digitales como las firmas a ciegas, ambas
utilizan la función de emparejamiento principalmente en la fase de verificación y la
multiplicación escalar en las distintas fases para la producción de la firma.
Por último se tiene la capa de los esquemas de votación electrónica, los cuales apoyan su seguridad en la buena elección de los esquemas de firma y consiguen una implementación eficiente de acuerdo a los campos definidos para establecer la aritmética en
las capas anteriores. Cabe mencionar, que antes de este trabajo, esquemas de votación
electrónica basados en emparejamientos bilineales no habían sido propuestos todavía.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Previo a esta tesis, los esquemas de seguridad para votaciones electrónicas basados en
firmas a ciegas utilizando emparejamientos bilineales no habían sido explorados todavía.
Por tanto, nuestra contribución principal es un esquema de seguridad novedoso, que
gracias a que los emparejamientos bilineales fue posible ofrecer una eficiencia significativa
en comparación con tres esquemas de votación electrónica propuestos recientemente.
La combinación de la firma a ciegas de Boldyreva y la firma corta de Boneh, ambas
basadas en emparejamientos bilineales, permitió cumplir con los requerimientos principales de los esquemas de votación electrónica como son autenticación, unicidad, anonimato, no coerción, integridad, verificación y auditoria, colocando a nuestra propuesta
como un esquema seguro de votación electrónica.
El uso de la firma a ciegas y la creación de llaves temporales como un seudónimo
permitió satisfacer el anonimato del votante y evitar la coerción de votos. Por su parte,
con la firma corta fue posible proteger la integridad del voto y autenticar al votante.
El análisis de seguridad y de eficiencia realizado para varios esquemas tanto de firma
digital como de firma a ciegas, permitió identificar las ventajas y desventajas de cada
uno.
Considerando la eficiencia de la firma ECDSA, se propuso un esquema de votación
electrónica que combina la criptografía sobre curvas elípticas y la criptografía basada en emparejamientos. La propuesta es una versión mejorada del esquema de Mu y
Varadharajan el cual pretendía detectar al votante tramposo. Sin embargo, el análisis de
seguridad del esquema propuesto permitió distinguir que la llave de sesión generada por
la firma digital abría la posibilidad de detectar al votante tramposo y al mismo tiempo
de permitir el doble voto y la pérdida del anonimato del votante. Concluyendo que, a pesar de que la firma ECDSA y las firmas a ciegas definidas sobre curvas elípticas son
las mejores opciones en eficiencia para utilizarse en un esquema de votación electrónica, el principal inconveniente es el uso de la llave de sesión que como se explica en el
Capítulo 5, abre la posibilidad de crear boletas falsas, en su aplicación a las votaciones
electrónicas.
Considerando tres esquemas de votación electrónica propuestos recientemente en la
literatura, realizamos una comparación general, de seguridad y eficiencia entre éstos esquemas y nuestra propuesta. Nuestro esquema realiza un número mínimo de operaciones
criptográficas en un número menor de interacciones para la generación y verificación de
la boleta y la contabilización de los votos que los demás esquemas.
Respecto a la seguridad de nuestra propuesta, dado que los esquemas de firma fueron
definidos sobre grupos aditivos, es decir utilizando puntos sobre una curva elíptica, fue
suficiente con definir la curva elíptica sobre un campo finito Fp con jpj = 254 bits, para
ofrecer un nivel de seguridad de 128 bits en los esquemas de firmas y producir con eso
una boleta electrónica con una longitud en bits mucho más pequeña que el resto de los
esquemas.
Además, obtuvimos una ventaja significativa en la eficiencia, debido a la implementación de la aritmética en curvas elípticas y el uso de la función de emparejamiento
más eficiente propuesta en la literatura. Con lo que, nuestro esquema de seguridad para
votaciones electrónicas ofrece una opción segura, eficaz y eficiente para ser implementado
en un sistema electrónico de votación.
Finalmente, muchos esquemas de votación electrónica han sido publicados en la literatura, no obstante, nuestra propuesta es el primer esquema que desarrolla un protocolo
entre las entidades electorales y el votante totalmente basado en emparejamientos bilineales a través de la firma a ciegas de Boldyreva [11] y de la firma corta de Boneh
et al. [12], permitiendo el paso de mensajes de longitud de aproximadamente 1500 bits,
ofreciendo un nivel de seguridad de 128 bits con una longitud para la llave privada de
254 bits y obteniendo una eficiencia mucho mayor a comparación de los demás esquemas
basados en firmas a ciegas que no utilizan emparejamientos bilineales.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Centro de Investigación y de Estudios Avanzados
del Instituto Politécnico Nacional
Unidad Zacatenco
Departamento de Computación
Diseño de un protocolo para votaciones electrónicas
basado en firmas a ciegas definidas sobre
emparejamientos bilineales
Tesis que presenta
M. en C. María de Lourdes López García
para obtener el Grado de
Doctora en Ciencias en Computación
Director de la Tesis
Dr. Francisco José Rambó Rodríguez Henríquez
México, D.F. Junio 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Algoritmo de checkpointing con base en ordenes parciales a nivel conjunto de eventos para sistemas heterogeneos</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de nuestro trabajo de investigación es:
Desarrollar un algoritmo de checkpointing eficiente para sistemas heterogéneos con mo-
delos de ejecución síncrono y asíncrono.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cómo determinar de manera eficiente conjuntos de checkpoints que formen snapshots
globales consistentes dentro de sistemas heterogéneos con modelos de ejecución síncrono
y asíncrono?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Cómo dos procesos, p y q, pueden determinar si su iésimo y j-ésimo checkpoint, respectivamente, cumple con los principios de snapshot global consistente, no importando
el modelo de ejecución (síncrono o asíncrono) al que pertenezca el proceso?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Cómo construir conjuntos de eventos que cumplan con los principios de un snapshot
global consistente?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Cómo disminuir el cantidad de checkpoints por medio de ordenes parciales a nivel
conjunto de eventos?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Cómo diseñar un algoritmo de checkpointing sin bloquear el cómputo de los procesos?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Siguientes pasos a seguir:
1. Análisis de algoritmos o mecanismos que aseguren el orden causal.
En este punto analizaremos los trabajos que aseguran mantener un orden causal como:
relojes lógicos ([13]) y vectores lógicos ([7] y [19]), dependencia causal inmediata ([22]),
etcétera.
2. Definición de una representación causal del sistema heterogéneo.
3. Diseño de un mecanismo para la representación compacta del historial causal del sis-
tema.
a. Definición de reglas para la agrupación de eventos en conjuntos.
b. Diseño de un protocolo en línea que segmente a los eventos de ejecuciones del sis-
tema heterogéneo en conjuntos de eventos casualmente consistentes. Considerando
los modelos de ejecución:
i. Asíncrono
ii. Síncrono
iii. Heterogéneo
4. Diseño de mecanismos para establecer conjuntos de eventos que formen snapshots glo-
bales y consistentes del sistema heterogéneo.
a. Definición de un estructura de control para el manejo de la información de check-
points entre procesos.
b. Análisis en línea del historial causal del sistema para inferir puntos de inserción
de checkpoints. Considerando los modelos de ejecución:
i. Asíncrono
ii. Síncrono
iii. Heterogéneo
5. Verificación formal del algoritmo de checkpointing desarrollado para asegurar su co-
rrecto funcionamiento.
Correctness
i. Safety
ii. Liveness
6. Simulación del algoritmo de checkpointing y análisis del comportamiento de los pa-
rámetros:
Número de checkpoints creados.
Overhead generado.
Cantidad de checkpoints útiles para generar snapshots globales consistentes.
Para la simulación utilizaremos el simulador de checkpointing ChkSim [32]. El simula-
dor ChkSim es implementado en Java y puede ejecutarse en cualquier plataforma con
una maquina virtual de Java; esta herramienta utiliza un modelo determinista para
garantizar la reproducir de cualquier simulación. Además, pueden realizarse compara-
ciones y análisis entre algoritmos de checkpointing (coordinado y semi-coordinado).
En la figura 11 podemos observar un diagrama de la metodología planteada anterior-
mente. Las líneas marcan la secuencia entre los principales pasos a seguir. Hay que notar en
esta figura que en algún momento podemos retomar algún paso anterior.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Presentamos una propuesta de investigación para desarrollar un algoritmo de check-
pointing para sistemas heterogéneos que corren sobre los modelos de ejecución síncrono y
asíncrono. Las recientes tendencias en la tecnología, en particular, de los procesadores multi-
núcleo y de sus posibles repercusiones en los sistemas heterogéneos y cómputo concurrente,
han motivado el desarrollo de esta investigación.
El problema de checkpointing es muy amplio, por lo que en nuestra investigación sólo
toma tres ejes principales: 1) construcción de snapshots globales y consistentes en sistemas
heterogéneos con modelos de ejecución síncrono y asíncrono; 2) diseño de algoritmos eficientes
de checkpointing; y 3) la eficiente creación del número de checkpoints por cada proceso.
La solución que proponemos para nuestro algoritmo de checkpointing se basa en establecer
un orden parcial a nivel subconjuntos de eventos, de tal forma, que la representación causal de
una ejecución del sistema heterogéneo, pueda ser capturada de manera eficiente y compacta;
y para ello, se requiere establecer las condiciones necesarias para formar snapshots globales
consistentes del sistema.
Los avances presentados hasta el momento, corresponden a establecer una representación
eficiente y compacta de lo que corresponde a una ejecución asíncrona del sistema heterogéneo.
El protocolo SEEA (Segmentación de Eventos para Ejecuciones Asíncronas), presentado en
la sección 7, segmenta los eventos de una ejecución asíncrona y los agrupa en conjuntos de
eventos causalmente relacionados. La representación compacta a nivel conjunto de eventos
es consistente con el orden causal global del sistema.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Algoritmo de checkpointing con
base en ordenes parciales a nivel
conjunto de eventos para sistemas
heterogeneos '
por
M. en C. Alberto Calixto Simon'
Propuesta de tesis doctoral
Instituto Nacional de Astrofísica, Optica y '
Electronica '
Diciembre 2010
Tonantzintla, Puebla
Supervisada por:
Dr. Saul Eduardo Pomares Hernández, INAOE '

c INAOE 2010</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Obtención y evaluación de regiones de transición usando imágenes satelitales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Las imágenes adquiridas remotamente son empleadas para la construcción de mapas 
temáticos. Estos mapas pueden ser generados a través de un proceso de análisis de 
imágenes. Normalmente, este proceso incluye etapas como segmentación, extracción de 
las características de las regiones de interés y la etapa de clasificación. La clasificación de 
imágenes sensadas remotamente se refiere al reconocimiento de las coberturas de cada 
tipo de suelo presentes en la superficie terrestre.
En el contexto de las imágenes sensadas remotamente, las coberturas presentes en la 
superficie terrestre incluyen zonas urbanas, zonas de vegetación (bosque, pastizales, 
manglares, etc.), cuerpos de agua (mares, ríos, lagos, etc.), y zonas dedicadas al cultivo de 
plantas, entre otras coberturas.
La extracción de mapas temáticos a partir de la clasificación de imágenes sensadas 
remotamente es un área de investigación importante por la aplicación que éstos tienen 
para la obtención de conocimiento del medio ambiente global, usos de suelo, recursos 
renovables y no renovables, desastres naturales, geología y muchos otros. Por lo anterior, 
se trabaja en la obtención mapas de alta calidad, lo cual no se ha logrado del todo con los 
esquemas de clasificación utilizados hasta el momento.
Históricamente, los ecologistas han estudiado regiones homogéneas para caracterizar y 
entender los procesos de los ecosistemas y han evitado las áreas heterogéneas entre 
ecosistemas. Como resultado, las zonas de transición han sido frecuentemente ignoradas 
o reducidas a líneas sobre un mapa temático. Sin embargo, estas zonas de transición, 
llamadas "ecotonos, controlan el flujo de materiales entre ecosistemas y la influencia de 
la biodiversidad [Fortin, 2000].
En la generación de estos mapas temáticos, la mejor calidad en la clasificación se presenta 
cuando se clasifican las coberturas que son básicamente homogéneas. Una cobertura 
homogénea se considera aquella que está constituida por una sola cobertura. El paso de 
una cobertura a otra, por ejemplo, al pasar de bosque a suelo descubierto, no se realiza 
de manera abrupta sino de una forma gradual, generándose una zona de transición 
formada por las coberturas homogéneas (en este caso, por el bosque y el suelo 
descubierto), ver figura 3.1. Esta zona de transición genera límites indeterminados y se 
encuentra compuesta de manera proporcional por las coberturas que la delimitan. Debido 
a esto determinar esta composición no resulta trivial para los esquemas usados.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar un método para la obtención y evaluación de las regiones de transición en 
imágenes sensadas remotamente y determinar la composición de las regiones de 
transición con respecto a las coberturas homogéneas.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cómo se pueden determinar los límites de las coberturas homogéneas presentes en 
imágenes sensadas remotamente y de esta manera delimitar las regiones de transición 
generadas por dichas coberturas?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Cómo evaluar las regiones de transición presentes en imágenes sensadas remotamente 
para determinar su composición a nivel píxel con respecto a cada cobertura homogénea
que la genera?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Hasta el momento se ha logrado determinar las regiones de transición presentes 
en una imagen satelital con solo dos coberturas, pero es posible obtener estas 
regiones de transición para más de dos coberturas siguiendo el mismo análisis.
 En la tarea de clasificación los errores son cometidos precisamente en la aparición
de los píxeles mixtos contenidos en una región de transición. Por lo que al 
determinar estas regiones de transición a través de la segmentación, es posible 
reducir parte de estos errores.
 La combinación de información a nivel píxel y a nivel región nos proporciona mayor 
grado de confianza para la toma de decisiones para la clasificación de regiones de 
transición. Esto ha sido utilizado en la segmentación de regiones usando las 
medidas de similitud basadas en umbralización y en los patrones LBPs.
 La medida de evaluación difusa ha generado resultados alentadores para la 
evaluación de regiones de transición, pero se buscará mejorarlo para que se pueda 
obtener una mejor precisión en la clasificación.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Propuesta de Tesis Doctoral
"Obtención y evaluación de regiones de transición usando 
imágenes satelitales
Jorge Morales Cruz
Asesores
Dr. Carlos A. Reyes García
Dr. Jesús A. González Bernal
Coordinación de Ciencias Computacionales
Instituto Nacional de Astrofísica Óptica y Electrónica
Luis Enrique Erro No. 1, Tonantzintla, Puebla, México
Mayo 2010</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Codificación fonética de sub-palabras para la Recuperación de Información en Documentos Orales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Gracias a los avances en el reconocimiento automático de habla es posible realizar 
la búsqueda de información en grandes colecciones orales. Por supuesto, la calidad de las 
transcripciones y, por ende, el éxito de la recuperación está fuertemente ligado al rendimiento del reconocedor de habla. Por el momento, para el inglés (la lengua más investigada 
y con el mayor número de herramientas) es posible obtener transcripciones casi correctas 
siempre y cuando se trate de elocuciones limpias y bien formadas. Por ejemplo, una computadora puede realizar la transcripción de un lector del Wall Street Journal con tan sólo un 
5% de error a nivel de palabra (WER, por sus siglas en inglés word error rate). Sin embargo, el WER empieza a elevarse en cuanto las condiciones son más cercanas a una conversación real. En el caso de una conversación en una transmisión radiofónica podemos tener del 
15% al 20% de error. Y en el caso de una conversación telefónica entre 30% y 60% de 
error. Estos datos nos dan una idea del reto en la recuperación de información en grabaciones de conversaciones espontáneas. 
El origen de los errores en una transcripción de habla es de muy variada naturaleza. 
A continuación se presentan brevemente los principales motivos que impactan en el desempeño de un reconocedor actual para posteriormente discutir su impacto en la RI. 
* Las palabras fuera del vocabulario. Un RAH propone una transcripción ortográfica del 
audio apoyándose en un diccionario de palabras a reconocer. Al no existir la entrada correcta en este diccionario de pronunciaciones, el reconocedor intentará aproximar el 
habla a las palabras contenidas dentro del diccionario. Así la transcripción de una palabra fuera del diccionario (OOV, por sus siglas en inglés out-of-vocabulary) será substituida por una palabra fonéticamente similar, o peor aún, por un grupo de palabras cuya 
pronunciación conjunta sea fonéticamente similar. Diversos trabajos han abordado especialmente el problema de palabras fuera del vocabulario como (Olson y Oard 2009, 
Olsson 2008). Los experimentos de (Witbrock y Hauptmann 1997) mostraron una caída 
del 31% en la efectividad de la recuperación debida a las palabras fuera del vocabulario. 
Cabe remarcar que las palabras fuera del vocabulario es un problema que siempre está 
presente en un RAH. Ya que prácticamente es imposible contar con un diccionario 
completo debido a la constante creación de nuevas palabras en el lenguaje. La creación de nuevas palabras es fácilmente observada cuando se trata de nombres propios tales 
como nombres de empresas, personas, lugares, organizaciones, etc. 
* La delimitación de las palabras. El lenguaje hablado no marca la separación entre palabras, de esta manera, una elocución se observa como una secuencia única de fonemas. 
Un RAH debe identificar correctamente las palabras pronunciadas a partir de dicha secuencia. Para ello el reconocedor elige las palabras a través de métodos probabilísticos, 
lo que puede provocar una transcripción incorrecta. El modelo probabilístico más 
comúnmente usado asigna probabilidades a secuencias de palabras, donde la probabilidad de una palabra viene condicionada por las palabras precedentes. De esta manera se 
incorpora información lingüística al proceso de decodificación mejorando las transcripciones propuestas. Desafortunadamente, un modelo de lenguaje genera probabilidades 
altas para secuencias de palabras comunes (tales como preposiciones, artículos, etc.). 
Estas partículas fácilmente podemos encontrarlas como parte de palabras más grandes. 
Por ejemplo, la secuencia de fonemas que dan origen a la palabra "elocuente, también 
puede interpretarse como las palabras "él lo cuente, y donde dada la alta probabilidad 
de las palabras frecuentes "el y "lo el reconocedor se inclinará por esta última opción * Fenómenos del habla espontánea. Finalmente, existen otro tipo de errores generados 
por el tipo de grabaciones a tratar, es diferente tener grabaciones cuyo contenido son 
noticias, las cuales mantienen una temática, buena entonación y repetición de los acontecimientos, que tener grabaciones cuyo contenido son entrevistas donde se tienen diversos fenómenos lingüísticos propios del habla espontánea (véase la tabla 1). Estos 
fenómenos pasan inadvertidos para una persona, pero son fuente de error para un RAH. 
Cabe recordar que un reconocedor intentará llevar a texto cualquier sonido presente en  la grabación, de esta forma una risa será transformada a una palabra (o palabras) alterando el contenido de la transcripción.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un método para la recuperación de información en documentos orales a 
través de una representación basada en unidades sub-palabra codificadas fonéticamente para abordar los errores de substitución y delimitación generados por el  RAH.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. Analizar el impacto de los errores del RAH en la RI de documentos orales. Para ello se 
ampliará la plataforma de experimentación existente, la cual servirá como base para el 
indexado y recuperación de los documentos usando códigos fonéticos. De igual forma 
esta plataforma será utilizada para la evaluación de los pasos subsecuentes de esta metodología. Para el análisis se realizará lo siguiente: 
* Conseguir un corpus con transcripciones de diferentes calidades (WER). En específico se tiene el corpus MALACH de entrevistas usado en el foro CLEF y se 
está en proceso de conseguir el corpus STD (Can, et al. 2009) usado en el foro 
TREC para recuperación en base a términos de documentos orales. 
* Implementar un sistema de RI a nivel palabra utilizando sólo transcripciones automáticas. 
* Evaluar el impacto que tiene la calidad de la transcripción en la recuperación de 
información. 
2. Evaluar el aporte de la codificación fonética a nivel palabra en la recuperación de información en documentos orales. 
* Aplicar los algoritmos de codificación fonética a las transcripciones automáticas 
y observar su desempeño. Se utilizarán los algoritmos: Soundex, DaitchMokotoff Soundex, Phonix, NYSIIS y Double-Metaphone. Estos algoritmos fueron seleccionados a partir de una revisión bibliográfica (Odell y Russel 1918, 
Holmes y McCabe 2002, Zobel y Dart 1996, Raghavan y Allan 2004, Kessler 
2005, Borgman y Siegfried 1992). 
* Medir la complementariedad y redundancia de los resultados obtenidos con las 
diferentes representaciones basadas en códigos fonéticos como en texto. 
* Analizar la relación entre frecuencia de códigos y la tasa de falsas alarmas. Con 
la intención de proponer una estrategia que limite las colisiones. 
* Combinar las representaciones textuales y de códigos fonéticos. Se espera aprovechar la complementariedad de los resultados de las diferentes representaciones para mejorar el rendimiento de la recuperación. Se experimentará con las siguientes estrategias: (i) expansión de documentos (fusión temprana), (ii) índices 
por separado para texto y códigos fonéticos, y donde la combinación se realizará 
ya sea en cascada (fusión intermedia), o utilizando técnicas conocidas de fusión 
de listas de resultados (fusión tardía). 
3. Representar los documentos orales con unidades sub-palabra codificadas fonéticamente 
y evaluar su desempeño en la recuperación de información. 
* Determinar empíricamente el aporte de las unidades sub-palabra codificadas 
fonéticamente. Bajo este estudio se realizarán experimentos para determinar: (i) 
el tamaño más adecuado de las unidades sub-palabra; y (ii) la conveniencia de 
utilizar unidades traslapadas. 
* Evaluar el comportamiento de las diferentes codificaciones fonéticas con las 
unidades sub-palabra. 
* Analizar la complementariedad de los resultados a diferentes tamaños (o traslapes). 
* Analizar la complementariedad con los obtenidos a nivel palabra (tanto al usar 
códigos como texto). 
* Determinar si la combinación/fusión de las representaciones textuales y códigos 
fonéticos a nivel sub-palabra es pertinente para disminuir las falsas alarmas. Al 
igual que el paso anterior se experimentarán las siguientes estrategias: 
i. Expansión de documentos (fusión temprana). En este caso se evaluarán 
diferentes expansiones usando códigos a nivel palabra y nivel subpalabra. 
ii. Construyendo índices por separado para texto y códigos fonéticos. En 
este último caso se experimentará con índices tanto a nivel palabra como 
sub-palabra. Con ellos se podrán evaluar esquemas de combinación ya 
sea en cascada (fusión intermedia), o utilizando técnicas conocidas de 
fusión de listas de resultados (fusión tardía).
Las contribuciones de este trabajo se orientan de manera general al área de recuperación de 
información en documentos orales. Las aportaciones se pueden resumir en los siguientes 
puntos: 
* Un estudio empírico para determinar el algoritmo de codificación fonética más 
apropiado para la representación de documentos orales en la tarea de RI. 
* El desarrollo de un método para la recuperación de información en documentos orales que aborde los errores de substitución y delimitación provocados por el reconocedor automático de habla. 
* El desarrollo de una estrategia de recuperación que combine diferentes representaciones de los documentos orales para disminuir la tasa de falsas alarmas.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo, se estudió la factibilidad de construir un método para recuperar información en documentos orales utilizando la codificación fonética. Gracias a esta codificación es posible mejorar el recuerdo al representar aquellas palabras con pronunciación 
similar a través del mismo código fonético. Esto ocurrió con casi todas las codificaciones 
fonéticas evaluadas, sin embargo, cabe resaltar el caso de NYSIIS. Con esta última codificación se obtuvo un MAP ligeramente superior que al usar únicamente texto. 
Con los experimentos realizados hasta el momento, se observó que la codificación 
fonética es complementaria no sólo al usar texto sino también entre las diferentes codificaciones fonéticas. De ahí la importancia de determinar una estrategia que permita aprovechar 
esta complementariedad para mejorar el orden de los documentos a entregar finalmente al 
usuario. Entre los primeros experimentos orientados a esta problemática se comprobó que 
bajo la estrategia de expansión es posible mejorar la recuperación. Sin embargo, es claro 
que deben hacerse más experimentos para evaluar otras posibles estrategias. 
Finalmente, entre los primeros experimentos orientados a representar los documentos con unidades sub-palabra codificadas se observó una ganancia respecto a la representación orientada a palabra completa. Este resultado preliminar nos permite afirmar que el uso 
de sub-palabras repercutirá en una mejora en la recuperación. Por supuesto, aún falta experimentar para determinar el tamaño apropiado de estas unidades sub-palabra así como si 
éstas deberán estar traslapadas o no. Además también falta determinar qué tipo de codificación conviene más para este tipo de unidades. 
En resumen, con toda la evidencia adquirida es posible afirmar que la codificación 
fonética es útil para la tarea de recuperación de información en documentos orales. Claro 
está, aún falta trabajo por hacer, sobre todo para determinar bajo qué circunstancias la codificación fonética alcanza la mayor efectividad.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Codificación fonética de sub-palabras 
para la Recuperación de Información 
en Documentos Orales 
Propuesta de Tesis Doctoral
por
Manuel Alejandro Reyes Barragán
InstitutoNacional de Astrofísica,
O'
ptica y Electro'nica
Abril 2009
Tonantzintla, Puebla
Supervisada por:
Dr. Luis Villaseñor Pineda
Investigador Titular del INAOE
Dr. Manuel Montes yGómez
Investigador Titular del INAOE</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Algoritmos dinámicos para el agrupamiento con traslape</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Sea C = {O1, O2, ... , On } una coleccion de objetos, cada uno descrito a través de un conjunto de rasgos 
R = { r1, r2, . . . , r1} ; un algoritmo de agrupamiento es aquel que organiza la coleccion antes mencionada 
en clases o grupos de forma tal que la semejanza entre objetos pertenecientes a un mismo grupo sea alta, en
contraposicion a la semejanza entre objetos de grupos diferentes. Se dice que el conjunto de grupos, obtenido 
por un algoritmo de agrupamiento, es solapado si en dicho conjunto existen objetos que pertenezcan a mas
de un grupo a la vez.
Sea C una coleccion de objetos como la descrita anteriormente; un algoritmo de agrupamiento es 
jerarquico  si el mismo construye una estructura compuesta de k niveles, siendo el nivel 1 el mas general o supremo y el nivel k el mas específico o infimo, que cumple las siguientes condiciones:
a) Cada nivel Ni = {gi1,gi2,..., giMi}i = 1..k - 1 satisface que:

-|Ni| = M
- g[i,j],j = 1..M, se cumple que g[i,j] C= C
-|Ni| &lt; |N[i+1]|


b) Nk = {{O1},{O2},...,{On}}: |Nk| = |C| = n
c) gij, i =1..k -1, j=1..M, existe una relacion padre-hijo con al menos un elemento g(i+1)q
; q = 1::Mi+1. Adicionalmente, si un grupo A tiene una relacion padre-hijo con un grupo B entonces se
cumple que B µ A y se puede decir tambien que  B tiene una relacion es-hijo-de con A.
d) gij, i = 1..k -1, j = 1.. Mi gi2 = U {g(i+1)q | q = 1..Mi+1 ^g(i+1)q es-hijo-de gij}

La condicion a) establece como est  a formado cada nivel de la estructura, qué características presentan
los grupos que conforman dichos niveles y que relación existe entre el n  umero de grupos de un nivel y el 
de su nivel inmediato inferior. La condicion b) plantea que el ultimo nivel (el nivel  k) de la jerarquía es
aquel en el cual cada elemento de C forma un conjunto unitario. La condicion c), por otra parte, establece la
relacion que existe entre cada uno de los elementos de un nivel y los elementos del nivel inmediato inferior. 
Finalmente, la condicion d) define que cualquier grupo g de un nivel se obtiene a traves de la unión de 
los grupos del nivel inmediato inferior con los cuales g tienen una relacion padre-hijo. Un ejemplo de una
estructura de agrupamiento jerarquico se muestra en la figura 1. 
Un algoritmo de agrupamiento jerarquico construye una jerarquía de grupos solapados si en dicha jerarquía se permite que el conjunto de grupos, obtenidos en cualquier nivel Ni, i = 1..k - 1, pueda ser
solapado.
El problema que se plantea en esta propuesta de tesis doctoral es el desarrollo de algoritmos de agrupamiento, jerarquicos y no jer  arquicos, que sean din  amicos y que obtengan un conjunto de grupos que 
pueden ser solapados.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de esta tesis de investigacion doctoral consiste en desarrollar un algoritmo de agru- 
pamiento jerarquico aglomerativo, que sea din  amico y que permita adem  as construir una jerarquía de grupos
que puede ser traslapada.
El algoritmo desarrollado debe de eliminar las limitaciones planteadas en la seccion de motivación. Adi-cionalmente, el algoritmo propuesto debe alcanzar, respecto a los algoritmos reportados en la literatura,
un rendimiento superior en cuanto a medidas de eficacia y un rendimiento similar o superior respecto a la
eficiencia.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es posible desarrollar un algoritmo de agrupamiento dinamico basado en grafos que permita obtener 
un conjunto de grupos que pueden ser solapados y que alcance mejores resultados de eficacia que los
algoritmos reportados?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Es posible determinar una representacion que sea  util o efectiva para los grupos de los niveles de una 
jerarquía de grupos?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Es posible desarrollar un algoritmo jerarquico aglomerativo, que sea din  amico, que permita obtener 
una jerarquía que puede ser traslapada y que obtenga mejores resultados de eficacia que los algoritmos
reportados?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. Recopilar colecciones reportadas en la literatura en las cuales exista traslape entre las clases etiquetadas manualmente. Consideraremos inicialmente las colecciones TDT2, TREC-5 y Reuters-21578.
2. Seleccionar medidas de evaluacion de algoritmos de agrupamiento, que estén reportadas en la li- 
teratura y permitan evaluar la calidad de los algoritmos de agrupamiento que obtienen grupos con
traslapes. Consideraremos inicialmente las medidas Jaccard-index [59] y Fmeasure [60].
3. Desarrollar un algoritmo de agrupamiento incremental.
(a) Desarrollar algoritmo de agrupamiento estatico basado en grafos que permita obtener grupos 
con traslapes.
i) Utilizar para representar la coleccion de objetos el grafo de  semejanza G. Este grafo
es simple de construir y ademas enél, la adición o eliminación de un vértice sólo provoca 
adiciones o eliminaciones de las aristas relativas a dicho vertice. 
ii) Utilizar para obtener un cubrimiento de G  los sub-grafos de tipo estrella [25]. Este tipo de
sub-grafo permite obtener grupos con traslape en los cuales el encadenamiento es reducido.
iii) Definir una propiedad sobre los vertices de  G que permita filtrar el numero de vértices 
candidatos a centro y que ademas permita definir un orden sobre los elementos de dicho  
conjunto. Dado que interesa obtener grupos densos la propiedad definida debe de utilizar
de cierta forma el grado de los vertices.
iv) Disenar una estrategia de agrupamiento que utilice la propiedad definida en iii) para se- 
leccionar un conjunto de vertices centros que cubran completamente a  G. La estrategia
disenada debe impedir la selección de vértices que  a priori forman grupos en los que todos
los vertices pertenecen al menos a otro grupo. 
v) Definir un criterio que permita filtrar el conjunto de centros seleccionados de forma que el
conjunto resultante cumpla que contiene a los centros mas densos del conjunto anterior que 
todavía pueden cubrir completamente a G. Este criterio debe de utilizar el grado de los
vertices centro. 
(b) Determinar condiciones que permitan actualizar el agrupamiento, formado por el algoritmo desarrollado en el punto 3.a, cuando se adiciona uno o mas objetos a la colección. Analizar inicialmente:
i) ¿Como varía el valor de la propiedad definida en el punto 3.a.iii para cada uno de los vertices 
de G?
ii) ¿Que grupos formados son los que necesitan ser actualizados? 
iii) ¿Que condiciones debe cumplir un vértice para ser considerado como candidato a centro? 
(c) Disenar e implementar, utilizando las condiciones determinadas en el paso 3.b, un algoritmo
incremental.
(d) Comparar el desempeno del algoritmo desarrollado, utilizando las colecciones recopiladas en 
el paso 1, respecto a los algoritmos reportados en la literatura. Estas comparaciones deben de
considerar:
i) La calidad de los grupos respecto a las medidas seleccionadas en el paso 2.
ii) La eficiencia de los algoritmos al agrupar una coleccion de forma incremental. 
(e) Analizar los resultados experimentales y determinar limitaciones que puedan afectar el funcionamiento del algoritmo. En caso de existir dichas limitaciones, modificar el algoritmo para
solucionarlas.
(f) Analizar la complejidad computacional del algoritmo desarrollado.
4. Desarrollar un algoritmo de agrupamiento dinamico. 
(a) Determinar condiciones que permitan actualizar el agrupamiento, formado por el algoritmo desarrollado en el punto 3.a, cuando se elimina uno o mas objetos de la colección. Analizar 
inicialmente:
i) ¿Como var  ía el valor de la propiedad definida en el punto 3.a.iii para cada uno de los vertices 
de G?
ii) ¿Que grupos formados son los que necesitan ser actualizados? 
iii) ¿Que condiciones debe cumplir un vértice para ser considerado como candidato a centro? 
iv) ¿Cuales son los vértices que pueden quedar no cubiertos? 
(b) Disenar e implementar, utilizando las condiciones determinadas en el paso 3.b y 4.a, un algoritmo dinamico. 
(c) Comparar el desempeno del algoritmo desarrollado, utilizando las colecciones recopiladas en 
el paso 1, respecto a los algoritmos reportados en la literatura. Estas comparaciones deben de
considerar:
i) La eficiencia de los algoritmos al actualizar el agrupamiento cuando se eliminan uno o
varios objetos de la coleccion. 
ii) La eficiencia de los algoritmos al actualizar el agrupamiento cuando se modifican uno o
varios objetos de la coleccion. 
(d) Analizar los resultados experimentales y determinar limitaciones que puedan afectar el funcionamiento del algoritmo. En caso de existir dichas limitaciones, modificar el algoritmo para
solucionarlas.
(e) Analizar la complejidad computacional del algoritmo desarrollado.
5. Desarrollar un algoritmo de agrupamiento jerarquico aglomerativo incremental. 
(a) Disenar un algoritmo jerárquico aglomerativo est  atico que: 
i) Utilice para agrupar los objetos de cada nivel el algoritmo estatico desarrollado en el paso 
3.a.
ii) Considere que los objetos de todo nivel Ni > 1 son los grupos del nivel anterior.
iii) Detenga la construccion de la jerarqu  ía cuando el grafo de semejanza, asociado a la
coleccion de objetos del nivel, no tenga aristas. 
iv) Pueda usar cualquier medida para el calculo de la semejanza entre los objetos de los niveles 
Ni > 1. Consideraremos inicialmente la medida group-average que ha sido usada en varios
algoritmos jerarquicos aglomerativos. 
(b) Seleccionar un criterio para representar los objetos de los niveles Ni > 1.
i) Estudiar los criterios, alternativos al utilizado en el paso 5.a.ii, que esten reportados en 
la literatura. Consideraremos inicialmente para representar a un grupo: (I) al objeto que
pertenece al grupo y que es el mas cercano del centroide del mismo y (  II) al vertice  centro
del sub-grafo que determina el grupo.
ii) Evaluar cada criterio sobre el algoritmo desarrollado en el paso 5.a.
iii) Analisis de los resultados y selección del criterio. 
(c) Seleccionar una medida para determinar la semejanza entre los objetos de los niveles Ni > 1.
i) Estudiar las medidas, alternativas a la utilizada en el paso 5.a.iv, que esten reportadas en 
la literatura. Consideraremos inicialmente: (I) la semejanza entre los representantes de los
grupos y (II) la semejanza entre un subconjunto de vertices de los sub-grafos que determinan 
a cada grupo.
ii) Evaluar cada medida sobre el algoritmo desarrollado en el paso 5.a.
iii) Analisis de los resultados y selección de la medida. 
(d) Determinar condiciones que permitan actualizar los grupos de la jerarquía formada por el algoritmo desarrollado en el paso 5.a cuando se adicionan uno o mas objetos a la colección. Analizar 
inicialmente:
i) ¿Como afecta la actualización de los grupos en un nivel a los objetos del nivel siguiente? 
ii) Cuando en un nivel se eliminan, se modifican o se forman nuevos grupos, ¿Que objetos del 
nivel siguiente necesitan ser eliminados y/o adicionados?
iii) ¿Como representar la relación entre los grupos de un nivel y los objetos del nivel siguiente 
de una forma eficiente que permita a la vez reflejar rapidamente los cambios que puedan 
ocurrir en un nivel?
(e) Disenar un algoritmo jerárquico aglomerativo incremental que: 
i) Utilice para agrupar los objetos de cada nivel el algoritmo dinamico desarrollado en el paso 
4.
ii) Utilice las condiciones determinadas en el paso 5.d.
ii) Represente los objetos de todo nivel Ni > 1 utilizando el criterio seleccionado en el paso
5.b.
iii) Detenga la construccion de la jerarqu  ía cuando el grafo de semejanza, asociado a la
coleccion de objetos del nivel, no tenga aristas. 
iv) Utilice para medir la semejanza entre dos objetos, en todo nivel Ni > 1, la medida seleccionada en el paso 5.c.
(f) Comparar el desempeno del algoritmo desarrollado, utilizando las colecciones recopiladas en 
el paso 1, respecto a los algoritmos reportados en la literatura. Estas comparaciones deben de
considerar:
i) La calidad de los grupos respecto a las medidas seleccionadas en el paso 2.
ii) La eficiencia de los algoritmos al agrupar una coleccion de forma incremental. 
(g) Analizar los resultados experimentales y determinar limitaciones que puedan afectar el funcionamiento del algoritmo. En caso de existir dichas limitaciones, modificar el algoritmo para
solucionarlas.
(h) Analizar la complejidad computacional del algoritmo desarrollado.
6. Desarrollar un algoritmo de agrupamiento jerarquico din  amico. 
(a) Determinar condiciones que permitan actualizar los grupos de la jerarquía formada por el algoritmo desarrollado en el paso 5.a cuando se eliminan uno o mas objetos de la colección. Analizar 
inicialmente:
i) ¿Como afecta la actualización de los grupos en un nivel a los objetos del nivel siguiente? 
ii) Cuando en un nivel se eliminan, se modifican o se forman nuevos grupos, ¿Que objetos del 
nivel siguiente necesitan ser eliminados y/o adicionados?
iii) ¿Como representar la relación entre los grupos de un nivel y los objetos del nivel siguiente 
de una forma eficiente que permita a la vez reflejar rapidamente los cambios que puedan 
ocurrir en un nivel?
(b) Disenar un algoritmo jerárquico aglomerativo din  amico que: 
i) Utilice para agrupar los objetos de cada nivel el algoritmo dinamico desarrollado en el paso 
4.
ii) Utilice las condiciones determinadas en el paso 6.a.
ii) Represente los objetos de todo nivel Ni > 1 utilizando el criterio seleccionado en el paso
5.b.
iii) Detenga la construccion de la jerarqu  ía cuando el grafo de semejanza, asociado a la
coleccion de objetos del nivel, no tenga aristas. 
iv) Utilice para medir la semejanza entre dos objetos, en todo nivel Ni > 1, la medida seleccionada en el paso 5.c.
(c) Comparar el desempeno del algoritmo desarrollado, utilizando las colecciones recopiladas en 
el paso 1, respecto a los algoritmos reportados en la literatura. Estas comparaciones deben de
considerar:
i) La eficiencia de los algoritmos al actualizar el agrupamiento cuando se eliminan uno o
varios objetos de la coleccion. 
ii) La eficiencia de los algoritmos al actualizar el agrupamiento cuando se modifican uno o
varios objetos de la coleccion. 
(d) Analizar los resultados experimentales y determinar limitaciones que puedan afectar el funcionamiento del algoritmo. En caso de existir dichas limitaciones, modificar el algoritmo para
solucionarlas.
(e) Analizar la complejidad computacional del algoritmo desarrollado.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>1. Recopilar colecciones reportadas en la literatura en las cuales exista traslape entre las clases etiquetadas manualmente. Consideraremos inicialmente las colecciones TDT2, TREC-5 y Reuters-21578.
2. Seleccionar medidas de evaluacion de algoritmos de agrupamiento, que estén reportadas en la li- 
teratura y permitan evaluar la calidad de los algoritmos de agrupamiento que obtienen grupos con
traslapes. Consideraremos inicialmente las medidas Jaccard-index [59] y Fmeasure [60].
3. Desarrollar un algoritmo de agrupamiento incremental.
(a) Desarrollar algoritmo de agrupamiento estatico basado en grafos que permita obtener grupos 
con traslapes.
i) Utilizar para representar la coleccion de objetos el grafo de  semejanza G. Este grafo
es simple de construir y ademas enél, la adición o eliminación de un vértice sólo provoca 
adiciones o eliminaciones de las aristas relativas a dicho vertice. 
ii) Utilizar para obtener un cubrimiento de G los sub-grafos de tipo estrella [25]. Este tipo de
sub-grafo permite obtener grupos con traslape en los cuales el encadenamiento es reducido.
iii) Definir una propiedad sobre los vertices de  G que permita filtrar el numero de vértices 
candidatos a centro y que ademas permita definir un orden sobre los elementos de dicho 
conjunto. Dado que interesa obtener grupos densos la propiedad definida debe de utilizar
de cierta forma el grado de los vertices.
iv) Disenar una estrategia de agrupamiento que utilice la propiedad definida en iii) para se-
leccionar un conjunto de vertices centros que cubran completamente a  G. La estrategia
disenada debe impedir la selección de vértices que  a priori forman grupos en los que todos
los vertices pertenecen al menos a otro grupo. 
v) Definir un criterio que permita filtrar el conjunto de centros seleccionados de forma que el
conjunto resultante cumpla que contiene a los centros mas densos del conjunto anterior que 
todavía pueden cubrir completamente a G. Este criterio debe de utilizar el grado de los
vertices centro. 
(b) Determinar condiciones que permitan actualizar el agrupamiento, formado por el algoritmo desarrollado en el punto 3.a, cuando se adiciona uno o mas objetos a la colección. Analizar ini- 
cialmente:
i¿Como varía el valor de la propiedad definida en el punto 3.a.iii para cada uno de los vertices 
de G?
ii¿Que grupos formados son los que necesitan ser actualizados? 
iii¿Que condiciones debe cumplir un vértice para ser considerado como candidato a centro? 
(c) Disenar e implementar, utilizando las condiciones determinadas en el paso 3.  b, un algoritmo
incremental.
(d) Comparar el desempeno del algoritmo desarrollado, utilizando las colecciones recopiladas en 
el paso 1, respecto a los algoritmos reportados en la literatura. Estas comparaciones deben de
considerar:
i) La calidad de los grupos respecto a las medidas seleccionadas en el paso 2.
ii) La eficiencia de los algoritmos al agrupar una coleccion de forma incremental. 
(e) Analizar los resultados experimentales y determinar limitaciones que puedan afectar el funcionamiento del algoritmo. En caso de existir dichas limitaciones, modificar el algoritmo para
solucionarlas.
(f) Analizar la complejidad computacional del algoritmo desarrollado.
4. Desarrollar un algoritmo de agrupamiento dinamico. 
(a) Determinar condiciones que permitan actualizar el agrupamiento, formado por el algoritmo desarrollado en el punto 3.a, cuando se elimina uno o mas objetos de la colección. Analizar 
inicialmente:
i¿Como varía el valor de la propiedad definida en el punto 3.a.iii para cada uno de los vertices 
de G?
ii¿Que grupos formados son los que necesitan ser actualizados? 
iii¿Que condiciones debe cumplir un vértice para ser considerado como candidato a centro? 
iv¿Cuales son los vértices que pueden quedar no cubiertos? 
(b) Disenar e implementar, utilizando las condiciones determinadas en el paso 3.  b y 4.a, un algoritmo dinamico. 
(c) Comparar el desempeno del algoritmo desarrollado, utilizando las colecciones recopiladas en 
el paso 1, respecto a los algoritmos reportados en la literatura. Estas comparaciones deben de
considerar:
i) La eficiencia de los algoritmos al actualizar el agrupamiento cuando se eliminan uno o
varios objetos de la coleccion. 
ii) La eficiencia de los algoritmos al actualizar el agrupamiento cuando se modifican uno o
varios objetos de la coleccion. 
(d) Analizar los resultados experimentales y determinar limitaciones que puedan afectar el funcionamiento del algoritmo. En caso de existir dichas limitaciones, modificar el algoritmo para
solucionarlas.
(e) Analizar la complejidad computacional del algoritmo desarrollado.
5. Desarrollar un algoritmo de agrupamiento jerarquico aglomerativo incremental. 
(a) Disenar un algoritmo jerárquico aglomerativo est  atico que: 
i) Utilice para agrupar los objetos de cada nivel el algoritmo estatico desarrollado en el paso 
3.a.
ii) Considere que los objetos de todo nivel Ni > 1 son los grupos del nivel anterior.
iii) Detenga la construccion de la jerarquía cuando el grafo de semejanza, asociado a la
coleccion de objetos del nivel, no tenga aristas. 
iv) Pueda usar cualquier medida para el calculo de la semejanza entre los objetos de los niveles 
Ni > 1. Consideraremos inicialmente la medida group-average que ha sido usada en varios
algoritmos jerarquicos aglomerativos. 
(b) Seleccionar un criterio para representar los objetos de los niveles Ni > 1.
i) Estudiar los criterios, alternativos al utilizado en el paso 5.a.ii, que esten reportados en 
la literatura. Consideraremos inicialmente para representar a un grupo: (I) al objeto que
pertenece al grupo y que es el mas cercano del centroide del mismo y (  II) al vertice  centro
del sub-grafo que determina el grupo.
ii) Evaluar cada criterio sobre el algoritmo desarrollado en el paso 5.a.
iii) Analisis de los resultados y selección del criterio. 
(c) Seleccionar una medida para determinar la semejanza entre los objetos de los niveles Ni > 1.
i) Estudiar las medidas, alternativas a la utilizada en el paso 5.a.iv, que esten reportadas en 
la literatura. Consideraremos inicialmente: (I) la semejanza entre los representantes de los
grupos y (II) la semejanza entre un subconjunto de vertices de los sub-grafos que determinan 
a cada grupo.
ii) Evaluar cada medida sobre el algoritmo desarrollado en el paso 5.a.
iii) Analisis de los resultados y selección de la medida. 
(d) Determinar condiciones que permitan actualizar los grupos de la jerarquía formada por el algoritmo desarrollado en el paso 5.a cuando se adicionan uno o mas objetos a la colección. Analizar 
inicialmente:
i¿Como afecta la actualización de los grupos en un nivel a los objetos del nivel siguiente? 
ii) Cuando en un nivel se eliminan, se modifican o se forman nuevos grupos, ¿Que objetos del 
nivel siguiente necesitan ser eliminados y/o adicionados?
iii¿Como representar la relación entre los grupos de un nivel y los objetos del nivel siguiente 
de una forma eficiente que permita a la vez reflejar rapidamente los cambios que puedan 
ocurrir en un nivel?
(e) Disenar un algoritmo jerárquico aglomerativo incremental que: 
i) Utilice para agrupar los objetos de cada nivel el algoritmo dinamico desarrollado en el paso 
4.
ii) Utilice las condiciones determinadas en el paso 5.d.
ii) Represente los objetos de todo nivel Ni > 1 utilizando el criterio seleccionado en el paso
5.b.
iii) Detenga la construccion de la jerarquía cuando el grafo de semejanza, asociado a la
coleccion de objetos del nivel, no tenga aristas. 
iv) Utilice para medir la semejanza entre dos objetos, en todo nivel Ni > 1, la medida seleccionada en el paso 5.c.
(f) Comparar el desempeno del algoritmo desarrollado, utilizando las colecciones recopiladas en 
el paso 1, respecto a los algoritmos reportados en la literatura. Estas comparaciones deben de
considerar:
i) La calidad de los grupos respecto a las medidas seleccionadas en el paso 2.
ii) La eficiencia de los algoritmos al agrupar una coleccion de forma incremental. 
(g) Analizar los resultados experimentales y determinar limitaciones que puedan afectar el funcionamiento del algoritmo. En caso de existir dichas limitaciones, modificar el algoritmo para
solucionarlas.
(h) Analizar la complejidad computacional del algoritmo desarrollado.
6. Desarrollar un algoritmo de agrupamiento jerarquico din  amico. 
(a) Determinar condiciones que permitan actualizar los grupos de la jerarquía formada por el algoritmo desarrollado en el paso 5.a cuando se eliminan uno o mas objetos de la colección. Analizar 
inicialmente:
i¿Como afecta la actualización de los grupos en un nivel a los objetos del nivel siguiente? 
ii) Cuando en un nivel se eliminan, se modifican o se forman nuevos grupos, ¿Que objetos del 
nivel siguiente necesitan ser eliminados y/o adicionados?
iii¿Como representar la relación entre los grupos de un nivel y los objetos del nivel siguiente 
de una forma eficiente que permita a la vez reflejar rapidamente los cambios que puedan 
ocurrir en un nivel?
(b) Disenar un algoritmo jerárquico aglomerativo din  amico que: 
i) Utilice para agrupar los objetos de cada nivel el algoritmo dinamico desarrollado en el paso 
4.
ii) Utilice las condiciones determinadas en el paso 6.a.
ii) Represente los objetos de todo nivel Ni > 1 utilizando el criterio seleccionado en el paso
5.b.
iii) Detenga la construccion de la jerarquía cuando el grafo de semejanza, asociado a la
coleccion de objetos del nivel, no tenga aristas. 
iv) Utilice para medir la semejanza entre dos objetos, en todo nivel Ni > 1, la medida seleccionada en el paso 5.c.
(c) Comparar el desempeno del algoritmo desarrollado, utilizando las colecciones recopiladas en 
el paso 1, respecto a los algoritmos reportados en la literatura. Estas comparaciones deben de
considerar:
i) La eficiencia de los algoritmos al actualizar el agrupamiento cuando se eliminan uno o
varios objetos de la coleccion. 
ii) La eficiencia de los algoritmos al actualizar el agrupamiento cuando se modifican uno o
varios objetos de la coleccion. 
(d) Analizar los resultados experimentales y determinar limitaciones que puedan afectar el funcionamiento del algoritmo. En caso de existir dichas limitaciones, modificar el algoritmo para
solucionarlas.
(e) Analizar la complejidad computacional del algoritmo desarrollado.

Los requerimientos y las disponibilidades de informacion existentes hoy en día, incrementan la necesidad
de desarrollar nuevos metodos de agrupamiento que sean cada vez m  as utiles a las aplicaciones que procesan 
dicha informacion. De aquí la importancia de los metodos que se desarrollen en esta investigación. 
Como parte de los resultados preliminares se presento un nuevo algoritmo incremental para el agru- 
pamiento de objetos llamado ICSD. El algoritmo propuesto no impone restricciones al tipo de objeto a
agrupar, al espacio de representacion deéste ni a la medida para determinar la semejanza entre dos objetos. 
ICSD permite obtener un conjunto de grupos solapados mediante el cubrimiento del grafo de semejanza
a traves de sub-grafos en forma de estrella; el uso de este tipo de sub-grafo le permite a ICSD reducir el 
encadenamiento en los grupos obtenidos.
ICSD, a diferencia de la mayoría de los algoritmos incrementales reportados en la literatura, permite el
procesamiento eficiente de adiciones multiples. 
El algoritmo propuesto fue comparado contra otros tres algoritmos basados en grafos en seis colecciones
de documentos. Los resultados experimentales mostraron que ICSD obtiene en las colecciones de prueba
mejores resultados de calidad, considerando las medidas Fmeasure y Jaccard-index, que los otros metodos. 
Adicionalmente, en los experimentos se mostro que ICSD es m  as r  apido en el procesamiento incremental 
de colecciones que los algoritmos incrementales basados en grafos usados en los experimentos.
Finalmente, con base en los resultados preliminares alcanzados, concluimos que siguiendo la metodología
propuesta se pueden alcanzar los objetivos de esta investigacion en el tiempo planteado.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Algoritmos dinámicos para el agrupamiento 
con traslape 
Airel Pérez Suárez, José Fco. Martínez Trinidad, 
José E. Medina Pagola, Jesús Ariel Carrasco Ochoa 
Reporte Técnico No. CCC-10-001 
25 de enero de 2010
© 2010 
Coordinación de Ciencias Computacionales 
INAOE 
Luis Enrique Erro 1 
Sta. Ma. Tonantzintla, 
72840, Puebla, México</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Desarrollo de Clasificadores basados en Reglas de Asociación</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Sea I un conjunto de ítems, C un conjunto de clases, TC un conjunto de transacciones de la forma
fi1; i2; :::; in; cg tal que 81·k·n[ik 2 I ^ c 2 C] (ver tabla 1), R un conjunto ordenado de reglas X ) c tal
que X µ I y c 2 C, W una funcion que asigna un peso a cada regla  r 2 R y D un criterio de decision que 
utiliza a R para asignar una clase a cada transaccion t que se desee clasificar.
Dados I, C y TC, construir un clasificador basado en CARs consiste en calcular R, ordenar R segun la 
funcion de asignación de peso W y definir el criterio de decisión D. El problema que se plantea en esta 
propuesta de tesis doctoral es la construccion de clasificadores basados en CARs.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Construir un nuevo clasificador basado en CARs a partir de una muestra de entrenamiento, que alcance
mayor eficacia que los clasificadores existentes basados en CARs</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es posible proponer una nueva medida de calidad para el calculo y ordenamiento del conjunto de 
CARs, que no tenga las limitaciones de la confianza?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Es posible desarrollar un algoritmo eficiente para el calculo de CARs y una estrategia de poda que 
permita generar mas reglas que obtengan valores significativos de la medida de calidad?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Es posible proponer una estrategia de cubrimiento que reduzca el numero de casos en que ninguna 
CAR cubra a la transaccion que se desea clasificar y así reducir el numero de asignaciones de la clase 
mayoritaria?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Es posible proponer un criterio de decision que no tenga los problemas de los criterios de decisión
existentes?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Es posible proponer un criterio de desambiguacion que reduzca la cantidad de asignaciones aleatorias 
cuando hay clases empatadas?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. Proponer una nueva medida de calidad para el calculo y ordenamiento del conjunto de CARs, que no 
tenga las limitaciones de la confianza.
a) Estudiar las medidas de calidad propuestas en la literatura para ARM y analizar si alguna reduce
las limitaciones de la confianza. Si el estudio realizado da como resultado alguna medida que no tenga las limitaciones de la confianza entonces se utilizara para el c  alculo de las CARs. Inicial- 
mente estudiamos las medidas Confianza, Soporte, Conviction, Interest, Chi-square, Certainty
factors y Netconf. Por el momento se han obtenido buenos resultados con la medida de calidad
Netconf propuesta en [19] para estimar la fuerza de una regla de asociacion. 
b) Independientemente del resultado obtenido en el paso anterior, se propondra una nueva medida 
de calidad para el calculo de CARs que no tenga las limitaciones de la  confianza.
c) Analizar los criterios de ordenamiento de CARs reportados en la literatura y proponer un nuevo
criterio de ordenamiento que haga uso de la(s) medida(s) de calidad resultante(s) en el paso 1a) y
1b). Inicialmente hemos obtenido buenos resultados utilizando la medida Netconf para ordenar
el conjunto de CARs.
d) Evaluar la combinacion de los criterios de ordenamiento existentes con el criterio propuesto 
en 1c). Los trabajos recientes [9, 14, 12] han mostrado que la combinacion de criterios de 
ordenamiento puede mejorar la eficacia de los clasificadores.
2. Disenar e implementar un algoritmo eficiente para calcular el conjunto de CARs que haga uso de la
medida de calidad para CARs del objetivo 1 y ademas, proponer una estrategia de poda que permita 
generar mas reglas con valores significativos de la medida de calidad que las estrategias de poda 
existentes.
a) Adaptar el algoritmo publicado en [18] para calcular el conjunto de CARs haciendo uso de la
medida de calidad del objetivo 1. Para ello se crearan sólo las clases de equivalencias [3] que 
involucren al conjunto de clases predefinido y se modificara la información asociada a cada clase 
de equivalencia para poder calcular eficientemente, para cada regla, los valores correspondientes
de la medida de calidad utilizada.
b) Proponer una estrategia de poda que permita generar mas reglas de buena calidad (seg  un la 
medida utilizada). En vez de podar el espacio de busqueda cada vez que se encuentra una regla 
que satisface los umbrales de soporte y confianza, se evaluara la siguiente estrategia: cuando se 
encuentre una regla que satisface el umbral de Netconf establecido, se continuaran generando 
reglas mientras que el valor de Netconf no disminuya. Esta misma estrategia se evaluara con la 
medida que se obtenga en 1b).
c) Evaluar el algoritmo desarrollado en el paso anterior. En la evaluacion se medir  a como influyen 
en la eficacia del clasificador la nueva medida de calidad y la nueva estrategia de poda.
3. Proponer una estrategia para reducir los casos en que ninguna CAR cubra a la transaccion que se 
desea clasificar y así reducir el numero de asignaciones de la clase mayoritaria. 
a) Considerar el cubrimiento inexacto de las nuevas transacciones que se deseen clasificar. Para
ello haremos mas accexible el criterio de cubrimiento de una transacción por una regla (ver 
definicion 2.9). Inicialmente probaremos permitir que una regla  fi1; i2; : : : ; ing ) c cubra a
una transaccion t si al menos n ¡ 1 ítems del antecedente de la regla pertenecen a t.
b) Comparar la eficacia en la clasificacion considerando el cubrimiento inexacto contra el cubri- 
miento exacto.
4. Proponer un nuevo criterio de decision que resuelva los problemas de los criterios de decisión existen- 
tes. Los problemas de cada uno de los criterios de decision existentes fueron discutidos en la sección
3.
a) Comprobar experimentalmente el analisis hecho por otros autores respecto a los criterios de 
decision existentes. Los  ultimos trabajos utilizan el criterio de las "Mejores  K Reglas" por clase
como criterio de decision y descartan los criterios de la "Mejor Regla" y de "Todas las Reglas". 
No obstante, cuando se utiliza el criterio de las "Mejores K Reglas" y existe gran desbalance en
el numero de CARs por clase que cubre a una nueva transacción, la eficacia del clasificador se 
puede afectar.
b) Seleccionar automaticamente un valor de  K para el criterio de "Las Mejores K Reglas" posiblemente diferente para cada clase y para cada transaccion, de esta forma se puede reducir el efecto 
del desbalance entre el numero de CARs por clase, lo cual puede repercutir directamente en el 
desbalance del numero de CARs por clase que cubre a una nueva transacción. Inicialmente eva- 
luaremos tomar la mejor regla y dado un umbral, tomar todas las reglas cercanas a ella respecto
al valor de la medida utilizada.
c) Comparar la eficacia del clasificador aplicando el criterio de decision propuesto en el paso ante- 
rior y aplicando los criterios de decision existentes. 
5. Proponer un criterio de desambiguacion de clases para reducir la cantidad de asignaciones aleatorias 
cuando hay clases empatadas.
a) Considerar la eliminacion de la CAR de menor calidad, mayor calidad o ambas mientras haya 
empate.
b) Considerar el uso de un segundo clasificador posiblemente tambien basado en reglas. 
c) Comparar la eficacia en la clasificacion usando los criterios de desambiguación de clases resul- 
tantes en los pasos anteriores contra la eficacia que se obtiene al asignar una clase aleatoria.
6. Disenar e implementar un clasificador basado en CARs, a partir de una muestra de entrenamiento, que 
utilice las propuestas de los objetivos anteriores y que alcance mayor eficacia que los clasificadores
existentes basados en CARs.
a) Integrar las propuestas hechas en los pasos anteriores para construir un clasificador basado en
CARs.
7. Evaluar la eficacia del clasificador obtenido.
a) Realizar una comparacion experimental del clasificador obtenido contra los clasificadores basa- 
dos en CARs, reportados en la literatura.
b) Se consideraran en la experimentación los conjuntos de datos del repositorio UCI [31] por ser 
los comunmente usados en los trabajos reportados.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Hasta el momento se desarrollo un algoritmo para calcular el conjunto de CARs utilizando la medida de 
calidad Netconf, que no presenta las limitaciones de la confianza, así como una nueva estrategia de poda que permite generar mas reglas de buena calidad. Adem  as, se propuso una estrategia de cubrimiento inexacto 
para reducir los casos en que ninguna CAR cubra a la transaccion que se desea clasificar. Estos resultados 
se integraron y se desarrollo un clasificador basado en CARs que alcanza valores de eficacia superiores a los 
alcanzados por otros clasificadores del estado del arte.
Teniendo en cuenta estos resultados preliminares, consideramos que los objetivos propuestos pueden ser alcanzados en el tiempo planteado y con la calidad deseada, siguiendo la metodología propuesta.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Desarrollo de Clasificadores basados en 
Reglas de Asociación 
Raudel Hernández León, Jesús A. Carrasco Ochoa, 
José Hernández Palancar, J. Fco. Martínez Trinidad 
Reporte Técnico No. CCC-10-002 
26 de enero de 2010
© 2010 
Coordinación de Ciencias Computacionales 
INAOE 
Luis Enrique Erro 1 
Sta. Ma. Tonantzintla, 
72840, Puebla, México.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Diseño de una Plataforma de Procesamiento Enfocada a Problemas de Optimización Global."</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La ejecución de los algoritmos de optimización esta dividida en dos secciones (6.1). La
primera parte es el tiempo de ejecución del algoritmo de optimización (ta), mientras que la
segunda parte corresponde al tiempo que se emplea en hacer las evaluaciones
correspondientes a la función objetivo (to).

A partir de esta descripción, se pueden tener diferentes escenarios que obedecen a las
características del problema en particular. El primero corresponde al tipo de problemas
donde el tiempo necesario para obtener la evaluación de la función objetivo es
considerablemente superior al tiempo de ejecución del algoritmo. Bajo este escenario, el
proceso de optimización esta dominado por las evaluaciones de la función objetivo. El otro
escenario se relaciona con el tipo de problemas donde el número de parámetros a encontrar
es considerablemente grande; además, el tiempo de ejecución del algoritmo de
optimización es comparable con el tiempo necesario para obtener un valor de la función
objetivo.
Una familia de problemas que presentan estas dificultades son los expresados mediante la
forma (6.3). Para calcular la función objetivo, primero se necesita resolver el sistema de
ecuaciones diferenciales ordinarias en un tiempo t. Un ejemplo representativo de este tipo
de sistemas se explica en la sección 7.1.3.

En la figura 6.1 se muestra una grafica representativa de los tipos de problemas
representados por la ecuación 6.2. La línea en rojo representa el tiempo necesario para
realizar una evaluación en la función objetivo, mientras que la línea en azul representa la
complejidad temporal del algoritmo de optimización. En la figura 6.1 se pueden distinguir
los dos escenarios antes mencionados. En la región A, donde el número de parámetros a
considerar es bajo y el tiempo de ejecución del algoritmo todavía no es comparable con el
tiempo requerido para evaluar la función objetivo. La zona B corresponde al segundo
escenario donde la relación de ambos tiempos no es tan pequeña.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Desarrollar una plataforma de procesamiento que permita resolver en forma práctica los
problemas de optimización global cuya función objetivo tiene un alto costo computacional.

Objetivos particulares

Además del diseño de la plataforma de procesamiento, se proponen las siguientes mejoras
al algoritmo de Gutmann para reducir el alto costo computacional debido a las llamadas de
la función objetivo y que permitirán que la plataforma de procesamiento a diseñar sea
viable:
o Encontrar dentro de las heurísticas conocidas, las apropiadas para la selección de los
puntos de inicio en el método de Optimización Global propuesto por Gutmann.
o Determinar las modificaciones a la interpolación de Funciones de Base Radial en el
algoritmo propuesto por Gutmann cuando los datos recopilados sean afectados por
ruido gaussiano para mejorar la convergencia del método.
o Determinar si se requiere de búsquedas locales en ciertas etapas del algoritmo de
Optimización Global propuesto por Gutmann para obtener reducciones adicionales
en el número de evaluaciones a la función objetivo definida por la superficie de
respuesta.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>Dentro de las Heurísticas conocidas,  ¿Cuáles son las apropiadas para la selección de los puntos de inicio en el método de Optimización Global propuesto por Gutmann?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>Cuando los datos tienen ruido gaussiano con media cero y desviación estándar uno.  ¿Cuáles serán las modificaciones en la interpolación con funciones de base radial cuando los datos recopilados sean afectados por ruido gaussiano?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Qué mejoras se tendrán si se emplean búsquedas locales en algunas etapas del método de Optimización Global con RBF propuesto por Gutmann?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los problemas de optimización industriales, en general, son muy demandantes en cuanto al
uso de los recursos computacionales. En la investigación hecha por Zabinsky [14] se
describe el ambiente donde operan y diseñan las aplicaciones de ingeniería. Estos
ambientes tienen restricciones en los tiempos de entrega de sus productos originadas por los compromisos con los clientes. Los desarrollos propuestos para encontrar las soluciones a los problemas técnicos, particularmente los de optimización, se ven afectados por dichas
restricciones, es necesario disponer de herramientas que permitan atacar y resolver los
problemas en los tiempos establecidos.
Una alternativa que permite superar las restricciones de tiempo de los problemas de
ingeniería es la implementación, en hardware especializado, de los algoritmos numéricos
utilizados. Bajo este enfoque se abre la posibilidad de disminuir el tiempo de ejecución del esquema seleccionado, comparado con una plataforma de procesamiento convencional,
cuando la implementación en hardware es soportada por el estudio detallado del algoritmo
seleccionado. Así, en la puesta en marcha en hardware se busca, usualmente, utilizar los
algoritmos con menor costo computacional.
Por lo anterior es primordial seleccionar el procedimiento que presente un bajo costo
computacional. Los algoritmos deterministas ofrecen la ventaja de tener tal característica, si se comparan con los métodos de exploración estocástica. Está propiedad es más notable cuando la demanda computacional está definida por la forma en que se plantea el problema a resolver. El método propuesto por Gutmann [11][29] pertenece a las estrategias deterministas para OG por lo que lo convierte en un buen candidato para dicha tarea.
El algoritmo de Gutmann utiliza un conjunto de puntos, { , ( )} i i x f x , 1... , m
i i = n x , que son empleados para generar una superficie de interpolación en la cual cada punto de entrada implica una evaluación del modelo f (x) . Al realizar una depuración del conjunto de puntos inicial se reduce el número de evaluaciones de f (x) , por lo que el costo computacional del método disminuirá. Dicha depuración debe realizarse de tal forma que la superficie de interpolación no sea alterada.
Por lo general, los problemas a resolver no son bien comportados. En condiciones reales,
los diseños están sometidos a diversas perturbaciones que alteran el funcionamiento de la
aplicación las cuales son, comúnmente, modeladas como ruido gaussiano. El algoritmo de
Gutmann, de acuerdo a la evidencia encontrada [28], presenta problemas de convergencia
al estar sometido a estas condiciones. Es deseable que el funcionamiento del algoritmo no
sea afectado por el ruido, de lo contrario se corre el riesgo de que el modelo no represente de manera realista el fenómeno bajo consideración.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Este proyecto se desarrollara en cuatro etapas:
Etapa1: Selección de Métodos de OG.
La selección del método de OG es una de las partes más importantes de esta investigación.
De acuerdo al trabajo relacionado, existen diversos métodos de OG que se emplean
frecuentemente. En muchos trabajos se ha buscado el reducir el costo computacional de los
algoritmos de tipo estocásticos y los híbridos; no obstante, se seleccionó el algoritmo de
Gutmann puesto que realiza menos llamadas al modelo matemático. Las actividades que se
realizarán en esta etapa son:
* Hacer una revisión comparativa de los métodos de Optimización Global más
empleados en diversas áreas de aplicación.
* Seleccionar los algoritmos de Optimización Global más empleados para obtener
conocimiento de sus características e indicar el método que será implementado.
Etapa 2: Mejoras al método de OG
Gutmann plantea en su trabajo varias preguntas abiertas en las que explica algunas de las
ventajas de responderlas. Las modificaciones al método mencionadas en la propuesta se
resolverán mediante las siguientes tareas:
* Implementar el algoritmo de ramificación y poda para realizar una selección de los
puntos de inicio del algoritmo de Gutmann.
* Integrar en la implementación del algoritmo de Gutmann diversas funciones de base
radial adicionales y verificar su funcionamiento.
Modificar la implementación del algoritmo de Gutmann que permita configurarlo de
tal manera que se pueda seleccionar el método de optimización interna.
* Modificar la interpolación con Funciones de Base Radial para que el método
soporte modelos con datos ruidosos y realizar pruebas con modelos matemáticos en
condiciones ruidosas para probar la factibilidad de la solución.
* Validar las mejoras del método con respecto a los algoritmos de OG más
empleados.
Etapa 3: Análisis de requerimientos, diseño e Implementación.
La implementación de la plataforma de procesamiento necesita de un análisis previo que
definirá sus parámetros y requerimientos de acuerdo a la última versión del algoritmo de
Gutmann. El análisis del algoritmo permitirá determinar:
* Partición de procesos.
* Nivel de paralelismo.
* Resolución de las variables.
* Requerimientos de Hardware.
* Costo.
El diseño y la implementación de acuerdo al análisis realizado requerirán de la búsqueda de elementos de hardware que permitan una implementación sencilla. No obstante, en este
punto se decidirá que tanto se renuncia a la precisión del sistema en general en la medida de una implementación adecuada.

Etapa 4: Pruebas y Validación.
Cuando la plataforma de procesamiento ya este diseñada e implementada, la etapa final
consistirá en hacer pruebas con diversos modelos de diferentes áreas. Los resultados
obtenidos se validarán comparándolos con una estimación de la solución entregada por el
mismo método en una plataforma de procesamiento convencional.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este documento se presentó la propuesta de investigación que esta enfocada a reducir el
costo computacional del proceso de optimización global. El tipo de problemas considerados
son aquellos en donde el cálculo de la función objetivo consume gran parte del tiempo de
ejecución del algoritmo. De acuerdo a la revisión de la literatura, se determinó que el
algoritmo de optimización global determinístico propuesto por Gutmann es un candidato
potencial para el desarrollo de esta investigación. Características como el uso de la
metodología de superficie de respuesta, el uso de funciones de base radial como
interpolantes, el empleo de una medición de la irregularidad de la superficie interpoladora, entre otras, permiten que el costo computacional sea bajo respecto a los métodos que emplean búsquedas estocásticas y los métodos híbridos (emplean estrategias
determinísticas como estocásticas).
Debido a que el método propuesto por Gutmann es todavía sensible a modificaciones. Se
están proponiendo mejoras de software y de hardware. Las modificaciones que se proponen
buscan reducir el tiempo de ejecución por lo menos en un orden de magnitud. Una de las
mejoras en el algoritmo tiene el objetivo de reducir el número de puntos de inicio del
método seleccionado. Otro de los objetivos es mejorar la convergencia del método ante
situaciones en donde los datos estén perturbados por ruido gaussiano. También para reducir
el costo computacional del método, la selección adecuada del algoritmo de optimización
interna del algoritmo de Gutmann.
La plataforma de procesamiento consiste en la aceleración por hardware del proceso de
encontrar la solución del sistema de ecuaciones que es necesario para encontrar varios
parámetros del método considerado. Para lograrlo, se ha propuesto un arreglo que procese
simultáneamente vectores de datos para resolver los dos sistemas de ecuaciones presentes
en el algoritmo de optimización. La reducción esperada es en un factor k3 en la complejidad del algoritmo, es decir O((n/k)3).

Las pruebas realizadas con los algoritmos de tipo estocástico mostraron como al ir
incrementando el número de dimensiones del problema las llamadas a la función objetivo
crecen en orden de magnitud. Con las pruebas se comprobó que el método recocido
simulado presenta mejores resultados (precisión y número de llamadas de la función
objetivo) que el algoritmo genético empleado. Además de que los algoritmos genéticos no
siempre convergen a la solución deseada.
Se han considerado tres casos de estudio con la finalidad de mostrar las propiedades de los
diferentes tipos de métodos de optimización en comparación con el método seleccionado.
En las pruebas realizadas se considera un error relativo permitido que 10-3. De acuerdo a los
resultados obtenidos de los casos de estudio se ejemplificó como en ciertos casos a pesar de
que el número de llamadas está en el mismo orden de magnitud, el tiempo de ejecución está
fuertemente ligado al modo en como se obtiene la evaluación de la función objetivo. Así
mismo, se comprobó que los métodos de tipo determinístico locales realizan un número
bajo de llamadas a la función objetivo. Sin embargo, este tipo de métodos están limitados a
que el punto de inicio se encuentre cerca de la solución deseada; de no ser así, el método
encuentra soluciones subóptimas.
De las pruebas realizadas con el algoritmo de Gutmann se observó que mantiene bajo el
número de llamadas a la función objetivo (hasta tres ordenes de magnitud respecto a los
métodos estocásticos). El método fue probado con los casos de estudio antes mencionados.
Al emplear cada uno de los casos de estudio se verificó que el método converge en distintas
situaciones: donde la solución se encuentra en regiones donde la convergencia a la solución
es lenta, zonas en donde se encuentran múltiples mínimos locales y donde el cálculo de la
función objetivo es complicada. Se comprobó que trabaja adecuadamente con las funciones
de base radial multicuádrico (posteriormente se trabajará con otras funciones de base
radial). En estos casos, el método encontró la solución deseada con la precisión adecuada.
De acuerdo a los resultados obtenidos en la selección de puntos, con el método de
ramificación y poda, observa que el método selecciona los puntos de inicio de forma
adecuada para el propósito planeado. Con está selección de puntos se reduce de forma cuadrática el número de puntos de inicio del algoritmo de Gutmann. Así, se retarda el
crecimiento del número de parámetros que debe encontrar el algoritmo para trabajar. De
acuerdo a las graficas 7.18 y 7.19, los puntos seleccionados se encuentran cerca del valor
deseado. Lo anterior es importante debido a que al continuar corriendo el algoritmo
propuesto por Gutmann tendera a añadir nuevos puntos en esa zona; en consecuencia,
encontrará la solución deseada.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Propuesta de Tesis Doctoral
Diseño de una Plataforma de Procesamiento Enfocada a Problemas de Optimización Global.
por
Alejandro Ramos Cabrera
INAOE
Coordinación de Ciencias Computacionales
INSTITUTO NACIONAL DE ASTROFÍSICA, ÓPTICA Y ELECTRONICA
Abril 2007
Tonantzintla, Puebla
Supervisada por:
Dr. Gustavo Rodríguez Gómez
Dr. René Armando Cumplido Parra
* © INAOE 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Algoritmos basados en tríos de minucias para la verificación e identificación de huellas dactilares"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>No existe algoritmo de comparación de huellas basado en tríos de minucias que sea
invariante a la traslación, rotación y al orden de las minucias en el rasgo; sensible a
la reflexión de los tríos, a las direcciones de las minucias relativas a los lados y a la individualidad de las huellas; tolerante a las distorsiones no lineales de las huellas, a rasgos ausentes, a la baja calidad de las huellas, así como a errores del extractor de
rasgos; insensibles a la elección de una sola alineación; determinista y que tenga costo
computacional aceptable.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Crear algoritmos de verificación e identificación de huellas basados en tríos de minucias
que tengan elevada eficacia en comparación con los algoritmos del estado del
arte.

Objetivos particulares
1. Crear una forma de representación basada en tríos de minucias, invariante a la traslación y rotación, y sensible a las direcciones de las minucias relativas a los lados.
2. Crear una función de comparación de tríos de minucias, basada en la representaci
ón anterior, tolerante a las distorsiones no lineales de las huellas, invariante al
orden de las minucias en el rasgo y sensible a la reflexión de los tríos.
3. Desarrollar un algoritmo de verificación de huellas basado en la nueva forma de
representación y función de comparación de tríos de minucias, con eficacia superior
a los reportados en la literatura y costos computacionales aceptables. La sección
3.7 describe los protocolos utilizados para la evaluación de la eficacia. Los costos
computacionales aceptables están definidos en la FVC2006 [10].
4. Adaptar el nuevo algoritmo de verificación para aplicarlo en la identificación de
huellas, con eficacia superior a los reportados en la literatura y costos computacionales
aceptables. La sección 3.7 describe los protocolos utilizados para la
evaluación de la eficacia. Consideramos como costos computacionales aceptables,
los definidos en la FVC2006 [10] porque no existe un consenso al respecto en la
comunidad científica internacional y algunos peritos entrevistados hallaron aceptable
esta consideración.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es posible mejorar la calidad de los algoritmos de comparacion de huellas basados en tríos de minucias para elevar la eficacia de la verificacion e identificacion de huellas?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Como representar los tríos de minucias, compararlos y usarlos para la verificacion e identificación de huellas?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. Crear la forma de representación basada en tríos de minucias. En esta etapa se
definirán los rasgos que se incluyen en la forma de representación:
a) Definir los rasgos de los tríos que hagan la representación invariante a la
traslación y rotación.
b) Evaluar la forma de incluir en los rasgos las direcciones de las minucias
relativas a los lados.
c) Evaluar la conveniencia de utilizar el tipo de las minucias y el conteo de crestas
en la nueva representación puesto que su estimación es muy inexacta en
áreas ruidosas, cerca de los puntos focales (núcleo y deltas), y cuando el segmento
sobre el cual se hará el conteo se encuentra parcialmente superpuesto
sobre una cresta [2, 21, 31, 54].
2. Crear la función de comparación de tríos de minucias.
a) Proponer una función que compare los rasgos de manera tolerante a las
deformaciones de las huellas.
b) Evaluar cómo hacer la función de comparación invariante al orden de las
minucias pero sensible a la reflexión de los tríos.
c) Estudiar las propiedades que permiten descartar a priori comparaciones de
tríos para acelerar el proceso.
d) Evaluar experimentalmente la nueva representación y función de comparación de tríos de minucias. Para esto se sustituirán estos componentes en
algoritmos existentes.
3. Desarrollar un algoritmo de verificación de huellas basado en la nueva forma de
representación y función de comparación de tríos de minucias, con eficacia superior
a los reportados en la literatura y costos computacionales aceptables.
a) Evaluar diferentes estrategias para calcular los tríos de minucias. Para esto
se implementarán diferentes esquemas como los vecinos más cercanos y la
triangulación de Delaunay. También se probarán diferentes combinaciones
de esquemas.
b) Proponer un algoritmo para determinar las coincidencias locales. Para esto
se probarán esquemas como: asociar cada minucia con aquellas cuyos correspondientes
tríos tenga similaridad mayor que cero o con aquella con la cual
tenga un mayor valor de similaridad.
c) Definir el algoritmo de alineación. Se probarán varias estrategias como: alinear
usando 3, 5 y 7 pares de tríos más similares, usar todos los pares de
tríos coincidentes o cada trío y su más similar de la otra huella.
d) Proponer un algoritmo para determinar las coincidencias globales. Se probar
án diferentes heurísticas que incluyan la similaridad entre los tríos de
minucias y otras que no usen esta información.
e) Definir la función de similaridad entre las huellas. Esta función puede incluir
la similaridad de los tríos, las minucias coincidentes en el área de solapamiento
de las huellas y el total de minucias en cada huella.
f ) Estudiar posibles optimizaciones para aumentar la rapidez del algoritmo.
Para esto se estudiarán las propiedades de los tríos de minucias y se probarán
diferentes estrategias de indexación como son la clasificación de los triángulos
según sus lados y sus ángulos. También se estudiará la posibilidad de crear
otras estrategias que estén relacionadas con las direcciones de las minucias
relativas a los lados y la diferencia entre las direcciones de las minucias.
g) Evaluar experimentalmente la eficacia del nuevo algoritmo de verificación. Se
comparará contra otros algoritmos existentes en bases de datos de competencias
de verificación a nivel internacional. Para esto se usarán los protocolos
aceptados internacionalmente [55] cuya descripción aparecen en la sección
3.7.

4. Adaptar el nuevo algoritmo de verificación para aplicarlo en la identificación de
huellas, con eficacia superior a los reportados en la literatura y costos computacionales
aceptables.
a) Evaluar diferentes estrategias para calcular los tríos de minucias teniendo
en cuenta que en los problemas de identificación las huellas son parciales,
la información de las crestas es de baja calidad y los extractores de rasgos
omiten o adicionan información falsa.
b) Proponer un algoritmo para determinar las coincidencias locales. Los errores
del extractor de rasgos provocan que toda la información sea menos fiable y se necesite considerar más coincidencias locales que en los problemas de
verificación.
c) Adaptar el algoritmo de alineación para las restricciones de calidad de las
huellas en los problemas de identificación.
d) Adecuar el algoritmo para determinar las coincidencias globales.
e) Ajustar la función de similaridad entre las huellas al contexto de la identificaci
ón.
f ) Estudiar posibles optimizaciones para aumentar la rapidez del algoritmo.
g) Evaluar experimentalmente la eficacia del nuevo algoritmo de identificación.
Se comparará contra otros algoritmos existentes en bases de datos propias.
Para esto se usarán los protocolos aceptados internacionalmente [7] cuya
descripción aparecen en la sección 3.7.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los algoritmos de comparación de huellas basados en tríos de minucias tienen una
eficacia y rapidez competitiva, y a veces superior, en comparación con otros tipos de
algoritmos de comparación. Además, tienen la ventaja de poderse usar tanto en los
sistemas basados en arquitecturas ligeras como en los sistemas basados en estándares de
almacenamiento de huellas representadas por minucias, tecnologías cuya popularidad ha
aumentado en los últimos años. No obstante, los algoritmos propuestos en la literatura
presentan varias limitaciones que degradan su eficacia.
Como avances preliminares de la investigación se propone una nueva forma de representaci
ón y función de comparación de tríos de minucias invariantes a la traslación y
rotación de las huellas. La nueva forma de representación es sensible a las direcciones de las minucias relativas a los lados y ordena las minucias de acuerdo a las manecillas del reloj, así en la comparación se prueban las tres posibles rotaciones de los tríos logrando la invarianza al orden de las minucias en el rasgo y la sensibilidad a la reflexión de los tríos. Los componentes de las tuplas que describen a los tríos son comparados usando umbrales que permiten la tolerancia a las distorsiones no lineales de las huellas. Además, la función de comparación propuesta incluye optimizaciones que evitan comparar todos los rasgos, aumentando en muchos casos la velocidad de comparación.
Un algoritmo basado en tríos de minucias que cumple con la mayor cantidad de
parámetros de calidad es modificado adicionándole la nueva forma de representación y
función de comparación propuestas. Experimentaciones en las bases de datos de la competencia FVC2004 muestran que el algoritmo modificado supera en eficacia al algoritmo
original y a otros tres algoritmos propuestos en la literatura.
Como trabajo futuro proponemos crear un algoritmo de verificación y otro de identificación basados en la nueva forma de representación y función de comparación de
tríos de minucias, que tengan eficacia superior a los propuestos en la literatura.
Basados en estos resultados preliminares podemos concluir que nuestros objetivos
son alcanzables siguiendo la metodología propuesta.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INAOE
Algoritmos basados en tríos de minucias para la verificación e identificación de huellas dactilares
Propuesta de Investigación Doctoral
M.C. Miguel Angel Medina Pérez
Asesores:
Dr. Leopoldo Altamirano Robles
Dr. Milton García Borroto
Ciencias Computacionales
INAOE, Puebla
2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Detección automática del decaimiento de habilidades motrices en la marcha de adultos mayores"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Problemática
Hasta donde sabemos, la toma de decisión sobre el tratamiento a seguir para una persona
con problemas de estabilidad es realizada por personas expertas en el dominio, y esta
decisión es hecha con base en la experiencia de los expertos apoyada con los datos
obtenidos por los instrumentos utilizados para realizar el análisis clínico de la marcha Hasta donde se sabe, la identificación de los parámetros relevantes y las relaciones entre
ellos que permitan detectar la pérdida de estabilidad, la cual implique un aumento en la
probabilidad de sufrir una caída en personas adultas no están definidos. Es por eso que se
propone el uso de técnicas computacionales para abordar la problemática del modelado y
determinación del tipo de riesgo de caída de un individuo, de acuerdo a los cambios
detectados en la marcha de éste.
En esta investigación nos centraremos en la predicción. La predicción es el proceso por el
cual el comportamiento futuro de un sistema es estimado con base en la caracterización de
éste. La tarea de predicción es compleja y difícil debido a múltiples razones, tales como:
alta sensibilidad a las condiciones iniciales en el sistema modelado, dificultad en la
determinación de las tendencias o en el reconocimiento de patrones en presencia de ruido
en los datos, para ello se propone el uso de modelos que permitan representar datos
temporales e inciertos.
Para el análisis del decaimiento de habilidades motrices es necesaria una representación
adecuada de la dependencia del tiempo con respecto a cambios en los parámetros
asociados a ser factores de riesgo de caída. Dado que los cambios pueden ocurrir en
distintos tiempos para diferentes personas, la inferencia del riesgo debería de ser hecha en
términos de los cambios que han ocurrido en el pasado o que podrían ocurrir en el futuro.
Por ejemplo, un cambio puede ocurrir más rápido en una persona que en otra y la
información acerca de la ocurrencia del cambio o la rapidez con la que está cambiando un
parámetro ayudarían a determinar si el riesgo de caída está asociado a una degradación
patológica o a una degradación normal de la marcha. Así, la inferencia del riesgo no
debería de estar sujeta a un determinado tiempo, sino al tiempo en el que el cambio ha
ocurrido.
Dado que los datos en dominios médicos como el nuestro son afectados por diferentes
fuentes de incertidumbre, e. g., errores de medición, variabilidad de los individuos dentro
del estudio, consideramos un modelo probabilístico, que permita una representación de
dichos datos así como la inferencia a partir de estos, en particular una EBN.
El problema principal es que la determinación de la estructura del modelo está
estrechamente ligada al conocimiento de los expertos en el dominio, pero existen dominios
como el nuestro en donde no se conoce con certeza la información necesaria para la
construcción del modelo, pues es necesario conocer las variables relevantes así como las
relaciones entre ellas y cómo los cambios de éstas afectan al aumento o disminución de
presentar un determinado evento, y finalmente saber la rapidez con la que está cambiando
las variables del problema y cómo esta rapidez afecta al riesgo de presentar un evento.
Es por eso que un reto es la construcción automática de un modelo a partir de técnicas de
análisis de los datos que nos permitan complementar así como corroborar el conocimiento
de los expertos en el dominio, para determinar cuáles son variables más significativas, las relaciones temporales entre ellas, y para determinar la rapidez de cambio y el efecto que tiene en un evento futuro de interés.de una persona, como son cámaras, sensores electromagnéticos, etc.

El problema principal para la construcción del modelo es que no se conocen las relaciones
entre las variables, ni en qué proporción están afectando el riesgo de presentar un evento de
interés. Aunado a lo anterior se tienen limitaciones por disponer en estos estudios de pocos
datos y muchas variables, lo cual afecta la construcción de un modelo a partir de dicha
información. Así, se tiene el problema de determinar cuáles son las variables más
significativas, cómo están relacionadas entre ellas y cómo afectan la aparición de futuros
cambios relacionados con la degradación de la marcha patológica.
Después de lo expuesto antes, optamos por el uso de EBNs pues este tipo de red nos
permite modelar e inferir con respecto a la ocurrencia de eventos, y aun más nos permite
saber qué tan rápido puede cambiar una variable y cómo la rapidez de ese cambio afecta a
otras variables, y finalmente determinar qué consecuencias y en qué tiempo aproximado
tendrán lugar, de acuerdo a los eventos registrados y a los tiempos en los que ocurrieron
estos. De este modo podemos saber si casos específicos presentan una degradación
patológica de la marcha o una degradación normal de la marcha.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
* El objetivo general de este trabajo es detectar y representar el decaimiento de
habilidades motrices en la marcha asociadas a la pérdida de estabilidad en adultos
mayores, así como determinar la probabilidad de riesgo de caída de una persona a lo
largo de un periodo de tiempo.
Objetivos específicos
* Representación de las variables de la marcha de personas adultas mayores.
* Representación de la evolución en el tiempo y las relaciones entre las variables de la marcha de personas adultas mayores.
* Aplicación de técnicas que permitan trabajar con pocos datos y muchas variables.
* Construcción automática de un modelo que represente datos de la marcha humana y
permita detectar cambios tempranos asociados con la pérdida de estabilidad, los cuales incrementen el riesgo de sufrir una caída, a partir de datos temporales
biomédicos proporcionados por los expertos del dominio.
* Desarrollo de métodos que permitan la construcción automática de un modelo
basado en una EBN de la marcha.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cómo detectar y representar los cambios en la marcha humana los cuales den indicio del decaimiento de habilidades motrices relacionado con pérdida de estabilidad, y en específico con el aumento de la probabilidad de sufrir una caída, a partir de datos biomédicos obtenidos con instrumentos para análisis de la marcha?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Cómo identificar los atributos relevantes, así como las relaciones entre ellos, del
conjunto de los datos biomédicos obtenidos con instrumentos para análisis de marcha que indican el decaimiento de las habilidades mencionadas?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Cómo modelar el cambio de la probabilidad de riesgo de caída, a partir del análisis de las tendencias de las variables en el tiempo?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Cómo construir un modelo de la marcha de manera automática a partir de los datos?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>A continuación se describen los métodos y técnicas a utilizar para alcanzar el objetivo
planteado.
1. Caracterización de datos biomédicos
1.1. Adquirir los datos de la marcha de personas adultas mayores, los cuales serán
proporcionados por el laboratorio de Análisis de Movimiento del INR de la ciudad
de México. Durante el análisis de la marcha los datos involucrados con la manera
de caminar de una persona son obtenidos y almacenados. Dado que en nuestro caso
estamos interesados en la detección de decaimiento, es necesario tener registros de
los datos de la marcha de diferentes personas en diferentes momentos.
1.2. Caracterizar los tipos de datos, los instrumentos con que son obtenidos, los rangos
de acción, los tiempos en que han sido tomados y el tipo de personas a quienes se
les ha dado seguimiento.
1.3. Trabajar periódicamente con los investigadores del INR para obtener información
del tipo de datos que nos están proporcionando.
2. Análisis y modelado de los datos
2.1. Revisar y seleccionar las técnicas para el modelado de datos biomédicos que
permitan representar y detectar cambios en los datos, así como técnicas que
proporcionen una medida de los cambios en el riesgo de presentar un determinado
evento como consecuencia de los cambios detectados con el modelado de los datos.
2.2. Probar las técnicas seleccionas sobre los datos de la marcha proporcionados por el
INR. Lo anterior es importante pues nos permitirá tener evidencia de la eficacia y
de las limitantes de las técnicas seleccionadas al comparar la clasificación de este
modelo con la clasificación de expertos, y así tener una base con la cual decir si las
técnicas utilizadas son suficientes para resolver la problemática planteada.
2.2.1. Prueba de técnicas de análisis de sobrevivencia.
2.2.2. Prueba de técnicas de Inteligencia artificial.
2.3. Proponer un modelo que permita considerar el cambio de los datos a través del
tiempo, así como la influencia que tiene dicho cambio en el riesgo de presentar un
determinado evento, y permita proporcionar una medida del riesgo asociado a una
degradación patológica de la marcha a lo largo de un periodo de tiempo. En esta
parte se encuentra una de las principales contribuciones de nuestro trabajo,
pues hasta donde sabemos no existen trabajos reportados que hagan una
representación de la evolución de la marcha humana y que permita evaluar el
riesgo de caída de una persona.
2.4. Proponer un método para la construcción automática del modelo anterior, esto es
deseable debido a que como hemos mencionado, las diversas opciones para
modelar dependen exclusivamente de la experiencia de los expertos en el dominio,
pero existen dominios como el nuestro en donde los expertos no tienen seguridad
de la información necesaria para la construcción de un modelo de la marcha
patológica. Para ello es necesario lo siguiente:
2.4.1 Determinación de atributos relevantes así como las relaciones entre ellos
2.4.1.1. Utilizar el conocimiento a priori proporcionado por los expertos en el
dominio para reducir el número de parámetros a considerar en el análisis.
2.4.1.2. Utilizar técnicas de selección de atributos discutidos en la literatura
especializada, donde encontramos dos métodos: los empacados (wrappers) y los filtros, [27], [54].
2.4.1.3. Utilizar técnicas de selección de atributos relevantes de datos temporales [34], [40] y evaluar la efectividad del uso de los parámetros encontrados como relevantes con dichas técnicas de selección.
2.4.1.4. Proponer técnicas de selección de atributos en datos temporales
2.4.1.4.1. Comparar clasificación aplicando los puntos de la
metodología 4.4.1.2 y 2.4.1.3.
2.4.1.4.2. Seleccionar aquellas variables que sean identificadas que influyen en el aumento del riesgo de caída, utilizando análisis de sobrevivencia.
2.4.1.4.3. Identificar cuáles son las variables que cambian la marcha de
personas que tienen un decaimiento normal y descartar dichas variables como significativas para determinar un decaimiento patológico asociado al aumento de riesgo de caída.
2.4.2 Determinación de la rapidez con la que ocurren los cambios Aplicación de técnicas de análisis de sobrevivencia
2.4.3 Determinación del efecto en el riesgo de los cambios en los atributos Aplicación de técnicas de análisis de sobrevivencia.
2.4.4 Determinación de los parámetros del modelo Aplicación de técnicas de aprendizaje paramétrico
2.4.5 Determinación de los intervalos Análisis de las graficas de la función sobrevivencia

3. Identificación de atributos relevantes
La necesidad de la reducción de atributos surge debido a que se tiene una gran cantidad
de atributos, de un orden mayor a 30, provenientes del análisis clínico de la marcha
considerando el número de casos de estudio; alrededor de 18 actualmente. Por ello, el
tratamiento de los datos para los algoritmos de minería tradicionales es costoso computacionalmente. El determinar aquellos atributos relevantes de la marcha para
detección de pérdida de estabilidad ayudará a la adquisición y análisis de un subconjunto de atributos.
3.1. Utilizar técnicas de réplica de datos, esto es, aumentar el número de casos
utilizando oversampling, para tener más casos y así evitar la degradación de tener muchos atributos contra pocos datos.
3.2. Hacer selección de atributos como alternativa al uso de oversampling, para tener pocos datos con pocos atributos.
3.2.1. Uso del punto 2.4.1 de la metodología.
3.3. Proponer métodos para la construcción de modelos con pocos datos y muchas variables
3.3.1. Combinación de oversampling con selección de atributos.
3.3.1.1. Utilizar oversampling sobre los datos que tenemos y posteriormente
hacer selección de atributos.
3.3.1.2. Realizar una selección de atributos inicial sobre los datos y posteriormente hacer oversampling.
4. Evaluación
4.1. Comparar la eficacia del modelo con respecto a técnicas de análisis de sobrevivencia.
4.2. Comparar la eficacia del modelo obtenido, con respecto a modelos similares
construidos con ayuda de los expertos en el dominio.
4.3. Calcular la precisión en la clasificación del modelo con los datos iniciales,
utilizando validación cruzada.
4.4. Calcular la precisión del modelo propuesto con datos de expedientes cerrados. Es
decir, datos distintos a los proporcionados inicialmente.
7. Trabajo realizado y resultados preliminares
Los resultados obtenidos hasta hoy consisten en lo siguiente:
* Caracterización de una parte de los datos de la marcha de personas adultas mayores.
* Selección de las técnicas a utilizar.
* Evaluación de las técnicas seleccionadas con los datos caracterizados.
* Construcción automática de un primer modelo basado en una EBN.
7.1 Caracterización de los datos.
Actualmente se está colaborando con los investigadores del Laboratorio de Análisis del
Movimiento del INR, con el fin de obtener las bases de datos del análisis de la marcha de
personas adultas mayores, concretamente mujeres de entre 50 a 70 años con diagnóstico de
osteoporosis, y algunas con registro de caídas previas al inicio del estudio. Esos datos se han registrado durante un estudio conducido por los investigadores del INR durante un periodo de 3 años</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Hasta el momento se ha probado la factibilidad del uso de técnicas de análisis de
sobrevivencia en específico el modelo de regresión de Cox, al igual que técnicas de IA, en
específico BN, para modelar los datos de la marcha de personas adultas mayores. Los
resultados muestran que ambas técnicas permiten obtener información útil para los
expertos en el dominio aunque dicha información es proporcionada de manera separada y
es obtenida de manera limitada pues se está trabajando con datos restringidos debido a los
requerimientos de los modelos.
También hemos corroborado la necesidad de técnicas tales como selección de variables
pues tenemos pocos datos con muchas variables. Como se ha mostrado, sólo se ha trabajado
con pocos datos pero al tener más datos se espera mejorar los modelos presentados en la
sección anterior.
Hasta el momento sólo se ha trabajado con datos de un año de seguimiento, es decir dos
registros de datos de la marcha. Ambos modelos se usaron para determinar si un nuevo caso
analizado tendría caída, con base en la información de los datos de la marcha.
Para ambos modelos se consideró únicamente si existió un cambio en algún parámetro de la
marcha de un registro a otro, no se sabía si el cambio detectado estaba asociado al aumento o disminución del parámetro estudiado. Esto hace suponer que se está perdiendo
información relevante para determinar las relaciones que puedan existir entre parámetros de la marcha asociados con el aumento en el riesgo de caída, es por eso que una de las
contribuciones de este trabajo es incorporar en el modelo utilizado la representación de este tipo de cambios.
Una alternativa para considerar la información para el modelo de regresión de Cox es el uso de variables adicionales asociadas al aumento, decremento o ningún cambio de un
parámetro, esto traería consigo un aumento considerable en el número de variables y ello
implicaría un mayor número de muestras para entrenar el modelo. Otra alternativa sería
utilizar representación de los cambios de los datos como variables binarias con información sobre el tipo de cambio que experimentó una variable. Por otro lado, esta situación es menos problemática para el modelo con una Red Bayesiana pues cada variable puede tener más de un valor y por lo tanto el número de nodos en el modelo no se modifica.
Uno de los objetivos de este trabajo es la representación de los cambios de la marcha asociados al aumento de riesgo de caída, que hasta el momento no se ha realizado. Para ello se aplicarán redes de eventos, las cuales permiten modelar cómo se relacionan la ocurrencia de eventos iniciales con eventos futuros. Una de las aportaciones de este trabajo es el diseño de un modelo que permita determinar los cambios en determinados parámetros de la marcha y la relación que tienen en el cambio de otros parámetros que inicialmente no se detectaban como indicativos de sufrir una caída. También dicho modelo permitirá determinar el aumento del riesgo de acuerdo a los cambios de la marcha. Y aún más como contribución principal de este trabajo se prevé la construcción de dicho modelo a partir de los datos.
De acuerdo a los resultados obtenidos, el modelo de regresión de Cox permite obtener
probabilidades de riesgo de caída de acuerdo al cambio que tienen las variables predictivas en un periodo de tiempo, aunque como se ha mencionado la limitante es que sólo se considera presencia o ausencia de dichas variables, y además no se permite modelar
relaciones entre las variables. Por otro lado, las BN permiten el modelado de relaciones
entre variables además de considerar no sólo variables binarias. De tal forma que ambas
técnicas pueden ser tratadas como técnicas complementarias para la construcción de un
modelo basado en EBN, pues una EBN permite un modelado de las relaciones entre
variables no binarias y además obtener una medida del riesgo de presentar un evento. El
reto en esto es que el diseño de dicho tipo de BN no se realiza de manera automática y
depende en su mayoría de la experiencia de los expertos en el dominio, y esto es una
desventaja en dominios en los que los expertos no tienen certeza o desconocen por
completo información fundamental para el diseño de un modelo, como el de la marcha
patológica, información como tipos de cambios, relaciones entre variables, efecto del
cambio de una variable en el riesgo de presentar un evento de interés.
Los resultados mostrados hasta el momento dan evidencia que es posible realizar la
construcción de un modelo basado en EBN que permite inferir el riesgo de caída de
acuerdo a los cambios detectados en la marcha. Se descubre de manera automática a partir
de los datos el modelo, y se puede recurrir a los expertos para validar y complementar este modelo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Detección automática del decaimiento de habilidades motrices en la marcha de adultos mayores
German Cuaya Simbro, Angélica Muñoz Meléndez,
Eduardo F. Morales Manzanares
Reporte Técnico No. CCC-10-003
16 de marzo de 2010
© 2010
Coordinación de Ciencias Computacionales
INAOE
Luis Enrique Erro 1
Sta. Ma. Tonantzintla,
72840, Puebla, México.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Búsqueda de Patrones Emergentes Extendidos en Problemas con Datos Mezclados e Incompletos"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Extender el concepto de patrones emergentes para dotarlo de una mayor capacidad
expresiva que permita enunciar propiedades más generales que los patrones emergentes
actuales para alcanzar mayor eficacia de clasificación. Encontrar dicho conjunto de
patrones emergentes extendidos en un conjunto de entrenamiento con DMI.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
Diseñar un método para extraer patrones emergentes de mayor capacidad expresiva, a
partir de un conjunto de entrenamiento con datos mezclados e incompletos, para obtener
clasificadores más eficaces que los existentes.

Objetivos particulares
Extender el lenguaje de representación de los patrones emergentes para poder expresar
propiedades más generales.
Diseñar un método de extracción de patrones emergentes con el lenguaje extendido en
bases de datos con DMI. 
Proponer un método de evaluación y filtrado de patrones emergentes que permita su
selección y pesado.
Obtención de métodos o heurísticas de estimación de buenos parámetros para los métodos propuestos.
Diseñar de un nuevo clasificador basado en patrones emergentes con eficacia superior a los clasificadores existentes.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Seleccionar, recopilar y analizar críticamente la bibliografía pertinente.
Extender el lenguaje de representación de patrones emergentes utilizado hasta el
momento para incluir propiedades como:

AtributoNoNumérico =v o AtributoNoNumérico != v
AtributoNumérico &#60;&#61; v o AtributoNumérico > v
AtributoNoNumérico =  v1 o AtributoNoNumérico =  v2 ... o
p AtributoNoNumérico =  vp

AtributoNumérico[v1,v2]
AtributoNoNumérico {v1,v2,...,vp}

Propiedades de este tipo fueron utilizadas en la definición de l -complejo de
Michalsky [37] para la construcción de conceptos.
Proponer un nuevo método de extracción de los patrones extendidos en matrices con
DMI:
Creación de un nuevo algoritmo de extracción de patrones emergentes extendidos en
bases de datos con datos mezclados e incompletos. Para esta tarea se realizará:
Estudio crítico de los métodos de extracción de patrones emergentes existentes.
Proponer un nuevo algoritmo para encontrar EP extendidos.
Evaluación de resultados mediante la realización de pruebas y comparación de
resultados. Retroalimentación.
Definir métodos o heurísticas para determinar cuándo un conjunto de EP es
representativo de una base de datos. Esto puede permitir detener anticipadamente el
proceso de búsqueda de EP, ahorrando tiempo de procesamiento. Para este punto se parte del presupuesto de que existe mucha redundancia en el conjunto de todos los EP, por lo que
un subconjunto puede representarlo sin pérdida de información. Para ello se realizará:
Estudio crítico de las medidas de calidad de EP existentes.
Proponer un nuevo método o heurística que permita determinar cuándo un conjunto de
EP es representativo de una base de datos.
Evaluación de resultados mediante la realización de pruebas y comparación de
resultados. Retroalimentación.
Buscar criterios de calidad de los EP encontrados, y aplicarlos al filtrado de EP postextracción.
Existen muchas medidas para evaluar la calidad de un EP [6, 14] que pueden
ser utilizadas para el filtrado de los patrones, o para la asignación de un peso a cada uno
para clasificar. Por la importancia de este tema, así como el impacto directo que tiene en la
calidad de un clasificador basado en EP se realizará:
Análisis crítico de las formas existentes de evaluar la calidad de un EP.
Evaluación del desempeño de cada medida de calidad en la clasificación de bases de
datos de prueba. En este paso se utilizará el clasificador propuesto por Fan y
Ramamohanarao [12] utilizando el subconjunto de los mejores EP evaluados por cada
medida o todos los EP con el peso calculado con dicha medida.
Desarrollo de nuevos criterios de calidad para patrones emergentes, que permitan
realizar una selección de los mejores o una asignación de pesos al clasificar.
Evaluación de resultados mediante la realización de pruebas y comparación de
resultados. Retroalimentación.
d) Buscar una estrategia de cálculo automático de los parámetros de extracción
de los EP. El concepto de EP incluye la expresión "soporte
significativamente superior" en una clase que en las demás. La forma
habitual de expresar numéricamente está propiedad es mediante el uso de
umbrales. En la mayoría de los trabajos previos estos umbrales son fijos o
definidos por el usuario. En este trabajo buscaremos un algoritmo adaptativo
de búsqueda inicial y refinamiento de estos umbrales. Para ello se realizará:
Estudio de las características de un problema de clasificación específico que pueden ser
útiles para la estimación de valores de los umbrales cercanos a los deseados.
Construcción de un algoritmo adaptativo de refinamiento de los valores de los
umbrales.
Evaluación de resultados mediante la realización de pruebas y comparación de
resultados. Retroalimentación.
Definir estrategias para clasificar los objetos en los que nuestro clasificador se
abstiene. El problema de la abstención al clasificar no ha sido muy estudiado en la literatura
de reconocimiento de patrones. La mayoría de los clasificadores definen una estrategia de
que hacer si no poseen evidencia que les permita distinguir la clase de un objeto dado. La
asignación de la clase mayoritaria o una elección al azar son las más utilizadas. No
obstante, en los trabajos de clasificación con patrones emergentes no se ha abordado este
problema, teniendo estos clasificadores según nuestros experimentos mayor tendencia a
abstenerse.
del presupuesto de que existe mucha redundancia en el conjunto de todos los EP, por lo que
un subconjunto puede representarlo sin pérdida de información. Para ello se realizará:
Estudio crítico de las medidas de calidad de EP existentes.
Proponer un nuevo método o heurística que permita determinar cuándo un conjunto de
EP es representativo de una base de datos.
Evaluación de resultados mediante la realización de pruebas y comparación de
resultados. Retroalimentación.
Buscar criterios de calidad de los EP encontrados, y aplicarlos al filtrado de EP postextracción.
Existen muchas medidas para evaluar la calidad de un EP [6, 14] que pueden
ser utilizadas para el filtrado de los patrones, o para la asignación de un peso a cada uno
para clasificar. Por la importancia de este tema, así como el impacto directo que tiene en la
calidad de un clasificador basado en EP se realizará:
Análisis crítico de las formas existentes de evaluar la calidad de un EP.
Evaluación del desempeño de cada medida de calidad en la clasificación de bases de
datos de prueba. En este paso se utilizará el clasificador propuesto por Fan y
Ramamohanarao [12] utilizando el subconjunto de los mejores EP evaluados por cada
medida o todos los EP con el peso calculado con dicha medida.
Desarrollo de nuevos criterios de calidad para patrones emergentes, que permitan
realizar una selección de los mejores o una asignación de pesos al clasificar.
Evaluación de resultados mediante la realización de pruebas y comparación de
resultados. Retroalimentación.
d) Buscar una estrategia de cálculo automático de los parámetros de extracción
de los EP. El concepto de EP incluye la expresión "soporte
significativamente superior" en una clase que en las demás. La forma
habitual de expresar numéricamente está propiedad es mediante el uso de
umbrales. En la mayoría de los trabajos previos estos umbrales son fijos o
definidos por el usuario. En este trabajo buscaremos un algoritmo adaptativo
de búsqueda inicial y refinamiento de estos umbrales. Para ello se realizará:
Estudio de las características de un problema de clasificación específico que pueden ser
útiles para la estimación de valores de los umbrales cercanos a los deseados.
Construcción de un algoritmo adaptativo de refinamiento de los valores de los
umbrales.
Evaluación de resultados mediante la realización de pruebas y comparación de
resultados. Retroalimentación.
Definir estrategias para clasificar los objetos en los que nuestro clasificador se
abstiene. El problema de la abstención al clasificar no ha sido muy estudiado en la literatura
de reconocimiento de patrones. La mayoría de los clasificadores definen una estrategia de
que hacer si no poseen evidencia que les permita distinguir la clase de un objeto dado. La
asignación de la clase mayoritaria o una elección al azar son las más utilizadas. No
obstante, en los trabajos de clasificación con patrones emergentes no se ha abordado este
problema, teniendo estos clasificadores según nuestros experimentos mayor tendencia a
abstenerse.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los clasificadores basados en patrones permiten clasificar objetos con calidad
competitiva y a veces superior a otros clasificadores, obteniéndose con ellos modelos fácilmente legibles e interpretables por los usuarios. No obstante su eficacia está
estrechamente relacionada con la calidad de los patrones que utilizan.
El proceso de extracción automática de patrones ha estado desde sus inicios limitado
por el costo computacional de las búsquedas exhaustivas. Para ello se han introducidos
numerosas soluciones, pero esto se ha hecho al coste de limitar el lenguaje de
representación de los patrones.
Como avances preliminares de la investigación proponemos una extensión al lenguaje
de representación de patrones emergentes, que permite expresar propiedades más generales,
y un algoritmo para encontrarlos en bases de datos con DMI. También se propone una
nueva forma de ponderar los votos emitidos por los patrones al momento de clasificar. Las
experimentaciones muestran que un clasificador simple basado en estos patrones supera a
los clasificadores contra los que se comparó en varias bases de datos de prueba.
Basados en estos resultados preliminares podemos concluir que nuestros objetivos son
alcanzables siguiendo la metodología propuesta</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Búsqueda de Patrones Emergentes Extendidos en Problemas con Datos Mezclados e Incompletos
Milton García-Borroto, J. Francisco Martínez-Trinidad,
José Ruiz-Shulcloper
Reporte Técnico No. CCC-08-002
27 de Febrero de 2008
© 2008
Coordinación de Ciencias Computacionales
INAOE
Luis Enrique Erro 1
Sta. Ma. Tonantzintla,
72840, Puebla, México.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Optimización global de coherencia en la desambiguación del sentido de las palabras"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Mejorar el desempeño de los métodos para la desambiguación del sentido de las palabras
basados en la aplicación directa del diccionario de sentidos, a través de aplicación de
mejores métodos de búsqueda de combinación óptima de sentidos en un rango de texto.
Objetivos específicos
1. Buscar y/o desarrollar nuevos métodos de optimización global que más se adecuen al
problema de WSD basada en Lesk y sus variantes.
2. Identificar ventajas y desventajas de estos métodos
3. Utilizar éstos para la desambiguación del sentido de las palabras
4. Evaluar los resultados obtenidos
5. Modificar el algoritmo original de Lesk para la obtención de mejores resultados.
6. Desarrollo e implementación de una variante del algoritmo de Lesk, modificando su
naturaleza lingüística.
7. Evaluación del método modificado de Lesk.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En este trabajo se utilizaron como algoritmos para desambiguación de sentidos de
palabras Lesk Completo y Lesk Simple, además del uso de dos estrategias de back-off;
sentido aleatorio y sentido más frecuente.
Para llevar a cabo la desambiguación, los métodos tipo Lesk requieren el uso de
un diccionario de sentidos, en nuestro caso, el diccionario utilizado fue WordNet.
La evaluación se hizo sobre el corpus etiquetado sintética y semánticamente
Senseval-2 English all-words.
La medida de similitud utilizada por ambos algoritmos es la medida original de
Lesk, traslape.
La ventana de contexto para ambos algoritmos es la oración.
Para la optimización del método de Lesk Completo se utilizó un método de optimización conocido como Algoritmos con Estimación de Distribuciones, los parámetros de dicho algoritmo se presentan a continuación.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este capítulo se describió la metodología utilizada para llevar a cabo evaluación sobre
los métodos tipo Lesk para desambiguación del sentido de las palabras. Se presentaron
los resultados para los métodos propuestos y se hizo un análisis sobre ellos.
Se presentó la comparación de los resultados obtenidos para los métodos
propuestos con los resultados reportados en el estado del arte.

A continuación se describen las conclusiones a las que llegamos con este trabajo:

1. El uso de más información en las definiciones del sentido de una palabra, se
ve reflejado en el desempeño de los métodos para desambiguación del sentido
de las palabras basados en la aplicación directa del diccionario de sentidos.
2. La estrategia de back-off influye en el rendimiento de los sistemas para
desambiguación del sentido de las palabras.
3. Dos métodos en cadena pueden dar mejor resultado que por separado.
4. Una estrategia de back-off con buena precisión mejora el funcionamiento de
un método con baja cobertura.
5. En la evaluación de métodos para desambiguación del sentido de las palabras,
no se trata solo de precisión, sino también de cobertura.
6. El método de Lesk Completo (optimizado con EDA) en comparación a Lesk
Simple es mejor cuando se utiliza una estrategia de back-off a sentido
aleatorio.
7. El método de Lesk Simple en comparación con Lesk optimizado es mejor
cuando se utiliza con estrategia de back-off a sentido más frecuente, debido a
la baja cobertura de Lesk Simple y la alta precisión de sentido más frecuente.
8. Al evaluar métodos no supervisados para desambiguación del sentido de las
palabras no se debe utilizar sentido más frecuente como estrategia de back-off,
ya que ésta es una técnica supervisada.
9. El uso de un mejor método de optimización, en este caso, Algoritmos con
Estimación de Distribuciones, mejora los resultados de Lesk Completo en
comparación con los de Lesk Simple, a diferencia de lo antes reportado en el
estado del arte.
10. Los resultados del método propuesto, Lesk Simple con back-off a Lesk
optimizado son muy similares a los que se obtienen para Lesk optimizado, ya
que, al ser tan baja la cobertura de Lesk Simple, Lesk optimizado decide la
mayoría de los casos.
11. El método propuesto de Lesk Simple modificado muestra mejores resultados en comparación con los de Lesk Simple.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Laboratorio de Lenguaje Natural
Optimización global de coherencia en la desambiguación del sentido de las palabras
DOCTORADO EN CIENCIAS DE LA COMPUTACIÓN
SULEMA TORRES RAMOS
Director:
Dr. Alexander Gelbukh
México, D.F.
Diciembre, 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Descripción y evaluación de un sistema basado en reglas para la extracción automática de contextos definitorios"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivos
El objetivo general de esta tesis es desarrollar una metodología para
la extracción automática de CDs en un corpus anotado
morfosintácticamente.
Para ello se han planteado los siguientes objetivos particulares:
1. Revisión de los principales conceptos involucrados en la
investigación desde una perspectiva terminográfica: contexto
definitorio, término, definición y patrón definitorio.
2. Revisión de estudios previos y trabajos en curso relacionados
con la extracción automática de conocimiento definitorio.
3. Búsqueda, análisis y evaluación de patrones verbales
recurrentes en CDs en español.
4. Desarrollo de una metodología para la extracción automática
de CDs.
5. Evaluación de la metodología propuesta para la extracción
automática de CDs.

Objeto de estudio
El objeto de estudio de esta tesis son CDs que incluyen algún patrón
verbal definitorio asociado a alguno de los siguientes tipos de
definición: analítica, extensional, funcional y sinonímica.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Hipótesis
Esta investigación parte de la premisa de que en textos
especializados se pueden extraer automáticamente CDs en los
cuales los autores definen términos mediante el uso de patrones
definitorios. A partir de esta premisa las hipótesis aquí planteadas
son:
1. Los candidatos a CDs pueden ser extraídos automáticamente a
partir de la búsqueda de las ocurrencias de patrones definitorios.
2. Es posible filtrar automáticamente excepciones en las ocurrencias encontradas.
3. Se pueden identificar automáticamente los elementos
constitutivos de los candidatos a CDs. En esta identificación,
a su vez, es importante notar lo siguiente:
3.1 Los elementos constitutivos, es decir términos y definiciones, suelen seguir patrones de formación sintáctica.
3.2 Los elementos constitutivos suelen ocupar una posición
recurrente dependiendo del patrón definitorio que los
conecta.
4. Se pueden clasificar automáticamente los resultados obtenidos
a partir de reglas lingüísticas para determinar los mejores
candidatos a CDs</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este capítulo presentamos las conclusiones de esta tesis. Para
ello comenzaremos con una recapitulación general de lo expuesto
donde retomaremos también algunas ideas expuestas en las
hipótesis que presentamos en el primer capítulo (7.1).
Posteriormente abordaremos lo que consideramos como las
principales aportaciones de esta investigación (7.2). Por último,
describiremos los puntos relacionados con el trabajo futuro (7.3).</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Descripción y evaluación de un sistema basado en reglas para la extracción automática de contextos definitorios
Rodrigo Alarcón Martínez
TESIS DOCTORAL UPF / 2009
DIRECTORES DE LA TESIS
Dra. Carme Bach Martorell (Departament de Traduccion
Ciebcias del Llenguatge, Instituto Universitario de Lingüística
Aplicada, Universitat Pompeu Fabra)
Dr. Gerardo Sierra Martínez (Grupo de Ingeniería
Lingüística, Instituto de Ingeniería, Universidad Nacional
Autónoma de México)</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"ANÁLISIS DEL USO UNIVERSITARIO DE PLATAFORMAS DE GESTIÓN DEL APRENDIZAJE. ESTUDIO DE CASO EN LA UNIVERSITAT DE VALÉNCIA"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivos
Dada la descripción anterior, podemos enunciar los objetivos del presente trabajo de la
siguiente forma:

Objetivos generales
1. Desarrollar un estudio descriptivo exhaustivo del uso de una plataforma de elearning
durante su período de implantación, considerando que dicho periodo
engloba dos cursos académicos completos.
2. Analizar la evolución del empleo de la plataforma en el período referido.
3. Detectar los módulos que presentan carencias informáticas y objetivar las
mismas para proponer las medidas tecnológicas correctoras oportunas.
4. Diseñar e implementar una herramienta informática que permita generalizar el
uso de los análisis realizados a otros centros de educación superior que así lo
deseen.
Objetivos específicos
1. Determinar indicadores de utilización en las diferentes áreas de estudio:
Medidas Generales, Medidas de Innovación Educativa y Medidas de
indicadores de Financiación ligada a Objetivos en el período estudiado.
2. Analizar la validez y potencia estadística de estos indicadores como
herramientas de gestión de calidad de la plataforma virtual.
3. Conseguir indicadores demostrativos de las relaciones entre la implantación de
los LMS y la calidad e innovación de los procesos de enseñanza-aprendizaje
generalizables.
4. Establecer comparativas con otras Universidades con respecto al uso de
plataformas similares.
5. Desarrollar instrumentos informáticos que, a través de la utilización de formatos
estándares, generalicen y semiautomaticen los procesos de evaluación de
plataformas virtuales de aprendizaje.
6. Exponer medidas informáticas correctoras para mejorar tanto los módulos más
utilizados, como los peor valorados de la plataforma.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Hipótesis

Hipótesis conceptual
El estudio y evaluación de la utilización de la Plataforma Aula Virtual permitirá
determinar su grado de implantación y la evolución de la misma. Asimismo, se
observará la eficacia de los instrumentos de evaluación de tal modo que puedan ser
integrados a partir de procesos informáticos de manera automática en los sistemas
propios de gestión de calidad y planificación de la Universitat de Valencia para
controlar su plataforma LMS.
Igualmente se plantea la generalización de los resultados de la evaluación a
través de la generación de un procedimiento informático como mecanismo para
evaluar la fase de implantación de otras plataformas en universidades.

Hipótesis generales y relacionales
1. El EEES y los planes de adecuación y mejora de la docencia y procesos de
enseñanza-aprendizaje, para seguir las líneas marcadas por el proceso de
convergencia, han llevado a una evolución positiva en la explotación de
herramientas en Aula Virtual, en la implantación y mejora de los proyectos de
innovación educativa. En consecuencia, y relacionado con ambos aspectos, a
una mejora en los indicadores evaluadores de calidad de las titulaciones y
centros que forman la Universitat de Valéncia.
2. Los resultados de uso, implantación y mejora de la Plataforma de Gestión del
Aprendizaje, Aula Virtual, en los centros de la Universitat de Valéncia están
íntimamente relacionados con los resultados obtenidos tanto en el desarrollo
de proyectos de innovación educativa, como en la evaluación de la calidad de
las titulaciones y centros.
3. El desarrollo de una herramienta informática que contribuya a la
automatización del proceso evaluativo facilita la generalización de resultados a
otras plataformas universitarias.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En el presente trabajo se han estudiado, en primer lugar, las características y
desarrollo de la plataforma Aula Virtual, los motivos de su elección y su
implementación. Asimismo, se ha aplicado una metodología estadística para analizar
su utilización a lo largo de los dos cursos que constituyen la fase de implantación, y su
relación con la mejora en innovación y calidad en la educación; y se ha comparado el
estudio descriptivo con los resultados de otra universidad. Finalmente se ha aplicado
una metodología informática con el fin de automatizar y generalizar parte de los
análisis para su posterior uso en otras universidades que se encuentren en situaciones
semejantes. Tras todos estos análisis y a la luz de los resultados obtenidos, llegamos
a las conclusiones que se exponen a continuación.
En primer lugar se desarrollarán de forman detallada, manteniendo el esquema
que se planteó en las hipótesis de trabajo y los objetivos propuestos, para,
seguidamente, agruparlas esquemáticamente en un resumen de los aspectos más
destacados.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>DEPARTAMENT DE INFORMÁTICA
ANÁLISIS DEL USO UNIVERSITARIO DE PLATAFORMAS
DE GESTIÓN DEL APRENDIZAJE. ESTUDIO DE CASO EN
LA UNIVERSITAT DE VALÉNCIA
PALOMA MORENO CLARI
UNIVERSITAT DE VALÉNCIA
Servei de Publicacions
2009
Aquesta Tesi Doctoral va ser presentada a Valencia el dia 28
dábril de 2009 davant un tribunal format per:
- Dr. Antoni Hervás Jorge
- Dr. Faraón Llorens Largo
- Dr. Rafael Pastor Vargas
- Dra. M. Dolores Sancerni Beitia
- Dr. Santiago Felici Castell
Va ser dirigida per:
Dr. Vicente Cerverón Lleó
Dra. Amparo Oliver Germes
Dr. Miguel Arevalillo Herráez
* ©Copyright: Servei de Publicacions
Paloma Moreno Clari
DipÁ²sit legal: V-3751-2009
I.S.B.N.: 978-84-370-7507-5
Edita: Universitat de Valencia
Servei de Publicacions
C/ Arts Gráfiques, 13 baix
46010 Valencia
Spain
Telefon:(0034)963864115</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"EVALUACIÓN DE LA CALIDAD DE LOS SITIOS WEB CON INFORMACIÓN SANITARIA EN CASTELLANO"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>OBJETIVOS

Objetivos Generales:
El objetivo principal de la presente investigación se basa, en conocer la calidad de los
sitios web sanitarios mediante la aplicación de diferentes cuestionarios atendiendo a
diferentes criterios. Unido a esto, es imprescindible profundizar en la relación
existente entre los diferentes métodos de evaluación de la calidad de los sitios web
sanitarios.

Objetivos Específicos:
1. Saber que nivel de accesibilidad tienen los sitios web sanitarios a través de la
herramienta HERA.
2. Conocer los sitio web sanitarios que aplican los criterios de calidad propuestos
por Bermúdez et al en su cuestionario.
3. Conocer los sitios web sanitarios que cumplen el código de conducta HonCode.
4. Saber qué sitios web sanitarios pueden ser acreditados a través del sistema de
acreditación de la Agencia Sanitaria de Calidad de Andalucía.
5. Analizar la calidad de los sitios web sanitarios en función de su origen.
6. Saber si hay relación entre el cuestionario de accesibilidad HERA y el
cuestionario de Bermúdez et al.
7. Conocer la relación existente entre el código de conducta HonCode y la
acreditación de un sitio web sanitario de la Agencia Sanitaria de Calidad de
Andalucía.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>HIPOTESIS
1. Los sitios web de información sanitaria tienen una calidad insuficiente para aportar
información al usuario.
2. La accesibilidad de los sitios web sanitarios es buena, siendo más elevado el nivel de
accesibilidad en los sitios web de las administraciones públicas, al ser obligadas por ley
a cuplir la norma UE de accesibilidad.
3. El cuestionario de Bermúdez et al, un test reciente que incluye criterios conocidos
de códigos de conducta y criterios recomendados por la UE para todos los estados
miembros, es cumplido por la mayoria de los sitios web sanitarios evaluados.
4. El código de conducta HonCode es aplicado por la mayoría de los sitios web debido a
que contiene criterios muy básicos de aplicar y universalmente conocidos y aceptados,
implicando su aplicación un aumento de la calidad del sitio web sanitario.
5. Los sitios web sanitarios no podrán acreditarse por la Agencia Andaluza de Calidad
Sanitaria. Los criterios que se han de aplicar son muchos y algunos difíciles de evaluar
por un usuario del sitio web sanitario.
6. Los test de evaluación de la calidad de los sitios web sanitarios estan relacionados
entre sí, ya que muchos de los criterios están incluidos en los cuatro cuestionarios
( HERA, HonCode,Acreditación y Bermúdez et al) a la vez.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Es un estudio descriptivo transversal. La población diana u objeto, son los principales
sitios web de información sanitaria incluyendo los sitios web institucionales.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Todos los trabajos citados anteriormente describen una serie de criterios
que se deben tener en cuenta en la evaluación de las páginas web con
información sanitaria. Y, aunque muchos artículos coinciden en la
descripción de los mismos criterios, las metodologías en cuanto a
procedimiento, puntuación de indicadores, aplicación etc., no son siempre
muy explícitas.
La idea de crear una autoridad o un sistema de acreditación general de
contenidos, para toda la red es un proyecto muy complejo si tenemos en
cuenta diversos factores que caracterizan a Internet, como por ejemplo el
inmenso número de páginas web existentes y su crecimiento exponencial,
la variabilidad en cuanto a los criterios que se consideran adecuados para
su evaluación en función de los evaluadores implicados y la proliferación
de estos sistemas de acreditación.
Otros factores que influyen a la hora de acreditar o certificar contenidos
web son el gran dinamismo que comportan Internet y la propia medicina,
que hacen difícil el hecho de poder mantenerse al día respecto al control
de esa información (Mayer, 2001).
Desde hace unos años, una serie de agrupaciones, organizaciones y sociedades interesadas en la veracidad de la información han desarrollado diferentes instrumentos (HONCode, URAC, etc.), que al utilizarse en la página web determinan un nivel mínimo de calidad tanto del contenido de la información, como de los aspectos formales del recurso web. La aplicación de los criterios descritos en estos instrumentos por el webmaster tendrá como consecuencia la creación de páginas web fiables que ofrezcan garantías para el usuario que consulte información sanitaria.
La aplicación de los instrumentos descritos anteriormente es beneficiosa tanto para el webmaster como para el usuario. Para el webmaster porque verá cumplido uno de sus objetivos que es que el mayor número de usuarios posibles visite la página web; y, para el usuario porque siempre accederá a las páginas web sanitarias que sean fiables y de calidad.
En conclusión, según nuestra opinión, es necesario unificar todas las recomendaciones o criterios de calidad que deben cumplir las páginas web sanitarias para ser consideradas de calidad, también sería necesario que la página web informara al usuario si la página web que está viendo se adhiere a un código de conducta y creemos que se debería de especificar si los contenidos científicos y divulgativos de la página web sehan contrastado y validado. Por tanto se debería de elaborar una herramienta fácil de aplicar por el usuario, para evaluar la calidad de una página web, y adiestrar al usuario para que desarrolle un sentido crítico y así poder diferenciar una página web sanitaria fiable de la que no lo es.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>TESIS DOCTORAL
UNIVERSIDAD DE MURCIA
FACULTAD DE COMUNICACIÓN Y DOCUMENTACIÓN
EVALUACIÓN DE LA CALIDAD DE LOS SITIOS WEB CON
INFORMACIÓN SANITARIA EN CASTELLANO
M* ª Carmen Conesa Fuentes
Director: Enrique Aguinaga Ontoso
MURCIA, FEBRERO 2010</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Árboles de Decisión para Grandes Conjuntos de Datos"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
El objetivo general de esta investigación es proponer dos algoritmos que permitan la generación de árboles de decisión para grandes conjuntos de datos dinámicos, que sean competitivos en calidad y más rápidos que los algoritmos existentes.

Objetivos Particulares
Los objetivos particulares de este trabajo son los siguientes:
1. Desarrollar un algoritmo que genere árboles de decisión multivaluados, con todo el conjunto de atributos, para conjuntos de datos numéricos.
2. Desarrollar un algoritmo que genere árboles de decisión multivaluados, con subconjuntos de atributos,
para conjuntos de datos numéricos.
3. Extender los algoritmos de los objetivos (1) y (2) para manipular conjuntos de datos mezclados.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es posible crear un algoritmo de generación de árboles de decisión para grandes conjuntos de datos
que procese todo el conjunto de entrenamiento, capaz de trabajar con conjuntos de datos din ámicos, el cuál
sea competitivo en calidad y más rápido que los algoritmos existentes?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Metodología 
La metodología para poder alcanzar los objetivos propuestos en este trabajo es la siguiente:
1. Obtener grandes conjuntos de datos reales, que puedan ser utilizados para probar los algoritmos.
b) Para el proceso de expansión, proponer diversas representaciones de los arcos generados por los
nodos expandidos, tomando en cuenta el subconjunto de atributos elegido en el paso (a).
c) Desarrollar estrategias para clasificar objetos nuevos a partir del AD generado, cuando se alcance
un nodo hoja en el recorrido que se haga del árbol.
d) Validar el algoritmo propuesto con grandes conjuntos de datos numéricos, para observar su comportamiento
con respecto al tamaño del conjunto de entrenamiento y de acuerdo al número de
atributos que describe al conjunto. Algunas de las posibles características a evaluar serán: el porcentaje
de aciertos del algoritmo, el tiempo de procesamiento y el tama ño del árbol de decisión
generado.
e) En caso de ser necesario, redefinir alguno de los puntos anteriores.
f ) Comparar los resultados del algoritmo propuesto contra C4.5 y con uno o más algoritmos presentados
en el trabajo relacionado.
4. En la extensión de los algoritmos para que puedan manipular grandes conjuntos de datos mezclados,
aplicar los siguientes pasos:
a) Desarrollar estrategias para datos mezclados que permitan encontrar representaciones de los
arcos generados por los nodos expandidos y que puedan manipular grandes conjuntos de datos.
b) Proponer técnicas que generen subconjuntos de atributos, que se utilicen en la representación
interna del árbol.
c) Proponer estrategias para clasificar objetos nuevos, descritos por atributos mezclados.
d) Validar el algoritmo propuesto con grandes conjuntos de datos mezclados, para observar su
comportamiento con respecto al tamaño del conjunto de entrenamiento y de acuerdo al número
de atributos que describe al conjunto. Algunas de las posibles características a evaluar serán:
el porcentaje de aciertos del algoritmo, el tiempo de procesamiento y el tama ño del árbol de
decisión generado.
e) Comparar los resultados del algoritmo propuesto contra C4.5, debido a que es un algoritmo que
también permite trabajar con datos mezclados, y con uno o más algoritmos presentados en el trabajo relacionado.
2. Para generar un AD multivaluado para grandes conjuntos de datos numéricos, que tenga todo el conjunto de atributos en sus nodos, se seguirán los siguientes pasos:
a) Definir la estructura de los nodos del AD. Buscar una representación para los nodos internos y sus arcos, así como para las hojas del árbol, de tal manera que puedan ser manipulados grandes conjuntos de datos.
b) Proponer la rutina de actualización del árbol generado, el algoritmo leerá uno a uno los objetos del conjunto de entrenamiento.
c) Para el proceso de expansión, crear diversas representaciones de los arcos generados por los nodos expandidos.
d) Para el proceso de clasificación de objetos nuevos, definir una estrategia para recorrer el árbol construido, de tal manera que se llegue a una hoja.
e) Desarrollar estrategias para clasificar objetos nuevos a partir del AD generado, cuando se alcance un nodo hoja en el recorrido que se haga del árbol.
f ) Validar el algoritmo propuesto con grandes conjuntos de datos numéricos, para observar su comportamiento con respecto al tamaño del conjunto de entrenamiento y de acuerdo al número de atributos que describe al conjunto. Algunas de las posibles características a evaluar serán: el porcentaje de aciertos del algoritmo, el tiempo de procesamiento y el tama ño del árbol de decisión generado.
g) En caso de ser necesario, redefinir alguno de los puntos anteriores.
h) Comparar los resultados del algoritmo propuesto contra C4.5 y con uno o más algoritmos presentados en el trabajo relacionado
3. Para generar un AD multivaluado para grandes conjuntos de datos numéricos, que tenga a un subconjunto de atributos en sus nodos, se seguirá el procedimiento anterior, con las siguientes modificaciones:
a) Proponer estrategias para encontrar subconjuntos de atributos para utilizarlos en la representación de los nodos internos y sus arcos, y que además permitan el manejo de grandes conjuntos de datos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los árboles de decisión son una herramienta útil para resolver problemas de clasificación supervisada. En la actualidad se han desarrollado diversos algoritmos que generan árboles de decisión, incluso algunos que trabajan con grandes conjuntos de datos, sin embargo, los algoritmos de generación de árboles de este tipo presentan algunas restricciones, como por ejemplo: el espacio que ocupan, el n úmero de veces que tienen que recorrer el conjunto de entrenamiento para construir el árbol o el no tener la capacidad de actualizarse si el conjunto de datos tiene un flujo continuo de informacion.
En el presente trabajo se propone desarrollar un algoritmo de generación de árboles de decisión para grandes conjuntos de datos que supere estas restricciones, es decir, un algoritmo que sea incremental, que no necesite almacenar todo el conjunto de entrenamiento en memoria para poder construir el árbol y que accese la menor cantidad de veces posible al conjunto. El algoritmo propuesto preliminarmente ha sido diseñado para trabajar con conjuntos de datos numéricos, pero se plantea aplicar ténicas que nos permitan trabajar con conjuntos de datos mezclados.
Los resultados experimentales que se han realizado, muestran que el algoritmo propuesto resulta competitivo en porcentaje de aciertos y genera árboles compactos en un tiempo razonable. Estos resultados son alentadores y muestran que es factible alcanzar los objetivos propuestos es esta tesis.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Árboles de Decisión para Grandes Conjuntos de Datos
Anilu Franco-Arcega, J. Ariel Carrasco-Ochoa,
Guillermo Sánchez-Díz, J. Francisco Martínez-Trinidad
Reporte Técnico No. CCC-08-001
13 de Febrero de 2008
© 2008
Coordinación de Ciencias Computacionales
INAOE
Luis Enrique Erro 1
Sta. Ma. Tonantzintla,
72840, Puebla, México.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"TALISMAN: Desarrollo ágil de Software con Arquitecturas Dirigidas por Modelos"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>OBJETIVOS
Partiendo de la hipótesis que acabamos de plantear en el apartado
anterior, el objetivo principal de la tesis es el siguiente:
Definir una metodología para el desarrollo de software que
permita la creación de software a partir del modelado de procesos
del negocio de acuerdo a la especificación MDA y el desarrollo
de software ágil.
Es importante aclarar que nuestra investigación se centra en las etapas
iniciales del proceso de desarrollo, aquellas relacionadas con los modelos CIM y
PIM de MDA, dado que, tal y como se ha señalado anteriormente, esa parte es
la que aún no ha sido suficientemente estudiada dentro de la comunidad
científica.
El objetivo principal se concreta en los siguientes objetivos parciales:
* Realizar un estudio de las áreas relacionadas con el objetivo
principal, es decir, MDA, desarrollo ágil de software, patrones de
diseño y estándares.
* Definir una metodología para el desarrollo de software ágil de
acuerdo a la especificación MDA.
* Crear un prototipo que implemente la metodología.
* Desarrollar una aplicación usando la metodología y utilizando el
prototipo.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Para que esta hipótesis pueda cumplirse consideramos que el desarrollo
de software debería realizase a partir del establecimiento de un contexto de
desarrollo ágil de software. Sólo de esa forma creemos que será posible partir
de modelos de tipo CIM, asociados a los procesos del negocio, y transformarlos
a modelos de software independientes de las plataformas, de tipo PIM.
Si se consigue la mencionada combinación entre MDA y desarrollo ágil
de software se estará contribuyendo a resolver el problema mencionado sobre
la falta de conexión entre el dominio del negocio y el tecnológico. Además se
favorecerá la definición de procesos de desarrollo de software más rápidos y
flexibles, que reaccionen rápidamente ante los cambios en el negocio y que no
dependan tanto de las tecnologías.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Tras analizar la propuesta MDA de OMG se han detectado dos
importantes carencias relacionadas con el tratamiento de los modelos iniciales:
* No se aclara cómo deben manejarse los modelos ligados al negocio,
lo que se conoce como modelos independientes de la computación ó
CIM.
* No se describe cómo deben asociarse esos modelos CIM con los
primeros modelos del software a desarrollar, es decir, con los PIM.
Debido a las carencias mencionadas la mayoría de trabajos relacionados
con el desarrollo de software basado en MDD ó MDA suelen enfocarse desde
una perspectiva puramente tecnológica. Salvo contadas excepciones no se
trabaja partiendo desde un punto de vista orientado al negocio y relacionado
con los objetivos empresariales.
La mayor parte de las investigaciones actuales sobre este campo están
centradas en aspectos técnicos que pueden mejorar la obtención de código a
partir de modelos más o menos abstractos. Esos modelos suelen estar
directamente relacionados con el software, como son los modelos de requisitos,
de información, de casos de uso, de análisis, de diseño, etc.

De todo lo anterior se deduce que la falta de conexión real entre el
dominio del negocio y el tecnológico provoca que, en la mayoría de los casos,
los modelos del negocio no sean utilizados como punto de partida de los
procesos de desarrollo de software. Este problema se aprecia claramente en el
ámbito de MDA, siendo evidente la falta de asociación entre modelos CIM y
PIM.
Las metodologías de desarrollo ágil de software tiene como punto fuerte
la conexión de los aspectos asociados al negocio con los tecnológicos. Sin
embargo, no se ajusta a la especificación MDA y por lo tanto no puede aportar
los beneficios inherentes, relacionados con el desarrollo de software dirigido por
modelos.
Por consiguiente, una vez expuestas de forma resumida las ventajas,
desventajas y deficiencias de los modelos de desarrollo mencionados y con el
objetivo de solventar sus puntos débiles y aprovechar sus aportaciones,
planteamos la siguiente hipótesis de partida en esta tesis:
La combinación de la especificación MDA con la idea del
desarrollo ágil resultaría muy ventajosa de cara a un desarrollo
de software flexible y asociado a los procesos del negocio.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El proceso metodológico que se ha utilizado en esta investigación está
basado en un método científico y puede dividirse en varias fases generales.
Concretamente las descritas a continuación:
1. Formulación del problema: En esta primera etapa se realiza un
planteamiento del problema, la definición de la hipótesis de partida y
de los objetivos principales de la investigación. La descripción de
toda esta información se recoge en este capítulo.
2. Recopilación y estudio de documentación: Durante esta fase se ha
hecho acopio de toda la información necesaria para elaborar la base
de conocimientos básicos necesarios para afrontar la investigación.
Toda esa documentación ha sido organizada y estructurada en
función a su contenido y relación con nuestra investigación de forma
que permita una cómoda consulta y actualización.
3. Documentar la base teórica. Una vez asimilada la información
recopilada en el apartado anterior, se procede a documentar en los
primeros capítulos aquellas disciplinas que están directamente
relacionados con el área de esta investigación, es decir, MDA,
desarrollo ágil de software y modelado del negocio. Esta información
servirá de soporte para el análisis y las investigaciones posteriores.
4. Analizar trabajos relacionados con la investigación. En esta etapa se
realiza un estudio de trabajos y propuestas muy relacionados con los
objetivos a alcanzar. Tras dicho análisis se pueden plantear, para
cada uno de esos trabajos, los puntos a favor y en contra en relación
a cómo pueden ser utilizados para la resolución del problema
formulado y la hipótesis de partida.
5. Desarrollo de la investigación. Con los conocimientos y el material
adquiridos en las etapas anteriores se desarrollará la investigación.
Este desarrollo desemboca en la documentación de los capítulos
principales de la tesis donde se especifica la metodología aportada.
6. Construcción de un prototipo. Una vez que la metodología ha sido
definida, se desarrolla un prototipo para probar la utilización de la
misma en casos prácticos.
7. Conclusiones. Esta etapa se expresan los resultados y aportaciones
de la investigación, fundamentados en las discusiones reflejadas en
el resto de la memoria de la tesis.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este capítulo se exponen las conclusiones principales de la tesis.
En primer lugar, en el apartado 15.1 se hace un repaso y evaluación de
las acciones realizadas a partir de la hipótesis de partida.
Por otro lado, en el apartado 15.2 se realiza una comprobación del nivel
de cumplimiento de los objetivos presentados en la introducción.
El apartado 15.3 describe de forma resumida las principales
aportaciones resultantes de la investigación, destacando aquellas características
inéditas que resultan novedosas o innovadoras.
Con el objetivo de dar a conocer a la comunidad científica los avances
obtenidos, en el apartado 15.4 se exponen los trabajos presentados en
congresos y jornadas internacionales, relacionados con los temas abordados en
esta tesis.
Por último, en el apartado 15.5, se presentan las líneas de investigación
futuras que se desea seguir a partir de esta tesis.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE OVIEDO
Departamento de Informática
TESIS DOCTORAL
TALISMAN: Desarrollo ágil de Software con Arquitecturas Dirigidas por Modelos
Presentada por
Dña. Begoña Cristina Pelayo García-Bustelo
Para la obtención del título de Doctora por la Universidad de Oviedo
Dirigida por el
Profesor Doctor D. Juan Manuel Cueva Lovelle
Oviedo, Mayo de 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"INTEGRACIÓN DE LOS MODELOS DE SIMULACIÓN EN EL DISEÑO DE LOS ENSAYOS CLÍNICOS"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo principal de este trabajo es el análisis y desarrollo de modelos de simulación para la optimización del diseño de los ensayos clínicos. Con este fin se desarrollan diferentes estructuras de modelos estocásticos asistidos por ordenador que permiten representar adecuadamente el ensayo clínico y simular los resultados con diseños e hipótesis alternativas antes de su realización. Con ello se pretende determinar el diseño que optimiza el coste y el tiempo de desarrollo de un ensayo clínico, analizando los resultados hipotéticos antes de su realización real.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La hipótesis que se pretende probar es que el desarrollo y la aplicación de los modelos de simulación propuestos permiten alcanzar de forma más eficiente los objetivos de los ensayos clínicos, es decir, obtener con una determinada probabilidad de éxito los resultados buscados lo que en el caso de nuevos productos es una condición necesaria para su comercialización en un periodo de tiempo y con un coste óptimos.
Los modelos de simulación estocásticos de estados discretos son especialmente adecuados para dicho objetivo, pues permiten incorporar fácilmente a los algoritmos de cálculo la información necesaria para representar supuestos muy complejos, tales como variaciones de los parámetros a lo largo del tiempo. En este sentido, dichos modelos permiten no sólo optimizar el diseño inicial del ensayo, sino que constituyen una herramienta de gran utilidad potencial en el seguimiento y gestión del ensayo a lo largo de su ejecución.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En este capítulo se define el proceso de planificación aconsejable para llevar a cabo un estudio de simulación para optimizar el diseño de un ensayo clínico. La figura 2.1 muestra la secuencia de pasos necesarios. El elemento central de la realización de un ensayo clínico es el protocolo, que describe los objetivos y las hipótesis, así como todos los detalles de cómo se lleva a cabo un ensayo clínico, incluso el análisis estadístico de los datos (González, 2003). Ya se ha mencionado anteriormente que no se suele integrar el diseño con los costes y la duración de forma explícita.
El enfoque de esta tesis es aplicar los modelos de simulación de estados discretos como método para cuantificar las respuestas de un sistema y manejar situaciones complejas en las que existe incertidumbre. Por otro lado, la utilización de estos modelos es útil para extrapolar los resultados de los ensayos clínicos cuando no es posible la realización de estudios de larga duración debido al coste elevado o bien cuando existe la necesidad de obtener los resultados en un plazo corto de tiempo.
La solución puede ser utilizar información y supuestos de distintas fuentes con el fin de realizar simulaciones basándose en la mejor información posible. En definitiva, el enfoque es ver el diseño del ensayo clínico como un proceso de optimización que, realizado en su momento, permita influir en el diseño, es decir en el protocolo.
Adicionalmente, la simulación puede permitir determinar la imposibilidad o baja probabilidad de que un determinado ensayo clínico termine con éxito, lo que sugería la consecuencia de descartar su realización para no incurrir los costes y utilidades. Finalmente, la simulación puede ayudar a tomar decisiones una vez iniciado el ensayo clínico, tales como, interrumpirlo, aumentar su duración, etc.
En base a nuestro análisis y experiencia, la metodología para planificar un modelo válido y creíble tiene que tener los pasos básicos que aparecen a continuación resumidos en la figura 2.1.
A modo de ilustración práctica se incluye un ejemplo de una propuesta para optimizar un ensayo clínico del microbicida en la prevención del VIH en mujeres, realizando durante una estancia en la Universidad de Ohio. Esta propuesta es una planificación real pendiente de ser iniciada, ya que la FDA no ha aprobado la solicitud de inicio del ensayo clínico.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los ensayos clínicos constituyen una herramienta imprescindible para garantizar la eficacia y seguridad de los nuevos fármacos y un requisito previo a su comercialización en la mayor parte de los países.
Los ensayos clínicos de Fase III suelen ser los más importantes dentro del proceso de desarrollo de un nuevo fármaco, tanto por su papel en la autorización de comercialización de un nuevo producto, como por sus elevados costes y por las consecuencias sanitarias y económicas que se derivan de sus resultados.
La metodología y diseño de los ensayos clínicos ha generado un gran volumen de investigación y de literatura científica. En el primer capítulo se han revisado los principales enfoques que se utilizan para determinar el tamaño de la muestra y otros elementos del diseño. Se ha revisado también la utilización de los modelos de simulación en el ámbito del diseño de ensayos clínicos.
La revisión realizada de la literatura ha permitido constatar que la mayoría de los enfoques que se aplican en la actualidad al diseño de ensayos clínicos, incluyendo los que utilizan modelos de simulación, no tienen en cuenta todos los aspectos relevantes para la optimización del diseño conjuntamente. Lo que propone precisamente esta tesis como principal aportación es integrar todos estos aspectos en un modelo general de simulación para la optimización del diseño de un ensayo clínico según criterios y supuestos explícitos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Tesis doctoral
Programa de doctorado "Aplicaciones Técnicas e Informáticas de
la Estadística, la Investigación Operativa y la Optimización
Departamento de Estadística e Investigación Operativa
Universidad Politécnica de Catalunya
INTEGRACIÓN DE LOS MODELOS DE SIMULACIÓN
EN EL DISEÑO DE LOS ENSAYOS CLÍNICOS
Ismail Abbas
Directores:
Dr. Josep Casanovas García
Dr. Joan Rovira Forns
Barcelona, diciembre de 2003</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Una propuesta de gestión integrada de modelos y requisitos en líneas de productos software"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo Global. Definir una propuesta de gestión de requisitos para líneas
de productos que integre modelos de ingeniería del software y requisitos
textuales en lenguaje natural.

Este objetivo global se desglosa en los siguientes objetivos parciales:
Objetivo 1. Estudiar la reutilización de requisitos textuales, definiendo un
método de IR basado en reutilización que incluya un metamodelo de
requisitos, un conjunto de técnicas, un modelo de procesos y una
herramienta de soporte.

Objetivo 2. Definir un metamodelo de variabilidad en una línea de
productos software, que sea compatible con los resultados del Objetivo 1, y
establecer un conjunto de técnicas que representen dicho modelo de
variabilidad en el nivel de los requisitos.
Objetivo 3. Definir un proceso de generación de documentación de
requisitos textuales, en un proceso de desarrollo de software iterativo e
incremental, que sincronice los modelos definidos en el Objetivo 2 con el
metamodelo de requisitos textuales establecido en el Objetivo 1.
Objetivo 4. Diseñar e implementar el prototipo de una herramienta que
soporte los modelos definidos en 2 y el proceso de generación definido en
3.Objetivo 5. Validar los resultados anteriores en al menos un caso de estudio
real.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La hipótesis de partida de esta tesis doctoral es que es factible definir una propuesta de
IR para líneas de productos que facilite la reutilización de requisitos y que integre
modelos de ingeniería del software y requisitos textuales. El fin último de esta
propuesta es la mejora de la calidad de las especificaciones de requisitos de líneas de
productos y de la productividad en el proceso de especificación, si bien no se prevé que
tales mejoras en la calidad y la productividad puedan ser medidas experimentalmente de
forma cuantitativa en el ciclo de desarrollo de software, sino sólo de forma cualitativa,
debido a la ausencia de registros previos en los casos de estudio involucrados en esta
investigación.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En un análisis de la literatura de ingeniería del software, Glass et al. [116] muestran que
tradicionalmente el método de investigación mayoritario en ingeniería del software ha
sido el análisis o estudio conceptual, concluyendo que "los investigadores en ingeniería
del software tienden a analizar e implementar nuevos conceptos, y hacen muy poco
más. Por el contrario, en la realización de esta tesis doctoral se han utilizado varios
métodos de investigación: casos de estudio, investigación en acción y revisiones
sistemáticas de la literatura. En este apartado se introduce brevemente cada uno de estos
métodos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En las secciones de conclusiones de los capítulos 2, 3 y 4 se han recogido las
reflexiones detalladas en relación con cada una de las tres líneas de investigación en
Ingeniería de Requisitos (IR) que confluyen en esta tesis doctoral, junto con una
descripción de posibles vías de trabajo futuro. En esta sección resumimos las principales
aportaciones de la investigación llevada a cabo en el marco de esta tesis doctoral, para
mostrar cómo tales aportaciones desarrollan los objetivos planteados en la Sección 1.2,
"Hipótesis y Objetivos.
Objetivo 1. Estudiar la reutilización de requisitos textuales, definiendo un método de IR
basado en reutilización que incluya un metamodelo de requisitos, un conjunto de
técnicas, un modelo de procesos y una herramienta de soporte.
* Después de una breve descripción del estado del arte en IR y en reutilización de
requisitos, en el Capítulo 2 de esta memoria se ha descrito en profundidad
SIREN, un método de IR basado en reutilización que trata con requisitos escritos
preferentemente en lenguaje natural. Esta descripción de SIREN incluye los
elementos mencionados en el Objetivo 1: (1) el metamodelo de requisitos o
modelo de referencia de requisitos de SIREN, que primero se describe
informalmente en el Capítulo 2 y después se formaliza en el Capítulo 4,
utilizando MOF y Ecore; (2) el conjunto de técnicas de SIREN, que incluye
guías para la definición de requisitos, atributos de los requisitos, requisitos
parametrizados, trazas, organización de los catálogos, reutilización de requisitos,
mejora de los catálogos y organización de los documentos de requisitos; (3) el
modelo de procesos de SIREN, con una descripción de sus tareas y subtareas; y
en último lugar (4) la herramienta de soporte de SIREN, SirenTool. Finalmente,
en el Capítulo 2 se ha mostrado SIRENgsd, una propuesta de extensión de
SIREN para el desarrollo global de software, junto con un repositorio de
amenazas y salvaguardas para la IR que se desarrolla en un entorno globalizado,
repositorio que se basa en una revisión sistemática de la literatura.
Objetivo 2. Definir un metamodelo de variabilidad en una línea de productos software,
que sea compatible con los resultados del Objetivo 1, y establecer un conjunto de
técnicas que representen dicho modelo de variabilidad en el nivel de los requisitos.
* En el Capítulo 3 de esta memoria se ha descrito SIRENspl, una evolución de
SIREN con el objetivo de soportar la IR para líneas de producto en el contexto
de un problema concreto, el dominio de los sistemas teleoperados para
mantenimiento de cascos de buques (STO). SIRENspl se basa en técnicas ya
existentes en análisis del dominio, que se han seleccionado y particularizado
después de un estudio del estado del arte en IR para líneas de productos, con el
objetivo de modelar adecuadamente el caso de estudio de los STO. En el
Capítulo 3 el metamodelo de variabilidad de la línea de productos se describe
sólo implícitamente a través de la presentación informal de las técnicas
involucradas en SIRENspl; realmente es en el Capítulo 4 cuando se formaliza.
dicho metamodelo los modelos de características y de casos de uso genéricos
utilizando MOF y Ecore. Por tanto el proceso seguido en la tesis doctoral ha
seguido este orden (el inverso al enunciado en el Objetivo 2): primero se han
seleccionado unas técnicas para el análisis del dominio, que incluyen la
definición de la variabilidad de la línea de productos, y después se han
formalizado.
Objetivo 3. Definir un proceso de generación de documentación de requisitos textuales,
en un proceso de desarrollo de software iterativo e incremental, que sincronice los
modelos definidos en el Objetivo 2 con el metamodelo de requisitos textuales
establecido en el Objetivo 1.
* En el Capítulo 4 de esta memoria se ha comenzado planteando intuitivamente el
interés de la integración de modelos de ingeniería del software con requisitos
textuales en lenguaje natural mediante la generación de requisitos textuales
candidatos a partir de modelos de ingeniería del software (preferentemente
modelos gráficos). Una vez planteado intuitivamente el interés de esta línea de
investigación, se ha realizado una revisión sistemática de la literatura sobre
generación de requisitos textuales a partir de modelos, que ha corroborado el
interés de esta línea de trabajo. Acto seguido, se ha propuesto lo que
denominamos una correspondencia de aplanamiento de los modelos de análisis
del dominio propuestos en SIRENspl en requisitos en lenguaje natural
compatibles con SIREN. Para formalizar esta propuesta se ha modelado
formalmente con MOF los dominios de inicio y destino de la correspondencia de
aplanamiento, esto es (1) como dominio de inicio, las técnicas de SIRENspl 
acotando a modelos de características y casos de uso genéricos; y (2) como
dominio de destino, el modelo de referencia de requisitos de SIREN. El deseo de
utilizar esta correspondencia de aplanamiento en un desarrollo iterativo e
incremental nos ha llevado a plantear el problema de la sincronización de los
modelos de inicio y destino.
Objetivo 4. Diseñar e implementar el prototipo de una herramienta que soporte los
modelos definidos en 2 y el proceso de generación definido en 3.
* En el Capítulo 3 de esta memoria se ha descrito un prototipo básico de
herramienta de soporte a SIRENspl, SirenSPLTool, desarrollado mediante
Eclipse EMF/GMF, con el cual se da soporte a los modelos de análisis del
dominio definidos en SIRENspl a raíz del Objetivo 2. Posteriormente, en el
Capítulo 4 se ha mostrado una implementación de la correspondencia de
aplanamiento propuesta, utilizando para ello Eclipse EMF junto con un lenguaje
declarativo de transformación de modelos, QVT-relations. Se proporciona
también una primera aproximación al problema de la sincronización de los
modelos de inicio y destino.
Objetivo 5. Validar los resultados anteriores en al menos un caso de estudio real.
* En relación con el Objetivo 1, SIREN ha sido validado en entornos industriales
fundamentalmente a través del proyecto GARTIC (Sección 2.7.3). En relación
con el Objetivo 2, el enfoque propuesto por SIRENspl ha sido validado con su
aplicación mediante Investigación-Acción en el caso de estudio de los STO, como se muestra en el Capítulo 3. Finalmente, en relación con los objetivos 3 y
4, la herramienta SirenSPLTool y la implementación de la correspondencia de
aplanamiento con QVT-relations han sido validadas con su aplicación
retrospectiva a los modelos generados en el caso de estudio de los STO.
La hipótesis de partida de esta tesis doctoral decía que es factible definir una propuesta
de IR para líneas de productos que facilite la reutilización de requisitos y que integre
modelos de ingeniería del software y requisitos textuales (Sección 1.2). Creemos que el
desarrollo de los objetivos anteriores permite probar dicha hipótesis de partida y
satisface el objetivo global de esta tesis doctoral: "Definir una propuesta de gestión de
requisitos para líneas de productos que integre modelos de ingeniería del software y
requisitos textuales en lenguaje natural.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Murcia 
Facultad de Informática
Una propuesta de gestión integrada de modelos y requisitos en líneas de productos software
Tesis Doctoral
Doctorando: Joaquín Nicolás Ros
Director: José Ambrosio Toval Álvarez
2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Aplicaciones de XML para la documentación periodística: efectos sobre los centros de documentación de prensa"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Esta tesis pretende analizar cómo la producción de los nuevos documentos digitales
difundibles en línea y sus respectivos usos por parte de un medio periodístico están
influyendo en el funcionamiento de los centros de documentación de medios
periodísticos. Y para ello, previamente, se analiza cuál ha sido la transformación en
esos nuevos documentos producidos digitalmente y difundidos en línea a cualquier
lugar del mundo, mediante Internet.
En concreto, se toman en consideración nuevos fenómenos como los siguientes: más
contenidos a tratar, la necesidad de una mejor relación entre el centro de
documentación de un medio y la redacción del mismo, el aumento de la urgencia en
la demanda de resultados al centro, la utilización de herramientas informáticas que
tengan en cuenta aspectos documentales, el requerimiento de acceso directo a los
contenidos por los usuarios, así como el aumento de la consideración periodística y
económica de los fondos documentales.
La alternativa ofrecida incluye la representación de esos contenidos medianté un
marcado de texto empleado simultáneamente por todos los sistemas de un medio
periodístico (no solo el centro de documentación, sino también la redacción, el
sistema de producción y el sistema de gestión: administración, recursos humaños,
contabilidad, márketing, planificación..., etc.) que facilite el análisis documental,
reduzca al tiempo el esfuerzo material de todos esos sistemas, permita la
representación más compleja de los contenidos, aumente las posibilidades de
recuperación de los mismos y facilite el uso múltiple de éstos a la medida de cada
posible usuario. El empleo de este marcado requiere el estudio de la influencia que
puede tener su uso en el funcionamiento tanto de un centro de documentación como
del resto de sistemas del medio.
Todas estas cuestiones provocan también la necesidad de analizar aspectos como:

* Los cambios en el conceptó de documento periodístico digital y de las
diversas partes que lo componen.
* La definición de las posibilidades genéricas del uso del marcado de texto
para reflejar tanto las estructuras de los documentos periodísticos como para
representar documentalmente sus contenidos.
* La aplicación de esas posibilidades en los lenguajes de marcado
periodístico ya existentes.
De todo ello, se deducirá el desarrollo de un modelo concreto de aplicación que sirva
como ejemplo para justificar la propuesta realizada.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La investigación realizada ha partido de las siguientes hipótesis:
1. El uso de documentos periodísticos digitales y su difusión a través de Internet
ha aumentado a su vez el empleo de la documentación periodística, tomando
como parámetros de medida el número de usuarios y la consideración por
parte de los medios periodísticos sobre el valor de la misma.
2. El aumento de las ofertas de productos documentales por parte de los medios
se ha producido en muchos casos mediante herramientas ajenas al ámbito
documental, como los buscadores; y en los casos en los que sí ha habido
tratamiento documental, éste no ha aprovechado todas las capacidades
derivadas del hecho de manejar documentos digitales.
3. La existencia de lenguajes de marcado de texto periodístico, basados en XML
y elaborados por instituciones que agrupan a entidades periodísticas, ofrece la
posibilidad de asumir en un futuro esas herramientas en el seno de medios.
Los lenguajes citados disponen de capacidades de tratamiento documental de
los contenidos representados, tanto por la propia definición de los elementos
que componen estos lenguajes como por el hecho de estar basados en XML y,
asimismo, por la combinación de ambos factores.
5. El empleo de este tratamiento documental por parte del centro de
documentación de un medio permitirá coordinar su actividad con la de otras
secciones de éste, como la redacción, la producción o la administración del
mismo.
6. La aplicación de un mayor tratamiento en profundidad de los contenidos
documentales de un medio ofrecerá mayores posibilidades de representación
y recuperación de los mismos, mejores capacidades de aprovechamiento de
los contenidos por parte de todo tipo de usuarios y, por último, facilitará la
elaboración de estrategias orientadas a la rentabilización de la información
documental en un grado mayor al obtenido hasta ahora.
7. El empleo de soluciones basadas en XML y en lenguajes con visos de ser
aceptados por multitud de medios en todo el mundo posibilitará el
intercambio entre los mismos tanto de sus contenidos como de información
documental sobre éstos.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La presente tesis se justifica, además del interés objetivo de la misma, en la
formación y el desarrollo académico del autor, así como en razones personales. En
1995, confluyeron una serie de hechos casi consecutivos: el final de la Licenciatura
en Ciencias de la Información, en la rama de Periodismo; el surgimiento del interés por la investigación en documentación periodística, fomentado por el Dr. Félix del
Valle Gastaminza en los citados estudios, en la Universidad Complutense de Madrid;
la introducción del autor en el uso de Internet, como usuario, y el surgimiento de la
prensa accesible a través de Internet en España, a semejanza de lo sucedido ya en
años anteriores en otros lugares.
La intersección de estas circunstancias produjo el interés por investigar la aplicación
de la documentación periodística a esos nuevos contenidos periodísticos. El
desarrollo de los mismos corrió paralelo a la formación posterior del autor, desde
1996, en los cursos del Doctorado en Documentación en la Universidad Carlos ifi de
Madrid.
Muy pronto, contó con la tutela del profesor Antonio Hernández Pérez, del
Departamento de Biblioteconomía y Documentación de la misma universidad, y
licenciado y doctor en Ciencias de la Información por la Universidad Complutense
de Madrid. A esta formación común a la del doctorado se unía también el interés por
los mismos temas, así como en otros relacionados con las Tecnologías de la
Información aplicadas documentalmente a contenidos accesibles en Internet.
Además de la dirección por el Dr. Hernández de la tesina de doctorado, esa tutela se
manifestó mediante la inclusión en varios proyectos en los que participaron ambos,
principalmente, dirigidos por la Dra. Mercedes Caridad Sebastián, del mismo
Departamento. En esos proyectos, el autor participó como becario FPI del Ministerio
de Educación y Ciencia, entre 1998 y 2000, bajo la tutela del Dr. Hernández, y desde
octubre de 2000, como Ayudante de Escuela Universitaria en el citado
Departamento.
Varios de esos proyectos estaban relacionados con el tratamiento de contenidos
periodísticos representados mediante marcado de texto, dirigidos por el Dr. Carlos
Delgado Kloos, del Dpto. de Ingeniería Telemática de la Universidad Carlos ifi de
Madrid. Ese fue el caso del titulado Periotrónico: Concepción y desarrollo de un
periódico electrónico personalizado (CICYT TEL97-0788), entre 1998 y 1999, y del
subsiguiente proyecto InfoMedia: un sistema multiplataforma para la publicación de
David Rodríguez Mazeos información (CICYT TEL99-0207), entre 2000 y 2002, así como del proyecto
InfoFlex: Gestión de contenidos flexible y distribuida basada en tecnologías Web
(T1C2003-07208), dirigido por el Dr. Luis Sánchez Fernández, del mismo
Departamento, que se desarrollará a partir de 2004.
En esos proyectos, se investiga el uso de lenguajes de marcado de texto periodístico
basado en XML. De esa investigación se derivó el interés definitivo del director y del
autor de esta tesis por desarrollar la aplicación de este metalenguaje, y de los
lenguajes derivados del mismo desarrollados por instituciones internacionales, en los
centros de documentación periodística pertenecientes a medios de comunicación
accesibles en Internet.
Este interés por la documentación periodística y por su desarrollo en Internet se ha
visto reflejado también en el plano docente, donde, tras participar corno A.E.U. en
diversas asignaturas relacionadas con las tecnologías de la información en la
Diplomatura en Biblioteconomia y Documentación, el autor participa en la docencia
de asignaturas, dentro de la Licenciatura en Periodismo de la Universidad Carlos ifi
de Madrid, que están relacionadas con la documentación €como Documentación
Informativa, desde el curso 2002/2003€, o con el uso de Internet con fines
periodísticos, como Aplicaciones de la Red al Periodismo (desde el curso
2003/2004).</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La investigación descrita se ha realizado mediante una serie de fases en la que se ha
combinado el método deductivo con el método inductivo. La primera de estas fases,
de tipo deductivo, consiste en un análisis de la documentación periodística como
proceso, con el fin de encontrar sus características comunes.
Una segunda fase, inicialmente deductiva, ha consistido en el análisis de la
producción de documentos periodísticos digitales difundida en línea, de la cual se
han extraído aquellos factores que han supuesto la modificación de algunas de las
características documentales descritas anteriormente. Este análisis ha inducido a la
definición de dos ideas clave: la necesidad de definir una división estructural del
documento periodístico, teniendo en cuenta sus posibles componentes desde un
punto de vista documental (el documento es divisible en piezas, objetos y
seudopiezas), y el planteamiento sobre cuáles eran los interrogantes documentales a
los que debía responder un centro de documentación de un medio periodístico para
aprovechar correctamente la división anterior.
En una tercera fase, en este caso inductiva, se han tomado algunos conceptos básicos
relativos al marcado de texto, y en concreto, a su formulación mediante XML,
definiendo tres modelos para la descripción de contenidos mediante marcado de texto
(inserto, adjunto y externo), teniendo en cuenta la posición del marcado respecto a la
posición del contenido descrito, así como las implicaciones derivadas del uso de uno
u otro mecanismo.
Una cuarta fase, en esta ocasión, deductiva, ha consistido en la descripción de cinco
lenguajes concretos de marcado de texto periodístico, RSS, PRTSM, NewsML, NLTF
y SportsML, que intercalan en su uso varios de los modelos descritos. Para cada uno de ellos, se ha realizado una breve mención histórica contextualizadora de su
nacimiento, se han descrito sus aspectos más destacables desde el punto de vista
documental, y se han mostrado sus utilidades más concretas.
Tras esta descripción individual, se han comparado estos lenguajes entre sí, así como
su uso combinado. Para ello, se han estudiado las capacidades de los lenguajes para
identificar y para describir contenidos €haciendo hincapié tanto en el modo empleado
para definir las estructuras de los documentos como en la forma de representar sus
contenidos, ya fuera mediante lenguaje libre o mediante lenguaje controlado€, y al
tiempo, para relacionar esos contenidos con otros, y pára integrar el funcionamiento
del centro de documentación con el de otras secciones del medio.
Una última fase, inductiva, ha consistido en la definición de un modelo de aplicación
concreto para representar contenidos periodísticos mediante marcado de texto que
respondiera a las necesidades planteadas y a las utilidades encontradas en los
lenguajes de marcado. Se ha empleado como punto de partida el uso combinado de
NewsML y NTTF, reduciendo el número de elementos de cada lenguaje.
El requisito principal del modelo de aplicación consiste en no reducir la utilidad
documental de ninguno de los lenguajes, sino, al contrario, mejorar la identificación
y la descripción de los contenidos con el fin de hacer posible un mayor número de
relaciones futuras entre los documentos marcados. Ello permitirá por lo tanto un
mayor número de alternativas de recuperación de los contenidos, de lo que se derivan
mayores posibilidades para la reutilización de los mismos.
En conjunto, el trabajo es en buena medida sincrónico, dado que el estudio se centra
en la utilidad actual de los lenguajes de marcado estudiados, si bien su justificación
ha requerido sendas introducciones históricas que contextualizaran la elección de las
tecnologías estudiadas, tanto en el aspecto más teórico, el de su aplicación a la
documentación periodística, como en el más pragmático, el desarrollo de las
tecnologías analizadas.
Asimismo, se trata de un trabajo especulativo, que aporta una solución genérica a los
problemas planteados mediante un modelo de aplicación susceptible de diferentes
implementaciones concretas en el futuro.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Aplicaciones de XML para la
documentación periodística:
efectos sobre los centros de
documentación de prensa
David Rodríguez Mateos
(kc)
Universidad Carlos III de Madrid
Departament ode Biblioteconom yi aDocumentación
Director :Dr .Antoni oHernánde Pz érez
Tesis para la obtenció dne ltítulo de
Doctor en Documentación
Diciembre de 2003</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Evaluación de los sistemas de acreditación de webs sanitarias. La experiencia de web medica acreditada"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Los objetivos de la presente tesis son los siguientes:
1. Revisar y analizar comparativamente las propuestas y
recomendaciones planteadas por las principales iniciativas de
acreditación de webs médicas a escala mundial.
2. Estudiar la percepción que los responsables de las webs
médicas acreditadas a través de Web Médica Acreditada tienen
respecto a la utilidad e influencia del sello de acreditación en
sus webs, así como las propuestas que dichos responsables
hacen para mejorar los criterios y procedimientos de
acreditación.
3. Estudiar la prevalencia de uso de sellos de calidad en webs
sanitarias y analizar las diferencias en cuanto a parámetros
relacionados con la calidad entre las webs acreditadas con
algún sello de calidad y aquellas que no disponen de estos
sellos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Hipótesis general
Los sellos de calidad presentes en las webs médicas otorgados mediante sistemas de acreditación, constituyen un indicador válido de calidad en los formatos y contenidos y un instrumento de mejora en los servicios de información sanitaria ofrecidos a través de dichas webs.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La información sobre las diferentes iniciativas analizadas y sobre las
recomendaciones que presentan, se obtuvo de sus respectivas webs
institucionales y se llevó a cabo durante el mes de mayo de 2005. Fue
necesaria la traducción del inglés al español de las recomendaciones de
URAC y de la American Medical Association, ya que no disponían de
una versión de su código de conducta en dicho idioma.
En cada iniciativa se describieron y analizaron los siguientes aspectos:
1) Las características generales de las organizaciones promotoras,
el momento de creación, el proceso general de acreditación y la
descripción del sello de calidad concedido.
La enumeración y clasificación de los criterios y aspectos que
caracterizan a cada una de las diferentes iniciativas analizadas.
Dicha clasificación se realizó en áreas temáticas, asociando un
color predeterminado a cada área. En total se distinguieron seis
áreas: transparencia y honradez, autoridad, intimidad y
protección de datos, actualización de la información, rendición
de cuentas y accesibilidad. La designación de estas áreas
temáticas se basó en los criterios de calidad utilizados en las
recomendaciones de e-Europe 2002: "Criterios de calidad para
los sitios web relacionados con la salud de la Unión Europea.74
Se marcaron en color gris todos aquellos criterios y aspectos
que no podían incluirse en ninguna de las áreas temáticas
definidas en eEurope 2002. El código de colores mostró las
coincidencias y discrepancias de los criterios de las iniciativas
estudiadas entre sí y con el documento de referencia. En el caso
de criterios que pudieran incluirse en diferentes áreas temáticas
se combinaron los colores correspondientes.
En la tabla 4.3 se muestran la relación de colores que se asignaron a
cada uno de los apartados y criterios de eEurope 2002.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>1. El interés existente por la calidad de las webs de contenido sanitario y la necesidad de realizar algún tipo de "control de esta calidad queda claramente reflejado por el amplio número de iniciativas, recomendaciones y soluciones propuestas que pretenden garantizarla.
2. Durante sus siete años de existencia, WMA ha acumulado una amplia experiencia valiosa para la evaluación de la calidad de páginas web de contenido sanitarios y para el desarrollo de estrategias de mejora.
3. El alto número de usuarios que se ha adherido al programa de acreditación WMA demuestra un interés continuado en la obtención de su sello de calidad.
4. Los profesionales responsables de webs de contenido sanitario
que solicitaron el sello WMA lo reconocen como una herramienta útil que influye de forma positiva en la calidad de sus webs.
5. La ética profesional constituye la razón fundamental declarada por los profesionales solicitantes del sello de calidad WMA.
6. El gran número de webs que presentan más de un sello de calidad demuestra el interés o la necesidad por parte de las webs acreditadas de disponer de diversas certificaciones lo que podría relacionarse con el hecho de que todavía no existe un estándar de calidad universalmente aceptado para la elaboración de webs sanitarias de calidad.
7. Existe un desconocimiento general, por parte de los responsables de webs médicas, de las recomendaciones concretas de WMA, lo que podría indicar la necesidad de elaborar estrategias para una mejor información sobre el significado de los sellos de calidad.
8. La complejidad y amplitud en el conjunto de recomendaciones de las iniciativas de calidad y la diversidad de los criterios y códigos de conducta utilizados, dificultan la estandarización y la
creación de un sistema común de acreditación. La diversidad de
criterios de calidad y recomendaciones así como, en algunos
casos, su complejidad, ponen en duda que los usuarios realicen
una correcta interpretación de dichas recomendaciones.
9. Los sistemas de acreditación requieren un gran esfuerzo en su
mantenimiento y actualización.
10. Aunque las primeras webs sanitarias ofrecidas por los motores
de búsqueda más habituales pueden clasificarse de aceptables
e incluso algunas de buena calidad, aparecen un número
importante de páginas web irrelevantes o incluso con
contenidos inadecuados o peligrosos.
11. De los resultados obtenidos con los buscadores más habituales
se infiere un uso entre moderado y bajo de los sellos de calidad,
si bien se observa que las webs que los presentan acostumbran
a ser de mejor calidad que el resto</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Memoria presentada por
Miguel Ángel Mayer Pujadas
Para optar al grado de Doctor
Tesis doctoral dirigida por el Prof. Ferran Sanz Carreras
Departament de Ciencias Experimentals i de la Salut
Universitat Pompeu Fabra
Barcelona 2006</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"TECNOLOGIAS DE INFORMACION Y COMUNICACION, UNIVERSIDAD Y TERRITORIO Construcción de "campus virtuales en Argentina"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Esta tesis parte de considerar que las tecnologías en general y las TIC en
particular no son neutrales ni autónomas.
En tal sentido, una de las premisas clave de las que se parte tiene que ver con
la idea de que una pieza de tecnología adquiere su significado dentro de grupos
sociales más amplios. Es decir, no se desarrolla bajo su propia lógica técnica
inmanente. Las tecnologías adquieren significados en el mundo social y estos
significados dan forma y constriñen su desarrollo. A menudo en las etapas iniciales de
su producción, se conciben diferentes significados de una tecnología, algunos
enfrentados entre sí.
La concepción "instrumentalista de la tecnología, es decir, aquella que
considera que las tecnologías son simples herramientas o artefactos construidos para
una diversidad de tareas, colabora a consolidar la percepción de la misma como algo
"neutral. El problema mayor de ese planteo es que considera que la tecnología es
independiente de cualquier sistema político o social y así cualquier tecnología puede
ser transferida de un espacio social a otro sin mayores consecuencias.
En esta tesis se considera que tales cosmovisiones son reduccionistas y
esconden el potencial peligro que acarrea ignorar las redes de intereses sociales,
económicos y políticos de aquellos que diseñan, desarrollan, financian y controlan la
tecnología (González García, López Cerezo y Luján López, 2000).
El desarrollo de la informática agudizó la percepción de la centralidad de lo
tecnológico en los procesos de cambio social. Al mismo tiempo, pareciera que tales
procesos demandan, promueven o motivan cambios tecnológicos. La importancia de lo
tecnológico ha alcanzado una relativa centralidad entre investigadores provenientes de
tradiciones disciplinares diversas, no obstante, la pregunta por el "cómo se articulan tales cambios junto a otros político institucionales, organizacionales, económicos,
cognitivos, simbólicos y territoriales, es crucial y aún permanece abierta.
Si bien pareciera haber cada vez más esfuerzos por sistematizar teorías en
torno a distintas problemáticas de las TIC, los términos, las formulaciones y hasta las
descripciones se revelan como insuficientes frente a las dimensiones de un fenómeno
que no termina de "estabilizarse. En efecto, apenas una configuración de sentido
comienza a consolidarse se produce algún cambio en la tecnología que obliga a
revisar lo que hasta hace poco se pensaba. Para Castells (2002) el "paradigma de la
tecnología de la información no evoluciona hacia su cierre como sistema, sino hacia
su apertura como una red multifacética. Es poderoso e imponente en su materialidad,
pero adaptable y abierto en su desarrollo histórico. Sus cualidades decisivas son su
carácter integrador, la complejidad, la interconexión y la generación de redes.
La incertidumbre surge en este escenario caracterizado por la complejidad de
un entorno cada vez más abierto. Así, resulta central contar con un adecuado corpus
teórico para el estudio específico de las TIC. Por supuesto, en esta finalidad hay
desafíos complejos ya que por un lado, se debe tener en consideración la "flexibilidad
que imponen los cambios tecnológicos y, por otro lado, debe ser posible enmarcar a
los distintos campos del desarrollo de las TIC. En un escenario marcado por la
inestabilidad y velocidad del cambio junto a la obsolescencia de teorías, es importante
contribuir en la consolidación de conocimientos así como en la descripción de
experiencias, a fin de ir constituyendo un corpus que permita aproximar a la
comprensión de los fenómenos en su singularidad. En tal sentido, a medida que las
TIC se "difunden en la vida cotidiana y se internalizan y "naturalizan determinados
usos tecnológicos, se desconoce cuán complejo es el funcionamiento interno de la
tecnología lo que contribuye a crear una "percepción mágica sobre las TIC. Esta idea
"fantástica se refuerza con otra que se encuentra arraigada en algunos estudios
orientados a analizar los "impactos o los "efectos que tienen las "nuevas tecnologías
sobre la sociedad. Estas tesis "deterministas tienden a naturalizar la aparente
neutralidad y autonomía de la tecnología.
En relación a la informática, se suele describir su desarrollo como un proceso
autónomo que sigue una lógica propia al margen de cualquier dinámica social. Se
piensa así que la tecnología sigue su propio curso al margen de la intervención
humana. La tesis de la "tecnología autónoma patrocina una relación unidireccional
entre tecnología y sociedad. Se considera que los desarrollos tecnológicos influyen
significativamente en el orden social, mientras que la tecnología se muestra, por el
contrario, impermeable a la influencia de factores sociales. La influencia de la
tecnología en el ámbito social se produce, pues, desde el exterior.
Emparentadas con estas ideas se encuentran otras que sostienen que el
cambio social está determinado por el cambio tecnológico. Estos supuestos
consideran que la base técnica de una sociedad es la condición fundamental que
afecta a todos los modos de existencia social y los cambios tecnológicos son la fuente
más importante de cambios sociales.
En esta tesis, por el contrario, se sostiene que la tecnología es "moldeada
socialmente. El problema radica en que los enfoques "tecnocráticos ignoran la
complejidad de los procesos sociales. Por consiguiente, "abrir la caja negra de las TIC
se vuelve crucial a fin de evitar recaer en el viejo dualismo de tecnología y sociedad.
Para ello, analizar cómo las opciones sociales se cristalizan y encastran, es decir, se
"instituyen dentro de la tecnología, es fundamental. El desafío, por tanto, deviene
comprender cómo las tecnologías en sí mismas son socialmente construidas.
Describir los procesos de adopción y cambio de tecnologías a través de
conceptualizaciones dinámicas que den lugar a un análisis en términos de "relaciones
y "procesos requirió de un abordaje analítico conceptual que se generó mediante un
procedimiento de triangulación teórica. En dicha triangulación se combinaron
conceptos acuñados en distintos "campos disciplinares: los estudios sociales de la
tecnología, los estudios sobre la educación superior y las universidades y los estudios
territoriales.
En relación a la educación superior y a la universidad en particular, en la
denominada "sociedad de la información o "del conocimiento se modifica la misión
institucional de las universidades y también se amplían sus alcances territoriales por
medio de la generación de propuestas "virtuales de enseñanza que, en algunos
casos, trascienden sus límites institucionales. En tal sentido, los "campus virtuales,
dependiendo de las distintas tramas de significados que lo co construyen, acrecientan
las posibilidades "expansivas de la universidad ramificando, en algunos casos más
que otros, el área de influencia geográfica de las instituciones universitarias.
Así, en esta tesis se considera que un "campus virtual no es una entidad
"desterritorializada sino que está física y "virtualmente situado en un soporte dado y
sus elementos componentes circulan mediante redes. El contexto local / regional fue
uno de los elementos que influyó en la localización de las universidades nacionales
argentinas. Así, en algunos casos, se privilegió la relación con el medio socio
productivo de la región de pertenencia de dichas instituciones o las relaciones
establecidas con distintos organismos vinculados al contexto local donde se
encuentran ubicadas. De esta manera, la dimensión territorial es un elemento
importante para el análisis del objeto de estudio de esta obra y que no siempre, como veremos en el capítulo uno, se ha incorporado al análisis de los procesos de
construcción social de la tecnología.
En Argentina en particular y en América Latina en general, un enfoque que
combine distintas disciplinas se vuelve necesario para comprender las características
de la tecnología local y las posibilidades de desarrollo que éstas pueden brindar a la
región. Las condiciones en las que se producen y utilizan tecnologías en nuestros
países presentan una dinámica muy distinta a la de los países centrales. Esta
situación demanda un análisis que tenga en cuenta distintas dimensiones cognitivas,
simbólicas, político institucionales, económicas, organizacionales, territoriales y
"artefactuales y consecuentemente, genere un pensamiento propio para la región,
adecuado a las condiciones sociotécnicas locales.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivos
Una enumeración posible de los objetivos que guían la realización de esta tesis
puede sintetizarse de la siguiente manera:
Objetivos Generales
1) Contribuir a la elaboración del instrumental teórico-analítico que permita dar cuenta de la complejidad y diversidad de los procesos de co construcción de tecnologías en
general y de las Tecnologías de la Información y la Comunicación (TIC) en particular y
sus procesos de incorporación en las universidades del país.
2) Conocer las características de los procesos de incorporación de TIC en la
construcción de "campus virtuales considerando tanto las finalidades con que fueron
creados, como las políticas institucionales específicas diseñadas por las universidades
argentinas respecto a su incorporación y las estructuras organizativas definidas para su gestión.
3) Analizar la heterogeneidad de elementos presentes en los procesos de adopción de las plataformas tecnológicas utilizadas como soporte de los "campus virtuales identificando los distintos intereses y orientaciones de los diferentes actores
involucrados en su selección.
4) Contribuir en el conocimiento de las implicancias territoriales de las propuestas
"virtuales de las universidades argentinas.
Objetivos Específicos
1) Identificar y analizar aquellas acciones que promovieron la incorporación de
"campus virtuales en las universidades nacionales argentinas seleccionadas teniendo
en cuenta: 1) Los actores intervinientes; 2) La formas de organización institucional
adoptadas; 3) las principales aplicaciones tecnológicas incorporadas y 4) los alcances
territoriales de las propuestas educativas "virtuales.
2) Analizar los distintos elementos intervinientes en las decisiones político-tecnológicas
acerca del tipo de plataforma utilizada -de distribución "libre, de distribución
propietaria o un desarrollo original producido por la universidad- en relación a las
diferentes misiones y objetivos institucionales de las universidades nacionales
seleccionadas y los significados conferidos por los actores involucrados a los "campus
virtuales que adoptaron.
3) Identificar las distintas modalidades de uso dadas a los "campus virtuales teniendo
en cuenta el uso "externo aquellas universidades nacionales que brindan oferta
académica en formato "virtual tanto de pregrado, grado y/o posgrado y cursos de
extensión - así como el uso "interno aquellas universidades nacionales que utilizan el
"campus virtual como apoyo a sus actividades "presenciales- y las implicancias
territoriales de las distintas propuestas.
4) Caracterizar los distintos intereses presentes en el espectro de opciones
tecnológicas posibles para los diferentes actores involucrados en la construcción de
los "campus virtuales estudiados.
5) Integrar en el análisis de los casos seleccionados los aspectos políticos
institucionales, organizacionales, económicos, "artefactuales, territoriales, simbólicos
y cognitivos inherentes a la construcción de los "campus virtuales a partir de
establecer los diferentes procesos que conducen a su desarrollo y consolidación.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Esta tesis está orientada por el siguiente juego de hipótesis de carácter
general:
En primer lugar, se parte de considerar que la historia de la tecnología y en particular de las llamadas Tecnologías de la Información y la Comunicación, deriva de las sofisticadas "innovaciones desarrolladas en los países "centrales pero también de los distintos usos alternativos y apropiaciones creativas de todo tipo de técnicas y artefactos que tienen lugar en los países "periféricos.
Así, en segundo lugar, si se tiene en cuenta un contexto que, desde mediados del siglo XX, pareciera caracterizarse por una creciente valorización del conocimiento científico como insumo económico y productivo a tal punto que algunos trabajos conceptualizan al conocimiento como una "materia prima" o un "nuevo factor de producción" junto a los cambios derivados, entre otros, de los avances en la automatización, la microelectrónica, la informática y las biotecnologías al ámbito productivo - una vía de entrada para dar cuenta de éste fenómeno es preguntarse por los desarrollos tecnológicos, en especial lo que atañen a las TIC y al software en particular, que se producen en universidades argentinas. No obstante, dar cuenta sólo
de la creación de software que se produce no es suficiente sino se tienen en cuenta
los usos alternativos y apropiaciones creativas que se realizan a uno existente.
En tercer lugar, se parte de sostener que los procesos de incorporación de
TIC que conducen a la construcción de "campus virtuales no se dan nunca de modo
homogéneo y dependen en gran parte de la disponibilidad de recursos -financieros,
tecnológicos, "humanos, simbólicos, cognitivos, entre otros" las características de la
institución, así como también de las distintas estrategias desplegadas por los actores
involucrados.
Asimismo, en cuarto lugar, se sostiene que la construcción de "campus
virtuales no es un acontecimiento aislado ya que refleja un estado determinado de
conocimiento, una cierta disponibilidad de aptitudes para definir un problema técnico y
resolverlo y una red de actores que además de desarrollar sus experiencias de forma
acumulativa, aprenden al desarrollarlas. En tal sentido se sostiene que las tecnologías
son "moldeadas socialmente. La interactividad inherente a las TIC es un rasgo que
debe considerarse para comprender los procesos de su incorporación. La morfología
de la red parece estar adaptada para una complejidad de interacción creciente y para
pautas de desarrollo impredecibles que surgen del poder creativo de esa interacción.
En el señalado marco hipotético y paralelamente a lo indagado en la
bibliografía existente, junto a la información empírica relevada proveniente de los
casos estudiados, se generaron distintos interrogantes que permitieron orientar las
indagaciones y delimitar el objeto de estudio de esta tesis. De esta forma, se
enumeran a continuación algunas de las preguntas que guiaron esta investigación:
-Si partimos de considerar que la tecnología en general y las TIC en particular son
construcciones sociales, * ¿de qué manera los procesos sociales influyen en el
contenido mismo de la tecnología?
¿Qué características asumen los "campus virtuales de las universidades nacionales
del país? * ¿Cuándo surgen y con qué propósitos? * ¿Qué actores intervienen en los
momentos fundacionales?
¿Qué tipo de tecnología utilizan las universidades nacionales argentinas? * ¿Cómo se
selecciona la tecnología? * ¿Cuál es el significado que los actores le atribuyen a la
tecnología empleada? * ¿Cómo se produce el cambio de tecnología?
¿Qué determina la adopción de un desarrollo tecnológico propio? * ¿Cómo se
construye la agenda de posibilidades tecnológicas? * ¿Qué establece que figure o no en
la agenda de los equipos de gestión responsables del "campus virtual de la
universidad la posibilidad de producir un desarrollo propio?
* ¿Por qué una solución tecnológica es más utilizada que otra? * ¿Por qué no otra? * ¿Por
qué se piensa que una nueva plataforma tecnológica "funciona mejor? * ¿Por qué se
adoptó o no software libre o propietario?
¿Cómo se puede explicar el hecho de que en ciertos casos las trayectorias de los
"campus virtuales sean "exitosas?
¿Cómo se desarrollan las redes de vinculación interinstitucional entre los actores
(universidades) involucrados en la construcción de "campus virtuales y otros actores
(universidades, empresas, organismos públicos) nacionales e internacionales?
¿En qué medida las políticas institucionales de las universidades nacionales
analizadas inciden en los procesos de consolidación de los "campus virtuales
estudiados?
¿Cuáles fueron los cambios en las estrategias de los actores técnicos responsables
del "campus virtual de la universidad frente a la crisis desatada producto del quiebre
del modelo de convertibilidad monetaria que se produjo en la Argentina en el año
2001?
¿Qué implicancias conlleva en el territorio nacional la creación de "campus virtuales?</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Esta tesis espera realizar distintas contribuciones. En primer lugar,
incorporando determinadas herramientas conceptuales desarrolladas en el campo de
los estudios sociales de la ciencia y la tecnología que no han sido mayormente
aplicadas al estudio particular de las Tecnologías de la Información y la Comunicación.
Añadir aquellos conceptos presentes en el estudio de las tecnologías en general
permitiría comprender lo particular de los procesos de incorporación de TIC y de la
construcción de "campus virtuales en universidades del país. Junto con ello,
proporcionar nuevos andamiajes teóricos vinculados con las características
inherentes a las TIC. Este propósito se presenta como inédito ya que pocos trabajos
en el ámbito de las ciencias sociales han analizado las dinámicas de desarrollo
tecnológico que posibilitan la construcción de entornos "virtuales que se gestan en las
universidades desde un enfoque que se encuadre en el propuesto por el campo de los
estudios sociales de la ciencia y la tecnología.
En ese sentido se aspira a establecer una ruptura con aquellas formas
"cristalizadas de determinismo tecnológico que consideran que la creciente "difusión
y proliferación de las Tecnologías de la Información y la Comunicación producen
"efectos en la "sociedad y se limitan a una mirada meramente instrumental dejando
de lado aquellas que hacen hincapié en la tecnología como construcción social,
entendiendo que no hay una relación sociedad-tecnología, como si se tratara de dos
entidades separadas y que la distinción misma entre social y técnico debe
considerarse como el resultado de un proceso de "co-construcción.
Por otra parte, esta investigación espera contribuir al campo de los estudios
de la educación superior y los estudios sobre universidad en el cual aparece
como vacante la problemática vinculada a la educación universitaria "virtual. La
utilización de la tecnología para la enseñanza y la investigación no es únicamente una
cuestión técnica, puesto que replantea cuestiones fundamentales no sólo sobre los
métodos de trabajo sino sobre las metas y los propósitos que persiguen las
instituciones universitarias. Así, el estudio de los "campus virtuales de las
universidades nacionales argentinas podría contribuir a replantear la dinámica de
formulación de políticas y toma de decisiones al interior de la universidad.
Por último se espera contribuir a los estudios que relacionan los procesos
de "innovación tecnológica y los territorios indagando en la nueva dimensión
espacio-temporal que generan las TIC y en las condiciones territoriales, económicas,
sociales, organizacionales y simbólicas, entre otras, que inciden sobre la
implementación de los "campus virtuales.

De esta forma, se busca abrir nuevas posibilidades de triangulación teórica,
entre la sociología de la tecnología, los estudios vinculados a la educación superior y a
la universidad en particular y los estudios orientados a comprender las distintas
dinámicas de los territorios contribuyendo a ampliar los alcances de los actuales
estudios sociales de la tecnología.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los procesos de incorporación de TIC que tienen lugar en las universidades
nacionales argentinas exhiben un alto grado de complejidad. En el primer capítulo de
la tesis se realizó un sucinto recorrido por la literatura que se ocupa de estudiar a la
tecnología incorporando diversas perspectivas disciplinares a su estudio. A través de
indagar los principales temas y problemas que abordan los estudios sociales de la
tecnología se mostró cómo, especialmente en los primeros años de la década del
2000, la relación entre tecnología-sociedad aparece como un campo de debate. Desde
las ciencias sociales, algunas investigaciones posicionan a la tecnología como
protagonista de gran parte de los problemas críticos que enfrenta la "sociedad. En
dichas investigaciones, las TIC suelen considerarse el detonante tecnológico de una
transformación social de gran alcance y similar en magnitud a la revolución industrial.
No obstante, en algunos casos, al estar impregnados de una concepción
"determinista esos enfoques hacen hincapié en los "efectos, "impactos o "difusión
de las TIC y al no proliferar una perspectiva constructivista acerca de la tecnología,
refuerzan la dicotomía entre "tecnología y "sociedad al tratar a las TIC como una
"caja negra.
Además, si bien se identificaron distintas investigaciones que estudian los
procesos de incorporación de TIC en la educación superior, a medida que la literatura
se acrecienta, no se registran estudios que analicen los procesos de incorporación de
TIC en las universidades a través del diseño de categorías teóricas específicas para
su análisis; por el contrario, en la mayoría de los casos, indagan en algunas de las
particularidades del fenómeno mediante descripciones y caracterización de
"tendencias. Al mismo tiempo, si bien se encuentran distintos estudios que indagan
sobre el vínculo entre "universidad y TIC, no abundan investigaciones que se ocupen
de analizar dichos vínculos desde una perspectiva que estudie a las TIC como
"socialmente construidas.
La "convergencia en Internet de redes de comunicación soportadas por las
TIC, así como los distintos cambios referidos a su acceso, ha sido una de las
características más remarcable desde sus inicios. No obstante, su relativa
"masificación en la Argentina data de los últimos años. Así, se consideró necesario
dar cuenta de cómo fueron virando los accesos a Internet en el transcurso de los años
sobre todo dado que el primer "campus virtual construido en una universidad nacional
argentina data del año 1999 cuando en el país los cálculos más "optimistas
establecían una relación de una población usuaria de 2.8% sobre la población total del
país. El hecho de indagar acerca de las características del acceso a Internet en la
Argentina también permite "contextualizar la dimensión territorial de las TIC.
Dado que esta tesis considera que tanto las tecnologías en general como las
TIC en particular son "moldeadas socialmente, la perspectiva puesta en un análisis
que tiende a privilegiar herramientas conceptuales y perspectivas teóricas de los
enfoques "constructivistas acerca de la tecnología, a la par de los estudios territoriales
y universitarios, permiten comprender a las TIC como parte de un proceso en
donde se ensamblan elementos que no son sólo "técnicos. En tal sentido, como
se trató en el capítulo 2, para comprender las particularidades de los "campus
virtuales de las universidades nacionales argentinas, se tornó necesario abordar su
estudio teniendo en cuenta que dichos "campus remiten a determinadas redes de
relaciones entre elementos heterogéneos "sociales y "técnicos- que se encuentran
"enredados en un tipo de espacio particular: un "espacio red.
Así, el "campus virtual, como "espacio red, es un entramado sociotécnico
soportado por las TIC en el cual el espacio-tiempo que éstas generan permite la
interacción en un tiempo artificial y asimismo, da lugar al surgimiento de nuevas
territorialidades "virtuales que traspasan las fronteras geográficas al vez que las
juridiscciones de las universidades en sus distintas escalas y que no en todos los
casos están reguladas. A su vez, dicha noción aporta otra perspectiva a los estudios
sociales de la tecnología, situando al territorio como parte importante de los procesos
de construcción de las tecnologías.
Al mismo tiempo, como se trató en el transcurso de esta obra, dicho "espacio
red está integrado por diversos elementos: actores individuales y colectivos
(docentes, alumnos, "gestores, técnicos, universidades, empresas proveedoras de
tecnología, empresas proveedoras de housing, municipios, gobiernos provinciales,
fundaciones, entre otros) y tecnologías (software desarrollados por las universidades,
u otros de distribución "libre o "propietarios, bases de datos, distintos lenguajes de
programación, etcétera). Considerar al "espacio red en tanto un espacio de flujos de
relaciones permitió, a su vez, mostrar que los elementos que lo componen se distintos momentos que atravesó el "campus y las distintas redes de relaciones
entabladas por ellos con la tecnología, se imbricaron en ese espacio-red confiriéndole
distintos sentidos.
La tesis muestra cuáles fueron los elementos presentes en la incorporación de
determinadas tecnologías y cómo finalmente quedaron "instituidas distintas
cosmovisiones al interior de los "campus virtuales estudiados. En tal sentido, cómo
fueron concebidos determinados problemas y sus respectivas soluciones, hizo
lugar a ciertas opciones y no otras. Esto se vincula, a su vez, a los diversos
sentidos que se construyeron en los orígenes de los "campus. Así, cada una de las
universidades analizadas partió de cierta concepción reduccionista acerca de lo que es
un "campus virtual que limitó su complejidad subyacente y se ensambló con el tipo de
tecnología seleccionada. La "flexibilidad interpretativa del "campus virtual se
materializó no sólo en el modo en que los distintos actores interpretaron los "campus
virtuales sino también en el modo en que dichos "campus fueron diseñados y se
basaron en la adopción y/o desarrollo de plataformas tecnológicas distintas.
De esta manera, "correr el velo que recubre a las distintitas opciones
cristalizadas en las selecciones "técnicas, permite dar cuenta que las tecnologías en
general y las TIC en particular no son neutrales ni autónomas. Detrás de las políticas
institucionales promovidas por las universidades se encuentran también ocultos
posicionamientos político-tecnológicos. En tal sentido, la "apertura de las cajas
negras es tanto una herramienta heurística como una estrategia que permite entrever
distintas alianzas políticas.
Las secciones siguientes se ocupan de señalar, por un lado, los puntos
divergentes y convergentes en las distintas opciones sociotécnicas acerca de los
"campus virtuales basados en software "propietario, "propios y "libres. Por otro, se
esbozan distintas reflexiones que conducen a generar nuevos interrogantes y la
posibilidad de continuar con distintas líneas de estudio.
Dado que la estrategia metodológica de esta tesis se basó en estudios de caso
se torna necesario aclarar que el análisis de los ocho casos que se seleccionaron
difícilmente podría arribar a conclusiones generales. No obstante, tal aclaración refiere
específicamente al hecho de que las afirmaciones de estas conclusiones ofrecerían
elementos e insumos a ser corroborados o utilizados en futuras indagaciones. Esta
tesis hace algo más que describir y analizar distintos desarrollos tecnológicos: ilumina
el carácter multidimensional de la tecnología.
encuentran acoplados y entrelazos en él, es decir, lo co-construyen.
Asimismo, como se trató en los capítulos 4, 5 y 6, las distintas relaciones que
en algunos casos quedaron "encastradas en los distintos software incorporados, no
fueron estables y "fijas en el tiempo, sino que fueron mutando y, de acuerdo a las
particularidades de cada situación analizada, otorgaron nuevos sentidos y significados
al "campus virtual. Así, los distintos actores que se fueron incorporando durante los</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad Nacional de Quilmes
Doctorado con mención en Ciencias Sociales y Humanas
Tesis de doctorado
TECNOLOGIAS DE INFORMACION Y COMUNICACION, UNIVERSIDAD Y TERRITORIO Construcción de "campus virtuales en Argentina
Luciana Mónica GUIDO
Directora: Ester SCHIAVO
Co Directora: Elsa LAURELLI</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Marco para la Elicitación de Requisitos Software en Procesos de Desarrollo Global"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La tendencia de la globalización de la empresa, y especialmente de los negocios
relacionados con la producción de tecnologías de software, ha producido un profundo
impacto tanto en las estrategias de marketing y distribución, como en la manera en
que los productos son concebidos, diseñados, construidos, probados y entregados a
los clientes [Herbsleb y Moitra, 2001]. Ejemplo de ello es que cada vez es más común
el desarrollo de software en forma distribuida y el desarrollo global de software (GSD
según sus siglas en inglés), donde quienes participan del proceso de desarrollo (usuarios,
clientes, desarrolladores) se encuentran localizados en sitios geográficamente distantes
entre sí. El principal motivo del crecimiento del desarrollo global de software es que
permite a las empresas disminuir los costos de desarrollo mientras se mantiene el nivel
de calidad del proceso [Audy et al., 2004], contando con profesionales a lo largo y ancho
del mundo sin necesidad de afrontar el costo de su traslado [Kobitzsch et al., 2001].
Además, el desarrollo global de software permite producir software para clientes
remotos sin necesidad de trasladar el equipo de desarrolladores, así como lograr mayor
productividad, por medio de jornadas de trabajo más extensas teniendo programadores
distribuidos en sitios con amplia diferencia horaria [Ebert y De Neve, 2001]. Sin
embargo, a la vez, el desarrollo global de software enfrenta una serie de problemas
ocasionados tanto por la distancia entre los participantes como por la diversidad
cultural de las personas involucradas, y donde la comunicación se ve limitada por la tecnología utilizada y por la diferencia horaria que obstaculiza la interacción sincrónica,
así como la problemática relacionada con la gestión del conocimiento en entornos
distribuidos [Damian y Zowghi, 2002, Ebert y De Neve, 2001].
Al analizar la literatura existente sobre el desarrollo global de software, se ha
notado que la investigación se ha centrado en determinar las limitaciones de la
comunicación interpersonal y la gestión del conocimiento en entornos distribuidos,
con el objetivo de definir su implicación en el proceso de desarrollo de software,
pero que por lo general estos trabajos se dedican a las etapas más avanzadas de la
ingeniería de requisitos (como la negociación y la especificación) [Damian et al., 2004,
Hargreaves et al., 2004, Peters, 2003] y son muy pocos los trabajos referidos a las
etapas iniciales [Audy et al., 2004]. Por ejemplo, respecto a la etapa de elicitación de
requisitos se han encontrado sólo algunas herramientas para dar soporte al proceso
[Lanubile, 2003, Togneri et al., 2002] y algunos experimentos conducidos con el objeto
de definir y comparar la efectividad de algunas técnicas de elicitación de requisitos sobre
otras [Audy et al., 2004, Lloyd et al., 2002]. Por lo tanto, las propuestas para mejorar
la etapa de elicitación de requisitos en entornos virtuales está aún en sus inicios y
existen muchas líneas de trabajo posibles.
Aunque la etapa de elicitación de requisitos comúnmente no se estima entre las
más importantes, estudios estadísticos revelan que más del 80 por ciento de los defectos
del software, una vez entregado a los usuarios, se debe a fallas al definir los requisitos
[Young, 2002]. Considerando que es una etapa donde la comunicación interpersonal es
crucial para obtener información sobre el sistema que se desea construir y las personas
que se verán envueltas en el proceso de ingeniería [Nuseibeh y Easterbrook, 2000], es
primordial lograr una interacción fluida entre desarrolladores, clientes, usuarios, y otros
miembros de la organización.
Nuestro objetivo, por lo tanto, es proponer una serie de etapas que mejoren la
comunicación durante la elicitación en entornos distribuidos y, como consecuencia,
obtener requisitos más precisos. Para ello se utilizaron conceptos innovadores en esta
etapa de la Ingeniería del Software que pertenecen a un área de investigación llamada
Informática Cognitiva, disciplina que se centra en el estudio del procesamiento de
la información en el cerebro, especialmente respecto a actividades como adquisición,
representación, y comunicación de la información [Chiew y Wang, 2003].
Por otro lado, dado que la calidad del software depende de la calidad de los requisitos
y esta, a su vez, de las técnicas utilizadas para su elicitación [Hickey y Davis, 2003a] se ha focalizado nuestro estudio en analizar una amplia cantidad de métodos y técnicas de elicitación existentes, y en buscar literatura relacionada al proceso de selección
de las técnicas a utilizar. Este tema en especial, el proceso de selección de técnicas
de elicitación apropiadas a una situación particular y de los factores que tienen
impacto en este proceso, se ha convertido en un foco de interés importante durante
los últimos años [Aurum y Wohlin, 2005]. Los trabajos encontrados en nuestra revisión
demuestran que se ha avanzado en la realización de análisis de casos y comparaciones
entre algunas técnicas de elicitación [Browne y Rogich, 2001, Browne y Ramesh, 2002,
Lloyd et al., 2002, Carrizo Moreno, 2004, Carrizo Moreno y Dieste, 2007], sin que se
hayan presentado aún estrategias de selección completamente validadas. Un trabajo
que se perfila como base de investigaciones relacionadas con la elicitación de requisitos,
es el modelo propuesto por [Hickey y Davis, 2003b] para las actividades de ingeniería de
requisitos en entornos de desarrollo co-localizado, que al ser genérico permite extenderse
para estudiar la selección de técnicas de elicitación en entornos distribuidos.
Particularmente, nuestro trabajo se ha enfocado en analizar en qué medida los
factores que desafían la comunicación en el trabajo global se presentan en un
equipo que debe realizar la tarea de elicitación de requisitos, y a partir de ese
diagnóstico se han estudiado las estrategias utilizadas para minimizar dichos problemas
[Herbsleb y Moitra, 2001, Carmel y Agarwal, 2001]. También se han propuesto nuevas
estrategias, por ejemplo, mediante el análisis de los aspectos cognitivos de las personas
participantes en este tipo de proyecto. Para ello se han utilizado técnicas psicológicas
que han sido aplicadas anteriormente en el campo de la informática para el estudio
de los estilos cognitivos en el aprendizaje de programación u orientar el diseño
de cursos [Bostrom et al., 1988, Wu et al., 1998, Moallem, 2002, Thomas et al., 2002,
Blank et al., 2003]) pero que, de acuerdo a nuestra investigación, nunca habían sido
relacionadas anteriormente con la selección de tecnología durante la etapa de elicitación</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Sobre la base del objetivo principal, se define el siguiente conjunto de objetivos
parciales:
Objetivo A
Estudiar las técnicas de elicitación de requisitos existentes, analizando ventajas y
limitaciones de su uso en un entorno de desarrollo global de software y teniendo
en cuenta los aspectos cognitivos de las personas involucradas.
Objetivo B
Estudiar las herramientas groupware existentes analizando ventajas y
limitaciones de su uso en la etapa de elicitación de requisitos en un entorno de
desarrollo global de software, respecto a los aspectos cognitivos de las personas
involucradas.
Objetivo C
Estudiar modelos cognitivos existentes y analizar su aplicación al análisis de un
grupo de personas trabajando en un proceso de elicitación de requisitos en un
entorno de desarrollo global de software.
Objetivo D
Proponer un marco que sirva para:
1. La detección de problemas que pueden surgir en un equipo que realiza
la tarea de elicitación de requisitos en un entorno de desarrollo global de
software
2. La implementación de un conjunto de estrategias que resuelvan o minimicen,
principalmente problemas relacionados a la comunicación y a la diversidad
cultural en equipos de desarrollo global de software que realizan la tarea de
elicitación de requisitos.
Objetivo E
Implementar una herramienta que soporte las etapas del marco propuesto.
Objetivo F
Experimentar y validar el marco propuesto</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Nuestra hipótesis de trabajo es que:
Es factible mejorar el proceso de elicitación de requisitos en un entorno de
desarrollo global de software, mediante la aplicación de estrategias seleccionadas
de acuerdo a las características del equipo virtual.
Basándonos en esta hipótesis se ha formulado el objetivo principal de nuestra
investigación que es:
Definir un marco que describa un proceso sistemático para la elicitación
de requisitos en un entorno de desarrollo global de software, basado en el
análisis de las características del equipo virtual, y destinado a mejorar la
comunicación entre los stakeholders y, como consecuencia, el proceso de
elicitación de requisitos.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Conclusiones
En este capítulo se analizan los resultados del trabajo llevado a cabo en esta tesis.
De esta manera, los apartados incluidos son los siguientes: análisis de la consecución
de objetivos, principales aportes de esta tesis, contraste de los mismos en publicaciones
científicas y líneas de trabajo abiertas.
Análisis de la consecución de objetivos
En el primer capítulo de esta tesis se han presentado los objetivos parciales que se
pretendían cumplir para satisfacer el objetivo principal de nuestra investigación, que
es el siguiente:
Definir un marco que describa un proceso sistemático para la elicitación
de requisitos en un entorno de desarrollo global de software, basado en el
análisis de las características del equipo virtual, y destinado a mejorar la
comunicación entre los stakeholders y, como consecuencia, el proceso de
elicitación de requisitos.
A continuación se presenta una valoración de la consecución de cada uno de los objetivos
parciales:
Objetivo A
Estudiar las técnicas de elicitación de requisitos existentes, analizando
ventajas y limitaciones de su uso en un entorno de desarrollo global de
software y teniendo en cuenta los aspectos cognitivos de las personas
involucradas.
Se ha revisado ampliamente en literatura a fin de conseguir una visión general
de los métodos y técnicas utilizados para elicitación de requisitos en entornos
co-localizados y multi-sitio. Además, se han analizado sus características más
importantes respecto a las categorías del modelo de estilos de aprendizaje de
Felder-Silverman. También se ha centrado esta revisión en el estudio de ejemplos
de su uso en entornos de desarrollo global de software y trabajos preliminares
sobre la adecuación de ciertos métodos y técnicas a los entornos globales (Sección
3.1). Este objetivo creemos que está completamente cubierto.
Objetivo B
Estudiar las herramientas groupware existentes analizando ventajas
y limitaciones de su uso en la etapa de elicitación de requisitos en
un entorno de desarrollo global de software, respecto a los aspectos
cognitivos de las personas involucradas.
Se ha realizado una búsqueda extensa en la literatura a fin de conseguir una lista
de herramientas groupware que puedan utilizarse durante la etapa de elicitación
de requisitos, analizando sus características más importantes (Sección 3.2.2.3).
Este objetivo creemos que está completamente cubierto.
Objetivo C
Estudiar modelos cognitivos existentes y analizar su aplicación al análisis
de un grupo de personas trabajando en un proceso de elicitación
de requisitos en un entorno de desarrollo global de software.
Se han estudiado cinco modelos de estilos cognitivos y de aprendizaje
ampliamente utilizados y se eligió uno de estos modelos como base de nuestra
propuesta. También, se han encontrado y analizado casos de aplicación de estos
modelos para plantear una solución a problemas en la enseñanza de informática
y en algunas actividades de la ingeniería de software, sin que se haya encontrado
trabajos relacionados con la etapa de elicitación de requisitos (Sección 3.2.1.1).
Consideramos que este objetivo está completamente cubierto.
Objetivo D
Proponer un marco que sirva para:
1. La detección de problemas que pueden surgir en un equipo que
realiza la tarea de elicitación de requisitos en un entorno de desarrollo
global de software Se han estudiado los desafíos más comunes reportados en la bibliografía
para equipos de desarrollo global y se ha propuesto una manera de ponderar
cuatro factores relacionados a la distancia geográfica y la diversidad cultural.
Estos factores son: el grado de solapamiento de horarios de trabajo, el
grado de conocimiento de un idioma común todos los stakeholders, el
grado de diferencia cultural y las diferencias respecto al estilo cognitivo de
los stakeholders. La valoración de cada uno de estos factores es utilizada
en el marco de nuestro trabajo para determinar el grado de necesidad
de aplicar estrategias que minimicen los problemas que dichos factores
pueden introducir en el comportamiento del equipo virtual (Sección 4.3.1).
Consideramos que este objetivo está completamente cubierto.
2. La implementación de un conjunto de estrategias que resuelvan
o minimicen, principalmente problemas relacionados a la comunicaci
ón y a la diversidad cultural en equipos de desarrollo global
de software que realizan la tarea de elicitación de requisitos.
Como parte de nuestro trabajo se ha propuesto un conjunto de estrategias
para minimizar los problemas ocasionados por la distancia geográfica y
la diversidad cultural (Sección 4.3.2). Algunas estrategias forman parte
de prácticas conocidas, como el uso de una ontología para facilitar la
comunicación entre stakeholders así como la gestión del conocimiento
del dominio bajo estudio. Otras estrategias son innovadoras, por ejemplo
respecto al uso de técnicas psicológicas para determinar la manera que
los stakeholders prefieren percibir el mundo que los rodea, como son los
modelos de estilos de aprendizaje. Para implementarlo se ha propuesto un
modelo basado en conjuntos difusos a fin de conseguir reglas que puedan
ser aplicadas a un grupo de stakeholders para sugerir qué herramientas
groupware y qué técnicas de elicitación son más acordes a sus aspectos
cognitivos. Respecto a este objetivo, aunque se ha presentado una propuesta
preliminar, queda pendiente el análisis de algoritmos basados en lógica
difusa y algoritmos de aprendizaje automático que nos permitan obtener
reglas de preferencias cuando los miembros de un equipo tienen estilos
cognitivos en extremos opuestos de una categoría. Además, se ha propuesto
una estrategia para ayudar a minimizar los problemas ocasionados por la
diversidad cultural, mediante el uso de simuladores para el entrenamiento de los stakeholders tanto en el uso del idioma como de las costumbre relacionadas a la cultura foránea. Finalmente, se ha propuesto un modelo de
selección de técnicas de elicitación de requisitos para entornos distribuidos,
basado en un modelo genérico previo propuesto por [Hickey y Davis, 2003b],
incorporando la consideración de la valoración de los factores analizados
previamente. Consideramos que este objetivo está completamente cubierto.
Objetivo E
Implementar una herramienta que soporte las etapas del marco propuesto.
Está en proceso de desarrollo una herramienta que soporta las primeras dos
etapas del marco, permitiendo el ingreso de datos de los miembros de un equipo
virtual, mediante los formularios diseñados en la etapa 1, así como el posterior
cálculo de las reglas de preferencia y su aplicación para seleccionar la tecnología
más apropiada para un equipo dado (Sección C). Además, está programada la
extensión de dicha herramienta para el ingreso de datos y la obtención del estilo
de aprendizaje de acuerdo al test de Felder y Soloman sin necesidad de la conexión
al sitio web de la Universidad de North Carolina.
Objetivo F
Experimentar y validar el marco propuesto.
Se ha realizado un experimento para validar la aplicación de la estrategia B
(uso de una ontología de dominio) y la estrategia C2 (selección de herramientas
groupware para equipos tipo 2, con preferencia fuerte, sin conflicto). En un
futuro es necesario replicar dicho experimento, a fin de contrastar los resultados
obtenidos, así como realizar varios experimentos más, de la misma familia, para
validar las estrategias restantes de nuestra propuesta.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Departamento de Tecnologías y Sistemas de Información
Universidad de Castilla-La Mancha
Tesis Doctoral
Marco para la Elicitación de Requisitos Software en Procesos de Desarrollo Global
Gabriela Noemí Aranda
Area de Conocimiento: Lenguajes y Sistemas Informáticos
Ciudad Real, España
Diciembre 2008</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Recuperación de información con resolución de ambigüedad de sentidos de palabras para el español"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La recuperación de información (Information Retrieval, IR, del inglés) consiste en la
tarea de ordenar los documentos, tanto de texto como de multimedia, que pertenecen a
una colección dada de acuerdo a la probabilidad estimada de relevancia para las
necesidades de información del usuario. Estas necesidades de información se expresan generalmente por el usuario en función de las respuestas obtenidas a un requerimiento de
un lenguaje no formalizado (por ejemplo, sentencias) o un conjunto de términos en un
lenguaje natural.
El esfuerzo requerido para la recuperación de la información es notoriamente
complejo debido a que la relación de "relevancia entre los documentos y las necesidades
de información son dependientes de las preferencias e interpretaciones subjetivas del
usuario. Además, esta relación es no formalizable [Saracevic, 1995].
La enorme disponibilidad actual de documentos almacenados electrónicamente,
especialmente en plataformas distribuidas, ha transformado la recuperación de la
información en una disciplina importante. La World Wide Web (WWW) contiene
grandes cantidades de información (unos 2000 millones de páginas que abarcan unos 38
terabytes de datos y que crece 7 millones de páginas diariamente-; también contiene
alrededor de 450 millones de imágenes, julio de 2000) [Pimienta, 2000], [Lawrence,
2000] potencialmente interesantes y accesibles para muchos usuarios (615 millones para
el 2002, de los que 48 millones hablan español; 1030 millones para el 2005, de los que
80 millones hablan español) [Global Reach, 2002].
Estas cifras actualizadas al 2006 por la University of California at Berkeley y
Whois.Net Domain-Based Research Services plantean la existencia de 1,000 millones de
usuarios en Internet, que alcanzará los 2 mil millones para el 2016, 530 mil terabytes de
información en Internet, 320 millones de búsquedas diarías y más de 95 millones de
dominios resgistrados (38 millones en uso continuo).
Uno de los problemas de recuperación de información en los portales de Internet
como los portales dinámicos Altavista, Google, Yahoo, etc., y en bibliotecas digitales
como la Biblioteca del Congreso de los USA, es el de brindar diversas respuestas con
muy baja pertinencia con respecto a los intereses del usuario.
Por ejemplo: un economista busca "historia del banco y obtiene respuestas sobre los "bancos de arena, "bancos de madera y las "instituciones financieras. Un músico busca "formato de letra y obtiene respuestas sobre el "documento comercial de pago, "letras del alfabeto y "letras musicales. Estas imprecisiones se deben a los
distintos sentidos que tienen las palabras.
La WSD es considerada como uno de los problemas de investigación más
importantes en el procesamiento del lenguaje natural [Wilks y Stevenson, 1996]. Es
esencial para las aplicaciones que requieren la comprensión del lenguaje y de mensajes,
la comunicación hombre-máquina, la recuperación de información y otros. Se requiere en
aplicaciones de:
* Traducción automática: se refiere más que nada a la traducción correcta de
información de un lenguaje a otro según lo que se quiera expresar en cada oración
y no sólo palabra por palabra. Una aproximación a este tipo de traductores en
Internet es el Babylon.
* Extracción de información y resúmenes. Los nuevos programas deben tener la
capacidad de crear el resumen de un documento sobre la base de los datos
proporcionados, con un análisis detallado del contenido sin truncar las primeras
líneas de los párrafos.
* Reconocimiento de voz. Es una de las aplicaciones de PLN que más éxito ha
tenido en la actualidad, ya que es común que las computadoras de hoy tengan esta
facilidad. El reconocimiento de voz puede tener dos usos posibles: identificar al
usuario o procesar lo que el usuario dicte y ya existen programas comerciales
accesibles por los usuarios, por ejemplo: ViaVoice.
* Recuperación de la información. Un ejemplo claro de esta aplicación es el
siguiente: una persona llega a la computadora y le dice en Lenguaje Natural qué
es lo que busca; ésta busca y le dice lo que tiene referente al tema.
En los últimos diez años se han multiplicado las investigaciones para desambiguar
palabras automáticamente, crear métodos de identificación y usar las irregularidades
encontradas. Los sistemas actuales de recuperación de información en línea carecen de un
método inteligente que permita mejorar su eficiencia. Por lo tanto, este trabajo de
investigación se concentra en crear un Método de desambiguación de sentidos de
palabras usando grandes recursos léxicos para ser aplicado en la recuperación de la
información y en la navegación en hipertexto.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Se cree que el disponer de servicios eficientes de recuperación inteligente de información
permite mejorar la respuesta a los usuarios que buscan información. En base a esta
hipótesis, el objetivo general de esta investigación es diseñar un nuevo Método de
desambiguación de sentidos de palabras usando grandes recursos léxicos que mejore
la pertinencia de la información recuperada.
Objetivos específicos
El desarrollo de métodos para la recuperación de la información es en la
actualidad una de las tareas de investigación en Ciencias de la Computación, ya que
permite mejorar la eficiencia en la obtención de la información pertinente.
Debido a esto, es estratégico crear métodos que contribuyan a alcanzar esas
metas y en consecuencia, es necesario diseñar nuevas técnicas de recuperación
inteligente, que permitan la resolución de ambigüedad de sentidos de palabras.
De forma genérica, la WSD consiste en la asociación de una palabra en un texto
con una definición o significado dado que la distingue de otros significados atribuibles a
esa palabra. La asociación de las palabras a diversos sentidos depende de dos recursos de
información: contexto y recursos de conocimientos externos.
El contexto de la palabra a ser desambiguada se considera como el conjunto de
palabras que acompaña a la palabra a desambiguar junto con las relaciones sintácticas,
categorías semánticas, etc. Los recursos de conocimiento externos son los recursos
léxicos (WordNet), enciclopédicos, etc., desarrollados de forma manual o automatizada,
que proporcionan datos valiosos para asociar las palabras con los diversos sentidos
posibles.
Los objetivos específicos de este trabajo se orientan hacia el diseño de un nuevo
Método de desambiguación de sentidos de palabras usando grandes recursos léxicos y
son:
1. Preparar los recursos léxicos (diccionarios) para usar en la desambiguación.
2. Diseñar un nuevo método de WSD teniendo en cuenta el contexto local del
documento donde se ponderen los posibles sentidos en función de esa área
limitada dentro del mismo al usar diferentes recursos léxicos (diccionario
explicativo Anaya, diccionario WordNet para el español, diccionario de
sinónimos ) y la semejanza entre las palabras y que permita:
a. Analizar y determinar los tamaños óptimos del contexto a usar en el
proceso de desambiguación.
b. Calcular los pesos de los sentidos asociados a cada palabra.
c. Aplicar la intersección de contextos y definiciones.
d. Aplicar el diccionario de sinónimos.
e. Limitar de forma automática la sustitución en profundidad.
f. Aplicar las primitivas semánticas.
3. Preparar una colección de documentos de prueba con ciertos criterios, aplicarle el
método y analizar su comportamiento.
4. Investigar el comportamiento del método para el contexto general de todo el
documento y realizar el refinamiento del método.
5. Realizar pruebas de eficiencia del método en una colección de documentos en
español.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Resultados, aportes y contribuciones
Método propuesto
El método propone la desambiguación de los sentidos de las palabras del contexto
basado en la comparación de los sentidos de la palabra analizada en relación al contexto
y a los sentidos de las palabras que conforman el contexto, teniendo en cuenta la
influencia de cada palabra del contexto según la distancia a la que se encuentra de la
palabra analizada y la influencia de la semejanza en función de muchos recursos léxicos.
Los resultados obtenidos demuestran que el método obtuvo mejor precisión que
los otros dos métodos con los cuales se comparó.
Semejanza o similitud entre dos textos
El método propone una nueva forma de determinar la semejanza usando diferentes
recursos léxicos tales como el diccionario explicativo con definiciones normalizadas,
sinónimos, antónimos, merónimos (parte de), holónimo (contiene a), hipónimos,
hiperónimos en el cual cada recurso aporta a la determinación de la semejanza
obteniéndose mejores resultados con valores más discretos normalizados entre 0 y 1.
Preparación y conversión de los recursos léxicos
El método usa algunos recursos léxicos que ya existen en la actualidad, los cuales
fueron convertidos a bases de datos normalizadas e indexadas, de forma que permite un
procesamiento acelerado de los análisis realizados en tiempos muy cortos desde el punto
de vista de la persona que espera por los resultados.
El corpus
Se creó un corpus con 100 documentos para evaluar los WSD para el español, de
este corpus se seleccionaron los contextos usados para hacer las mediciones para los
distintos métodos de desambiguación.
Distancia y tamaño del contexto
El método propone una atenuación de la influencia en el peso de las palabras
según la distancia a la que se encuentren de la palabra analizada, de forma tal, que
palabras más cercanas tienen mayor influencia y que palabras más lejanas tienen menor
influencia, expresándose de forma discreta bajo una curva exponencial con tendencia a
cero hacia el infinito.
El español en el análisis
El método fue diseñado 100% para ser usado en textos en español, pues existen
pocas investigaciones en el tema de la desambiguación en comparación con otros
idiomas, específicamente con el inglés.
Uso práctico
Disponer de un método de desambiguación de sentidos de palabras para el idioma
español para la recuperación inteligente de información en buscadores de Internet ya que
en la actualidad tanto el volumen existente como la demanda en su consulta son grandes.
Rumbos de investigaciones posteriores
Aplicar algoritmos genéticos para la determinación del peso total combinatorio
que existen entre todas las palabras del contexto de forma tal que se determine la mejor
combinación de significados para todas las palabras al mismo tiempo y no el mejor peso
para cada palabra de forma independiente.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Recuperación de información con resolución de ambigüedad de sentidos de palabras para el español
T E S I S
QUE PARA OBTENER EL GRADO DE
DOCTOR EN CIENCIAS DE LA COMPUTACIÓN
PRESENTA
YOEL LEDO MEZQUITA
Director:
Dr. Grigori Sidorov
Codirector:
Dr. Alexandre Guelboukn</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"COMPUTACIÓN EVOLUTIVA PARA EL PROCESO DE SELECCIÓN DE VARIABLES EN ESPACIOS DE BÚSQUEDA MULTIMODALES"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Hasta la actualidad se han venido empleando diferentes técnicas de selección
de variables. Algunas basadas en métodos matemáticos, otras basadas en
métodos evolutivos con el fin de explotar su funcionamiento intrínsecamente
paralelo.
Sin embargo, ambos grupos de técnicas presentan una limitación a la
hora de trabajar con problemas que presentan un espacio de búsqueda
multimodal, es decir, que presentan más de una solución global, o bien una
solución global y múltiples soluciones parciales. En estos casos, tanto los
métodos matemáticos, como los métodos evolutivos clásicos tienden a
proporcionar una única solución válida. Si se desea obtener varias soluciones,
una opción es la ejecución del proceso de selección tantas veces (variando en
aquellos casos en los que sea necesario los parámetros iniciales) como
soluciones distintas se deseen, con el consiguiente incremento de costes, tanto
en el plano de los recursos como del tiempo.
En cambio, son prácticamente nulos los métodos que traten de manera
simultánea el proceso de selección de variables y la multimodalidad del
espacio de búsqueda. Ésta es la principal causa por la que se ha planteado la
realización de esta tesis doctoral: buscar un mecanismo de selección que
permita determinar, no sólo aquellas que aportan una mayor cantidad de
información de la muestra que describen, sino el máximo número posible de
combinaciones diferentes de variables que permitan obtener, sino el mismo
nivel de bondad que la solución global, sí uno lo más cercano posible. De esta
manera la búsqueda no se centra únicamente en el entorno de la solución
global, sino que abarca también a las soluciones parciales extraerse de manera directa e inmediata las siguientes ventajas:
* Reducción del coste de adquisición de datos: como resultado directo
del proceso de selección de variables, se necesita una menor cantidad
de datos para caracterizar una muestra, con lo cual el tiempo de
adquisición se ve significativamente reducido. Además, al realizarse
una búsqueda multimodal, puede escogerse de entre las soluciones
aquel conjunto de variables cuya adquisición lleve asociada una menor
complejidad o coste.
* Incremento en la eficiencia del sistema clasificador: por lo general los
procesos de selección de variables son el paso previo a la clasificación
de una muestra. En estos casos, a menor cantidad de información de
entrada del sistema clasificador, menor tiempo requerido para su
procesado.
* Mejora en la comprensión del modelo clasificatorio: aquellos modelos
que empleen menor cantidad de información para realizar una misma
tarea serán más fácilmente comprendidos. Adicionalmente, si el
sistema de selección de variables ofrece diferentes soluciones, será
posible la construcción de diferentes modelos, es decir, podrá
explicarse lo mismo de diferentes maneras, con lo cual el conocimiento
acerca del dominio del problema se ve incrementado.
* Mejora en la eficacia: en ocasiones, dotar de demasiada información a
los sistemas clasificadores, en vez de mejorar su rendimiento, produce
una pérdida en su capacidad de generalización. Nuevamente, al disponer de un conjunto de soluciones, en vez de una única solución,
será posible emplearlas todas ellas con el objetivo de determinar cuál
es la más eficiente bajo cada circunstancia concreta.
Por lo tanto, se plantea la obtención de un sistema que aborde el
proceso de selección teniendo en cuenta la posible multimodalidad presente
en el espacio de búsqueda. Se buscará, además, aportar una metodología
clara y concisa sobre las diferentes tareas que deben llevarse a cabo en el
citado proceso.
Se comprobará el correcto funcionamiento del sistema propuesto, así
como su metodología, con casos experimentales basados en problemas
genéricos, representativos dentro de los problemas multimodales. Una vez
verificado y contrastado su correcto funcionamiento en dichos problemas, se
probarán casos reales basados en tareas de clasificación, contrastándose los
resultados obtenidos con otras técnicas existentes.
Concretamente, se mostrará el funcionamiento del sistema propuesto
en un problema, con una alta tendencia a la multimodal, basado en la
clasificación de muestras en atención a su concentración real en zumo.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En aquellos problemas cuya resolución implica la búsqueda en espacios
multimodales, los algoritmos evolutivos, en general, y los Algoritmos
Genéticos, en particular, pueden quedar atrapados en un óptimo local, sin ser
capaces de alcanzar la solución global del problema. Para que se produzca
una exploración realmente eficaz del espacio de búsqueda multimodal, y la
búsqueda converja al óptimo global  sin que ello implique que se descarten
el resto de óptimo locales  es un requisito indispensable mantener e
incrementar la diversidad de la población en evolución.
Es también altamente interesante mantener el máximo número de
óptimos locales en la población a lo largo de las generaciones, puesto que
ellos también representan soluciones potenciales al problema que se plantea
resolver que, en ocasiones, y bajo ciertas circunstancias, quizás sean más
factibles de implementar, bien por cuestiones de simplicidad, de eficiencia, de
menor coste de implantación, o simplemente por ser más fáciles de
interpretar.
Ambos aspectos, por un lado evitar que se produzca la caída del
método de búsqueda empleado y por otro proporcionar no sólo una única
solución global sino las múltiples existentes (bien sean globales o locales), son
especialmente cruciales cuando los problemas son relativos a procesos de
selección de variables.
Tal y como se planteaba en uno de los objetivos de la presente Tesis, se
ha procedido al desarrollo de dos métodos para la selección de variables en
problemas que impliquen la exploración de espacios de búsqueda
multimodal. Ambos métodos están basados en técnicas de Computación
Evolutiva.
El primero de estos métodos parte de un Algoritmo Genético clásico,
modificándolo mediante la inclusión de una nueva población genética que
actuará a modo de repositorio de información con el objetivo de mantener la
diversidad genética a lo largo de toda la ejecución del algoritmo. Los
operadores genéticos habituales de selección, cruce y mutación han sido
adecuados para que trabajen con información proveniente de las dos
poblaciones genéticas, la tradicional y la añadida, denominada piscina
genética.
camino totalmente distinto. En este caso se parte de la premisa que la
Naturaleza es uno de los sistemas más intensamente multimodales que
puedan encontrarse. Múltiples organismos, generalmente agrupados en torno
a una especie, han encontrado diferentes maneras de alcanzar el objetivo
final, la supervivencia de dicha especie. Y lo que es más importante, las
diferentes especies conviven y evolucionan en un mismo entorno, cada una
defendiendo su territorio y luchando por mejorar la especie generación a
generación. Este ha sido el esquema que se ha llevado a la práctica, de manera
simplificada, de tal forma que diferentes especies de individuos genéticos
representen las diferentes maneras de dar solución a un problema dado.
Ambos métodos, tal y como se planteaba como punto de partida, han
demostrado su validez en los problemas planteados en la sección 6 de la
presente Tesis, en primer lugar con una serie de ejemplos típicos cuando se
habla de espacios de búsqueda multimodal como son las funciones de
Rastrigin, Ackley y Schewefel. A continuación, se han probado en un ejemplo
con una vertiente más cercana a aspectos prácticos y no meramente teóricos,
que involucraba la selección de variables sobre un espacio de búsqueda
netamente multimodal.
Tal y como se puede comprobar en base a los resultados obtenidos,
ambos métodos propuestos ofrecen resultados altamente satisfactorios.
En este último ejemplo enfocado a la clasificación de muestras en base
a su contenido en zumo natural, se ha empleado en los dos métodos
desarrollados la misma métrica para definir la bondad de cada una de las
soluciones aportadas. Concretamente se han empleado redes de neuronas
artificiales para determinar si el conjunto de variables seleccionadas como
significativas por el individuo genético eran o no una buena elección. Dichas redes de neuronas tomaban como parámetros de entrada los valores
especificados por el individuo genético y comenzaban el proceso de
entrenamiento a partir de ellos. Aunque no es necesario finalizar dicho
proceso de entrenamiento, los ciclos realizados permiten establecer si las
variables seleccionadas configuran un punto de partida válido para realizar la
clasificación.
A pesar de ocasionar unos tiempos de ejecución considerablemente
elevados, este hecho aporta la considerable ventaja de ser trasladable a
prácticamente cualquier otro problema, debido a la alta versatilidad de las
redes de neuronas artificiales. Además, es aplicable en problemas de selección
de variables en los que se emplea una técnica guiada o no guiada de
búsqueda únicamente con unos mínimos cambios (en el primer caso la
estructura de la capa de entrada de la red es fija, mientras que en el segundo
caso la definirá cada uno de los individuos genéticos en el momento de su
evaluación).</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE DA CORUÑA
FACULTAD DE INFORMÁTICA
DEPARTAMENTO DE
TECNOLOGÍAS DE LA INFORMACIÓN Y LAS COMUNICACIONES
COMPUTACIÓN EVOLUTIVA PARA EL PROCESO DE SELECCIÓN DE VARIABLES EN ESPACIOS DE BÚSQUEDA MULTIMODALES
Tesis Doctoral
Directores
Dr. Julián Dorado de la Calle
Dr. Juan Ramón Rabuñal Dopico
Doctorando
Marcos Gestal Pose</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Sistemas hipermedia para el aprendizaje de la Lectoescritura"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivos
Los objetivos que nos proponemos alcanzar en el presente trabajo son:
* Identificar las diferentes metodologías, materiales y recursos pedagógicos
utilizados en diferentes centros educativos para la enseñanza-aprendizaje de la
lectoescritura en sus fases iniciales (Educación Infantil). En el capítulo 8 se
analiza en profundidad este objetivo, presentando a su vez un estudio
comparativo, de carácter cualitativo, entre los diferentes centros.
* Diseñar el modelo conceptual subyacente a un sistema hipermedia adaptativo
que permita la introducción, realización, evaluación y seguimiento de
actividades concretas de lectoescritura El capítulo 6 presenta el modelo
arquitectónico de este sistema hipermedia, materializándose una implementación
de un subconjunto del mismo en el capítulo 7.
* Desarrollar parte del sistema hipermedia propuesto, en base a la información
recopilada en los diferentes centros, y verificar la bondad del mismo en la
práctica educativa. Con el subsistema creado en el capítulo 7 se estudia su
validez en la realidad educativa diaria en el capítulo 8.
* Identificar las aportaciones que ofrece la herramienta informática objeto de
estudio en la etapa de Educación Infantil en su aplicación al proceso
lectoescritor. Una vez verificada la validez educativa del sistema hipermedia
propuesto se estudian qué nuevas aportaciones ofrece al mundo de la
lectoescritura en el capítulo 8.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Las hipótesis que se plantean, y que han sido objeto de demostración o
refutación durante la realización de la presente Tesis Doctoral, son las siguientes:
* Existencia de un modelo abstracto para la construcción de un sistema hipermedia
adaptativo que favorezca el aprendizaje del proceso lectoescritor a cualquier
usuario y en cualquier contexto de uso. El capítulo 6 demuestra que,
efectivamente, existe una solución arquitectónica para este sistema. Sin embargo, su obtención no fue directa, sino que se realizó tras el análisis de la
información recogida en los diferentes centros de Educación Infantil (ver
capítulo 8), para garantizar la concordancia funcional con las situaciones
educativas del mundo real.
* Viabilidad técnica para desarrollar un sistema hipermedia en base al modelo
anterior. Tras analizar las diferentes posibilidades tecnológicas existentes en el
mercado se optó por utilizar Authorware como herramienta para la
implementación del sistema y el modelo Entidad-Relación como formalismo
para la representación de la información existente en los diferentes dominios. En
el capítulo 7 se analizan estas cuestiones en profundidad.
* Identificación de la idoneidad de uso de cada metodología para la enseñanzaaprendizaje
de la lectoescritura, en base a los resultados cualitativos obtenidos
en los diferentes centros educativos. El capítulo 8 indica claramente la relación
existente entre la información recogida en los centros de Educación Infantil y las
diferentes propuestas metodológicas.
* Corrección e interés del modelo conceptual propuesto en la etapa de Educación
Infantil, en base a las aportaciones anteriores. Una vez analizado el punto
anterior, el capítulo 8 concluye indicando las aportaciones del sistema
hipermedia estudiado en el proceso lectoescritor.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para demostrar nuestra hipótesis hemos utilizado la siguiente metodología de
trabajo:
* Revisión del estado del arte de las diferentes metodologías existentes para la
enseñanza-aprendizaje de la lectoescritura. El capítulo 2 recoge toda la
información relevante acerca de este punto.
* Revisión del estado del arte del uso de la informática en el ámbito educativo. En
el capítulo 3 se presenta aquella información relevante que hemos considerado
más relacionada con nuestro contexto de estudio.
* Revisión del estado del arte de los sistemas hipermedia adaptativos y su utilidad
en el ámbito educativo. El capítulo 4 se ocupa de esta cuestión. En ambas
revisiones del estado del arte hemos incluido referencias bibliografías a trabajos
recientes y/o significativos.
* Aplicar técnicas de investigación cualitativa para la recogida y estudio de la
información de los diferentes centros. El estudio comparativo entre centros del
capítulo 8 ha sido posible gracias a estas técnicas cualitativas. Se ha considerado
como material de estudio todo aquel que utiliza cada centro con sus alumnos
para la enseñanza-aprendizaje de la lectoescritura: libros, fichas, murales,
software educativo, etc. También se han realizado entrevistas al profesorado.
* Experimentar, mediante sucesivas aproximaciones, hasta alcanzar el modelo
conceptual más adecuado. Se pretende alcanzar un primer modelo conceptual
válido, sobre el que crear la aplicación hipermedia y realizar las modificaciones
necesarias en función de los resultados obtenidos. El modelo arquitectónico
presentado en el capítulo 6 y la implementación, de un subconjunto del mismo,
del capítulo 7 han sido obtenidos siguiendo este proceso. Para la implementación
parcial hemos utilizado una herramienta de autor, por su facilidad, potencia e
idoneidad de uso en el desarrollo de aplicaciones educativas.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Como colofón del presente estudio de investigación, a continuación se detallan
las principales conclusiones que se pueden extraer de los capítulos anteriores, indicando
además las limitaciones y líneas de trabajo futuro.
1. La implementación de una versión reducida del SHA propuesto y su aplicación a
la realidad educativa diaria en un centro de educación infantil ha demostrado ser
positiva y provechosa para alumnos y profesores. El programa desarrollado ha
demostrado facilitar y favorecer el proceso de aprendizaje en los alumnos y les
ha permitido actuar con mayor independencia, liberando al profesor de buena
parte de las tareas de exposición y explicación repetitiva de contenidos,
supervisión y retroalimentación.
2. A pesar de las ventajas obtenidas con la utilización del programa educativo
desarrollado, no debe pensarse que los SHAs son la panacea para el aprendizaje
de la lectoescritura o que pueden sustituir completamente la valiosa labor del
profesor en el aula. Así, el programa desarrollado, en su versión actual, presenta
algunas limitaciones:
* Existen actividades que no se han incorporado al SHA porque no se
prestan o son muy difíciles de llevar a cabo con el uso del ordenador
o, simplemente, todo parece indicar que es mejor realizarlas usando
otros medios. Así, muchas actividades que requieren la participación
colectiva de la clase resultan difíciles de controlar completamente
mediante un programa informático, sin la intervención del profesor Algunos ejemplos son: cantar, tocar las palmas para separar una
palabra en sus sílabas, crear un diccionario de clase, etc. La primera
actividad (cantar) requiere el uso de técnicas de reconocimiento de
voz sofisticadas que en la actualidad dan buenas resultados con frases
pronunciadas (a ser posible de forma clara y pausada) pero no
funcionan suficientemente bien cuando deben extraer el texto de
canciones. Este problema se debe a las limitaciones de los sistemas
de reconocimiento de patrones utilizados en la actualidad. La segunda
actividad (tocar las palmas) podría ser llevada a cabo por un
ordenador, pero en las edades tempranas en las que se trabaja es más
conveniente el trato directo y cálido del profesor, a quien además le
resulta más sencillo, en el estado actual de la tecnología, controlar los
errores y progresión de la clase. En la última actividad, el profesor lee
textos de varios libros referentes a la palabra que quiere incorporarse
en el diccionario y luego pide a la clase que confeccione una
definición entre todos usando la técnica del torbellino de ideas. Dadas
las limitaciones derivadas del estado actual de la inteligencia artificial
para gestionar adecuadamente el entramado complejo de
interacciones que se requieren en esta actividad (usando técnicas de
procesamiento de lenguaje natural) es considerablemente mejor la
relación clásica profesor-alumno prescindiendo del ordenador o
usándolo en todo caso para presentar los múltiples textos referentes a
cada palabra o/y permitiendo escribir delante de la clase la definición
final consensuada entre todos.
* Otras actividades puntuales no han sido incorporadas al SHA por
considerar que pueden realizarse con la utilización de otros
programas ya existentes: escribir pequeños textos (cuentos, poemas,
adivinanzas, etc.), dibujar, elaboración de álbumes de fotos de la
clase, colecciones gráficas, etc. Para estas actividades ya existen
aplicaciones informáticas como procesadores de textos, programas de
dibujo, bases de datos, etc. que permiten realizarlas, actuando el
profesor como mediador y guía. Los procesadores de textos son una
herramienta especialmente interesante, como alternativa o
complemento a los textos que los alumnos ya escriben manualmente
en clase, pues les libera de su concentración en aspectos ortográficos
y de trazado y les permite dar rienda suelta a su imaginación.
* El sistema desarrollado adolece, en su estado actual, de un mayor
número de actividades destinadas a las fases iniciales del aprendizaje,
esto es, en aquellas etapas más tempranas del aprendizaje
lectoescritor según el paradigma constructivista. Esto se ha debido en
parte a las razones anteriormente expuestas. El principal motivo por
el que no se ha dedicado un mayor esfuerzo en solventar este
inconveniente es que se ha considerado que estas lagunas quedan
convenientemente resueltas con el uso de otros programas
informáticos y medios existentes en el mercado y la intervención del
profesor. Así, por ejemplo, no se ha hecho especial énfasis en
incorporar actividades consistentes en escribir gráficamente (usando
el ratón, tableta gráfica u otro dispositivo de entrada adecuado) letras
sueltas, sílabas, palabras (nombre propio o de algún compañero de
clase, objetos del aula, etc.) o frases muy cortas. La incorporación de estas actividades habría supuesto utilizar avanzadas y complejas
técnicas de reconocimiento óptico de caracteres (OCR) que estarían
muy limitadas en su funcionamiento. En las fases iniciales los
alumnos no sólo no realizan siempre un trazado correcto o
suficientemente legible de las grafías, lo que dificulta enormemente
el reconocimiento óptico, sino que además es habitual, en la
construcción de las hipótesis iniciales sobre la lectoescritura, que sus
trazos (garabatos aparentemente sin sentido) no se correspondan con
ninguna grafía del alfabeto oficial. Por lo tanto, es muy difícil crear
un sistema que supervise la correctitud de las hipótesis iniciales de
aprendizaje de los alumnos. Sin embargo, todas estas dificultades
desaparecen usando otros medios (como el clásico papel para escribir
a mano) o aplicaciones informáticas como programas de dibujo
(Paint, por ejemplo) que permiten realizarlas fácilmente siempre y
cuando el profesor actúe como mediador y guía. Usar un programa de
dibujo es una experiencia muy positiva en estos niveles de
aprendizaje pues fomenta la creatividad e imaginación de los niños,
acerca y facilita un primer contacto con los ordenadores y les permite
practicar grafomotricidad.
* Aunque el sistema se adapta automáticamente, en la mayor medida de
lo posible, a las características individuales y a la evolución en el
aprendizaje de los alumnos, gran parte del proceso de seguimiento y
evaluación de los alumnos queda en manos del profesor. Resulta muy
difícil, dadas las limitaciones actuales en el campo de la informática,
construir un sistema que tenga en cuenta todos los parámetros
existentes en la realidad educativa y que filtre los más relevantes en
cada caso para establecer con propiedad los avances logrados. Por
tanto, la opción considerada en el presente trabajo ha sido dejar en
manos del profesor dicha labor de seguimiento proporcionándole un
módulo software para modificar en cualquier instante aquellos
parámetros que considere han cambiado de otra forma que la
calculada por el propio SHA. Aunque el sistema desarrollado es
bastante flexible, en la actualidad está limitado a un número fijo de
parámetros preestablecidos.
* Aunque el SHA desarrollado es bastante flexible y permite la adición
de nuevas actividades no contempladas en la versión actual, éstas
deberán seguir fielmente los principios de implementación del
capítulo 7. Así, cualquier nueva actividad deberá programarse usando
el lenguaje Authorware y gestionando adecuadamente las variables
ya existentes para la comunicación bidireccional entre el nuevo
módulo software y el módulo principal de gestión, denominado
"eligeJuego.
* Aunque los resultados obtenidos en el centro educativo en el que se
probó la presente aplicación informática han sido satisfactorios, es
conveniente verificar la bondad de la aplicación educativa a mayor
escala. Esto no ha sido posible debido a dos importantes barreras
encontradas en la realidad educativa: escasez de centros con
profesorado especializado en la aplicación práctica en el aula del enfoque constructivista y conveniencia de observar de la forma más
directa posible los avances logrados en el aprendizaje.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE CÁDIZ
Escuela Técnica Superior de Ingeniería Informática
Dpto. de Ingeniería de Sistemas y Automática, Tecnología Electrónica y Electrónica
Sistemas Hipermedia
para el aprendizaje de la Lectoescritura
TESIS DOCTORAL
Francisco Damián Ortega Molina
Licenciado en Informática
Dpto. de Lenguajes y Sistemas Informáticos
Cádiz, noviembre/2005
Profesores Directores de la Tesis:
Gabriel Nuñez Ruiz
Profesor Titular de Universidad del área de conocimiento de
Didáctica de la Lengua y la Literatura de la Universidad de Almería
Antonio Jorge Tomeu Hardasmal
Profesor Titular de Escuela Universitaria del área de conocimiento de
Ciencias de la Computación e Inteligencia Artificial de la Universidad de Cádiz</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Aplicacion de los algoritmos Geneticos a la Sociedad del Problema de DEsiciones Multicritico Individual y en grupo"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Sea A un conjunto finito de alternativas o acciones potencial es considerables por un grupo y evaluaciones con multiples criterios. El grupo esta compuesto por un conjunto de M={1,2....N} de miembros, cuyo trabajo era de alguna manera controlada pir un "Supra DEsiciones MArker" sea nfinito= AxA ---> [0,1] una relacion binaria fuzzy la cua integra las preferencias del i-esimo miembro sobre los multiples criterios que describen los elementos de A. El porblema es:
i) Para cada uno de los individuis y segun si apreciacion de las alternativas obtener un ranking completo O derivada de explotar la relacion en orden preferenciales.

ii)En base a las N parejas establecer un modelo de integracion y balance de preferencias del grupo que determina un ranking completo o que refleje el mejor compromisooo del grupo o que refleje el mejor compromiso del grupo de acuerdooo a las preferencias del "Supra Desicion Marker"</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General.
Crear un nuevo método de solución que integre técnicas robustas de busqueda y método reflexibles de modelacion de preferencias, en un procedimiento unico que aporte a la solución del problma del ranking indivual y en grupo de un conjunto de alternaivas por multiples atributos.

Objetivos Especificos.
1.-Desarrollar un algoritmo genericos para obtener un ranking final a partir deuna relacion binaria borrosa.
2.-Crear un método de solución para problemas de desicione multicriterio y un grupo con mejores propiedades que otros enfoques reportados.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El empleo de los algoritmos genericos para explorar una relacion binaria borrosa junto con el enfoque de integracion de preferencias basado en el principio de concordancia discorcia permiten obtener un metodo para derivar un ranking consistente en problema de desicion multicriterio y en grupo.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>De acuerdo con las pruebas y comparaciones empiricas realizadas con el método, el trabajo aqui desarrollado sutenta una nueva herramienta confiable de analisis de desiciones multicriterio basada en algoritmos geneticos, que ayuda a un grupo de desiciones a alcanzar cin consenso.
Entre las lineas de investigacion y de desarrollo naturalespara tabajos a futuro se encuentran las siguientes:
i)Busquedas de propiedades del algoritmo generico como métodos de ranking.
ii) Analisis, diseño y desarrollo de un GDSS cuyo nucleo sea el método aqui propuesto para utilizarse en una primera fase en un  "Computarized Room Desicion" condusido para un facilitador. En una segunda fase se pretende que permita a los Desision Markers
iii) Variantes del algoritmo genetico que permita encontrar informacion multicriterio automaticamente para el problema del ranking individual y en grupo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>"Aplicacion de los algoritmos Geneticos a la Sociedad del Problema de DEsiciones Multicritico Individual y en grupo"
Resumen de Tesis DOctoral 
Jan Crlos Leyva Lopez 
Fc. de Ingenieria de la Universidad Autonoma de Sinaloa
Asesor: Eduardo Fernandez Gonzalez</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"ARQUITECTURA SIMD RECONFIGURABLE"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>4.1 Objetivo General

Diseño e implementación de de una arquitectura dinámicamente reconfigurable, de alto rendimiento que permita aprovechar el paralelismo inherente en algunos algoritmos de visión basados en ventanas y mapearlos de manera automática a un arreglo sistólico de procesadores especializado en operaciones locales bajo restricciones de tiempo real. 


4.2 Objetivos Particulares

*	 Implementación de una arquitectura basada en módulos reconfigurables dinámicamente, que permita ejecutar una secuencia de algoritmos de acuerdo a una programación definida
*	 Implementación de un arreglo sistólico parametrizable que permita modificar el tamaño de las ventanas utilizadas en las operaciones
*	Determinar la métricas necesarias par reducir el costo de reconfiguración asociado a las técnicas de reconfiguración dinámica
*	Implementación de una métodología de codiseño Hardware/Software para una arquitectura dinámicamente reconfigurable
*	Implementación de un mecanismo de mapeo automático de un algoritmo, o parte de él, a la arquitectura generada basado en el uso de loop unrolling
*	Realizar pruebas exhaustivas para obtener resultados cuantitativos que muestren el grado de mejora logrado con la técnica comparado con arquitecturas estáticas</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La necesidad de cada vez más capacidad de cómputo para muchas aplicaciones importantes, incluyendo modelado, simulación y sistemas inteligentes, tales como los que realizan máquinas de visión, ha motivado que  la investigación en el área de arquitecturas de cómputo haya avanzado de manera considerable en los últimos años. Los sistemas requieren mejor desempeño del que puede proporcionar las máquinas que existen  actualmente o las que podrán ser construidas en un futuro cercano. Actualmente se están alcanzando los límites de las capacidades de procesamiento de los diseños seriales, una opción para alcanzar las velocidades de procesamiento deseadas, es físicamente posible es a través de procesamiento paralelo [hm94].

Las arquitecturas Reconfigurables prometen ser una valiosa alternativa  a los dispositivos de cómputo convencionales. El hardware no es estático sino que se adapta a las aplicaciones, a diferencia de los procesadores o ASICs. Las técnicas de reconfiguración se utilizan ampliamente en procesamiento paralelo, particularmente en arreglos de procesadores tales como los sistólicos.
La necesidad de suspender la operación de los dispositivos, aún para realizar pequeños cambios, tiene un impacto crítico si se van a usar como aceleradores de un algoritmo en aplicaciones donde la naturaleza de las tareas que van a ejecutar es cambiante. La reconfiguración dinámica permite realizar cambios en la configuración de un dispositivo "al vuelo durante la operación del sistema [mr01].

A partir de estos conceptos, este trabajo se propone reducir el cuello de botella de los sistemas basados en el modelo de Von Neumann, explorar y analizar las técnicas de reconfiguración dinámica junto con la implementación de algoritmos basados en operaciones de ventanas  para poder proponer soluciónes más acordes a las restricciones de tiempo real. De manera específica el uso de la reconfiguración parcial. 

Existen una serie de problemas que se deben manejar asociados con el uso de FPGAs acoplados a un procesador, primero el proceso de mapeo de un algoritmo en los FPGAs no es automático. Típicamente el programador es quien debe cuidar  la forma de identificar un algoritmo (o una parte de él) para ser implementado en hardware y las herramientas especializadas para convertir el algoritmo en una descripción de hardware. La presente propuesta pretende determinar la mejor forma de resolver el problema del codiseño HW/SW [nj02].

Otro  desafío interesante asociado con la computación reconfigurable es la reconfiguración dinámica en sí misma, y sobre todo como manejarla, lo cual es el objetivo principal de este proyecto.

Los FPGAs tradicionales tienen tiempos de configuración relativamente largos, por lo que para aplicaciones que dependen de modificaciones en línea, cualquier retardo de configuración incrementa el tiempo total de cómputo y por tanto el desempeño global. Otro de los objetivos de este trabajo es disminuir este efecto utilizando reconfiguración parcial que permite modificar sólo un área del FPGA sin perturbar el funcionamiento del resto del dispositivo. El desafío consiste en determinar la granularidad apropiada para el sistema en relación al desempeño obtenido.

Finalmente, se plantea la posibilidad de generar un sistema que permita optimizar área, reducir el consumo de potencia, de bajo costo y que cuente con la habilidad de hacerlo compatible con otras tecnologías.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO NACIONAL DE ASTROFÍSICA, 
ÓPTICA Y ELECTRÓNICA
DOCTORADO EN CIENCIAS DE LA COMPUTACIÓN
PROPUESTA DE TESIS
ARQUITECTURA SIMD 
RECONFIGURABLE
ALUMNA:M. C. GRISELDA SALDAÑA GONZÁLEZ
DIRECTOR DE TESIS:
DR. MIGUEL O. ARIAS ESTRADA
Agosto 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Métodos para el cálculo y reducción de conjuntos de representantes."</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General

Este trabajo tiene como objetivo general desarrollar herramientas que permitan la reducción del conjunto de representantes  de tal manera que sean adecuados para la clasificación y/o la caracterización. 

	Objetivos Secundarios
En dependencia de lo logrado en el objetivo principal de este trabajo de investigación se propondrán métodos para el cálculo del conjunto completo de representantes ya que en la actualidad la única opcion es  verificar la definición contra todo el espacio de búsqueda.

Además, se propondrán métodos para  calcular únicamente los subconjuntos de representantes que permitan una clasificación y/o caracterización adecuada, sin tener que calcularlos todos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es posible reducir el conjunto de representantes de tal manera que se mantenga la calidad de la clasificación?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Es posible reducir el conjunto de representantes de tal manera que se pueda caracterizar de manera adecuada a las clases?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El aumento del volumen y variedad de información en las Bases de Datos ha incrementado la dificultad para la clasificación, caracterización y comprensión de las mismas, una solución a este problema es el uso de técnicas de análisis de datos, Reconocimiento de Patrones o minería de datos entre otras. 

 	En base al estudio de los trabajos relacionados, una forma de abordar el problema es encontrar regularidades que puedan representar a los conjuntos de datos, permitiendo caracterizarlos y de esta manera reducir información. Una forma de hacer esto es utilizar conjuntos de representantes los cuales son una herramienta útil para la clasificación supervisada y la caracterización de universos estructurados. Sin embargo,  no se cuenta con un algoritmo para su cálculo, exceptuando la verificación de la definición contra todo el espacio de búsqueda. 


Otro problema  es que los representantes pueden incluir mucha redundancia y por tal motivo pueden llegar a ser demasiados (por ejemplo en un conjunto de 798 objetos, con 31 rasgos y 5 clases se encontraron 15362 representantes)


Por todo esto,  es necesario contar  con una herramienta que permita encontrar un subconjunto de representantes que sea adecuado para la clasificación y/o la caracterización.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En esta sección se expone la métodología propuesta para alcanzar los objetivos planteados.

Inicialmente se trabajará en la reducción del conjunto de representantes teniendo como objetivo la clasificación.

o	Se utilizarán Algoritmos Genéticos para buscar un subconjunto de representantes del conjunto original.
o	Se buscarán estrategias de eliminación de redundancia.
o	Se buscarán  otras formas de reducción.
o	Se compararán las estrategias propuestas.

 
Posteriormente, se desarrollarán estrategias para reducir el conjunto de representantes, teniendo como objetivo la caracterización adecuada de las clases.
 
o	Se buscará una o varias medidas de calidad de caracterización de los representantes.
o	Se utilizarán Algoritmos Genéticos para buscar un subconjunto de representantes, que caractericen adecuadamente a los objetos.
o	Se buscarán estrategias de eliminación de redundancia.
o	Se buscarán  otras formas de reducción.
o	Se compararán las estrategias propuestas.
 
Finalmente se trabajará en el cálculo de todos los representantes. Además se buscarán estrategias para calcular directamente subconjuntos de representantes sin tener que calcularlos a todos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Coordinación de Ciencias Computacionales
Métodos para el cálculo y reducción de conjuntos de representantes.
Propuesta de Tesis Doctoral
Que presenta:
M. en C. Leticia Mendoza Alonso
Directores
Dr. Jesús Ariel Carrasco Ochoa, Dr. José Francisco Martínez Trinidad.
Marzo  -  2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"SEGMENTACIÓN DE COBERTURAS DE LA TIERRA ESPECTRALMENTE SIMILARES EMPLEANDO CAMPOS ALEATORIOS DE MARKOV, CARACTERÍSTICAS Y GEOMETRÍA ESTOCÁSTICA"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>En consecuencia a la preguntas de investigación, el objetivo general de esta
investigación es:
Desarrollar un algoritmo para la segmentación de coberturas de la tierra
con comportamiento espectral similar basado en Campos Aleatorios
de Markov que involucre información de geometría y/o características
estocásticas de los objetos que se están clasificando.
El concepto de comportamiento espectral similar es relativo a que el algoritmo
propuesto debe ser capaz de reconocer subtipos de una clase en particular y si es
posible los subtipos mixtos, por ejemplo los tres tipos de manglar anteriormente
expuestos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es posible desarrollar un algoritmo que diferencie coberturas de la tierra espectralmente
similares usando imágenes de satélite y que obtenga resultados de clasificación
satisfactorios?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Es posible desarrollar un algoritmo basado en Campos Aleatorios de Markov que involucre además de la información contextual información de geometría y/o características estocásticas de los objetos que están siendo clasificados, que permita incrementar los porcentajes actuales de clasificación de las clases espectralmente similares?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Es enorme la cantidad de imágenes en crudo que se tiene almacenada de la
superficie terrestre. La interpretación de las imágenes es laboriosa y consume tiempo,
por lo cual es inimaginable la interpretación sin la ayuda del procesamiento de
imágenes[23]. Pongamos el ejemplo de las imágenes satelitales que se tienen del
territorio nacional tomadas por la constelación de satélites SPOT, sin considerar
las imágenes que se siguen adquiriendo. Hasta el primer trimestre de este año,
las imágenes tomadas por el satélite SPOT 5 alcanzan la cantidad de 10,200 imágenes aproximadamente. Entre ellas se encuentran imágenes pancromáticas y
multiespectrales de resolución espacial de 20, 10, 5 y 2.5 metros sin contabilizar las
de los otros dos satélites que permanecen en órbita SPOT 2 y 44. Del satélite SPOT
4 se tienen 825 imágenes pancromáticas de 10m de resolución que cubren casi todo
el territorio nacional.
Por otro lado, las herramientas que se han diseñado tienen algunas limitantes al
ser aplicadas a la clasificación de coberturas espectralmente similares, obteniendo
resultados de clasificación entre el 62.7% [17] y 73% [20].
Una de las desventajas de los algoritmos comúnmente empleados en la clasificación de
coberturas, es que no involucran información espacial de los objetos que están siendo
clasificados, por lo cual un modelo probabilístico resulta atractivo en este dominio.
Sin embargo, los objetos en imágenes satelitáles presentan además geometrías y/o
características estocásticas, la cual es información a considerar para incrementar los
porcentajes de reconocimiento [25].
Por último, el algoritmo aplicado sobre algún tipo de cobertura en particular para
obtener su clasificación fina podría ser extendido para otros tipos de cobertura con
comportamiento similar.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La métodología a seguir para alcanzar el objetivo de la investigación es la siguiente.
1. Investigar y analizar los métodos de segmentación basados en Campos Aleatorios
de Markov para la segmentación de imágenes satelitales con base al menor
error de clasificación.
2. Seleccionar y programar los modelos de Campos Aleatorios de Markov que
menor error de clasificación reportaron en la literatura.
3. Aplicar los modelos de Campos Aleatorios de Markov programados en el punto
anterior a la clasificación de clases espectralmente similares.
4. Seleccionar el modelo CAM que obtuvo el menor error de clasificación del punto
anterior. Evaluar también el modelo seleccionado realizando comparaciones con
mapas temáticos existentes.
5. Investigar y analizar diferentes enfoques que modelen información del objeto
y definir qué información se va a modelar (triángulos, elipses, cuadrados,
carcaterísticas estocásticas, etc.).
6. Analizar el modelo seleccionado en el paso 4 y definir durante el proceso
interno de su segmentación el o los procesos que serán extendidos para usar
la información de forma.
7. Crear el algoritmo de segmentación que conjunte el paso 5 y 6.
8. Definir dos dominios de trabajo de clases espectralmente similares con base a
los más estudiados en la literatura y a los datos disponibles. Recopilación de
datos de los dominios seleccionados.
9. Extender el algoritmo creado en 7 para el uso de características de textura.
10. Investigar, analizar y seleccionar los métodos de evaluación de segmentación y
clasificación de coberturas más empleados en la literatura para la segmentación
de imágenes satelitales.
11. Aplicar los algoritmos desarrollados en 7 y 9 a los dominios de trabajo
seleccionados y validar sus resultados con base a los métodos de evaluación
investigados en el punto anterior</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Hasta el momento se ha demostrado que los modelos de Campos Aleatorios de
Markov seleccionados de la literatura con base a su menor error de clasificación,
resuelven parcialmente la segmentación de clases espectralmente similares.
Además, se ha seleccionado el modelo CAM que será la base de nuestro algoritmo de
segmentación de coberturas espectralmente similares.
Al mismo tiempo, hemos podido establecer que el modelo de geometría y/o
características estocásticas será involucrado en el modelo a priori de los CAM,
el cual interviene directamente con la ganancia Gfi y Mfi del modelo CAM-árbol
estructurado.
Finalmente, estos primeros resultados nos dan indicios que vale la pena explorar
los Campos Aleatorios de Markov para la segmentación de clases espectralmente
similares.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>SEGMENTACIÓN DE COBERTURAS DE LA TIERRA
ESPECTRALMENTE SIMILARES EMPLEANDO
CAMPOS ALEATORIOS DE MARKOV,
CARACTERÍSTICAS Y GEOMETRÍA ESTOCÁSTICA
Por
Erika Danaé López Espinoza
Asesor
Dr. Leopoldo Altamirano Robles
REQUISITO PARCIAL PARA OBTENER EL
GRADO DE
DOCTOR EN CIENCIAS
EN EL
INSTITUTO NACIONAL DE ASTROFÍSICA, ÓPTICA Y ELETRÓNICA
STA. MA. TONANTZINTLA, PUEBLA
NOVIEMBRE 2006</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Métodos Libres de Malla para Ecuaciones Diferenciales Parciales Evolutivas"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo principal
El objetivo principal de la propuesta de esta tesis es el estudio y formulación de métodos numéricos, de descomposición de dominio, para ecuaciones diferenciales parciales de tipo convección difusión, a base de técnicas de aproximación radial.
Este objetivo implica la investigación de algoritmos numéricos libres de mallas que se reflejen en la estructuración de procedimientos computacionales paralelos, considerando mejorar el número condición del sistema algebraico resultante de la discretización.
Objetivos secundarios
Para satisfacer al objetivo principal de la tesis, se consideran los siguientes objetivos secundarios:
Análisis y diseño de algoritmos de nodos adaptivos para leyes de conservación: lineales, semi-lineales y cuasi-lineales.
Formulación de algoritmos numéricos conservativos basados en técnicas de aproximación de tipo base radial.
Diseño y formulación de técnicas computacionales paralelas cuya partición de memoria dependa de los algoritmos numéricos formulados.
Determinación de cubiertas -subáreas- del domino global que reflejen el comportamiento de la EDP como resultado de los algoritmos adaptivos.
El énfasis de esta tesis está en el desarrollo demétodos numéricos para problemas lineales o problemas no lineales susceptibles de ser linealizados. En el contexto de esta investigación no descartamos el análisis y desarrollo de métodos numéricos conservativos.
Con base a los métodos numéricos desarrollados o investigados, se generará una biblioteca de funciones con su respectiva documentación. Esto se traducirá en reportes técnicos internos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La idea general es llegar a resolver problemas en 3D, para ello se resolverán una serie de problemas en
1D y 2D paulatinamente.
En primera instancia se planteó un problema difusivo ut = uxx en 1D, en donde se conocía la solución
analítica, se resolvió por diferencias finitas y el método de Kansa, implementándolo en modo serial y
posteriormente en paralelo. Con este sencillo ejemplo se logró comprender cómo se emplea MPI para dar
solución a una EDP en particular y analizar los pasos que conllevan a la implementación en paralelo.
Posteriormente se planteó la solución del problema de Poisson en 2D, por colocación no simétrica de
Kansa, que se muestra en la sección de resultados preliminares. En dicho problema se distinguieron dos
conceptos importantes: partición de datos e implementación en paralelo.
De lo expuesto anteriormente, se tienen los dos primeros problemas a resolver: proponer un esquema de
comunicación en paralelo para el envío de datos de puntos de frontera en común y dada la partición de datos
determinar los puntos de frontera en común entre particiones. Ambos problemas deben considerar datos en
3D.
Por otro lado y desde el punto de vista numérico, los resultados analizados para el caso lineal de convección
difusión en 1D, nos plantea el estudio y desarrollo de nuevos algoritmos radiales, seriales y paralelos,
en 2D y 3D basados en distintos núcleos radiales y así analizar sus ordenes de convergencia. Esto con objeto
de desarrollar algoritmos más eficientes.
Empleando distintos núcleos radiales se determinará experimentalmente el orden de convergencia para
un problema 1D temporal. Este enfoque numérico es importante para determinar la convergencia ya que no
hay resultados mostrados en esta dirección.
Los esquema de mallas adaptivas son conocidos en el área de elemento finito, se analizarán cuáles de los conceptos implícitos en estas técnicas son aplicables a los algoritmos libres de mallas. Ejemplo de ello es la adición/remoción de nodos que se adapten al tipo de solución obtenida.
Con respecto a descomposición de dominio, se explorarán algunas variantes del algoritmo de Schwarz que resulten apropiadas para el problema a resolver. Dentro de las variantes del algoritmo de Schwarz se contemplan los casos de fronteras tipo Dirichlet-Dirichlet y Neumann-Neumann.
En particular, y para el caso de leyes de conservación, primero analizaremos casos en 1D hiperbólicos en diferencias finitas, en donde se tenga formación de ondas de choque. Una vez analizados los casos de estudio, se investigarán esquemas que consideren el uso de aproximación radial planteado en forma conservativa que utilice partición de dominio.
Finalmente, para validar que la solución numérica obtenida por los algoritmos propuestos sea correcta, se debe satisfacer que la solución obtenida sea mejor o igual a lo obtenido con otro métodos numéricos, por ejemplo volumen finito. Para ello se debe seleccionar varios casos de prueba y analizar los resultados.
La selección de la ecuación diferencial parcial de prueba se divide en dos casos: se conoce la solución analítica y no se conoce la solución analítica. En el segundo caso, la solución numérica se ha validado en distintos artículos quedando así establecida la certeza de que la solución numérica obtenida es correcta.
Cuando se conoce la solución analítica, podemos medir el error cometido empleando por ejemplo el error cuadrático medio</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Métodos Libres de Malla para Ecuaciones
Diferenciales Parciales Evolutivas
J. Antonio Muñoz-Gómez, Pedro González-Casanova
Henríquez, Gustavo Rodríguez-Gómez
Reporte Técnico No. CCC-05-001
12 de enero de 2005
© Coordinación de Ciencias Computacionales
INAOE
Luis Enrique Erro 1
Sta. Ma. Tonantzintla,
72840, Puebla, México.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo> "Un modelo del estudiante basado en mapas cognitivos"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Planteamiento de la investigación
En esta sección se precisa el problema de investigación a resolver, se determina el objetivo del proyecto y se ofrece un conjunto de razones que justifican el desarrollo de la investigación.
1.2.1 Problema a resolver
El problema que se aborda en la investigación doctoral se define por medio de la siguiente cuestión principal:
* ¿Cómo estimular positivamente el aprendizaje del estudiante derivado de las experiencias provistas por un SEBW?
Adicionalmente, la investigación se dedica a responder las siguientes cuestiones específicas:
1. * ¿Cuáles son los dominios de influencia que se involucran en la provisión de experiencias?
2. * ¿Cuáles son los factores que se deben considerar provenientes de dichos dominios?
3. * ¿Cómo se valoran tales factores?
4. * ¿Cuáles son las relaciones causales que existen entre los factores?
5. * ¿Cómo representar las relaciones causales?
6. * ¿Cómo modelar, a manera de un sistema dinámico, el escenario de factores y relaciones causales?
7. * ¿Cómo anticipar el efecto que producen los factores con base en sus relaciones causales?
8. * ¿Cómo estimar las alteraciones de los factores como resultado del efecto causal?
9. * ¿Cuál es la secuela de influencias causales que se desencadena entre los factores al cambiar el estado de alguno de ellos?</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo central de la tesis doctoral se define de la siguiente forma:
Proponer un modelo que represente cualitativamente factores que inciden en el aprendizaje del estudiante durante una experiencia, y que anticipe mediante el razonamiento causal-difuso su impacto en la adquisición de los conocimientos provistos por un SEBW a efecto de elegir la opción que mejor estímulo ofrezca al aprendizaje del estudiante.
A través de este modelo se ofrece una solución al problema planteado y se brinda una respuesta a las cuestiones formuladas en la Sección 1.2.1. Adicionalmente, la investigación busca satisfacer los siguientes objetivos teóricos:
1. Formular un conjunto de hipótesis sobre el impacto que un modelo del estudiante basado en mapas cognitivos ejerce en el aprendizaje del individuo.
2. Aportar evidencia empírica para fundamentar las hipótesis.
3. Establecer un vínculo conceptual entre la Teoría de la Actividad (Leont'ev, 1978), el modelado del estudiante y los mapas cognitivos.
4. Evaluar la influencia que un modelo del estudiante basado en mapas cognitivos ejerce sobre un SEBW para proveer educación centrada en el estudiante.
Adicionalmente, la investigación persigue satisfacer los siguientes objetivos prácticos:
1. Concebir una imagen mental del individuo basada en los dominios que revelan rasgos de su personalidad, habilidades cognitivas y preferencias de aprendizaje.
2. Recrear un perfil descriptivo de las experiencias que tipifique sus modalidades de secuencia, contenido y evaluación.
3. Representar el dominio del conocimiento adquirido por el estudiante como resultado de las experiencias que el SEBW le proporcione.
4. Diseñar una ontología para describir los conceptos que recrean un modelo del estudiante.
5. Formular un modelo dinámico que describa los factores y sus relaciones causales.
6. Proponer un método para identificar, evaluar y seleccionar las opciones de experiencias acordes con el perfil del estudiante.
7. Recrear la función proactiva en el modelo del estudiante para contribuir en la planeación, provisión y control de las experiencias a cargo de un SEBW.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La hipótesis de investigación causal y multivariada que se formula en la tesis es la siguiente:
La selección de la experiencia que un SEBW provee al estudiante, estimula positivamente su aprendizaje cuando se toma en cuenta el perfil del individuo.
En tanto que la hipótesis nula respectiva, encargada de negar la suposición anterior, es:
La selección de la experiencia que un SEBW provee al estudiante, no estimula positivamente su aprendizaje aunque se tome en cuenta el perfil del individuo.
La hipótesis de investigación se compone de tres tipos de variables: 1) independiente: selección de la experiencia; 2) dependiente: aprendizaje; 3) interviniente: perfil del individuo.
La definición constitutiva y operacional de las tres variables se establece en la Tabla 1.1. En la segunda columna se ofrecen las definiciones teóricas genéticas (Chávez, 2005) para los tres términos introducidos en la presente investigación, mientras que en la tercera columna se identifican las actividades o criterios que se utilizan para medir la variable respectiva.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La justificación de la investigación doctoral se fundamenta en los siguientes criterios:
1. Conveniencia. Se pretende elevar la eficacia de la enseñanza provista al estudiante a través de los SEBW. Para ello, se evalúan las experiencias y se eligen aquellas, que se supone, habrán de estimular positivamente el aprendizaje del estudiante de la mejor manera.
2. Relevancia social. Se busca mejorar el aprendizaje del estudiante al personalizar la enseñanza que un SEBW le provee. Para este fin se consideran las capacidades cognitivas, las preferencias de aprendizaje y ciertos atributos de la personalidad del estudiante.
3. Implicaciones prácticas. Se procura adecuar la enseñanza a cada estudiante con base en las variedades de contenido, secuencia y evaluación disponibles en un SEBW. De esta forma, se promueve un paradigma general para diversos dominios de enseñanza.
4. Valor teórico. Se concibe el proceso de enseñanza-aprendizaje como un fenómeno causa-efecto. Por lo tanto, además de formular un modelo causal para caracterizar a dicho proceso, se recolecta evidencia empírica que sustente la verificación de la hipótesis.
5. Utilidad métodológica. Se define un marco de trabajo para evaluar los conceptos que componen un modelo del estudiante. Con base en exámenes psicológicos se emite un diagnóstico del individuo acerca de su personalidad, sus preferencias de aprendizaje y sus capacidades cognitivas.
6. Consecuencias. La investigación busca tratar al estudiante en forma individualizada. Bajo este precepto, se impulsa un paradigma de servicio alterno al tradicional, ofertado por la educación presencial y a distancia, en el cual a los estudiantes se les trata sin distinción.
7. Viabilidad. El desarrollo de la investigación y su comprobación experimental se lleva a cabo en su propio dominio: la educación basada en Web.
8. Comprobación. Para verificar la tesis se crea un prototipo8 del modelo del estudiante basado en mapas cognitivos. El prototipo forma parte de un SEBW que provee enseñanza sobre un dominio de conocimiento específico a una muestra de voluntarios. La muestra se organiza en dos grupos: el primer grupo recibe el soporte del modelo del estudiante; mientras que el otro no cuenta con su respaldo. Al final del experimento, se comparan los alcances logrados por los dos grupos para identificar la ventaja del modelo del estudiante.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En esencia, la tesis se encaminó a proponer un modelo de cómputo para recrear un modelo mental orientado a representar y anticipar el desarrollo de un fenómeno complejo: el ciclo de enseñanza-aprendizaje.
Por tanto, en aras de comprobar la hipótesis planteada, se identificó un conjunto de conceptos derivados de siete dominios de análisis que fueron evaluados en forma cualitativa conforme a ciertos criterios de estudio. Mediante el conjunto de dominios, se establecieron las relaciones causa-efecto entre sus conceptos. De esta forma se diseñaron mapas cognitivos para caracterizar el fenómeno de enseñanza-aprendizaje, y mediante la inferencia causal-difusa, se buscó anticipar el impacto que las experiencias producen en el aprendizaje del estudiante.
La razón para proponer un modelo del estudiante basado en mapas cognitivos es el considerar el ciclo de enseñanza-aprendizaje como la actividad en que un SEBW transmite conocimientos a través de su exteriorización, los que el estudiante adquiere mediante su interiorización. Este ciclo fue analizado desde la perspectiva causal, como una relación causa-efecto. Por lo tanto, con base en la Teoría de la Actividad y los mapas cognitivos, en la tesis se propuso un modelo que recrea el escenario en donde se caracteriza al estudiante como individuo y a la experiencia en turno, a efecto de estimar consecuencias causales. De esta forma, se alienta la provisión de educación centrada en el estudiante a través de un SEBW.
La propuesta se distingue por incorporar la representación de la experiencia en el modelo del estudiante. Generalmente, las aplicaciones afines se concentran en representar el conocimiento adquirido por el estudiante GRUNDY (Rich, 1979) , sus atributos personales Smex Web (Albrecht et al., 2000) , conductas observadas ELM-PE (Brusilovsky, 1995b) , distorsiones en el aprendizaje Modelos de Diagnóstico (Brown y Burton, 1978)  y el conocimiento de enseñanza ADAPS (Brusilovsky y Cooper, 1999) . En cambio, en el presente trabajo los atributos de la experiencia forman parte del propio modelo del estudiante. Como resultado, se obtiene una representación más rica del fenómeno de estudio, puesto que se consideran a dos protagonistas: el emisor y el receptor del conocimiento que se transmite y adquiere.
Adicionalmente, en el modelo propuesto se introdujo la lógica difusa como el medio formal para representar conocimiento cualitativo. Mediante la lógica difusa se definieron los conceptos como variables lingüísticas y las relaciones por medio de reglas difusas. Gracias al marco teórico de la lógica difusa, se recreó una representación formal de los atributos que describen al estudiante y a la experiencia; además de proveer la base para el razonamiento causal-difuso que se implementó.
En contraparte, el común de los modelos del estudiante representa conocimiento cualitativo por medio de redes semánticas Scholar (Carbonell, 1970)  y mapas conceptuales DynMap (Rueda et al., 2003) . Esta clase de modelos carece de un modelo formal que ofrezca un sólido respaldo para la representación de conocimiento cualitativo. Además, el razonamiento aplicado emplea relaciones identificadas por términos que representan una jerarquía abstracta para definir atributos que se heredan a través de asociaciones de inherencia. Por tanto, esta clase de inferencia es de carácter conceptual, mientras que la empleada en la propuesta disfruta del respaldo matemático de la lógica difusa. Por lo tanto, la ventaja radica en el respaldo formal que goza el paradigma empleado para la representación de conocimiento y el razonamiento cualitativo usado en la propuesta.
Al comparar la propuesta contra los modelos del estudiante que usan la estructura episódica ELM-ART (Weber y Brusilovsky, 2001) , patrón del perfil del usuario EPK (Timm y Rosewitz, 1998)  y estereotipos  CALL (Murphy y McTear, 1997) ; se advierte que éstos emplean estructuras para representar conocimiento muy simples, tales como: conceptos relacionados lógicamente en una red y conceptos definidos como parejeas atributo-valor, mientras que la representación hecha en el prototipo descansa en el uso de ontologías y acervos de conocimiento. Con estos paradigmas, se establece una robusta representación semántica de los conceptos y de las relaciones envueltas en el modelo del estudiante.
En relación con los modelos del estudiante orientados al razonamiento causal, tales como las tres versiones de SOPHIE (Brown et al., 1982), se advierte que éstos emplean un sistema de reglas de producción para representar conocimiento cualitativo y realizar razonamiento causal. Este paradigma es ampliamente utilizado en el ámbito de los sistemas expertos, por ser un modelo que facilita la manifestación del conocimiento del experto humano en la solución de un problema. Generalmente esta clase de representación de conocimiento y forma de razonamiento es empírica, intuitiva y parcial, además de carecer de un modelo formal para sustentar las inferencias derivadas. Por tanto, es un paradigma limitado para representar conocimiento y razonamiento derivado de la experiencia y sentido común del individuo que realiza el modelo. En contraparte, el razonamiento causal que se aplica en la propuesta no solo disfruta del soporte matemático de la lógica difusa, sino que además incorpora la modalidad de razonamiento causal-difuso proveniente de los mapas cognitivos difusos con bases de reglas difusas. Por tanto, no solamente se estima un efecto de agregación, propio de las relaciones de inferencia difusa, sino que además se calcula el efecto de acumulación derivado de las relaciones causales-difusas. Por consiguiente, se ofrece un mecanismo de razonamiento cualitativo y causal que goza del marco formal de la lógica difusa y de los mapas cognitivos difusos con bases de reglas difusas. Gracias a este paradigma, el prototipo genera conclusiones que disfrutan de un mayor soporte formal que los modelos del estudiante que usan sistemas de producción.
Al comparar el enfoque de predicción causal que emplea la propuesta contra el aplicado por modelos del usuario, se evidencia la diferencia entre los alcances y el impacto que se persigue. Mediante el modelo del estudiante basado en mapas cognitivos se anticiparon efectos de carácter cognitivo que se supone ocurrirán cuando el estudiante afronte una experiencia. En cambio, los modelos del usuario basados en modelos lineales (Orwant, 1995) sólo pretenden anticipar los ingresos del individuo al sistema; aquellos que usan frecuencias sólo buscan recomendar contenidos que puedan resultar atractivos al usuario (Moukas y Maes, 1998); los que emplean modelos de Markov tratan de predecir eventos como la próxima página que la persona visitará (Bestavros, 1996); los modelos basados en redes neuronales buscan prever las preferencias de contenido del usuario. Por tanto, estos paradigmas son muy limitados.
Con respecto al empleo de mapas cognitivos, la propuesta introdujo los mapas cognitivos en el terreno del modelo del estudiante. En consecuencia, no solamente se inicia un nuevo campo de investigación y aplicación, sino que además se lleva al terreno experimental la versión de los mapas cognitivos difusos con bases de reglas difusas, debido a que una vez definido su modelo formal por parte de Carvalho (2001), esta versión de mapas cognitivos no se ha empleado en casos reales como la enseñanza y el aprendizaje. Así mismo, al comparar este trabajo contra los realizados por García et al. (2003) y Laureano et al., (2004), se advierte una vez más la diferencia en el alcance. Como primera diferencia, este par de trabajos emplean la versión de mapas cognitivos difusos. Esta versión carece del soporte de la lógica difusa para realizar el razonamiento causal, pues éste se realiza de una manera parecida al empleado por las redes neuronales. Por lo tanto, la inferencia es de carácter numérico en vez de cualitativo y causal. Otra diferencia radica en el grado de automatización, pues a excepción del cálculo numérico para la inferencia, el diseño del mapa cognitivo y la interpretación del comportamiento revelado durante la simulación, son manuales. En cambio, en el prototipo se implementó un mecanismo que se encarga de dichas tareas.
En relación con las lecciones aprendidas, éstas se resumen de la siguiente manera: El proceso para formular modelos mentales es una tarea compleja, sobretodo cuando el objeto a modelar es abstracto, como ocurre con la enseñanza y el aprendizaje. Así mismo, esta clase de objeto es por demás relativo, incierto, impreciso y no monotónico. Es relativo porque la modelación de la persona que aprende, así como el material y forma de enseñanza, están sujetos al punto de vista particular de quien realiza el modelo. Debido a que no existe un modelo único y aceptado de manera absoluta que establezca la forma de representar al individuo y a la experiencia, es incierto, pues todas las valoraciones que se hacen del estudiante y de la experiencia son aproximadas, ya que a pesar del uso de métricas para medir la inteligencia, éstas no son garantías de que en efecto una persona es más inteligente que otra. Es impreciso, puesto que a pesar de la exactitud de las apreciaciones hechas del estudiante y de la experiencia, no hay garantía de que el desempeño que ocurrirá corresponda con la predicción. No es monotónico, puesto que la conducta del individuo puede ser inestable e inconstante, amén de verse afectada por toda clase de imponderables que están fuera del alcance de cualquier modelo por muy completo que éste sea. Por ejemplo, si el individuo durmió mal, siente ansiedad, está cansado o simplemente si en el día que afronta una experiencia no está concentrado por cualquier causa, su desempeño será diferente al regular.
Adicionalmente, la determinación de los dominios y conceptos que componen el modelo del estudiante afronta los siguientes conflictos: enfoques de estudio, número y variedad de los dominios, cantidad y diversidad de conceptos, referente para la valoración, precisión de la valoración, marco de referencia para establecer relaciones y validación de criterios. El primer conflicto se relaciona con la pregunta: * ¿cuáles son los puntos de vista necesarios para modelar al estudiante? Una respuesta puede proponer enfoques relacionados con aspectos intelectuales, culturales, fisiológicos, anímicos, familiares, sociales e históricos, entre otros muchos. Con relación al número y variedad de los dominios, en el prototipo se incorporaron tres: personalidad, capacidades cognitivas y preferencias de aprendizaje. Adicionalmente, se pueden agregar dominios tales como: salud física, inteligencia emocional, estado de ánimo, estado nutricional, factores genéticos, situación familiar, herencia, sentimientos, intereses, espiritualidad, actitud, necesidades fisiológicas, estrés, afectivas y muchos otros enfoques más que revelan tan solo una parte del individuo. A los conflictos anteriores, se agrega el determinar para cada enfoque-dominio, la cantidad y diversidad de conceptos necesarios para caracterizarlos fielmente. También, se afronta el reto de establecer el marco formal, método e instrumentos necesarios para valorar los conceptos y determinar el grado de precisión requerida. Por ejemplo, ante la variedad de exámenes psicométricos que determinan la personalidad del individuo, * ¿cuál de ellos es el más acertado?, * ¿cuál es el más útil?, * ¿cuál es el más completo? Estas son tan solo tres cuestiones que también se aplican al identificar el marco de referencia para establecer relaciones y validar criterios.
Ante esta muestra de conflictos, todo se traduce a una conclusión: el intento para modelar el estudiante se reduce a un esfuerzo parcial, limitado, relativo, monotónico y cerrado que solo tiene validez para quien realiza el modelo. Es decir, el responsable de diseñar el modelo debe estar consciente de que la representación a la que arribe, responde a sus propios intereses, que ésta no es exhaustiva y tan solo corresponde a los medios que emplea, que ofrece un comportamiento estable que en la práctica suele suceder de otra manera y que además diseña un modelo cerrado a condiciones exógenas, con lo cual no es congruente con la realidad.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Tesis doctoral:
Un modelo del estudiante basado en mapas cognitivos
Autor:
Alejandro Peña Ayala
Directores de Tesis:
Dr. Juan Humberto Sossa Azuela
Dr. Agustín Francisco Gutiérrez Tornés
Diciembre, 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Un algoritmo de inducción de reglas para la ayuda al diagnóstico médico"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un algoritmo de aprendizaje inductivo para resolver problemas de clasificación binaria a partir de conjuntos de datos no balanceados, donde los resultados obtenidos alcancen un consenso apropiado entre precisión y comprensibilidad.
Objetivos Específicos.
* Aplicar el algoritmo propuesto en diferentes conjuntos de datos médicos.
* Evaluar los resultados obtenidos apropiadamente, en términos de precisión y comprensibilidad.
* Comparar el desempeño del algoritmo propuesto con el de los principales algoritmos de aprendizaje inductivo utilizados para este tipo de dominios.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cuál es el método de aprendizaje inductivo más adecuado para extraer conocimiento útil a partir de conjuntos de datos médicos?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Es posible desarrollar un algoritmo para el método de aprendizaje inductivo seleccionado, que permita obtener resultados que alcancen un consenso apropiado entre precisión y comprensibilidad?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Qué ventajas ofrecería este algoritmo con respecto a otros algoritmos de de aprendizaje inductivo existentes?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Qué estrategias se pueden seguir para que el algoritmo propuesto vaya más allá de lo teórico, y represente un beneficio real en la práctica clínica?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Tomando en cuenta la revisión que se hizo del estado del arte y generalizando el problema de clasificación en este tipo de dominios del diagnóstico médico, nuestra justificación es la siguiente:
Existe la necesidad de desarrollar un algoritmo de aprendizaje inductivo que permita resolver problemas de clasificación binaria a partir de conjuntos de datos no balanceados, alcanzando un consenso apropiado entre precisión y comprensibilidad. Por otro lado, la extracción de conocimiento útil a partir de conjuntos de datos médicos, puede ayudar a predecir la incidencia de algún tipo de enfermedad, lo que representaría la oportunidad de aplicar a tiempo un tratamiento médico, capaz de evitar, eliminar o disminuir en cierto grado la aparición de la misma.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Una vez que se revisó y analizó el estado del arte, se llegó a la conclusión de que los métodos inductivos más apropiados para extraer conocimiento útil para la ayuda al diagnóstico médico, son los denominados métodos simbólicos, ya que a través de ellos podemos obtener modelos inteligibles para los seres humanos, capaces de proveer al staff médico un nuevo punto de vista que les ayude a tomar decisiones acertadas en la difícil tarea del diagnóstico médico. Por lo tanto, la métodología para desarrollar nuestro algoritmo inductivo se divide en tres etapas.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Un algoritmo de inducción de reglas para
la ayuda al diagnóstico médico
Propuesta de Tesis Doctoral
M. en C. Luis Javier Mena Camaré
Director
Dr. Jesús A. González Bernal
Coordinación de Ciencias Computacionales, INAOE
{lmena,jagonzalez}@inaoep.mx</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Una métodología para la búsqueda de respuestas en fuentes documentales multilingües"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Proponer un esquema para la búsqueda de respuestas en situación multilingüe, donde la
pregunta es expresada en un lenguaje, y la búsqueda se realiza en una o varias colecciones
de documentos escritos en lenguajes diferentes al lenguaje de la pregunta.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es posible proponer un esquema de búsqueda de respuestas multilingüe construido a partir de un sistema de búsqueda de respuestas monolingüe basado únicamente en información léxica?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Cómo puede aprovecharse esta información léxica en un contexto de búsqueda multilingüe?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Los métodos estadísticos son basados en redundancia, si abrimos la búsqueda a más de un lenguaje * ¿mejorarán los resultados?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Cuál es el impacto de la traducción en el proceso de búsqueda de respuestas?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Cómo elegir la mejor respuesta de las listas de respuestas candidatas multilingües?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En esta sección se expone la métodología propuesta y menciona el alcance y las
limitaciones de nuestro trabajo. Para efectos de exposición, hemos divido la métodología en
tres grandes pasos: (i) preprocesamiento y traducción de la pregunta; (ii) búsqueda de
pasajes y recopilación de respuestas candidatas; y (iii) selección de la respuesta. En la
figura 3 se muestra un esquema de esta métodología</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Coordinación de Ciencias Computacionales
Laboratorio de Tecnologías del Lenguaje
Una métodología para la búsqueda de
respuestas en fuentes documentales multilingües
Propuesta de Tesis Doctoral
que presenta:
M. en C. Rita Marina Aceves Pérez
Directores
Dr. Luis Villaseñor Pineda,
Dr. Manuel Montes y Gómez</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"HERRAMIENTAS Y MÉTODOS PARA LA PRODUCCIÓN MULTIMEDIA Modelo centrado en el Autor MCA"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Introducción
Se define el entorno multimedia y se sitúa el trabajo realizado en el
contexto de la investigación en el campo de las aplicaciones multimedia.
Se describen los paradigmas de la información y de la comunicación
multimedia y se exponen las tendencias de futuro del mismo, analizando
con detenimiento la figura del autor de contenidos.
* Aportaciones e interés de la investigación
Partiendo de la génesis de la propuesta de investigación se exponen los
objetivos que se pretenden alcanzar con esta investigación, consistentes
en la definición, diseño y desarrollo de los diversos componentes de un
modelo integral de producción de contenidos, centrado en el autor</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>métodología de investigación
El proceso seguido a lo largo de los trabajos de investigación ha
consistido en realizar diversas iteraciones sobre la experiencia empírica
desarrollando y ensayando de forma gradual diversas versiones de los
modelos. Los límites de la investigación quedan establecidos por el tipo
de entorno en el que se realiza la investigación.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Evaluación respecto de los datos iniciales
Se considera cumplida la hipótesis de mejora del rendimiento del
proceso de producción en la fase de pedido con respecto a los sistemas
de producción tradicionales expuesta en los apartados 2.2.1. y 2.3.2. de
este documento.
Si se comparan los resultados obtenidos agregados con el criterio
empleado en la valoración del esfuerzo en las producciones previas de
referencia, puede comprobarse que en el marco del estudio han
participado más perfiles y que su dedicación es más repartida. Por otra
parte se ha invertido la proporción de participación de algunos perfiles.
Puede observarse un aumento de las tareas del diseñador visual y del
editor y una disminución considerable de las tareas de diseño y de
gestión de la producción. Por otra parte se manifiesta la actividad del
director de pruebas y de los expertos en contenidos.
Evaluación respecto de los objetivos de la investigación
El objetivo central de la investigación consiste en definir un modelo que
facilite a cualquier usuario de información digital, en calidad de experto
en contenido, su implicación en el desarrollo de documentos digitales
con distintos niveles de integración de recursos multimedia y grados de
interactividad, apartado 2.2.2.
Puede considerarse que el esquema derivado del estudio es más
completo que el aportado en los datos iniciales y que se basa en una
actividad sustancialmente más simple, lo que permite formular con
mayor precisión y eficacia procedimientos orientados a aumentar la
participación del autor en las tareas de producción y garantizar al mismo
tiempo la eficacia del sistema de producción empleado.
Definición de la producción multimedia desde el punto de
vista del autor de contenidos.
Las modificaciones efectuadas en el proceso de producción de
referencia han permitido en primer lugar la ejecución de los proyectos
con un nivel de calidad y complejidad equiparable al de los proyectos
editoriales de la fase previa y con un esfuerzo sustancialmente menor en
las tareas de diseño, programación y gestión. En la ejecución de los
proyectos que componen el estudio se ha realizado sistemáticamente
una transferencia de competencias propias del equipo de producción al
colectivo de autores. Esta transferencia ha posibilitado una mayor control
del proceso y del resultado por parte del autor.
Definición, desarrollo y ensayo de las plantillas de
producción.
Puede considerarse que el modelo de producción propuesto es eficaz
en:
Tesis doctoral: Herramientas y métodos en la producción multimedia. Modelo centrado en el autor.
Joaquín Fernández, Director: Josep Maria Monguet 235/245
* La fase de pedido, al reducir la iteración del productor ejecutivo
con el autor-cliente.
* La fase de diseño, al reducir el esfuerzo de los diseñadores y
desplazarlo al autor.
* La fase de producción, al estandarizar las tareas de edición.
Definición desarrollo y ensayo de un proceso de seguimiento.
El sistema de seguimiento dinámico ha facilitado la información
cuantitativa detallada de las producciones realizadas. Este seguimiento
en detalle ha requerido de un gran esfuerzo y disciplina por parte del
equipo de producción durante un periodo largo de tiempo. Las
principales dificultades que presenta el seguimiento de las producciones
multimedia radican sobre todo en la variedad de las tareas y en su poca
duración. Además existe una relación no siempre directamente
proporcional entre la duración de la tarea y su importancia, hecho que no
queda reflejado necesariamente en el sistema de seguimiento empleado
en este estudio y que puede dar pie a futuras mejoras del sistema en
esta dirección.
El objetivo general requería para este estudio un registro y análisis
cuantitativo de las tareas ejecutadas, y esta necesidad consideramos
que se ha cubierto suficientemente, en tanto que el sistema recoge
actualmente información no tratada hasta el momento.
Prescripción de acciones curriculares para la formación del
autor.
Entendemos que este objetivo se ha cubierto de forma exhaustiva al
aportar información curricular basada en acciones concretas y
agregadas por perfiles, tipos de tarea, tipos de producción y tipos de
aplicaciones.
Esta información permite elaborar currículos de forma muy
personalizada atendiendo a la formación previa del autor (por proximidad
a un perfil o a los tipos de tarea) y al tipo de documento que desea
realizar (por proximidad a los tipos de producción o de aplicación). De
Tesis doctoral: Herramientas y métodos en la producción multimedia. Modelo centrado en el autor.
Joaquín Fernández, Director: Josep Maria Monguet 236/245
hecho, el resultado obtenido en este apartado puede prescribir un
sistema automático que facilite:
* la acreditación y evaluación previa del autor y
* la definición del currículo en función de los objetivos
y que se reserva para futuros desarrollos.
Cumplimiento de las prescripciones de los sistemas
En este apartado se valora la consecución de las prescripciones
establecidas en el apartado 4.1.2 de este documento.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Tesis Doctoral
HERRAMIENTAS Y MÉTODOS PARA LA
PRODUCCIÓN MULTIMEDIA
Modelo centrado en el Autor MCA
Joaquín Fernández Sánchez
Director: Josep Maria Monguet Fierro
Barcelona a 15 de febrero del 2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"PROPUESTA DE NUEVOS MODELOS DINÁMICOS DE CABLES DE ELEVACIÓN PARA SIMULACIÓN EN TIEMPO REAL"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo principal de esta investigación es conseguir modelos dinámicos
de sistemas de elevación basados en las propiedades físicas de los cables
y de las poleas, que sean adecuados para la simulación interactiva en tiempo
real.
El fin de los modelos desarrollados no debe ser la obtención de una simulaci
ón muy precisa de los diferentes aspectos involucrados en la dinámica,
sino conseguir animaciones visualmente creíbles, que permitan reproducir
los aspectos más identificativos de la dinámica de estos sistemas, muchos de
los cuales han sido obviados hasta el momento en simulaciones en tiempo
real.
Este trabajo contribuirá a solucionar las deficiencias detectadas en la
simulación de sistemas de poleas y cables en entornos virtuales, por medio
del diseñ de las técnicas y los modelos necesarios para ello. No obstante,
para alcanzar este objetivo es necesario plantear un conjunto de objetivos
más concretos. A continuación se describen estos objetivos.
Analizar y definir adecuadamente el problema. Se plantea como
objetivo preliminar conocer, de la manera más amplia posible, las
técnicas existentes relacionadas con el problema abordado en este trabajo,
para determinar de forma precisa los motivos por los que aún
no existe una solución satisfactoria.
Proponer una estrategia para la resolución del problema. Proponer
una metodología que permita desarrollar modelos dinámicos
adecuados al problema y a las situaciones que se abordan.
Definir un modelo abstracto de polipasto. Definir un modelo
abstracto del sistema, que sirva como marco para el desarrollo de modelos
de simulación adecuados a las necesidades planteadas. Obtener
un modelo a partir de un conjunto mínimo de requisitos, de forma que
sea aplicable a un mayor número de situaciones.
Mostrar en qué manera los nuevos modelos proporcionan mejoras en
la descripción del sistema simulado y en las prestaciones computacionales,
en comparación con otros modelos anteriores.
Aplicar los resultados obtenidos. Desarrollar aplicaciones de demostraci
ón, que muestren las principales propiedades de los modelos.
Para ello se deberán desarrollar las técnicas auxiliares necesarias para
alcanzar este objetivo, como la representación gráfica o la detección
de colisiones. Se plantea también como objetivo integrar los modelos
en aplicaciones reales.
Extender las principales contribuciones a otros ámbitos, como los entornos
virtuales interactivos y la animación, cuyos requisitos son, en
muchos casos, comunes con los de la simulación para entrenamiento</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El esquema metodológico seguido está enfocado al desarrollo de nuevos modelos para la simulación en entornos de realidad virtual. Esto se conseguir á por medio del diseñ de un modelo abstracto que facilitará especialmente la consecución de un modelo flexible, robusto y estable.
Con el fin de poder abordar los problemas planteados, se realizará en primer lugar un cos para los diferentes módulos definidos. Cada nuevo modelo se ajustará
a las especificaciones determinadas en el modelo abstracto y éste actuará
como vía de comunicación entre los distintos módulos.
Los modelos desarrollados se implementarán con el fin, en primer lugar,
de validar y mostrar su comportamiento. La validación permitirá analizar y
rediseñar, cuando sea necesario, los modelos desarrollados. Esta implementaci
ón servirá también para desarrollar las técnicas necesarias para garantizar
su aplicación en sistemas de realidad virtual y simulación. Se tratarán
especialmente la interacción y la representación gráfica del modelo, y se
desarrollará un demostrador de la tecnología, consistente en una aplicación
informática.
Por medio de la definición de las estrategias adecuadas, esta metodología
permitirá alcanzar los principales objetivos propuestos. El modelo abstracto
obtenido será flexible, ya que se define en base a un conjunto muy reducido
de suposiciones previas sobre el sistema. Se verá que la utilización de diferentes
modelos dinámicos para cada módulo permite ampliar el rango de
situaciones que pueden simularse. Por otra parte, la descomposición de la
dinámica en módulos permitirá identificar y controlar las fuentes de posible
inestabilidad de forma independiente.análisis de los trabajos anteriores relacionados con la tesis.
Este análisis permitirá conocer las soluciones propuestas hasta el momento en diferentes campos científicos y técnicos y determinar de forma precisa
cuales son las carencias que deben ser cubiertas.
Una vez analizado convenientemente el problema, se abordará el modelado del sistema. Para ello se recurrirá a la metodología de refinamiento sucesivo, o metodología Top-Down. Como resultado, se obtendrá un modelo abstracto del sistema, basado en su descomposición en módulos y en la identificación de los flujos de información.
Este modelo abstracto permitirá posponer el modelado de la dinámica
de los subsistemas, evitando así la influencia de éstos en el diseñ del modelo global. Una vez definido el modelo abstracto, se obtendrán modelos dinámi.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>A lo largo del trabajo de investigación se ha desarrollado un modelo de
polipasto adecuado para la simulación de maquinaria de elevación, especialmente
en el marco de la simulación para entrenamiento. Esta investigación
ha sido motivada principalmente porque, entre los modelos existentes, ninguno
de ellos considera la posibilidad de simular sistemas de poleas contemplando,
simultáneamente, la oscilación de los cables.
Se ha presentado un detallado análisis del problema y se ha propuesto una metodología que ha permitido la obtención de modelos de polipasto adecuados para este problema. Las contribuciones alcanzadas incluyen nuevos modelos para la simulación de las oscilaciones, simulación en las situaciones en las que la tensión es más elevada y técnicas para su implementación e integración en aplicaciones de realidad virtual. Por otra parte, se han presentado un conjunto de experimentos numéricos que proporcionan un análisis y validación del modelo propuesto.
A continuación, en este capítulo se realiza un resumen de las principales contribuciones obtenidas a lo largo del trabajo de investigación realizado.
En primer lugar se analiza la consecución de los objetivos planteados en el Capítulo 1 de la memoria. A continuación se discuten las principales aportaciones científicas que ofrece el presente trabajo en el campo de la simulación y animación de cables. Por último, se indican algunas líneas de trabajo futuro que surgen como a partir de los resultados obtenidos a lo largo de este trabajo de investigación.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>DEPARTAMENT INFORMÁTICA
PROPUESTA DE NUEVOS MODELOS DINÁMICOS DE
CABLES DE ELEVACIÓN PARA SIMULACIÓN EN TIEMPO
REAL
IGNACIO GARCÍA FERNÁNDEZ
UNIVERSITAT DE VALENCIA
Servei de Publicacions
2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"TEORÍA DE MODELADO DEL E-LEARNING Y APLICACIÓN A UN SISTEMA DE PISTAS ADAPTATIVO EN TUTORIá INTELIGENTE UTILIZANDO TE'CNICAS DE WEB SEMÁNTICA"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Para realizar estos objetivos, se ha realizado un esfuerzo en serie que ha requerido conseguir ordenadamente las siguientes metas generales:
1. Estudio del estado del arte de los diferentes aspectos relacionados con nuestros objetivos, para conocer los trabajos previos relacionados existentes.
2. Formulación de la teoría de modelado del e-learning.
3. Definición de la especificación de pistas para aprendizaje basado en problemas, e implementaci
ón de un reproductor de pistas acorde con dicha especificación, como extensi
ón del tutor inteligente XTutor.
4. Evaluación del modelo de pistas definido desde diversos puntos de vista.
5. Aplicación de técnicas de Web semántica para conseguir pistas adaptativas, combinando el tutor inteligente XTutor junto con la extensión del reproductor de pistas y el razonador de Web semántica CWM.
A continuación, para cada una de las metas generales, se exponen las metas específicas
que de forma ordenada se hubieron de conseguir para lograr las metas generales. En primer
lugar, en relación con el estudio del estado del arte se hubieron de conseguir ordenadamente las siguientes metas específicas
Estudio del estado del arte sobre ingeniería Web e ingeniería Software. De esta forma
se puede comprender quécosas se han modelado y cómo, en relación con aplicaciones
software y aplicaciones Web. De ello se pueden extraer ideas interesantes para
aplicarlas de forma análoga en el e-learning.
Estudio del estado del arte sobre diseño instruccional del aprendizaje. Ello conlleva
conocer los diferentes modelos de planificación de un curso, así como las diferentes
teorías pedagógicas y psicológicas asociadas al aprendizaje. Todo ello se debe tener
muy en cuenta a la hora del modelado de e-learning, ya que el e-learning tiene que nutrirse
de las teorías tradicionales del proceso de aprendizaje, pero así mismo habrá que
complementarlas con nuevos ingredientes fruto del empleo de las tecnologías de la informaci
ón.
Estudio del estado del arte sobre la concepción global del e-learning.
Estudio del estado del arte sobre las especificaciones y estándares existentes para
e-learning y otras especificaciones existentes que aunque no son específicas de elearning,
están relacionadas.
Estudio del estado del arte sobre trabajos que incluyen modificaciones y extensiones a
especificaciones y estándares existentes para e-learning, debido a carencias detectadas
en las especificaciones de e-learning.
Estudio del estado del arte sobre modelados en UML existentes para aspectos de elearning.
Estudio del estado del arte sobre modelados existentes utilizando técnicas formales
para aspectos de e-learning.
Estudio del estado del arte sobre herramientas de autor para e-learning.
Estudio del estado del arte sobre adaptabilidad y personalización en e-learning.
Estudio del estado del arte sobre herramientas de competición en educación.
Estudio del estado del arte en relación con el concepto de sistema de pistas en tutoría
inteligente y la importancia de tales sistemas.
Estudio del estado del arte de sistemas de tutoría para la provisión de pistas que no
proporcionan funcionalidades adaptativas de pistas.
Estudio del estado del arte de sistemas de tutoría para la provisión de pistas que implementan
características adaptativas de pistas.
Estudio del estado del arte sobre experiencias o/y evaluaciones realizadas en el aula o
a distancia, con sistemas de pistas para sacar conclusiones sobre la efectividad de los
sistemas, información de las interacciones con los sistemas, comportamientos de los
alumnos, opiniones de los alumnos, etc.
Estudio de los conceptos generales sobre Web semántica.
1.3. Planificación de Tareas 11
Estudio del estado del arte sobre ontologías definidas para aplicaciones educativas.
Estudio de herramientas para la anotación en lenguajes deWeb semántica en el ámbito
educativo.
Estudio de herramientas de autoría para Web semántica en educación.
Estudio del estado del arte sobre aplicaciones de Web semántica en educación.
Estudio del estado del arte de arquitecturas de Web semántica en educación para personalizaci
ón y adaptación.
Estudio del estado del arte sobre aplicaciones adaptativas de Web semántica en educaci
ón.
En relación con la teoría de modelado del e-learning se hubieron de conseguir ordenadamente
las siguientes metas específicas:
Hacer un compendio de las diferentes funcionalidades y características presentes en
diferentes sistemas de gestión de aprendizaje y tutores inteligentes, y relacionarlo con
los diferentes aspectos cubiertos por las diferentes especificaciones y estándares de
e-learning, viendo así cuáles están modeladas, y cuales faltan por modelarse.
Ver los diferentes modos en que se ha modelado la Web, el software y algunas funcionalidades
de e-learning, y ver si los diferentes modos de modelado tienen sentido
su aplicación para las diferentes funcionalidades y características del compendio del
punto anterior.
Analizar cómo utilizar la especificación IMS Content Packaging para poder modelar
un curso completo con sus diferentes servicios y proponer un modelo para usar IMS
Content Packaging para tal fin.
Analizar cómo utilizar UML para poder modelar un curso completo con sus diferentes
servicios y proponer un modelo para usar UML para tal fin.
Contribuir en la realización de una herramienta de autor fácil de utilizar para usuarios
sin conocimientos tecnológicos, de forma que cubra aspectos de e-learning para los
cuales había una carencia de modelado en las especificaciones de e-learning existentes
Contribuir en la realización de una herramienta de autor fácil de utilizar para usuarios
sin conocimientos tecnológicos, de forma que cubra la planificación de un curso
pasando por las fases de objetivos, métodología, contenidos y evaluación, siendo compatible
con la especificación IMS Learning Design.
Definición de la nueva teoría de reglas adaptativas.
Definición de la nueva arquitectura que permita combinar técnicas de Web semántica
con los tutores inteligentes.
Definir una visión ideal de e-learning combinando todos los servicios y aspectos a
modelar.
Definir un ciclo de vida que combine todos los elementos a modelar y diseñar, y diga
claramente las fases de ejecución de cada uno de los modelados.
Definir los criterios generales de evaluación para los diferentes aspectos del modelado.
Diseñar y ejecutar una experiencia de evaluación para calificar la importancia de diferentes
funcionalidades de sistemas de gestión del aprendizaje.
En relación con la definición de la especificación de pistas para aprendizaje basado en
problemas, e implementación de un reproductor de pistas acorde con dicha especificación,
como extensión del tutor inteligente XTutor, se hubieron de conseguir ordenadamente las
siguientes metas específicas:
Una vez conocidas las características de otros sistemas de pistas del estado del arte
(sin considerar aspectos de adaptación en un primer momento) se realiza un compendio
para integrar todos los conceptos, agrupar conceptos similares con diferentes
términos según los sistemas pero idéntica funcionalidad, permitir la combinación de
los diferentes conceptos, y añadir nuestras nuevas ideas para la provisión de pistas,
entonces se especifican las características del nuevo modelo de pistas y se define un
nuevo modelo de datos, tratando que sea lo más completo posible.
Se definen los mapeos a XML, y UML del modelo de datos definido, siguiendo las
mejores prácticas posibles.
Se estudia el lenguaje de programación python (que es el lenguaje sobre el que está escrito
el tutor XTutor) así como las librerías que acompañan al tutor inteligente Xtutor.
Se implementa un reproductor de pistas que sea capaz de cargar y entender ficheros
XML de acuerdo con la especificación definida. Este reproductor se implementará utilizando
python y las librerías de XTutor, como una extensión a la funcionalidad inicial
proporcionada por XTutor.
Se contribuye en la realización de una herramienta de autor capaz de generar ficheros
XML de acuerdo con la especificación definida y que sea fácilmente utilizable por
profesores y diseñadores de cursos.
Se contribuye en la realización de una herramienta para competición entre alumnos a
base de ejercicios donde se pueden añadir pistas.
En relación con la evaluación del sistema de pistas para el aprendizaje se hubieron de
conseguir ordenadamente las siguientes metas específicas:
Realización de una experiencia en Laboratorio de Arquitectura de Ordenadores durante
los cursos 2004/2005 y 2005/2006 donde los alumnos interaccionaron con un
sistema de preguntas sin pistas, siguiendo la especificación IMS-QTI.
Realización de dos experiencias en Laboratorio de Arquitectura de Ordenadores durante
los cursos 2006/2007 y 2007/2008 donde los alumnos interaccionaron con el
sistema de pistas implementado. Durante el año 2006/2007, adicionalmente un grupo
de alumnos trabajo sólo con los ejercicios sin pistas, pero era el profesor quien
proporcionaba las pistas en clase.
Obtener datos generales de utilización del sistema, tanto de ejercicios como de pistas.
Comparar datos obtenidos entre experiencias utilizando sistema con pistas y sin pistas
para ver si hay diferencia significativa.
Comparar datos obtenidos de incremento de aprendizaje en el año 2006/2007 entre
grupo que utilizó el sistema y grupo que usó tutores humanos dando pistas, para ver si
hay diferencia significativa.
Comparar datos obtenidos en los cursos 2006/2007 y 2007/2008 sobre la utilización de
diferentes técnicas de provisión de pistas para averiguar cuáles y bajo qué condiciones
resultaron más beneficiosas para los alumnos, así como conocer que diferencias de
comportamiento implica utilizar diferentes técnicas de provisión de pistas.
Procesar los resultados de encuestas de las diferentes ediciones experimentales, donde
se recoja la opinión de los alumnos sobre el sistema, su usabilidad, su beneficio, la importancia
de las diferentes técnicas de provisión de pistas, la justicia de las diferentes
técnicas, las preferencias de los alumnos, etc.
Finalmente, en relación con la aplicación de técnicas deWeb semántica para pistas adaptativas
se hubieron de conseguir ordenadamente las siguientes metas específicas:
Particularización de la arquitectura general definida en la teoría de modelado del elearning
al caso de pistas adaptativas que combina el tutor inteligente XTutor junto
con la extensión del módulo de pistas implementado, y el razonador deWeb semántica
XTutor.
Implementación de un prototipo de la arquitectura específica definida para pistas adaptativas,
implementando el motor de llamadas de XTutor hacia el razonador CWM, los
transformadores de formatos, etc.
Definición de la información necesaria sobre la que razonar para la adaptación,
plasmándola mediante ontologías de los diferentes aspectos.
Definición de las reglas de adaptación para pistas adaptativas siguiendo la teoría general
de reglas definida en la teoría de modelado del e-learning.
Implementación de algunos ejemplos de reglas de adaptación de pistas en Notation 3
(N3) siguiendo la teoría general de reglas.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Esta tesis doctoral tiene dos objetivos finales, los cuales se pueden desglosar en diferentes
subobetivos. A continuación se presentan ambos objetivos junto con su desglose correspondiente
en subobetivos:
1. Definición de una teoría de modelado del e-learning, que por un lado aporte una visión
global de modelado del e-learning, y por otro lado modele de forma completa desde
diferentes puntos de vista aspectos para los que se detectan carencias. En concreto,
esto implica los siguientes subobjetivos:
a) Proporcionar una visión global que incluya qué aspectos del e-learning se pueden
modelar, categorizándolos y enumerándolos, y para cada uno de dichos aspectos
de qué formas diferentes se pueden modelar.
b) Proporcionar una visión ideal de evolución del e-learning, teniendo en cuenta el
modelado de los diferentes aspectos.
c) Establecimiento de la relación entre los diferentes estándares de e-learning existentes
(así como de otras especificaciones del estado del arte), y los aspectos de
modelado que cubren y aquellos en los cuales tienen carencias.
d) Proponer un nuevo modelado de cursos de e-learning mediante XML, utilizando
la especificación IMS Content Packaging.
e) Proponer un nuevo modelado de cursos de e-learning mediante UML, utilizando
una extension de UML.
f ) Contribuir en la realización de diferentes herramientas de autor de e-learning
(abarcando diferentes servicios, y aspectos de la especificación IMS Learning
Design), las cuales pueden ser vistas como un modelado de diferentes aspectos
en lenguaje de personas con bajos conocimientos tecnológicos.
g) Definir una nueva teoría de reglas adaptativas, que permita el modelado de reglas
de adaptación de un sistema de forma sencilla y clara para los diseñadores, y de
forma que sea fácilmente extensible con nuevas reglas, flexible, basado en reglas
atómicas, reusables, interoperables, combinables según condiciones, e intercambiables.
h) Definir una nueva arquitectura que permita combinar técnicas deWeb semántica
con sistemas de tutoría inteligente, integrando de esta forma a los sistemas de
e-learning en la Web semántica aprovechando todas sus ventajas. Para ello, la
arquitectura debe proporcionar una solución que combine el modelado en XML
que típicamente usan muchos sistemas de e-learning con el modelado mediante
técnicas formales de la Web semántica. La arquitectura definida debe enumerar
y explicar los diferentes elementos involucrados, sus interrelaciones, así como
los diferentes criterios de diseño a decidir durante la implementación de la arquitectura
junto con algunas guías para ayudar en dichas decisiones.
i) Definir un nuevo ciclo de vida del e-learning para la generación de contenidos,
reglas, assessments, etc., el cual incluya todos los aspectos involucrados en el
e-learning, su orden de ejecución, ciclo para la mejora de los materiales, etc.
j) Establecimiento de los criterios generales para la evaluación de los diferentes
aspectos de e-learning, y aplicación de la evaluación a la calificación de la importancia
de diferentes funcionalidades de sistemas de gestión del aprendizaje.
2. Crear un nuevo sistema de pistas adaptativo en tutoría inteligente utilizando técnicas
de Web semántica, donde se apliquen varios de los diferentes aspectos de la teoría
comentada. En concreto, esto implica los siguientes subobjetivos:
a) Definir un nuevo modelo de datos como parte de una nueva especificación de
pistas para aprendizaje basado en problemas. Dicho modelo de datos debe describir
las diferentes posibilidades de provisión de pistas abarcando los conceptos
recogidos en otros sistemas de pistas del estado del arte, así como incluyendo
otros nuevos aspectos basados en nuestras propias ideas y experiencia como
profesores.
b) Definir un mapeo a XML del modelo de datos establecido de la especificación
de pistas, de forma que cualquier instancia de ejercicio con pistas pueda ser representado
en XML. Dicho mapeo a XML debe cumplir que los archivos XML
obtenidos, representando instancias de ejercicios con pistas, deben ser de la menor
longitud posible y deben poder ser fácilmente insertables unos con otros para
poder componer nuevos ejercicios con pistas a partir de otros iniciales, dependiendo
de los criterios de adaptación considerados.
c) Definir un modelo que permita representar en UML cualquier instancia de ejercicio
con pistas.
d) Definir un mapeo a RDF del modelo de datos establecido de la especificación
de pistas, de forma que cualquier instancia de ejercicio con pistas pueda ser
representado en RDF.
e) Implementar una herramienta de autor que sea de fácil utilización por profesores
y diseñadores de ejercicios sin conocimientos tecnológicos, de forma que
puedan crear las diferentes instancias de la especificación de pistas definida, y
que permita importar y exportar en formato XML de acuerdo al mapeo XML
definido para la especificación de pistas. Esta herramienta se puede ver como un
modelado de alto nivel en palabras habituales para personas sin conocimientos
tecnológicos.
f ) Implementar un reproductor de pistas como extensión al tutor inteligente XTutor
desarrollado en el MIT (Massachusetts Institute of Technology), de forma
que pueda cargar y ejecutar ejercicios con pistas definidos en XML según la
especificación definida. Los alumnos podrán acceder vía Web a dicho sistema
implementado para resolver los ejercicios propuestos, pudiendo visualizar los
ejercicios, introducir las soluciónes, pedir pistas correspondientes, etc.
g) Diseñar una aplicación de competición entre alumnos basada en ejercicios con
pistas.
h) Conocer parámetros generales sobre el uso y satisfacción de los alumnos con el
sistema de pistas.
i) Comparar la efectividad del sistema con pistas con respecto a otro sistema sin
pistas para observar si se produce un beneficio significativo.
j) Comparar el incremento de conocimiento que produce el sistema con pistas con
respecto a tutores humanos dando pistas en clase. De esta forma se quiere corroborar
que dicho sistema de pistas es tan efectivo para el aprendizaje como un
tutor humano, y por lo tanto se podría dejar a los alumnos solos interaccionando
con el tutor obteniendo al menos similares resultados que con tutores humanos.
k) Evaluación de diferentes estrategias de provisión de pistas para conocer
qué técnicas de provisión de pistas pueden ser mejores que otras porque hacen
incrementar más el conocimiento de los alumnos, y si siempre dicha mejora es
en la misma dirección o depende de ciertos factores y es necesaria por lo tanto la
adaptación. En concreto se quieren evaluar para diferentes materias, las técnicas
de no penalizar por ver pistas con respecto a penalizar por ver pistas cuando en
ningún caso hay un límite máximo de pistas a visualizar, no penalizar por ver
pistas y con un límite máximo de pistas a visualizar con respecto a penalizar por
ver pistas pero sin un límite máximo de pistas a visualizar, dejar pedir la pista
directamente con respecto a sólo dejar pedir la pista ante un intento incorrecto,
tener efecto en la puntuación del problema inicial según la resolución de las pistas
asociadas con respecto a no tener ningún efecto en la puntuación, bonificar
en el problema inicial por resolución correcta de pistas con respecto a penalizar
por resolución incorrecta de pistas, y dejar ver todas las pistas menos una con
respecto a sólo dejar ver una pista.
l) Evaluación del comportamiento de los alumnos ante diferentes técnicas de provisi
ón de pistas. Las técnicas consideradas que se comparan son las mismas que
en el punto anterior, y sobre algunas de ellas se compara el número de pistas
vistas, o/y el número de problemas pistas contestadas y si se resolvieron mal o
bien.
m) Conocer la opinión de los alumnos ante las diferentes características de provisi
ón de pistas del sistema, incluyendo la importancia que le dan a cada una, sus
preferencias, la justicia, o su comportamiento esperado.
n) Definir una arquitectura para combinar el tutor inteligente XTutor junto con la
extensión de pistas implementada con el razonador de Web semántica CWM,
de forma que se puedan conseguir pistas adaptativas en aprendizaje basado en problemas. Dicha arquitectura será una instancia particular o realización de la
arquitectura general definida en la teoría del modelado del e-learning
ñ) Implementar un prototipo de la arquitectura definida en el punto anterior, que
permita lograr pistas adaptativas utilizando nuestra extensión de pistas en XTutor
y el razonador de Web semántica CWM, y combinando los formatos XML de
descripción de pistas definida, y el lenguaje Notation 3 (N3) propio de la Web
semántica y entendible por CWM. Así mismo, se deberán dar solución a los
diferentes problemas de implementación comentados en la arquitectura general,
adoptando las soluciónes oportunas.
o) Definición de las ontologías oportunas para el sistema de pistas adaptativas, incluyendo
ontologías de alumnos, problemas, pistas, conceptos del curso, o sobre
las diferentes técnicas de pistas.
p) Definición de las reglas de adaptación para el sistema de pistas adaptativo, incluyendo
la adaptación de contenidos de pistas o de las diferentes técnicas de
pistas, dependiendo de los materiales específicos o los alumnos.
q) Implementación de algunas reglas de adaptación para el sistema de pistas adaptativo,
utilizando el lenguaje Notation 3 (N3) entendible por el razonador deWeb
semántica CWM.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Conclusions
This chapter presents the conclusions of this PhD. First of all, it provides a list of the
more relevant contributions found in this PhD, next a summary of the main applications
related to the contributions, and finally a discussion about the limits of the contributions.
7.1. List of Contributions
The contributions of this PhD can be divided in four general blocks: theory of the elearning
modeling, new specification of hints for problem-based learning, evaluation of the
hinting model and the implemented hinting tutor, and application of semantic Web techniques
for achieving adaptive hints.
For each one of these four general blocks, there is a subsection explaining the specific
contributions.
7.1.1. Theory of the E-learning Modeling
The list of contributions regarding the theory of the e-learning modeling is the following:
It provides a global view of the e-learning modeling that includes a classification of
what issues to model, how to model such issues, and the relationships between different
concepts. This global view takes into account existing work about software and
Web modeling from software andWeb Engineering sciences, as well as existing works
on e-learning modeling. It tries to give a general overview about e-learning modeling
that allows researchers to have a clear idea about the topic.
It provides an ideal vision of the e-learning that includes interoperability between
different issues, authoring tools that are easy to use by teachers without high technological
expertise, enabling the best practices to improve students0 learning gains,
maintainable resources, etc. As part of this contribution, there are some illustrations
with specific case studies. This ideal view takes into account previous e-learning studies.
Study of the relationships between main LMS functionalities and present e-learning
specifications, determining some lacks. For each main LMS service or functionality, its relationship to present e-learning specifications is provided. In addition, some lacks
of educational standards for some LMS services are determined, explaining the need
of standardization for such functionalities. Among the possible categories in which
we divided the different LMS functionalities, are the following: LMS services for
which it has no sense to define a new data specification within an LMS course, LMS
services that are covered completely by one or more specifications, LMS services that
are covered partially by one or more specifications, and LMS services that are not
covered by present specifications.
A proposal of how to use the IMS-CP specification in order to define the general
configuration information of an LMS course, how to set the layout, or how to describe
the structure of the different resources, and services of an LMS. This is a different use
to the regular one for IMS-CP which is for assembling different contents and resources
of a course.
A proposal of a modified UML-based model for describing courses in LMSs. This
proposal allows describing complete LMS courses graphically based on UML class
and package diagrams with some modifications with respect to UML in order to cover
all the needs.
A proposal of a combined graphical and textual model for describing rules for adaptive
and personalized e-learning systems. In addition, this model defines best practices
on the definition of such types of rules, implying rules that are atomic, reusable,
interchangeable, interoperable, and that can be combined to form bigger rules. The
modeling method enables a way for describing individual rules with such features in
a formal, clear, and easy way; for grouping them in subsets; and for determining in a
clear way how they can be combined or interchanged.
Definition and design of new authoring tools which can be presented as modelings
close to teachers. This contribution has been made in collaboration with two Master's
Thesis [14], [15]. The final results were two software authoring tools which are easy
to use by teachers without high technological knowledge. One of the authoring tools
integrates several new defined specifications for LMS educational services for which
there were some lacks of e-learning specifications (such as forums, assignments, or
subgroups), and can generate XML files according to such new defined specifications.
The another implemented tool was CourseEditor, a course authoring tool that let a teacher
describe the complete planning of a course passing through its different phases
(objectives, contents, methodology and evaluation) and to make graphical representations
in an easy way different aspects of IMS-LD. The tool generates files compatible
with the IMS-LD e-learning specification and also other XML files with extra information
about the course.
A proposal of a new architecture for combining semantic Web techniques with ITSs.
There are several advantages in using semantic Web techniques with ITSs, which justifies
its combination. At present, few works have discussed architectures for enabling
this combination. Some of the state of the art works show Web service-based architectures
to enable semantic Web methods in adaptive hypermedia. Our architecture approach presents a different point of view, because it focuses on the relationship between
semanticWeb reasoners and existing ITSs, explaining its architectural elements,
design criterions, implementation problems, etc. Moreover, a few works exist which
explain specific system solutions but they do not cover the different architectural design
criterions and implementation problems. In this work, we contribute to these
issues, explaining an architecture for combining semanticWeb with ITSs that is general
enough. Furthermore, we explain some challenges that semantic Web techniques
present when applied with ITSs; those challenges require architecture implementation
decisions, and we give some recommendations.
A proposal of an e-learning development process. A development process in elearning
has been proposed which includes an extensive list of elements to be modeled
in e-learning in the correct order. By means of this development process, a course can
be created in an ordered and formalized way and it can also be improved in the next
phases of the life cycle, taken into account the evaluation process.
A study of a general plan for the evaluation of the different aspects involved in elearning.
Some general guidelines are provided in order to execute the evaluation of
different e-learning issues. Based on it, the e-learning models and its use can be improved.
A specific evaluation for rating the importance of different LMS functionalities, made
as a experience in the context of the Computer Architecture Laboratory course. In this
way, different LMS functionalities were compared, and an estimation of its importance
was presented.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD CARLOS III DE MADRID
DEPARTAMENTO DE INGENIERIá TELEMÁTICA
TESIS DOCTORAL
TEORÍA DE MODELADO DEL E-LEARNING Y
APLICACIÓN A UN SISTEMA DE PISTAS ADAPTATIVO EN
TUTORIá INTELIGENTE UTILIZANDO TE'CNICAS DE
WEB SEMÁNTICA
Autor: Pedro José Muñoz Merino
Ingeniero de Telecomunicación
Director: Dr. Carlos Delgado Kloos
Doctor Ingeniero de Telecomunicación
Leganés, Mayo de 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"LAS TECNOLOGÍAS DE LA INFORMACIÓN COMO FUENTE DE VENTAJAS COMPETITIVAS. UNA APROXIMACIÓN EMPÍRICA"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de la presente investigación es desarrollar un
modelo que permita evaluar e identificar las aplicaciones de las tecnologías
de la información y la forma en que estas tecnologías afectan al
rendimiento de las empresas, a través del logro y aprovechamiento de
ventajas competitivas. Se evaluarán también potenciales factores o condiciones que favorecen el logro y sostenimiento de dichas ventajas,
utilizando para ello el Enfoque de Recursos.
El objetivo anterior se desglosa en los siguientes objetivos específicos:
A nivel teórico, determinar:
* Qué son las TI y los Sistemas de Información y su relevancia dentro
del contexto de estudio de la Administración de Empresas.
* Cuáles son los antecedentes de la literatura estratégica que sustentan
el papel de las tecnologías de la información como fuente de
ventajas competitivas en las empresas.
* La conveniencia de adoptar el Enfoque de Recursos para centrarnos
en el estudio de las TI y su relación con el Rendimiento de las
empresas.
* El modelo integrador de análisis multivariable que se propone para
analizar tal relación.
* Los potenciales factores que favorecen el aprovechamiento de las
potenciales ventajas competitivas que las TI pueden generar en las
empresas.
* Definir las hipótesis pertinentes que serán contrastadas en la parte
empírica.
A nivel empírico y en relación a la población analizada:
* Identificar las TI utilizadas por las empresas.
* Identificar y evaluar los efectos que producen las TI en el ámbito
interno de las empresas según las hipótesis propuestas que serán
contrastadas.
Identificar y evaluar los efectos que producen las TI en el ámbito
externo de las empresas según las hipótesis formuladas que serán
contrastadas.
* Identificar las condiciones o atributos que favorecen el
aprovechamiento y obtención de las ventajas competitivas
generadas por las TI en las empresas y contrastar las hipótesis
planteadas al efecto.
* Determinar las conclusiones relevantes y sus implicaciones
plasmadas en las futuras líneas de investigación que se proponen</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Se aplicó el Análisis Cluster no Jerárquico de K-medias y se
calcularon los correspondientes Análisis Discriminantes a cada uno de los
Cluster. El resultado del discriminante mostraba que la agrupación de las
empresas en dos grupos era mejor, ya que determinaba dos muestras de
empresas representativas (N > 30), clasificaba a las empresas por encima
(grupo 1) y alrededor, aunque algo por debajo (grupo 2) de la mediana de la
muestra total analizada(tabla 5.1) y clasificaba correctamente el mismo
porcentaje de casos ( 97%).
La caracterización de los grupos se muestra en la tabla 5.2. Además el
Análisis Discriminante ponía de manifiesto que las variables que más
diferenciaban a los grupos determinados eran por este orden, el gasto anual en
formación en TI de los directivos o propietarios de las empresas (Lambda =
0.257) y el gasto anual en formación de TI del personal (Lambda = 0.320).
En consecuencia se constata una clara diferenciación de los dos
grupos de empresas, que identifica a un grupo por encima del otro con
respecto al esfuerzo inversor continuado en TI, que destaca en dos aspectos:
a) El esfuerzo global y continuado en el gasto en TI.
b) El esfuerzo continuado en el gasto en formación en TI.
Por otro lado, las Empresas que presentan un nivel más elevado de
inversión en TI llevan más tiempo funcionando (15-20 años), más de la mitad
son sociedades y más de la mitad lleva más de 10 años invirtiendo en TI.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>A fin de proceder a contrastar las distintas hipótesis determinadas en el
capítulo anterior se procedió a desarrollar un análisis empírico sobre una
realidad concreta del mundo empresarial.
El objetivo del presente capítulo es mostrar la métodología empleada en
dicho análisis. Así, en primer lugar se expone la justificación de la población
elegida, la elaboración del cuestionario, la descripción del trabajo de campo, la
determinación de la muestra y la métodología estadística aplicada sobre los
datos obtenidos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>6.1.-CONCLUSIONES DE LOS APARTADOS TEÓRICOS.
Con respecto a la parte teórica de la Tesis es posible establecer las
siguientes aportaciones.
Se han definido a las TI, como recursos de la empresa, como aquel
conjunto de tecnologías específicas (ya que derivan de otras tecnologías
fundamentales y genéricas) integradas por un complejo grupo de
conocimientos, medios y Know-how organizado, que basadas en los
desarrollos tecnológicos derivados de la interrelación entre otras tecnologías
fundamentales o genéricas como la microelectrónica, la informática y las
telecomunicaciones, desarrollan innovaciones en los procesos de elaboración,
manipulación, transmisión y presentación de datos, todo ello en el ámbito de
las actividades relacionadas con la comunicación, el cálculo o procesamiento
de datos y el control.
Se ha definido el concepto de SI, como capacidades de la empresa,
como aquel conjunto de recursos de TI, humanos, materiales e intangibles, que
se movilizan de forma conjunta a fin de optimizar la gestión del componente
informativo de todas las actividades operativas o de decisión de la empresa.
Para ello, los SI se estructuran de forma jerárquica desde las actividades
individuales hasta las interfuncionales y desde el proceso simple de los datos
hasta la gestión integrada de los mismos que genera información de valor
añadido para la dirección de la empresa.
Se ha revisado la literatura que ponen de manifiesto que tales TI y
SI han ido ganando una relevancia mayor en la gestión de las empresas,
que deriva de su potencialidad de generar ventajas competitivas en las empresas y la consecuente mejora que experimentan diversos aspectos del
rendimiento.
A este respecto, se ha revisado la literatura estratégica y se ha puesto de
manifiesto la falta de la existencia de evidencias claras o modelos empíricos
que contrastaran de forma contundente la relación entre el esfuerzo inversor en
TI de las empresas y su efecto sobre el rendimiento.
Así, por un lado aparecían aproximaciones teóricas o exposición de
casos donde la literatura estratégica ponía de manifiesto el aspecto estratégico
de las TI como elemento que afectaba a la competitividad de las empresas
desde el enfoque industrial basado en el análisis de Porter (1982), que mostraba
cómo las TI se integraban en las empresas modificando la forma en que éstas
competían en el sector industrial.
Sin embargo, el Enfoque de Recursos nos mostraba que tales ventajas
no resultaban gratuitas, sino que respondían al nivel de disponibilidad de los
recursos de TI que las empresas posean y que además, el hecho de que unas
empresas hubieran alcanzado éxito en el logro de estas ventajas y otras no,
debía fundamentarse en el desarrollo y posesión de unos factores motivadores
de las TI.
De esta manera, se revisó la bibliografía que mostraba diversos intentos
de medir el efecto de las TI sobre el rendimiento de las empresas, que
argumentaba como problemas principales a resolver: la determinación de las
variables que operacionalicen tales conceptos y el establecimiento de un
modelo que establezca de forma clara la relación lineal, si existe, entre las TI ,
las ventajas fundadas en su utilización y el efecto sobre los resultados de las
empresas.
La principal aportación teórica de esta Tesis, resulta ser el modelo
planteado que establece relaciones directas entre el esfuerzo inversor en TI y sus efectos sobre el Rendimiento de la empresa derivados del nivel de
ventajas competitivas conseguidas.
Para ello y como paso previo se ha propuesto una nueva clasificación
instrumental de diferentes categorías de TI derivado del análisis realizado
centrado en las TI como recursos que persigue diferentes objetivos y causan
diferentes efectos sobre los diversos aspectos de los resultados de las empresas.
Así se definen las Aplicaciones Transaccionales Internas de TI que se
relacionan con las actividades operativas y de proceso de datos internas de la
empresa, potenciando ventajas competitivas derivadas directamente de su uso
de reducción de costes, aprovechamiento de los datos que se procesan y
servicios de valor añadido, y que se relacionan con el rendimiento de la
empresa.
Se han definido las Aplicaciones Transaccionales Externas de TI
que se relacionan con los actividades operativas y de comunicación externas de
la empresa con proveedores, clientes y otras instituciones, potenciando ventajas
de reducción de costes, rapidez y fiabilidad de tales actividades y
comunicaciones y que se relacionan con determinados aspectos del
rendimiento.
Se han definido las Aplicaciones Informativas Internas de TI que se
relacionan con las actividades de gestión de la información y toma de
decisiones en el interior de la empresa, que potencian ventajas de mayor
confianza de los directivos y eficiencia de sus decisiones, que se relacionan con
el funcionamiento general de la empresa y su rendimiento.
Se han definido las Aplicaciones Informativas Externas de TI que se
relacionan con las actividades de gestión de la información externa de la
empresa y su influencia en la capacidad de los directivos, que supuestamente
mejorarían sus decisiones y el funcionamiento de la empresa.
El modelo también recoge la apreciación de nuevas formas de hacer las
cosas, es decir, la posible determinación de nuevas capacidades. Para ello se ha
seguido el esquema que diferencia distintas actividades básicas de la empresa
donde las TI pueden influir propuestos por Porter y Millar (1986) y el carácter
interfuncional de las capacidades propuesto por Grant (1995).
Se han propuesto una serie de factores motivadores del desarrollo
de las TI, que presentaban la potencialidad de explicar las diferencias
observadas entre las empresas con relación al desarrollo de las TI. Así se han
definido como tales, los requerimientos financieros para invertir en TI, la
adecuada formación y apoyo de consultores en TI, el interés de los directivos y
propietarios por las TI, la necesidad de planificar las inversiones en TI y la
influencia de otras entidades relacionadas con el sistema de valor de la empresa
en la adopción de las TI.
Por último con base en tal planteamiento y sobre la base de la distinción
entre tecnologías básicas, claves y emergentes que establece Little (1981) y una
vez analizados los resultados empíricos se establecerán las TI más relevantes
desde un punto de vista competitivo para las empresas del comercio minorista
murciano analizadas.
También resulta importante la diferenciación de las tres etapas de
desarrollo de las TI (Ward et al., 1990) en las empresas que establece la etapa
de administración de datos, de sistemas de información y de sistemas
estratégicos. Ello sirve para determinar el nivel de desarrollo global de las
empresas analizadas.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE MURCIA
FACULTAD DE CIENCIAS ECONÓMICAS Y EMPRESARIALES
DEPARTAMENTO DE ECONOMÍA DE LA EMPRESA
"LAS TECNOLOGÍAS DE LA INFORMACIÓN COMO FUENTE DE
VENTAJAS COMPETITIVAS. UNA APROXIMACIÓN EMPÍRICA.
TESIS DOCTORAL
Presentada por: Antonio Paños Álvarez.
Dirigida por: Dr. D. Ramón Sabater Sánchez.
Murcia, septiembre de 1999.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"PLANIFICACIÓN GLOBAL EN SISTEMAS MULTIPROCESADOR DE TIEMPO REAL"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>1.6 Objetivos 

El objetivo global de esta tesis es desarrollar un método teórico de planificación de
sistemas de tiempo real estricto en arquitecturas multiprocesador con memoria
compartida. Este método debe garantizar los plazos estrictos y tiene que aprovechar
al máximo la capacidad de proceso que proporcionan los distintos procesadores del
sistema. 

La planificación de sistemas monoprocesador puede ser realizada fuera de línea de
forma estática, o durante la ejecución, dinámicamente. La planificación fuera de
línea es el método más comúnmente utilizado en sistemas de tiempo real crítico,
como por ejemplo en aviónica o automovilismo, debido a que es el método más 
determinista. A su vez, por el hecho de realizarse de forma estática, es el método 
menos flexible, puesto que difícilmente pueden realizarse cambios. En los últimos
años, los sistemas de tiempo real han crecido en complejidad y funcionalidad,
precisando de una mayor capacidad de proceso. Frecuentemente gran parte de esta
carga adicional viene dada por tareas que no son críticas y de ejecución aleatoria. Por
eso, junto con la madurez de los conocimientos, cada vez más se ha ido utilizando la
planificación dinámica en entornos monoprocesador. Sin embargo, en arquitecturas
multiprocesador se siguen utilizando los métodos más deterministas y menos
flexibles: se reparten de forma estática en tiempo de diseño las tareas entre los
procesadores para que en tiempo de ejecución se pueda tratar cada procesador como
un monoprocesador. Esta metodología reduce la complejidad pero no permite
aprovechar al máximo la capacidad de proceso disponible. Por ejemplo, puede darse
la circunstancia de tener unos procesadores muy cargados y, al mismo tiempo, otros
procesadores completamente ociosos en el sistema. Además, esta metodología
difícilmente se adapta a cambios. Es por esta razón que en esta tesis proponemos
llevar a la práctica métodos que hagan un uso más flexible del sistema
multiprocesador. 

El primer objetivo de la tesis es diseñar un método de planificación que haga posible 
una máxima utilización de los procesadores. Por este motivo nos hemos centrado en 
la planificación on-line, basada en prioridades, con desalojo y con migraciones. 

Además, todos los capítulos de la tesis se centraran en usar la capacidad de proceso
sobrante de las tareas periódicas para dar servicio a algún tipo de tareas adicionales.  
El segundo objetivo es comprobar que modalidad de distribución de las tareas es la
más conveniente según el tipo de tareas. Así, en el tercer capítulo se analizan
diversas posibilidades de distribución de los diversos tipos de tareas entre los
procesadores. En este capítulo también se establecen los algoritmos que servirán de
referencia en los estudios de rendimiento de los capítulos posteriores. 
El tercer objetivo de la tesis es conseguir una planificación global de tareas
periódicas, de forma que esta planificación sea flexible, pero al mismo tiempo
garantizando los plazos de ejecución. El planificador global ha de ser simple, no 
realizar complicados cálculos de forma dinámica y no debería necesitar grandes 
estructuras de datos, de forma que pueda utilizarse con número de procesadores
elevado de forma eficiente.  

Si bien en algunas aplicaciones puede ser deseable conseguir el primer objetivo,
tampoco se puede pretender conseguir que las tareas periódicas utilicen totalmente
todos los recursos de computación puesto que, por un lado, los sistemas
multiprocesador pueden llegar a tener una gran capacidad de cálculo y, por otro lado,
debemos dejar recursos libres para permitir la ejecución de las tareas no críticas, las
tareas aperiódicas y las tareas asociadas a las aplicaciones de uso común. 
En consecuencia, el cuarto objetivo consiste en permitir la planificación conjunta de
tareas periódicas y aperiódicas. Gracias a la planificación de las tareas periódicas
flexible del tercer objetivo se podrá atender a eventos no periódicos. Así, el objetivo
no solo consiste en la planificación conjunta, sino que se desea obtener buenos
tiempos de respuesta para las tareas aperiódicas y unos altos índices de aceptación de
tareas esporádicas. De está forma, combinando la carga procedente de tareas
periódicas, aperiódicas y esporádicas, se propicia la máxima utilización del
multiprocesador, que es el primer objetivo. 
Para cumplir con los dos últimos objetivos, en el cuarto capítulo se diseña un nuevo 
método de planificación global del sistema multiprocesador que obtiene suficiente 
flexibilidad en las tareas periódicas para dar buen servicio a las aperiódicas pero que
no cumple con el requisito se los sistemas de tiempo real estrictos de garantizar todos
los plazos. 

Así, en el quinto capítulo se diseña un nuevo método de planificación híbrido entre la
planificación basada en monoprocesadores y la planificación global del
multiprocesador que sí logra cumplir con todos los objetivos propuestos. </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Esta tesis trata de la planificación de sistemas de tiempo real utilizando sistemas 
multiprocesador de memoria compartida. La principal motivación para iniciar la
investigación en la planificación de sistemas de tiempo real está fundamentada en el
plan estratégico del departamento en el que trabajo, el Departament d’Enginyeria
Informàtica i Matemàtiques de la Universitat Rovira i Virgili. Esta universidad está
ubicada en un entorno geográfico donde las instalaciones industriales con entornos
de tiempo real críticos abundan: refinerías de petróleo, industrias petroquímicas,
diversas centrales nucleares, etc. Así, las líneas del departamento estaban
principalmente enfocadas al “tiempo-real”, tanto en la parte docente como en la parte 
investigadora. 

En lo referente a la planificación de sistemas de tiempo real, la teoría para los
sistemas monoprocesador ya estaba muy consolidada. Para los sistemas distribuidos
había abundante bibliografía pero para los sistemas multiprocesador con memoria
compartida al principio apenas se encontraban referencias. Esto era debido a los
enormes problemas que conlleva y a una praxis que utilizaba el multiprocesador
como si fuesen monoprocesadores independientes o como si fuese un sistema
distribuido.  

Recientemente las arquitecturas multiprocesador se han ido popularizando. Su uso va
desde los ordenadores personales hasta grandes servidores y computadores, pasando
por los sistemas empotrados. Con los límites actuales del hardware, es más fácil y
con un menor coste incrementar la capacidad de cálculo con un multiprocesador que
realizarlo con sistemas de un procesador de equivalente capacidad. Así, han ido
apareciendo diversos chips Multi-Core o Chip-level Multiprocessors (CMP). Los
fabricantes de procesadores con mayor volumen, como son Intel y AMD, primero
lanzaron al mercado chips con dos procesadores (por ejemplo AMD Athlon X2 e Intel
Core Duo) y en la actualidad ya son frecuentes con cuatro procesadores (Quad-Core
AMD Opteron y Quad-Core Intel Xeon). En la Tabla 4 se resumen los procesadores
multicore actuales, donde se aprecia la gran evolución en los últimos años. </Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1.5 Marco y Metodología 

Para los experimentos de esta tesis se han utilizado simuladores desarrollados en
C++ por el autor. Para contrastar el correcto funcionamiento de la planificación
realizada, cuando fue posible se utilizó el simulador RTSIM3  de la Scuola Superiore
Sant'Anna [PLA01]. Aunque no se ha simulado ningún hardware específico, se 
asume que este tendrá la estructura de un sistema multiprocesador de memoria
compartida con procesadores homogéneos (véase la Figura 1.5). Los costes
derivados del propio hardware se consideraron incluidos en el cálculo del peor
tiem po de ejecución de cada una de las tareas (WCET, C). En la mayoría de
planificadores usados, el número de desalojos está acotado y, por lo tanto, los costes
también pueden incluirse en el WCET. Las tareas consideradas en esta tesis no
utilizan recursos compartidos, con lo que se simplifican los posibles problemas de
coherencia de cache. No obstante, en el Capítulo 5 se podrían incluir. En este caso el
acceso a los recurso s compartidos se tendría en cuenta en el análisis de la
planificabilidad mediante el factor de bloqueo [DaW95] y a nivel de planificador se
usarían mecanismos de exclusión mutua (Priority Ceiling Protocols, [SRL90]). Los 
i costes de las migraciones entre procesadores se consideraron nulos. En todo caso, los 
sistemas de tiempo real usualmente permiten las técnicas de replicación del código
de las tareas periódicas en las memorias caches para minimizar estos costes [Bus96].
No obstante, las modernas técnicas de las memorias caches en sistemas multicore,
como son la compartición de la memoria cache L2 o L3 (véase al columna Cache
Compartida en la Tabla 4), pueden ayudar a minimizar los costes de la migración
entre cores e incluso entre procesadores. 

La carga de los experimentos se generaró sintéticamente con diversos generadores
desarrollados por el autor. La entrada de estos generadores son los parámetros que
especifican la caracterización del conjunto de tareas y el número de procesadores (en
las tablas del Anexo B se especifican los principales parámetros de los diferentes 
tipos de conjuntos de tareas utilizados). La salida de los generadores son los
parámetros que especifican las características generales de un cierto número de tareas
(principalmente el periodo, el plazo y el peor tiempo de ejecución). Los simuladores
utilizan esta información para realizar la planificación en un modelo guiado por
eventos discretos. 

La parametrización utilizada se eligió a base de recopilar información de gran
diversidad de artículos, según nuestra propia experiencia y según marcaban algunos
benchmarks [KaW91]. Las principales características que influyen en la
planificación son: el period ratio, la multiplicidad de estos y el hiperperiodo. En la
Tabla 5 se muestra un resumen de estas características para los parámetros de los
conjuntos de tareas detallados en el Anexo B. El period ratio influye en la máxima
utilización del procesador alcanzable [LSD89]. Los conjuntos de tareas armónicos
aumentan la utilización del procesador [LiL73] y hacen que los eventos estén
sincronizados. Los hiperperiodos largos requieren simulaciones más largas y
requieren mayor cantidad de memoria para algunos planificadores [LRT92,RtL94]. 

En las tablas del Anexo B se especifican al menos los periodos mínimo y máximo y
el hiperperiodo. El plazo de las tareas se ha fijado igual a su periodo (D). El resto
de los periodos deberán estar dentro del rango [Tmin,T] y ser múltiples de los
factores en que se descompone el hiperperiodo, de forma que el mínimo común
múltiplo de todos los periodos será justamente el hiperperiodo. Por ejemplo, con
PCT # 3 se generan periodos utilizando los factores 24, 33max, 53 y 71. Con estos números
primos se pueden generar 76 periodos en el rango [100,3000].  

Los parámetros de los conjuntos de tareas se han elegido de manera que sean
similares entre ellos excepto en una de las características, para poder experimentar
los efectos de esta característica diferenciadora sobre el rendimiento. Los parámetros
PCT#6 son distintos al resto y se utilizaron para poder comparar con los resultados
de un artículo concreto ([AnJ00b]).  

El número de tareas periódicas normalmente se fijó entre 8 y 15 tareas por el número
de procesadores. El factor de utilización de una tarea (U) se genera aleatoriamente
siguiendo una distribución normal en el rango (0,Umaxi] pero se garantiza que al
menos una de ellas utilice U. El peor tiempo de cálculo de cada tarea (WCET oCimax)
se calcula como el producto del periodo por el factor de utilización de la tarea
Ci=TiU). En todas las simulaciones cada tarea periódica ejecuta siempre hasta su
WCET. Aunque los planificadores podrían utilizar el eventual tiempo sobrante de las
tareas periódicas, el objetivo de los experimentos era ver su comportamiento en los
casos límite. 

Las llegadas de las peticiones aperiódicas fueron generadas siguiendo una
distribución de Poisson. Normalmente los tiempos intermedios entre llegadas
elegidos estaban en el mismo rango que los periodos de las tareas periódicas. Sin
embargo, en algunos experimentos se explicita la utilización de tareas aperiódicas de
dimensiones fijas, más pequeñas y frecuentes, por ejemplo para una carga del 25% se
usó  Caper=1 y un tiempo entre llegadas de 4. 

Para un conjunto de tareas se realizaba una simulación en la que se tomaba un valor
de una métrica concreta. La simulación se repetía, variando la semilla inicial para la 
i =Ti generación aleatoria de las llegadas de tareas aperiódicas, un mínimo de 5 veces,
calculando la media aritmética de los resultados. Las simulaciones se repetían hasta
alcanzar un nivel de confianza del 95% utilizando la prueba estadística T-Student. En
el caso de no alcanzar el nivel de confianza en 35 simulaciones, se continuaba el
proceso pero utilizando la prueba de la Normal. Así, se obtenía un valor para la
métrica en un conjunto de tareas. El proceso se repetía para otros conjuntos de tareas. 
El número de conjuntos de tareas generados y simulados para obtener cada punto de
las gráficas de los experimentos fue variable entre 1000 y 100000, dependiendo de la
métrica utilizada y de los valores obtenidos. El número particular se estableció
buscando un margen de error del 1%.  Cuando la métrica era la media aritmética de
los tiempos de respuesta medios se utilizaba la prueba estadística de la Normal con
un nivel de confianza del 95%. Cuando la métrica era un porcentaje de éxito o un
porcentaje de aceptación, se utilizaba la distribución Binomial. 
La longitud de cada simulación se estableció entre 10 y 50 hiperperiodos,
dependiendo de la parametrización, para poder comprobar bien la planificación de
las tareas periódicas y para que el número de tareas aperiódicas finalizadas fuera
suficientemente grande.  

En cada uno de los experimentos se utiliza un PCT del Anexo B y se detalla que
parámetro se varió. Generalmente se realizaron experimentos variando uno de los
siguientes parámetros: 
• el número de procesadores (m)
• la carga periódica (U)
• el factor de carga máxima de las tareas periódicas (Uper)
• la carga aperiódica (U)
• el tiempo de ejecución requerido por las tareas aperiódicas (Caper)max aper</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>CONCLUSIONES 

En esta tesis se ha abordado el problema de la planificación de sistemas de tiempo
real utilizando una arquitectura de multiprocesador de memoria compartida. La
complejidad y las anomalías en la planificación de los sistemas multiprocesador
junto con el determinismo requerido por los sistemas de tiempo real estrictos derivan
en un problema realmente difícil de solucionar. Tradicionalmente se ha venido
simplificando la problemática tratando el multiprocesador como monoprocesadores
independientes, distribuyendo la carga de forma estática y planificándola de forma
local. Sin embargo, con la reciente proliferación de sistemas multiprocesador de bajo
coste, sistemas multicore y sistemas on-chip-mp, a lo largo de los últimos años la
planificación global del multiprocesador ha centrado la atención y los esfuerzos de
diversos investigadores. El objetivo general es el poder hacer un uso más exhaustivo
de la capacidad de proceso de este tipo de arquitecturas. La teoría de la
planificabilidad ha madurado pero todavía tiene algunas lagunas. Por un lado, el
grado de utilización para garantizar las tareas periódicas es bajo. Por otro lado, la
capacidad de proceso sobrante es difícilmente aprovechable para dar servicio a tareas
no periódicas. 

En numerosos experimentos de simulación hemos observado que si los recursos de
cómputo son abundantes entonces los planificadores que obtenían buenos
rendimientos con sistemas monoprocesador también son capaces de obtenerlos en
sistemas multiprocesador. Incluso pueden llegar a mejorar su rendimiento si las 
cargas son medio-bajas (digamos inferiores al 65%) o si disponen de una cantidad
considerable de procesadores (pongamos a partir de 8 procesadores). Concretamente,
para estos casos, en esta tesis se ha diseñado una planificación con servidores y se ha
abordado el problema de la distribución dinámica de tareas aperiódicas entre los
procesadores, tanto para planificadores con servidores como sin ellos. 
Cuando las cargas son altas y el número de procesadores es medio (entre dos y ocho)
entonces los planificadores más extendidos sufren un empeoramiento importante en 
su rendimiento. En estos casos se ha detectado la necesidad de diseñar nuevos
métodos basados en la planificación global, puesto que esta permite una mayor
flexibilidad. Sin embargo, se ha desestimado la planificación global con servidores
por la complejidad en su dimensionado. Así, hemos estudiado la posible adaptación
de otros tipos de algoritmos de planificación para monoprocesadores. Se han
considerado algoritmos que (i) proporcionen tiempos de respuesta de las tareas
aperiódicas (ii) su complejidad y sus costes fueran bajos. 

En esta tesis hemos propuesto dos algoritmos de planificación que no sufren de este
efecto: GDP y HDP. El primero no garantiza los plazos de las tareas periódicas y por
eso solo es aplicable a sistemas de tiempo real no estrictos. Sin embargo, al ser
global, proporciona una gran flexibilidad y un buen rendimiento para las tareas
aperiódicas. El segundo, HDP, sí que garantiza los plazos de las tareas periódicas y,
en consecuencia, es aplicable a sistemas de tiempo real estrictos. Además, se ha
diseñado un test de aceptación de tareas aperiódicas con plazo y los experimentos
muestran que puede conseguir unos porcentajes de aceptación altos. Además, el
servicio conjunto de tareas aperiódicas sin plazo es posible y el rendimiento para
estas es equiparable al obtenido con GDP. 

HDP consigue garantizar los plazos de las tareas periódicas ya que es un método de
planificación híbrido que basa sus garantías en la distribución de las tareas de forma
pseudo-estática. Así, en tiempo de ejecución el mecanismo de promoción de las
tareas las fuerza a ejecutarse en un procesador concreto. El precio a pagar por esta
garantía de plazos es una sobrecarga adicional motivada por las migraciones y
expulsiones. Así, se ha diseñado un mecanismo de renombrado de procesadores sin 
apenas coste alguno que reduce drásticamente el número de expulsiones y
migraciones  
En esta tesis también se han diseñado diversos métodos de distribución pre-runtime
de las tareas periódicas para el algoritmo HDP, ya que estas conducen a
distribuciones de las promociones diversas. Los experimentos han mostrado que el
tipo de distribución pre-runtime determina en gran medida el rendimiento. En 
general, hemos observado que las distribuciones balanceadas favorecen al servicio a 
tareas aperiódicas sin plazo mientras que las distribuciones que concentran la carga
favorecen la admisión de nuevas tareas periódicas o aperiódicas con plazo. Así, se ha
diseñado el algoritmo MaxPromo+LLFF que distribuiye balanceadamente la carga
buscando promociones tardías y, cuando no lo consigue, concentra la carga en
algunos procesadores, de forma que HDP puede dar servicio a los tres tipos de tareas. 
Ambos algoritmos de planificación, GDP y HDP, requieren de muy poca
información adicional. En concreto, necesitan saber cuando se producirán las
promociones de las tares periódicas. Las necesidades de proceso también son muy
bajas, puesto que los cálculos se realizan en tiempo de diseño. En consecuencia
escalan bien y se podrían utilizar en sistemas con un gran número de procesadores. 
En definitiva, se ha presentado un método de planificación que elude la problemática
de la utilización conjunta de sistemas de tiempo real en sistemas multiprocesador y
que cumple con los objetivos propuestos para esta tesis. 

En cuanto al trabajo futuro, estamos considerando varias posibles líneas. La primera
y más natural, consistiría en expandir la planificación global buscando el ahorro
energético cuando no hay tareas aperiódicas ni esporádicas que servir, ralentizando el
reloj de algunos procesadores. La segunda línea consistiría en incluir recursos
compartidos entre tareas, puesto que el cálculo de los peores tiempos de respuesta
permite añadir un factor de bloqueo. En este caso, el algoritmo de planificación en
tiempo de ejecución descrito en el Capítulo 5 no variaría pero se debería estudiar la
combinación de los métodos de distribución en tiempo de diseño de las tareas que
acceden a los recursos compartidos con las técnicas de distribución propuestas en 
esta tesis. Una tercera línea interesante debida al auge de los sistemas multicore, sería
el optimizar la planificación inter-chip, entre los diversos cores dentro de un chip, y
intra-chip, utilizando una planificación jerárquica. Finalmente, también estamos
considerando el extrapolar estos métodos de planificación a entornos no de tiempo
real, ni con tareas periódicas, pero si con tareas que tengan patrones repetitivos, que
se puedan retrasar permitiendo la ejecución más temprana de otras tareas.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSITAT POLITÈCNICA DE CATALUNYA 
PROGRAMA D’ARQUITECTURA I TECNOLOGIA DE COMPUTADORS 

PLANIFICACIÓN GLOBAL EN SISTEMAS 
MULTIPROCESADOR DE TIEMPO REAL 
 
Tesis propuesta para el doctorado en informática 
 
DOCTORANDO: Josep Maria Banús i Alsina 
CO-DIRECTORES: Dr. Jesús José Labarta Mancho 
               Dr. Alejandro Arenas Moreno 
  
 Diciembre de 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Un sistema interactivo para la búsqueda de información en idiomas desconocidos por el usuario"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El auge de Internet en la llamada Sociedad de la Información, supone la disponibilidad
de cantidades prácticamente ilimitadas de información accesible, principalmente, a
través de la World Wide Web. Para que toda esa información sea realmente accesible
y útil, los motores de búsqueda o sistemas de Recuperación de Información juegan un
papel fundamental.

Tradicionalmente, la Recuperación de Información se ha entendido como el proceso,
totalmente automático, en el que, dada una consulta (expresando las necesidades
de información del usuario) y una colección de documentos, se devuelve una lista
ordenada de documentos supuestamente relevantes para la consulta. Un motor de
búsqueda ideal recuperaría todos los documentos relevantes (lo que implica una cobertura
completa) y sólo aquellos documentos que son relevantes (precisión perfecta).
Este modelo tradicional lleva consigo muchas restricciones implícitas; entre ellas, la
suposición de que la consulta y el documento están escritos en el mismo idioma.
La mayoría de los motores de búsqueda en Internet, de hecho, tienen la limitación
de encontrar documentos sólo en el idioma en que se escribe la consulta. Algunos
incorporan sistemas de traducción automática, que sólo resultan útiles cuando los
documentos ya han sido localizados, pero no facilitan un medio efectivo para salvar
la barrera del idioma en el proceso de búsqueda.

Por este motivo, la información a la que facilitan el acceso estos motores de búsqueda
queda limitada a la escrita los idiomas en los que el usuario sea capaz de expresar sus
consultas. Esto puede suponer un problema más o menos grave según el idioma del que
se trate (ver figura 1.1) pero, en general, cualquier usuario de Internet que no pueda
formular consultas en inglés con fluidez tendrá dificultades a menudo para realizar sus
búsquedas. Incluso para hablantes nativos de inglés, el volumen de datos inaccesible
por causa de las barreras idiomáticas crece cada a~no, si no porcentualmente, sí en
términos absolutos.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Hipótesis planteadas

Se plantean las siguientes hipótesis preliminares:
* La utilización de software de traducción automática produce traducciones muy
densas y ruidosas: la cantidad de información que le llega al usuario y los errores
en la traducción complican la tarea de decidir acerca de la relevancia de un
documento. Esa decisión debe ser posible con traducciones más simplificadas,
o resumidas, y con menor coste computacional que una traducción automática
completa.

* La traducción palabra por palabra de las consultas es una tarea difícil, lenta
  e incómoda para un usuario. Es interesante estudiar alternativas con menos
  carga cognitiva y quizás más eficientes.

La motivación fundamental de este trabajo es verificar experimentalmente estas hipótesis
y encontrar mecanismos de interacción con el usuario que resuelvan los problemas
planteados de forma óptima. Para ello planteamos un modelo de interacción en el
que el usuario monolingue trabajará con sintagmas en su propio idioma.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Evidencias que apoyan la propuesta

* Para la indexación se ha estudiado que las unidades óptimas son las palabras,
mientras que para la traducción es mejor emplear unidades más grandes (Verdejo
et al., 2000), ya que el uso de palabras individuales para traducir no resuelve
el problema de la ambiguedad semántica.
* La estadística sobre coocurrencia de términos permite realizar traducciones más
fiables (Ballesteros and Croft, 1998) que la simple utilización de diccionarios.
Aplicarla sobre sintagmas nominales va a mantener esta característica (recordemos
que un sintagma nominal se compone de varios términos).
* La utilización de sintagmas nominales, con un contenido semántico más claro
para el usuario, proporcionará una mejor información a efectos de interacción
que el simple uso estadístico de la coocurrencia de términos próximos que se
hace en (Ballesteros and Croft, 1998).
* Diversos estudios como (Pe~nas et al., 2001; Dennis et al., 2002) muestran que
los sintagmas son una forma natural para los usuarios de refinar una consulta.
Representan conceptos de una forma más clara y con un mayor contenido
semántico que los términos aislados.
* Además, la determinación de los sintagmas y su utilización como índices puede
hacerse de manera eficiente (Pe~nas, 2002).</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>4.2 Metodología de trabajo

Simultáneamente al desarrollo de este trabajo de tesis se puso en marcha una evaluación
conjunta de sistemas interactivos de recuperación translingue de información
dentro del marco del CLEF. El calendario de nuestra investigación se ajustó al de la
competición, para aprovechar las ventajas de disponer de una metodología consensuada
por la comunidad científica con la que realizar la evaluación comparada de los
aspectos esenciales del sistema en comparación con otros enfoques.
La metodología de trabajo empleada fue una metodología de prototipado: se construyeron
una serie de prototipos con los que comprobar experimentalmente las hipótesis
iniciales, así como la utilidad que tienen los sintagmas nominales como forma de
resolver los problemas comentados.

Los pasos que se han seguido para realizar esta investigación han sido los siguientes:

1. Desarrollar un algoritmo de alineación de sintagmas nominales entre dos idiomas
que utilice sólo la siguiente información lingüística:

    * Un sistema de extracción de sintagmas nominales.
    * Un diccionario bilingue.
    * Corpus escrito en ambos idiomas (preferiblemente comparable).

Los requisitos iniciales para este algoritmo son: que tenga la mínima dependencia
del par de idiomas considerados, por un lado, y que pueda aplicarse a
cantidades realistas de texto (en el orden del GB), por otro.

2. Desarrollar un algoritmo de creación y traducción de resúmenes basado en los
resultados producidos por el algoritmo de alineación, y evaluarlo en la tarea de
selección interactiva de documentos en un idioma desconocido por el
usuario, comparándolo con un sistema de traducción automática profesional.

3. Desarrollar un sistema interactivo de formulación y refinamiento de la consulta
basado, igualmente, en sintagmas nominales alineados. Evaluar este sistema en
un entorno de búsqueda translingue de información, comparándolo con enfoques
conocidos.

4. Partiendo de los resultados experimentales obtenidos, diseñar e implementar
un sistema completo de asistencia a la búsqueda de información en idiomas
desconocidos.

El resto de esta memoria se estructura como sigue:

En el capítulo 5 se describe el desarrollo del algoritmo de alineación de sintagmas, así
como un análisis de la calidad de las traducciones producidas por el mismo.
En el capítulo 6 se presenta el algoritmo de construcción y traducción de resúmenes,
así como los resultados de la evaluación del mismo dentro de la tarea de selección
documental translingue del iCLEF'2001.

En el capítulo 7 se describe el sistema de formulación y expansión de la consulta,
evaluado en el marco del iCLEF'2002, así como los resultados de su evaluación.
En el capítulo 8 se describe el sistema NOODLE, que es un asistente para la búsqueda
de información en idiomas desconocidos. NOODLE basa su funcionamiento en los
dos prototipos estudiados en los capítulos 6 y 7 introduciendo algunas variantes sobre
éstos para trabajar sobre consultas más realistas que las consultas CLEF utilizadas en
las evaluaciones, que son más extensas de lo normal.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Conclusiones y trabajo futuro

En esta memoria se ha presentado el diseño, implementación y evaluación de un sistema
interactivo de recuperación de información en idiomas desconocidos basado en
sintagmas nominales, que aporta soluciones originales y efectivas para todos los aspectos
esenciales de la tarea: selección documental translingue, formulación, traducción
y refinamiento de la consulta. Las aportaciones principales del sistema son:
*  Un sistema de generación de pseudo-resúmenes indicativos translingües basado
en la traducción de los sintagmas nominales del texto. Estos resúmenes son
una alternativa a la traducción automática a la hora de facilitar la selección de
documentos por parte del usuario, y tienen varias ventajas principales:
*  Permiten juicios de relevancia más rápidos al ofrecer información condensada.
*  Se generan mucho más rápidamente y ocupan mucho menos espacio que
una traducción automática completa, lo que favorece tanto un procesado
o-line (menos tamaño) como on-line (respuesta más rápida).
*  A pesar de proporcionar información resumida, guardan la información
sobre todos los sintagmas nominales maximales del documento, por lo que
podrían ser utilizados como método para realizar búsquedas translingues
basadas en traducción de documentos.
*  Permiten diseñar estrategias de refinamiento de la consulta basadas en
sintagmas relevantes, como de hecho se hace en nuestro sistema.
*  Un sistema de formulación de la consulta en el que la interacción se realiza
en el idioma del usuario, ayudándole a expresar su necesidad de información
en términos de sintagmas nominales que expresen conceptos relevantes. Al
contrario que en los sistemas de traducción asistida de la consulta, en nuestro
enfoque la traducción de los sintagmas nominales se hace automáticamente,
utilizando las restricciones de coocurrencia para encontrar traducciones de los
términos que los componen. Como los documentos se examinan a través de sus
sintagmas nominales traducidos, el refinamiento de la consulta se produce de
forma natural sin más que seleccionarlos del contenido de los documentos.
Los aspectos individuales del sistema han sido evaluados en el contexto de las experiencias
de evaluación iCLEF'2001 y 2002, que constituyen el mayor estudio comparativo
realizado hasta la fecha sobre el papel de los usuarios en el acceso multilingue
a la información. En conjunto, en este estudio han participado cinco grupos de investigación,
con 78 evaluadores que han realizado un total de 302 sesiones de búsqueda
translingue de información, involucrando cinco idiomas diferentes: inglés, castellano
y sueco como idiomas de los usuarios; y alemán y francés (junto con el inglés) como
idiomas de los documentos.

Los resúmenes basados en sintagmas, como estrategia de traducción de los documentos,
fueron evaluados en el marco del iCLEF'2001. Nuestra aproximación utilizando
sintagmas nominales para construir un pseudo-resumen translingue indicativo del contenido
de los documentos obtuvo un 25% de mejora en la medida oficial del iCLEF
frente a las traducciones proporcionadas por un sistema profesional de traducción
automática (Systran Professional 3:0). Los usuarios realizaron juicios de relevancia
con una precisión similar para ambos enfoques, pero eran capaces de juzgar sobre los
resúmenes con más rapidez, lo que finalmente se reejaba en una mayor cobertura
para sesiones con un tiempo fijo de búsqueda. Los cuestionarios cumplimentados
por los evaluadores confirmaron que, en general, los resúmenes ofrecían menos información
pero resultaban más ágiles para tomar decisiones sobre la relevancia de los
documentos.

Nuestra estrategia contrasta con el experimento de la Universidad de Maryland, también
dentro del iCLEF'2001, en el que se comparaban las traducciones de Systran
con traducciones palabra por palabra.

Estas últimas daban un rendimiento casi un
50% inferior al Systran en la tarea de selección documental. En conjunto, los resultados
del iCLEF'2001 sugieren que las estrategias basadas en resumir los documentos
manteniendo una adecuada selección léxica, ejemplificadas en nuestro sistema, son las
más prometedoras para la selección documental translingue. En efecto, aunque sólo
ha transcurrido un año desde la publicación de estos resultados, ya existe al menos
un proyecto (Language and Media Processing Laboratory), financiado por el Departamento
de Defensa norteamericano y dirigido por la Dra. Amy Weinberg, en el que se
utiliza nuestra aproximación como sistema de referencia para desarrollar algoritmos
de traducción de entidades nombradas y sintagmas nominales.

La formulación y refinamiento interactivos de la consulta basados en selección monolingue
de sintagmas nominales, fue evaluada en el marco del iCLEF'2002, comparándose
con un asistente para la selección de las traducciones de los términos individuales
de la consulta. En la medida oficial del iCLEF, nuestra aproximación obtuvo
un 64% de mejora sobre la traducción asistida. Los datos mostraron, entre otras
cosas, que los evaluadores formulaban la consulta con más rapidez y mayor acierto,
e interaccionaban más (a través de refinamientos sucesivos) con el sistema. Si bien
es cierto que el sistema de traducción asistida no era óptimo (por ejemplo, el sistema
de la Universidad de Maryland en esta misma experiencia disponía de mejores
recursos y ofrecía, por tanto, un soporte más sofisticado a la selección de términos de
traducción), la evidencia adicional proporcionada por los cuestionarios y el estudio
observacional de las sesiones de búsqueda, indica de forma nítida que los usuarios sin
conocimiento de la otra lengua prefieren interaccionar sólo a nivel monolingue, y que
el sistema de sintagmas resulta una forma natural y efectiva de hacerlo.
A partir de los resultados cuantitativos y cualitativos obtenidos de los datos, cuestionarios
y estudios observacionales de las experiencias iCLEF, hemos diseñado un
sistema final, NOODLE, que recoge todos los principios de diseño de los sistemas
experimentales (que resultaron satisfactorios) y que mejora aspectos puntuales, como:

1. la expansión inicial de los sintagmas potencialmente relacionados, de forma que
pueda funcionar satisfactoriamente para consultas mucho más cortas que las
usadas en el CLEF.
2. la posibilidad de añadir términos individuales al proceso de formación de sintagmas,
como alternativa a la realimentación cuando no se encuentran documentos
relevantes.
Se puede acceder a una versión de demostración del sistema que permite realizar
búsquedas sobre las colecciones CLEF en http://terral.lsi.uned.es/noodle

Aparte de los aspectos interactivos de la búsqueda de información translingue, la
aportación principal de esta tesis al terreno de la Ingeniería Linguística es el sistema
de traducción de sintagmas nominales, que se basa en un conjunto mínimo de recursos
linguísticos (un diccionario bilingue, un lematizador y corpora comparable) para construir
una base de datos a gran escala con alineaciones entre conjuntos equivalentes
de sintagmas nominales en los dos idiomas contemplados. El sistema de traducción
se basa en esas alineaciones, sustituyendo iterativamente en cada sintagma completo
los subsintagmas maximales para los que existe una traducción alineada.

A través de su evaluación indirecta en las experiencias iCLEF, hemos demostrado
que, a partir de un recurso relativamente abundante como los corpora comparables,
es posible realizar traducciones parciales efectivas de forma eficiente con técnicas
extremadamente sencillas. Hasta ahora, la mayoría de los trabajos en traducción
automática no basada en conocimiento, sino en el estudio estadístico de corpora, se
basan en corpora paralelo, recursos que sólo se encuentran para idiomas y dominios
muy determinados. Nuestros resultados sugieren que se debe prestar más atención a
las técnicas que trabajan sobre corpora comparable.

En conjunto, nuestros resultados cuestionan dos suposiciones implícitas en buena parte
de la investigación en Recuperación de Información Multilingue: la primera, que
una vez encontrados los documentos en el idioma destino, la traducción automática
es la forma óptima de informar al usuario sobre su contenido para descartar o seleccionar
un documento; y la segunda, que en un entorno interactivo la forma óptima de
formular y traducir la consulta es ayudando al usuario a seleccionar las traducciones
adecuadas para cada término de la consulta.

Por supuesto, nuestros resultados han sido obtenidos en el marco de una de las primeras
evaluaciones sistemáticas de los aspectos interactivos de este tipo de búsquedas.
Como en el caso de toda metodología de evaluación novedosa, hay muchos matices
que añadir a los resultados cuantitativos que hemos obtenido:

1. A diferencia de lo que ocurre con las colecciones de evaluación estándar en
Recuperación de Información, los experimentos que involucran usuarios reales
no son reproducibles. No están claras las variaciones que sufrirían los resultados
de los experimentos utilizando consultas y usuarios distintos, ni cómo debe
valorarse la relevancia estadística de los resultados sobre un conjunto de datos
necesariamente reducido.
2. El uso de sólo cuatro consultas para las evaluaciones no permite garantizar la
utilidad del sistema de modo general. Como en otras experiencias de recuperación
de información, hemos podido comprobar que la in
uencia de la consulta
es decisiva en el comportamiento de los sistemas. Para aumentar la fiabilidad de
los resultados debería aumentarse sensiblemente el número de consultas, aunque
el coste de evaluación con usuarios se vuelve rápidamente prohibitivo.
3. Las diferencias observadas entre los dos sistemas de formulación y refinamiento
de la consulta en el experimento llevado a cabo en el iCLEF'2002 serían probablemente
menores empleando recursos linguísticos más ricos en el sistema de
traducción asistida de la consulta.
4. Aunque nuestro enfoque no ha tenido en consideración las características específicas
del dominio sobre el cual se han realizado los experimentos (las noticias
de prensa), no está claro cuales serían los resultados si se las búsquedas se
realizasen sobre un dominio diferente o con usuarios especializados. Como se
comentó en el capítulo 2, éste es un problema generalizado de la investigación en
este campo, que se ha realizado mayoritariamente sobre colecciones de noticias.

Futuras líneas de investigación

Los tres aspectos esenciales en los que se desarrollará nuestro trabajo son: la mejora
de los recursos y algoritmos básicos de alineación, la aplicación de nuestros resultados
a otros aspectos del tratamiento de información multilingue, y la mejora de la
metodología de evaluación. En concreto, algunos temas de interés son:

1. Mejoras en el sistema de traducción e interacción.

      (a) Generar una nueva versión del diccionario bilingue de palabras que subsa-
      ne los errores detectados en la versión utilizada en los experimentos. En
      este sentido se están realizando experimentos preliminares acerca del uso
      del algoritmo de alineación como un medio de obtener parejas de palabras
      candidatas a ser traducción la una de la otra, a partir de sintagmas semi
      alineados que contengan palabras desconocidas (es decir, ausentes de nuestros
      diccionarios). Aplicando un algoritmo de detección de cognados sobre
      estos datos sería posible ampliar el diccionario bilingue de palabras, aumentando
      así la cobertura de nuestro sistema y de cualquier otro enfoque
      de traducción de consultas o documentos.

      (b) Mejorar la eficiencia del algoritmo de alineación (y, sobre todo, de su implementación)
      de forma que el procesado de una colección del tamaño del
      CLEF sea abordable en un período de tiempo razonable. En estos momentos,
      nuestro sistema es eficiente una vez que se dispone del diccionario
      bilingue de sintagmas, pero la generación de este diccionario es todavía
      ineficiente. El análisis de cerca de 21 millones de sintagmas en castellano
      e inglés supuso dos meses de procesamiento en una estación de trabajo
      Sun Sparc con dos procesadores; según nuestros cálculos, la eliminación
      de cálculos redundantes y otras optimizaciones podrían permitir que ese
      mismo análisis se llevara a cabo aproximadamente en tres días.
      Una vez que se tenga una versión más eficiente del algoritmo se pretende
      comprobar su comportamiento entre pares de idiomas distintos al castellano
      e inglés. Por ejemplo, disponiendo de lematizadores para idiomas
      aglutinativos se podría aplicar entre sintagmas y compuestos léxicos, aumentando
      el rango de idiomas accesible al sistema.

      (c) Probar nuevos criterios para mejorar la alineación. En particular:

            i. Detección de nombres propios: algunos errores en la alineación eran
            debidos a la traducción de nombres propios como \bill " por \anuncio".
            Una primera aproximación consistiría en considerar las palabras que
            comiencen por mayúscula, mientras que una alternativa más elaborada
            involucraría un proceso de reconocimiento de entidades que sería más
            costoso. A la vista de los resultados, creemos que una técnica sencilla
            que distinga términos con mayúscula y que compruebe la presencia el
            el corpus destino del sintagma inalterado puede resolver la mayoría de
            los errores detectados en la evaluación manual.

            ii. Mejora en los criterios de ordenación de los conjuntos de sintagmas alineados:
            en sintagmas como \woodland hills " que se alinea con \colinas
            y montes" y con \woodland hills " (ambas con la misma frecuencia de
            aparición en el corpus en castellano), se ve que es necesario reforzar el
            criterio de utilizar simplemente la frecuencia como método de ordenación,
            ya que el algoritmo devuelve el sintagma incorrecto como mejor
            alineación.

      (d) Mejorar la legibilidad de las traducciones de los sintagmas maximales: por
      ejemplo para el sintagma \advances in treatment of a wide variety of diseases",
      el algoritmo de traducción devuelve \avances en el tratamiento
      amplio tipo de enfermedades". Se puede ver que hay dos partículas
      de unión \of " y \a" que no han sido consideradas para la traducción,
      pero que si se tradujesen producirían una traducción perfecta del sintagma
      maximal completo:\avances en el tratamiento de
      un amplio tipo de enfermedades". La comprobación de concordancia morfológica también
      puede ser una forma de mejorar la legibilidad sin un excesivo coste
      computacional asociado.

      (e) Utilizar las relaciones que establece EuroWordnet entre distintas categorías
      gramaticales para enriquecer las consultas. De esta forma sería posible
      establecer una relación entre la consulta \genes y enfermedades" y el sintagma
      \enfermedades genéticas", incrementando, posiblemente, tanto la
      precisión como la cobertura de las búsquedas.

Sin embargo la cobertura de estas relaciones es baja y, además, dependiente
del idioma. Una aproximación alternativa sería realizar un proceso de
stemming sobre el diccionario bilingue de palabras, aunque este tipo de
técnica suele ser demasiado agresiva y generar demasiado ruido.

2. Estudiar el problema de la búsqueda translingüe para consultas específicas, en
las que un único documento relevante puede satisfacer la necesidad de información.
Este escenario de búsqueda, tan realista como el de las consultas amplias
y más cercano al concepto de Question&amp;Answer (sistemas de pregunta y respuesta),
permitirá reducir drásticamente el tiempo de cada búsqueda en las
evaluaciones con usuarios, por lo que podrá utilizarse un mayor número de consultas.
En este sentido, por ejemplo, los procesos de realimentación de nuestro
sistema deben ser adaptados para proporcionar al usuario una forma de realimentar
al sistema sin disponer de un documento relevante del cual obtener
posibles sintagmas. Está previsto realizar este experimento en el marco del
iCLEF'2003, que se dedicará a consultas específicas.

3. Contrastar el enfoque de nuestro sistema, basado en la traducción de las consultas
para realizar búsquedas interactivas monolingues en el idioma de los documentos,
con un enfoque en el que los términos de la consulta no se traducen,
sino que se utilizan para realizar una búsqueda monolingüe en el espacio de
traducciones de los documentos de la colección. De acuerdo con nuestras experiencias
previas, pensamos que esta estrategia puede dar buenos resultados,
y sería una forma de estimular la investigación en traducción automática especializada
en tareas de búsqueda. Creemos que, hasta ahora, se ha puesto
demasiado énfasis en la traducción de la consulta y muy poco esfuerzo en la
traducción de documentos.

4. Comprobar la utilidad de nuestros pseudo-resúmenes como material de indexación
para un enfoque de búsqueda translingüe clásico (no interactivo) basado
en traducción de documentos.
La investigación en los aspectos interactivos de la recuperación multilingüe de información
está todavía en sus inicios. En este sentido, creemos que nuestro trabajo es
una aportación prometedora, que debe consolidarse mejorando sus técnicas básicas,
consolidando el diseño experimental de las evaluaciones y contrastándola con un rango
más amplio de hipótesis. En conjunto, esperamos que sirva de estímulo para incorporar
el papel de los usuarios a los retos que plantea la búsqueda multilingüe, un tema
que seguramente tomará especial relevancia en un futuro inmediato.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad Nacional de Educación a Distancia
Escuela Técnica Superior de Ingenieros Industriales
Departamento de Lenguajes y Sistemas Informáticos

UN SISTEMA INTERACTIVO PARA LA BÚSQUEDA DE 
INFORMACIÓN EN IDIOMAS DESCONOCIDOS POR EL
USUARIO

TESIS DOCTORAL

Fernando López Ostenero
Licenciado en Cc. Matemáticas por la Universidad Complutense de Madrid

Directores:
Julio Antonio Gonzalo Arroyo
Profesor Titular de Universidad del Departamento de Lenguajes y Sistemas Informáticos
de la Universidad Nacional de Educación a Distancia

María Felisa Verdejo Maíllo
Catedrática de Universidad del Departamento de Lenguajes y Sistemas Informáticos 
de la Universidad Nacional de Educación a Distancia

2002</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"APRENDIZAJE DE PARTICIONES"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Planteamiento 

La Teoría de Conjuntos Difusos y la Lógica Difusa [KY95, Zad65, Zim96,
DP97] son una gran herramienta para manejar situaciones de incertidumbre
inherentes a sistemas complejos. Cualquier sistema dinámico o estático que hace uso
de estas herramientas, se considera un Sistema Difuso [DP97]. 

Los Sistemas Difusos han demostrado su habilidad para resolver diferentes 
tipos de problemas tales como control [Zha95, DHR93, SYZL05, SGB05, PRG+04],
modelado [Ped96, SY93, TS85, Jin00, HD97a] o clasificación [MB05, Kun00,
CYP96, Leo99, Jin03]. Asimismo, han sido empleados en un amplio rango de
aplicaciones, por ejemplo, procesamiento de señales e imágenes [CYP96, DP97,
ST99, SISM01], evaluación de riesgos [LH04, Leo99, Jin03],  recuperación de
información [CHL01, Miy89], procesos industriales [Jin03, Leo99, HS95, BBC+95,
DO01], etc. En muchos de los casos, la clave del éxito era la incorporación del
conocimiento experto humano. Sin embargo, muchas de las investigaciones
realizadas en el campo durante los años 90 no contienen capacidad de aprendizaje y
adaptación. En la última década, ha existido un gran interés en incluir capacidad de
aprendizaje a los sistemas difusos. Esto se ha logrado por medio del desarrollo de
técnicas híbridas que incluyen los sistemas difusos junto con técnicas
complementarias como lo son las redes neuronales, los algoritmos evolutivos  o  los
métodos probabilísticos. 

Existe consenso entre los investigadores en que se pueden obtener sistemas más 
inteligentes por medio de la hibridación de metodologías de Soft Computing
Ova04], haciendo de este modo que las debilidades de unos sistemas se compensen
con las bondades de otros.  Los  Sistemas Neurodifusos (SNDs) y los Sistemas
Difusos Evolutivos (SDEs) constituyen la más notoria representatividad de sistemas
híbridos dentro de Soft Computing. Estos sistemas mezclan los métodos de
razonamiento aproximado de los sistemas difusos con las capacidades de aprendizaje
de las Redes Neuronales (RNs) y los Algoritmos Evolutivos (AEs), respectivamente.
Hay numerosas investigaciones en ambos temas pero los más usados han sido los
SNDs [JSM97, NKK97, Ful99]. Sin embargo, se han obtenido resultados muy 
prometedores en los últimos años para los SDEs [CHL96, CHL97a, CHL97b, HV96, 
Ped97, SSZ97, Tet95, CHHM01, CPC04, CCdJH05, ACP06].  

Un Sistema Difuso Evolutivo [CH95, CHHM01, HM97] es básicamente un 
sistema difuso robustecido por un proceso de aprendizaje basado en un AE [Bäc96],
en particular los Algoritmos Genéticos (AGs) [Gol89, Hol75, Mic96], los cuales están
considerados actualmente como la técnica de búsqueda global más conocida y
empleada. Este tipo de algoritmos presentan la capacidad de explorar y explotar
espacios de búsqueda complejos, lo que les permite obtener soluciones muy próximas
a la óptima en dichos espacios. Además, la codificación genética que emplean les
permite incorporar conocimiento a priori de una forma muy sencilla y aprovecharlo
para guiar la búsqueda. 

En esta memoria, se proponen SDEs que mejoran la técnica de modelado y 
simulación denominada Razonamiento Inductivo Difuso (FIR^2). Se persigue
aprovechar las potencialidades de los AGs para aprender los parámetros de
discretización de la metodología FIR, es decir, el número de clases por variable
(granularidad) y las funciones de pertenencia (landmarks) que definen su semántica.
Debido al hecho que es una metodología basada en lógica difusa, la eficiencia en el
modelado y predicción de FIR está influenciada de forma directa por estos
parámetros de discretización.  

FIR surge o emerge del enfoque del Resolvedor de Problemas de Sistemas
Generales (GSPS^3) propuesto por Klir [Kli85] y es una herramienta para el análisis
de sistemas generales que permite estudiar los modos de comportamiento de los
sistemas dinámicos. Realiza dos tareas principales, la primera es identificar las
relaciones causales entre las variables del sistema y construir el modelo cualitativo
del sistema observado; y la segunda es predecir el comportamiento futuro del sistema
a partir de las observaciones pasadas y del modelo previamente identificado. Para
cumplir con estas tareas, la metodología  FIR  cuenta con cuatro funciones básicas
llamadas: fusificación, modelado cualitativo, simulación cualitativa y defusificación. 
El proceso de fusificación se encarga de convertir los valores cuantitativos del
sistema (datos sin procesar) en sus valores cualitativos equivalentes (datos
discretizados). En el proceso de codificación difusa un valor cuantitativo se convierte
en una tripleta cualitativa, donde el primer elemento de la tripleta es el valor de la
clase, el segundo elemento el valor de pertenencia difusa y el último elemento
corresponde al valor de lado. El valor de lado permite conservar en la tripleta
cualitativa el conocimiento completo del valor cuantitativo original, determinando
con mayor precisión donde se encuentra el valor cualitativo, esto es, a la derecha, al 
centro o a la izquierda del máximo de la función de pertenencia. De esta forma no se
pierde información durante el proceso de fusificación.   

El proceso de modelado cualitativo es capaz de obtener las relaciones 
cualitativas entre las variables que componen el sistema, construyendo la base de
reglas patrón que guiará el proceso de simulación cualitativa. 

El proceso de simulación cualitativa realiza la predicción del comportamiento 
del sistema. El motor de inferencia de FIR es una  especialización  del método  de los  
k-vecinos más cercanos, comúnmente usado en el campo de reconocimiento de
patrones [Das91, DHS01, MMV03, SDI05]. La defusificación es simplemente el
proceso inverso al de fusificación.   
 
En los últimos años la metodología FIR ha sido aplicada a diferentes tipos de
problemas (control, biomedicina, ecología, etc.), obteniendo generalmente buenos
resultados [NCL96, MC94, NMG01, etc.]. Cuando se tiene acceso a la opinión de
expertos del problema en estudio es recomendable que los parámetros de
discretización sean determinados directamente por ellos.  Sin embargo, éste no es
frecuentemente el caso. En la mayoría de las ocasiones se dispone de un conjunto de
datos del sistema a analizar y modelar, sin mayor información adicional. Es entonces,
el investigador quien asume la tarea de establecer los parámetros del proceso de
discretización. Existen trabajos en los que se prueba que estos parámetros tienen una 
influencia determinante en el comportamiento de los Sistemas Basados en Reglas
Difusas (SBRDs) [CHV00, Vil00]. Por lo tanto, una discretización inadecuada
conlleva muy frecuentemente a la obtención de un mal  modelo del sistema, el cual
no representa adecuadamente la dinámica del problema estudiado. Sin embargo,
cuando no se dispone de información adicional ni del acceso a expertos, una opción
ampliamente usada, es definir valores por defecto para cada uno de los parámetros, de
manera que siempre se usen los mismos valores de estos parámetros para llevar a
cabo la discretización.  

En FIR el valor por defecto del parámetro número de clases para cada variable 
del sistema es tres y se emplea el método EFP^4 para obtener las funciones de
pertenencia. Este método distribuye uniformemente las clases (etiquetas o términos
linguísticos) a lo largo del universo de discurso de la variable.  

Pese a que los resultados obtenidos por FIR en diferentes aplicaciones usando 
valores por defecto de estos parámetros fueron buenos (desde el punto de vista de la
precisión en la capacidad de predicción de los modelos obtenidos) y en muchas
ocasiones mejores que los logrados por otras metodologías, la experiencia ha 
demostrado que en algunas de estas aplicaciones, especialmente las referidas como 
soft sciences,  como son las biomédicas y las ecológicas, la determinación de los
parámetros necesarios en el paso de discretización es relevante para la identificación
de un buen modelo que capture el comportamiento del sistema de manera precisa. En
este sentido, en muchas de las aplicaciones estudiadas fue necesario seleccionar
modelos FIR subóptimos con un poder de predicción mayor al obtenido por el
modelo óptimo e incluso se comprobó como pequeños cambios en los valores de los
landmarks producían consecuencias importantes en la capacidad de predicción de los
modelos identificados, tanto óptimos como subóptimos. Por todo ello, la
determinación automática de parámetros adecuados de discretización en la
metodología FIR surge como una alternativa de gran interés y  utilidad al uso de
valores heurísticos y/o por defecto. Más aún, automatizar la selección de los valores
adecuados para estos parámetros permite el uso de la metodología FIR a usuarios no 
expertos en modelado de sistemas ni en lógica difusa garantizándoles el mejor 
rendimiento de esta metodología. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivos de la tesis 

El objetivo principal de esta tesis doctoral es desarrollar métodos evolutivos de
aprendizaje automático de particiones difusas para la metodología FIR.  

Para llevar a cabo este objetivo general se desprenden los siguientes objetivos 
específicos: 

            1. Analizar  los diferentes métodos de aprendizaje de particiones difusas, con
            el fin de profundizar en el tema de estudio. 

            2. Diseñar e implementar en el entorno de FIR un método de aprendizaje
            automático de particiones difusas usando algoritmos genéticos, considerando
            para ello: a) El aprendizaje de la granularidad con las funciones de pertenencia
            uniformes, b) El ajuste de las funciones de pertenencia con un número fijo de
            clases,  y c) El aprendizaje conjunto de la granularidad y  de las funciones de
            pertenencia que definen su semántica. 

            3. Probar los métodos de aprendizaje de particiones difusas propuestos sobre
            un conjunto de benchmarks, que permitan analizar los resultados obtenidos en
            función de los resultados anteriores y en comparación con los resultados
            obtenidos con otras metodologías para los mismos conjuntos de prueba. 

            4. Aplicar la herramienta obtenida a un caso real complejo: Predicción de
            concentraciones de ozono en la ciudad de México, a objeto de estudiar el 
            comportamiento de los métodos de aprendizaje desarrollados en esta memoria y
            evaluar su eficacia a través de la comparación de los resultados obtenidos con
            estudios previos realizados con el mismo problema.  </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Metodología aplicada 

Para cumplir con los objetivos planteados se procedió a la elaboración de un
plan de trabajo. Esta tesis doctoral se llevó a cabo en (9) etapas: 

            1.- Revisión bibliográfica y documental: En esta etapa se realizó una extensa
            búsqueda bibliográfica relacionada con métodos que permitían realizar particiones
            difusas y sus algoritmos de aprendizaje, con el fin de establecer un marco conceptual
            y observar las aportaciones realizadas sobre el tema en estudio.  

            2.- Análisis de las diferentes técnicas de aprendizaje de particiones difusas: Esta
            actividad contempló un estudio de diferentes técnicas de aprendizaje de particiones
            difusas con el fin discernir la aplicabilidad y funcionalidad de cada una de ellas. 

            3.- Elaboración de una propuesta: En esta etapa se determinaron las posibilidades de
            aplicación de los algoritmos genéticos al aprendizaje automático de particiones
            difusas. Asimismo, implicó el diseño y desarrollo de cada una de las alternativas
            propuestas, y la selección de la más adecuada. Considerando para ello: a) El
            aprendizaje del número de etiquetas con las funciones de pertenencia uniformes, b) El
            ajuste de funciones de pertenencia con un número fijo de etiquetas,  y c) El
            aprendizaje conjunto del número de etiquetas y de la semántica de las mismas. 

            4.- Implementación de las mejores alternativas: Esta actividad comprendió la
            incorporación de los métodos de aprendizaje automático de particiones difusas
            seleccionados en el contexto de la metodología  FIR a objeto de generar una
            herramienta que mejore la habilidad de predicción de FIR. Considerando para ello: a)
            El aprendizaje del número de etiquetas con las funciones de pertenencia uniformes, b)
            El ajuste de funciones de pertenencia con un número fijo de etiquetas,  y c) El
            aprendizaje conjunto del número de etiquetas y de la semántica de las mismas. 

            5.- Aplicación práctica: Este paso consistió en el uso de la herramienta desarrollada
            sobre un conjunto de benchmarks así como también su aplicación a un problema real
            complejo con miras a realizar un análisis comparativo de los resultados obtenidos con
            otros sistemas de modelado. 

            6.- Presentación y discusión de resultados: Su objetivo fue mostrar los resultados de
            la investigación con el fin de realizar posteriormente su análisis, discusión e
            interpretación. 

            7.- Elaboración de conclusiones: Esta etapa consistió en presentar las conclusiones
            que se desprendieron del análisis de los resultados, considerando los aspectos más
            relevantes de éste y en función de los objetivos de la investigación. 

            8.- Elaboración de recomendaciones: En esta actividad se pretendió mostrar las
            recomendaciones que puedan surgir del proceso investigativo, con el objeto de
            sugerir ante cualquier limitación suscitada,  a fin de que otros investigadores
            interesados en profundizar en el tema o replicar la investigación, pudieran realizar 
            estudios más completos. Incluyó además,  sugerencias, interrogantes o aspectos de 
            interés develados por los resultados que pudieran ser ampliados o investigados en
            profundidad en otros trabajos o extensiones futuras de la tesis. 

            9.- Redacción de la memoria de la tesis: Finalmente, se procedió a la elaboración de
            la memoria de la tesis doctoral. En la misma se plasmaron los antecedentes del tema,
            las técnicas utilizadas, los métodos propuestos, los resultados y las conclusiones
            obtenidas, y las extensiones futuras de la tesis. </Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Resultados obtenidos y conclusiones 

Los resultados obtenidos por los SDEs desarrollados en esta tesis son muy
buenos. Recordemos que en las cuatro aplicaciones estudiadas los tres AGs
propuestos son los que obtienen mejores modelos con mayor eficiencia en el proceso
de predicción. En general, el AG3 y la combinación AG1+AG2, en ese orden, son
los que han mostrado mejores resultados en todas las aplicaciones, seguidos por el
AG1+EFP. Sin embargo el AG3 es el que presenta mayor costo computacional.

Enla aplicación del sistema nervioso central humano, los tres SDEs presentados en esta
tesis han conseguido resultados superiores a otras metodologías usadas en trabajos
previos y al FIR DEFECTO, en ese orden. Así mismo, en la aplicación de la línea de
media tensión en núcleos urbanos los tres AGs propuestos consiguen resultados de
predicción notablemente superiores a los conseguidos por FIR DEFECTO y por
otras metodologías en trabajos previos.  También hacen un buen papel los SDEs
diseñados en la aplicación de la estimación de las concentraciones de ozono en
Austria. A pesar de la pobreza de los datos (tanto en cantidad como en calidad) el
AG3 y AG1+AG2 son los que lideran la capacidad de predicción, excepto en  uno de
los casos del cross validation donde FIR MANUAL consigue el menor error de
predicción. Sin embargo, una solución de búsqueda manual no es para nada deseable
y no permite la utilización de la metodología FIR por usuarios inexpertos en lógica
difusa que requieran modelar un sistema de complejidad elevada. Finalmente, en la
aplicación de la estimación de las concentraciones de ozono en México, una vez más,
se confirma que las diferentes combinaciones propuestas de los AGs diseñados en
esta investigación superan ampliamente los resultados obtenidos por FIR DEFECTO
y también, cosa no esperada, por FIR EXPERTOS, es decir, por FIR cuando los 
parámetros de fusificación han sido definidos por expertos en el área. Por lo tanto 
como conclusión general, debemos decir que los SDEs diseñados e implementados en
esta tesis consiguen excelentes resultados para la tarea que les ha sido encomendada 
en el entorno de la metodología FIR. Es pues el usuario quien debe decidir qué SDE
resulta más conveniente para la aplicación que tiene entre manos, en función de las
necesidades temporales y de precisión. 

A continuación se presentan algunas conclusiones adicionales que queremos 
remarcar:  

A.- Influencia de la granularidad de las particiones difusas en el
comportamiento de la metodología FIR
 
      A lo largo de esta memoria, se ha resaltado la importancia de la granularidad de 
      las particiones difusas dentro del proceso de obtención del modelo FIR que represente 
      mejor el sistema bajo estudio. Sobre este aspecto, se han extraído las siguientes
      conclusiones: 

      En todos los sistemas difusos, el número de etiquetas (clases) por variable
      influye decisivamente en el comportamiento final del sistema difuso resultante.
      FIR no es la excepción, debido a que también está basada en lógica difusa y la
      determinación de los parámetros necesarios en el paso de fusificación
      (discretización) llega a ser relevante para la identificación de un buen modelo
      que capture el comportamiento del sistema de manera precisa.  

      El nivel de granularidad óptimo para cada partición difusa depende del
      problema que se esté tratando, por lo tanto los buenos resultados obtenidos en
      un problema no son directamente extrapolables a otros problemas. 

      Las diferencias de precisión entre los modelos FIR obtenidos con un buen
      nivel de granularidad en las particiones difusas y los obtenidos usando FIR
      DEFECTO (mismo número de etiquetas en todas las variables, siendo este
      valor por defecto de 3) son considerables. Por lo tanto, la selección de un nivel
      de granularidad adecuado es una tarea fundamental para el proceso de
      fusificación (discretización) de la metodología FIR. 

      El método de búsqueda del nivel de granularidad óptimo propuesto en el
      Capítulo III (AG1) requiere poco esfuerzo computacional si se tiene en cuenta
      la dimensión del espacio de búsqueda manejado, y ha demostrado una buena
      eficiencia, especialmente en los problemas con un número elevado de
      variables. 

B.- Aprendizaje de las funciones de pertenencia para razonamiento inductivo 
difuso 

      El  AG propuesto para el aprendizaje de las funciones de pertenencia ha tenido 
      un buen comportamiento. El método propuesto en el Capítulo IV (AG2) ha logrado
      muy buenos resultados en la mayoría de los problemas estudiados en comparación al
      uso de la distribución uniforme de las funciones de pertenencia a lo largo del universo
      de discurso de las variables (Método EFP). El mecanismo de ajuste “local” ha
      permitido refinar las definiciones de las funciones de pertenencia manteniéndose
      inalterable la granularidad (número de clases) de las variables. La granularidad ha
      sido proporcionada en algunos casos por expertos en el área de interés o sugerida por
      el método presentado en el Capítulo III (AG1). Aunque se ha indicado que la
      adaptación no lineal de contextos en sistemas difusos resulta costosa
      computacionalmente, ésta nos ha permitido representar mejor los conceptos en
      entornos reales con alta sensibilidad en las clases extremas o en las clases
      intermedias, por lo tanto estas variaciones han influido directamente en el 
      comportamiento de los modelos FIR resultantes. Es así como, una selección adecuada 
      de las funciones de pertenencia es, también, una tarea de vital importancia para el
      proceso de fusificación (discretización) de la metodología FIR.  

C.- Aprendizaje conjunto de la granularidad y las funciones de pertenencia para
razonamiento inductivo difuso 

      El método propuesto en el Capítulo V (AG3) ha demostrado ser eficaz en 
      problemas complejos. La filosofía de ajuste “global” que subyace en dicho método ha
      proporcionado unos buenos resultados y ha permitido obtener sistemas difusos con
      buena capacidad de generalización. Sin embargo, el proceso de aprendizaje ha sido
      lento y dificultoso porque se trabaja con un espacio de búsqueda extenso. Asimismo,
      el tamaño de la población se incrementa al tener que considerar más diversidad para
      la población inicial. 

      Los valores de los parámetros escogidos para los SDEs propuestos (AG1, AG2 
      y AG3) han originado un buen comportamiento general para todos los problemas
      tratados en esta investigación, pero estos métodos pueden ofrecer mejores resultados
      si se ajustan dichos valores realizando una experimentación específica. 

      Por último, es necesario señalar que no se puede concluir que un SDE 
      específico o combinación de éstos es mejor que los demás,  ya que ninguno tuvo
      mejor rendimiento (encontró soluciones más precisas y fiables) que otro en todos los
      casos estudiados.  La excepción es quizá el AG1, pues ha sido el peor de los SDEs
      desarrollados en la mayoría de las situaciones. Sin embargo mejora substancialmente 
      el uso de parámetros por defecto en FIR. Por todo ello, es importante tomar en
      consideración las ventajas e inconvenientes de cada método propuesto y las 
      características del problema de optimización a tratar para determinar qué SDE usar en 
      cada caso. 

D.- Funciones objetivo implementadas

      En esta memoria se han propuesto para la evaluación de los cromosomas en 
      todos los SDEs desarrollados (AG1, AG2 y AG3) dos funciones objetivos: a) la
      calidad de la máscara óptima y b) el error de predicción MSE de parte del conjunto de
      datos de entrenamiento. La calidad de una máscara Q,  es un valor entre 0 y 1, donde
      1 indica la más alta calidad. Por lo tanto, la primera función objetivo propuesta es 1Q,
      ya que la tarea era minimizar ésta. La segunda función objetivo corresponde  a la
      predicción del error de una porción del conjunto de datos de entrenamiento.

      El error cuadrático medio normalizado en porcentaje (MSE),se ha usado para este fin. Es por
      ello, que esta función objetivo se denomina MSE.

      Se ha confirmado que el tiempo de CPU necesario por el AG2 para efectuar el conjunto de 30 ejecuciones está relacionado directamente con el número predefinido de clases por variable. Por otro lado, en general se observa que la función objetivo 1Q
      necesita menos tiempo para ser evaluada pero tiene menor eficiencia en la predicción  del conjunto de datos de prueba.
      Contrariamente, la función objetivo MSE  es más costosa desde la perspectiva del tiempo de CPU pero la eficiencia es
      más alta. Sin embargo, el desempeño de ambas funciones objetivo puede verse
      influenciado por las características, cantidad y complejidad de los datos disponibles
      del problema en estudio, por ser FIR una metodología de minería de datos. De este
      modo, se recomienda al usuario escoger la función objetivo más conveniente en
      función del tamaño del problema de optimización y las necesidades del propio
      usuario. Si se requiere la máxima precisión en el problema que se está estudiando
      será conveniente usar el AG3 con la función objetivo MSE. Contrariamente, si la
      precisión no es el parámetro primordial, y se requiere no excederse en el tiempo de
      cómputo, el AG1 y/o el AG2 con la función objetivo 1-Q serán mejores alternativas.  

E.- Método multiobjetivo de aprendizaje para razonamiento inductivo difuso

      El método multiobjetivo propuesto en el Capítulo VIII (AGMO3) ha 
      demostrado también ser eficaz en problemas complejos. A pesar de estar aún en
      proceso de ajuste y pruebas, ha permitido con los parámetros que se le han
      establecido obtener buenos resultados para la aplicación estudiada. La filosofía de
      ajuste “global” que subyace en dicho método ha permitido obtener también sistemas
      difusos con buena capacidad de generalización (aunque con errores MSE
      más altos que los otros SDEs contemplados en esta memoria). Sin embargo, el tamaño de la
      población se incrementa al tener que considerar más diversidad para la población
      inicial, lo que puede derivar (dependiendo de la aplicación o problema que se trate) 
      en un aumento del número de evaluaciones de cromosomas (y por ende, del número 
      de generaciones) para que haya un proceso evolutivo más largo y se puedan explorar
      otras zonas prometedoras y explotar más el espacio de búsqueda. Hay que recordar 
      que el  AGMO3 no tiene un mecanismo elitista, por lo cual el conjunto de soluciones
      no dominadas en la última generación son las únicas que se tendrán a la mano para
      realizar la selección de aquellas que cumplan con nuestras necesidades. 

      El AGMO3 ha permitido en una sola ejecución (con un tiempo similar a los que 
      tardan los otros AGs implementados) obtener un conjunto de soluciones (unas más
      simples y otras más precisas para poder escoger) al alcanzar el criterio de parada que
      se le ha estipulado. De este modo, no fue necesario ejecutarlo tantas veces como los
      métodos AG1, AG2 y AG3 contemplados en esta memoria. 

      Se observa que las soluciones que tienen el menor valor de sumatoria de 
      etiquetas o clases por variable (considerando las relaciones causales de la máscara 
      óptima), es decir, los modelos más simples son los más propensos a poseer un error 
      de predicción elevado con respecto al conjunto de datos de prueba. Se debe estudiar
      la posibilidad de colocar una restricción para ésta parte de la función objetivo del
      AGMO3, de manera tal que descarte soluciones que tengan sólo relaciones causales
      en instantes de tiempo previos en la misma variable de salida. 

Trabajos futuros 

A continuación, se comentan algunas líneas de trabajo que quedan aún abiertas
sobre la temática tratada en esta memoria: 

Sobre la medida de calidad usada por la metodología FIR:

En virtud de los resultados obtenidos se han comenzado a realizar esfuerzos 
para establecer otras medidas de calidad Q de la máscara en el proceso de modelado
cualitativo de la metodología FIR. En las aplicaciones presentadas en esta memoria se
ha hecho patente que esta medida no refleja con la precisión suficiente la capacidad
de predicción de las máscaras.  

Sobre otros posibles casos de estudios o aplicaciones de especial interés:

En estos momentos, hay disponibles unos datos simulados sobre una aplicación 
de control en sistemas industriales que corresponden a una turbina de gas.  Se
requiere obtener modelos para distintas situaciones de funcionamiento de esta planta
con el fin de realizar posteriormente detección de fallos. Se pretende aplicar los 
métodos propuestos en esta memoria para resolver la problemática planteada. 

Sobre posibles extensiones de los métodos propuestos: 

Por un lado, se puede realizar un estudio comparativo para optimizar los 
parámetros de los SDEs propuestos. Mientras que para parámetros como la
probabilidad de cruce o de mutación se pueden seguir recomendaciones de diversos
autores duchos en el tema [Gre86], el tamaño de la población es un parámetro
interesante a estudiar puesto que depende del problema que se esté tratando, de la
extensión del espacio de búsqueda, de si se utilizan o no nichos, etc. De todas formas,
no se debe perder de vista que los AGs deben terminar en un tiempo razonable,
máxime cuando la función objetivo es costosa en tiempo de ejecución. En este
sentido, el tamaño de la población es un factor que no debe tenerse en cuenta en
forma aislada, sino junto a la condición de parada del AG (sea por número de
generaciones o por número de evaluaciones de cromosomas). Además, puede resultar 
que los parámetros óptimos no sean los mismos en las dos funciones objetivo que se 
han implementado.

Por otro lado, se pueden estudiar como actúan los SDEs propuestos con otros 
operadores genéticos de cruce o mutación con codificación real [HLV98] o con
nuevos mecanismos de selección frente a la resolución de los problemas estudiados
en esta memoria, así como para analizar la diversidad de la población. 

Por su parte, el AG multiobjetivo presentado en el Capítulo VIII debe probarse 
sobre un conjunto amplio de aplicaciones para estudiar cabalmente su rendimiento.
Asimismo, sería interesante desarrollar otro AG multiobjetivo en donde se aprenda
solamente la granularidad (como el AG1) y se persigan los mismos objetivos
descritos. 

Por último, es posible desarrollar un AG multiobjetivo elitista que sea más 
eficaz que el presentado en esta memoria como aportación adicional, ya que estos
encuentran mejores conjuntos de soluciones (más cercanos al Pareto).</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSITAT   POLITÈCNICA DE  CATALUNYA 

Departament d’Enginyeria de Sistemes, Automàtica i  
Informàtica Industrial 
 
APRENDIZAJE  DE  PARTICIONES DIFUSAS  PARA  RAZONAMIENTO 
INDUCTIVO 
 
MEMORIA  QUE  PRESENTA 
JESUS ANTONIO ACOSTA SARMIENTO 
PARA  OPTAR  AL  GRADO  DE  DOCTOR  

DIRECTORES 

Dra. ÀNGELA NEBOT CASTELLS 
Dr. JOSEP Mª FUERTES ARMENGOL

DEPARTAMENT  D’ENGINYERIA DE SISTEMES, AUTOMÀTICA I 
INFORMÀTICA INDUSTRIAL 

FACULTAT 
D'INFORMÀTICA DE
BARCELONA (FIB)              

UNIVERSITAT   
POLITÈCNICA DE 
CATALUNYA (UPC)
 
DICIEMBRE  2006 </Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Análisis de la Fractura en Soldaduras de una Aleación de Al-Zn (7075-T651)</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Determinar las características del crecimiento de grietas bajo la aplicación de una carga cíclica
(fatiga uniaxial), y la tenacidad a la fractura bajo la aplicación de una carga estática sobre una
aleación de Al-Zn (7075-T651), soldada por medio del proceso de soldadura de arco eléctrico y
la técnica por arco eléctrico indirecto modificado.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Empleando la técnica de soldadura por arco eléctrico indirecto modificado (AEIM) es posible
retardar la nucleación y el crecimiento de grietas por fatiga, e incrementar la tenacidad a la
fractura en las uniones soldadas de la aleación Al-Zn (7075-T651), respecto a la técnica
convencional de arco eléctrico (GMAW). Esto es posible, debido a que la técnica AEIM
produce uniones soldadas en espesores de más de nueve milímetros empleando un solo cordón
de soldadura, aspecto que es atribuido a la preparación de junta y a la elevada eficiencia
térmica que presenta. El elevado aporte térmico de la técnica y la elevada conductividad
térmica del aluminio, permiten un aumento en la velocidad de enfriamiento, con lo cual se
mejoran las características de solidificación en el metal fundido generando un refinamiento de
grano y en la zona afectada térmicamente se produce un menor engrosamiento de los
compuestos intermetálicos. </Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Las estructuras metálicas usadas en la industria del transporte, tales como bastidores de pared y
piso de vehículos automotores, están sometidas a cargas cíclicas durante su vida de servicio,
induciendo la aparición de grietas por fatiga. En las estructuras soldadas de aleaciones de
aluminio tratables térmicamente el riesgo de aparición de una grieta es probable, debido a la
heterogeneidad en las propiedades mecánicas de las zonas que comprenden la unión. Por lo
anterior surge la necesidad de realizar uniones experimentales de soldaduras que permitan
retardar el crecimiento de grietas por fatiga en uniones soldadas en aleaciones de este tipo. Para
ello se establecen estudios comparativos entre dos procesos de unión; por ejemplo, el proceso
convencional de soldadura de arco eléctrico con material de aporte y gas de protección, la
implementación de la técnica de arco eléctrico indirecto modificado. Con ello se busca
identificar una técnica de aplicación de soldadura que permita incrementar la vida a la fatiga de
los componentes soldados.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Por medio de mapas y perfiles de dureza fue posible identificar las diferentes zonas que
comprenden la soldadura por GMAW y la técnica de AEIM en la aleación Al-Zn 7075-T651.
Se observó la presencia de una zona blanda con una dureza de ~ 100 HV0.1, localizada a una
distancia aproximada de 15 y 20 mm del centro del cordón de soldadura para GMAW y AEIM,
respectivamente.
Las propiedades de tensión de las uniones soldadas disminuyen drásticamente comparadas con
el metal base debido a las condiciones de solidificación y la elevada porosidad en el metal
fundido en ambas técnicas de soldadura.
El porcentaje de dilución obtenido por la técnica AEIM es 2.6 veces mayor que el proceso
GMAW, lo cual favorece la incorporación de elementos de aleación (Zn y Cu) a partir del
material base. Esta característica genera un endurecimiento en la zona de fusión, incrementando
la dureza y la resistencia a la tensión de las uniones por AEIM, posterior al tratamiento térmico
de solubilización y envejecimiento artificial. Sin embargo, prevalecen las condiciones de
fragilidad (baja ductilidad) debido al efecto de la porosidad.
El crecimiento de grietas por fatiga para la zona blanda y el metal fundido en AEIM presenta
una ganancia en el número de ciclos para dar inicio a la nucleación de grietas por fatiga. En el
caso de la zona blanda, esta mejoría se atribuye al cambio microestructural que ocasiona un
incremento en el tamaño de los precipitados. Mientras que para el metal fundido, el aumento en
cuanto al número de ciclos está relacionado con la formación y tamaño de la porosidad.
La rapidez de propagación de grietas por fatiga en la zona blanda, es similar en ambos procesos
de soldadura. Sin embargo, en el metal fundido en AEIM es ~ 46 %, mayor con respecto al
proceso GMAW.
El espaciamiento de las estrías en las superficies de fractura, mantiene una correcta
correspondencia con la rapidez de propagación de grietas por fatiga en los materiales
ensayados.
La tenacidad a la fractura determinada por medio de los criterios de integral J y COD en AEIM
presenta una ligera mejoría en la zona blanda y metal fundido con respecto al proceso GMAW.
Así mismo, se ha observado que los valores son similares a los obtenidos para la aleación AlZn
7075-T651. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>IPN
CENTRO DE INVESTIGACIÓN E INNOVACIÓN TECNOLÓGICA
Análisis de la Fractura en Soldaduras de una
Aleación de Al-Zn (7075-T651)
TESIS
PARA OBTENER EL GRADO DE:
DOCTORADO EN TECNOLOGÍA AVANZADA
PRESENTA: NORMA ALATORRE TORRES
ASESOR DE TESIS: DR. RICARDO RAFAEL AMBRIZ ROJAS
México D.F. 2014 </Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>CONTROL DE SISTEMAS MECANICOS USANDO ´
COMPENSACION NEURONAL</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Diferentes modelos de redes neuronales artificiales son ampliamente estudiadas en [1, 2],
las cuales se pueden clasificar dependiendo del flujo de datos a través de la red, como redes
neuronales estáticas y redes neuronales recurrentes. En las redes neuronales estáticas, el flujo
de datos siempre es en dirección de entrada a la salida de la red neuronal, mientras que en la
red neuronal recurrente el flujo de datos se mueve de entrada a la salida de la red neuronal más
una realimentación de los datos de salida a la entrada y capas interiores de la red neuronal.
Otra forma de clasificar las redes es por el tipo de aprendizaje, si la red neuronal se entrena
con un conjunto de datos, previo a su trabajo con el sistema o proceso se le conoce como
“aprendizaje fuera de proceso”, en cambio si la red neuronal aprende en tiempo real cuando se
trabaja mutuamente con el sistema se llama “aprendizaje durante el proceso” [3, 4].
La clase de sistemas o procesos que se tratan en esta tesis son modelados por ecuaciones
diferenciales [5, 6, 7, 8], sin embargo, los mecanismos que se tratan en esta tesis solo se conocen
la estructura del modelo. Por ejemplo, si
x˙ =f(x) + g(x)u,
y =h(x),
es un modelo de algún sistema en particular, donde f : IRn → IRn
, g : IRn → IRn×r
, el vector de
control u ∈ IRr
para r ≤ n, con variables de estado x ∈ IRn
y el vector de salida y : IRn → IRm,
entonces dada la forma de la representación del modelo dinámico se conoce su estructura, pero
las funciones f(x), g(x) y h(x) son desconocidas.
Los sistemas dinámicos se clasifican de acuerdo a su grado de actuación (número de actuadores
respecto a los grados de libertad del sistema), en sistemas completamente actuados [6] y
sistemas subastados [9]. Los sistemas completamente actuados tienen igual número de actúa dores que grados de libertad del sistema, mientras que los subastados tienen menos actuadores
que grados de libertad y son más difíciles de controlar. Los modelos dinámicos con alto grado de
incertidumbre, no integran todas las dinámicas del sistema físico, como la fricción, zona muerta,
saturación, vacilas (“perdida de movimiento en un mecanismo causado por la holgura entre
sus partes”) y dinámicas desconocidas, entonces es conveniente el uso de redes neuronales, dado
que tienen la capacidad de aprender y adaptarse al sistema.
Recientemente los avances en el ´área de redes neuronales han dado un gran impulso al
control de sistemas no lineales a través de procesos de aprendizaje. En los sistemas de control,
se aprovecha la capacidad de la red neuronal de modelar funciones diferenciales no lineales,
la habilidad de aprender y adaptarse en tiempo de ejecución del proceso [3, 4, 10, 11]. Por
ejemplo, Narendra y Parthasarathy [4] demuestran que las redes neurales pueden ser usadas
efectivamente para la identificación y control de procesos dinámicos no lineales.
Un problema en el control de mecanismos es la determinación del modelo dinámico, ya sea
por lo difícil de obtener un modelo dinámico del sistema [5, 6], por el desconocimiento de los
parámetros del modelo o de algunas de sus dinámicas. Las redes neuronales, como una clase
de sistemas adaptativos [12, 13], son ´útiles en tales situaciones, ya que pueden adaptarse a la
dinámica del sistema que se desconoce.
Los controladores neuronales con aprendizaje durante el proceso, permiten controlar los
sistemas dinámicos sin la necesidad de un entrenamiento previo de la red neuronal, además, el
desempeño de la red neuronal no está sujeto a un entrenamiento realizado por un conjunto de
datos delimitados.
Un controlador basado en retroalimentación, es confiable si ´este es respaldado por un estudio
de estabilidad. Es importante notar que algunos trabajos de investigación de controladores
neuronales que están en la literatura, no tienen un estudio de estabilidad del sistema dinámico
en lazo cerrado, por lo que la confiabilidad de los controladores propuestos es degradada.
Las redes neuronales para el control de sistemas subastados es un campo de conocimiento
que necesita trabajo de investigación. Se encuentran trabajos de investigación para el control
de mecanismos como pendubot, acrobot, péndulo de Furuta y péndulo de rueda inercial, pero
muchos trabajos no tienen un estudio de estabilidad, sobre todo si es un problema de seguimiento
de trayectorias. Además, es importante conocer el desempeño de los controladores neuronales
en esta clase de sistemas. 
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Diseñar controladores neuronales para realizar el seguimiento de trayectorias de referencia
en sistemas mecánicos cuyo modelo dinámico es desconocido.
Objetivos específicos
Para mecanismos cuyo modelo dinámico es desconocido, los objetivos específicos de la presente
investigación son
Diseñar controladores neuronales de auto aprendizaje.
Diseñar controladores neuronales aplicados a sistemas completamente actuados y subastados.
Resolver problemas de sistemas dinámicos usando control neuronal.
Comparar y evaluar el desempeño del controlador neuronal.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Los controladores formados por un término adaptativo neuronal, un par de términos proporcional derivativos más un termino con función signo, aplicados a mecanismos cuyo modelo se desconoce, pueden asegurar que, en lazo cerrado, los errores de seguimiento de trayectorias converjan a cero o posean acotamiento ´ultimo uniforme. La manera de comprobar esta hipótesis es a través de la obtención de las ecuaciones de lazo cerrado y su análisis usando la teoría de Lyapunov y la teoría de sistemas con soluciones acotadas en forma ´ultima</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En la literatura hay una gran diversidad de controladores adaptativos que usan redes neuronales, sin embargo, es importante el desarrollo de nuevos controladores con mejor desempeño, menor tiempo de convergencia y más confiables.
Los sistemas que son difíciles de modelar o de los que no se dispone de un modelo dinámico, pueden ser objeto de estudio usando redes neuronales para la identificación del mismo. En el caso de incertidumbres en el modelo dinámico, las dinámicas no modeladas pueden ser aproximadas por redes neuronales. El control neuronal puede aplicarse a sistemas robóticos en la industria, que ayudaría a la reducción de tiempos en los procesos productivos, con la reducción del mantenimiento a los sistemas robóticos, dado que las dinámicas de estos sistemas cambian con el tiempo debido al desgaste de sus mecanismos o factores externos como la adhesión de polvo o suciedad. También, el control neuronal se puede usar para el control de sistemas humano-robóticos con la finalidad de ayudar a las personas con discapacidades físicas, y al control de dinámicas complejas como los robots bípedos o similares. También, esta investigación puede ser usada en el campo educativo como una referencia en investigaciones en el ´área de control neuronal aplicada a mecanismos.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El método para la solución de los problemas es similar al usado por Lewis et al. [3], el
cual consiste en el uso de redes neuronales en un lazo de recompensación más un controlador
en un lazo de retroalimentación que permite estabilizar el sistema mecánico o electromecánico
mientras la red neuronal aprende y compensa la dinámica del sistema. La ventaja de este
método es que la red neuronal no necesita de un conjunto de datos para su entrenamiento
previo y por consecuencia el desempeño de la red no está delimitado a la calidad y cantidad
de datos seleccionados para su entrenamiento. La diferencia entre los controladores neuronales
propuestos y los reportados en la literatura, está en el tipo de controlador usado en el lazo de
retroalimentación y en el tipo de redes neuronales usadas en el lazo de recompensación para
formar un controlador adaptativo que tenga un desempeño mejor o comparable respecto a los
controladores neuronales reportados en la literatura. La estabilidad del sistema es estudiada
con la teoría de Lyapunov. Además, se usa la teoría de sistemas con señales uniformemente
acotadas. El procedimiento para alcanzar los objetivos es el siguiente:
Proponer un algoritmo de control para un sistema en particular.
Obtener las ecuaciones de lazo cerrado.
Encontrar las condiciones de estabilidad o acotamiento de soluciones.
Hacer simulaciones.
Hacer experimentos.
Comparar los resultados con otros controladores en la literatura. 
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se puede enumerar para expresar con mejor claridad las conclusiones generales de este
trabajo de tesis:
1. Los controladores neuronales propuestos tienen tres tipos de acciones: proporcional derivativa,
acción adaptativa que es dada por las redes neuronales y una acción robusta
para compensar los términos residuales y de error de la red neuronal que se derivan del
análisis de convergencia.
2. Las redes neuronales empleadas son del tipo perceptrón de dos capas con funciones tanh(∗)
en la capa oculta. Las propiedades de la función tanh (∗) motivaron su uso en la acción de
control PD no lineal del controlador neuronal de robots manipuladores.
3. Las redes neuronales empleadas son de la clase “aprendizaje durante proceso”, por lo
tanto, los pesos se actualizan en tiempo real en base a leyes de adaptación de pesos
derivadas del análisis de convergencia.
4. El diseño de los controladores fue en la asunción de que los modelos dinámicos de los
sistemas físicos son desconocidos.
5. Se demuestra que los errores articulares de posición y velocidad del sistema de lazo cerrado
del brazo manipulador, convergen a cero para ganancias suficientemente grandes de
Kp, Kd y ∆ del controlador neuronal.  del péndulo de Furuta, son uniformemente acotados en forma ´ultima (UUB) dentro
de una bola de radio µ, bajo las condiciones del análisis de convergencia de la
proposiciones 1 y 3.
del péndulo de rueda inercial, converge a cero en la primera articulación que corresponde
a la varilla del péndulo, mientras que la posición de la rueda es desacotada.
Las condiciones bajo la cual se cumple esta afirmación son dadas en la ecuación (5.41)
y al uso del lema de Barbalat.
6. Se demuestra que los pesos estimados de las redes neuronales están uniformemente acotados
en forma ´ultima (UUB).
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL ´
CENTRO DE INVESTIGACIÓN Y DESARROLLO ´
DE TECNOLOGÍA DIGITAL
DOCTORADO EN CIENCIAS EN SISTEMAS DIGITALES
“CONTROL DE SISTEMAS MECÁNICOS USANDO ´
COMPENSACIÓN NEURONAL” ´
TESIS
QUE PARA OBTENER EL GRADO DE
DOCTOR EN CIENCIAS EN SISTEMAS DIGITALES
PRESENTA
MR. SERGIO ALBERTO PUGA GUZMÁN´
BAJO LA DIRECCIÓN DE ´
DR. EDUARDO JAVIER MORENO VALENZUELA
DR. VÍCTOR ADRIÁN SANTIBÁÑEZ DÁVILA ´
OCTUBRE, 2014 TIJUANA, B.C., MÉXICO
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Estrategias de adaptabilidad estéticamente estables al cambio de ´ terreno para el robot caminante hexapodo Hex-piderix</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>OBJETIVO GENERAL
Diseñar, analizar y construir un robot caminante hexápodo que sea capaz adaptarse de cambios en el terreno, garantizando la estabilidad estática. 
OBJETIVOS PARTICULARES
Diseñar, analizar y construir el robot hexápodo.  
Estudiar y seleccionar una de las estrategias de locomoción del robot hexápodo basándose en el comportamiento de los insectos.  
Elaborar estrategias de adaptabilidad al terreno que garanticen la estabilidad
estática de la postura.  
Validar las estrategias de adaptabilidad.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En las ultimas décadas se ha constatado el creciente interés que presentan los  
robots caminantes, tanto desde el punto de vista de la investigación básica como  
de sus potenciales aplicaciones. Una de las principales características de los robots
equipados con patas es su capacidad para poder desplazarse sobre una amplia
variedad de terrenos no estructurados. Esta particularidad los hace muy atractivos
para su aplicación practica como robots de servicios en los que se requiere la  
locomoción sobre terreno natural, el cual se caracteriza por presentar superficies  
irregulares, diversos tipos de obstáculos y distintas pendientes. Sin embargo, el caminar sobre terreno natural implica que tanto la geometría como las propiedades
físicas de los elementos que rodean al robot son desconocidas y muy variables, por
lo que es necesario dotar al robot de una estrategia que adapte su comportamiento
con respecto al entorno. Si bien el modelo del robot puede ser conocido con
precisión razonable, resulta difícil obtener una descripción detallada del entorno,  
especialmente en lo que respecta a sus propiedades físicas.
La estabilidad de los robots caminantes durante la realización del ciclo de  
locomoción, es de gran importancia. Por lo tanto, la consideración de una determinada medida de estabilidad durante la locomoción influye de manera directa en 
la seguridad del robot.
En este trabajo se presenta el planteamiento de estrategias de adaptabilidad
para el robot hexápodo Hex-piderix, basadas en un modelo geométrico, en el cual  
no es necesario conocer el ambiente, pues con tan solo saber que extremidades  
están tocando la superficie es posible estimar el plano de apoyo.  
En este documento se presenta el diseño y simulación de estrategias de adaptabilidad para el robot caminante hexápodo Hex-piderix basadas en un modelo
geométrico. La validación de cada una de ellas se realiza sobre un plano con distinto grado de inclinación; además de que se analiza la estabilidad estática de cada  
postura con el criterio de estabilidad estático normalizado por el peso (SNE, por  
sus siglas en ingles)
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El proyecto se divide en tres fases: diseño del robot caminante, construcción 
del prototipo y diseño de estrategias de adaptabilidad al terreno estáticamente estables. Estas etapas son descritas brevemente a continuación.  
1. Diseño del robot caminante. 
Esta etapa del proyecto comprende:
Búsqueda y análisis de información en la literatura especializada y en campo.
Análisis conceptual de la arquitectura del robot. Se partir   a de una topología
existente.
Análisis cinemático mediante la convención de Denavit-Hartenberg. Se opta por llevar a cabo este análisis pues las extremidades se pueden ver como 
manipuladores seriales independientes.
2. Construcción del prototipo.  
Se plantea construir un prototipo a partir de una arquitectura comercial.
3. Estrategias de adaptabilidad al terreno estáticamente estables.  
Esta es la etapa central del proyecto de tesis y está dividida como se muestra  
a continuación:  
Búsqueda bibliográfica.  
Análisis y discusión de los criterios de estabilidad estática.  
Diseño de las estrategias de adaptabilidad. 
Simulación y validación de las estrategias.
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este documento se presentó el diseño y validación experimental de estrategias de adaptabilidad estáticamente estables para el robot   Hex-piderix.
Para garantizar la estabilidad de las estrategias de adaptabilidad fue relevante
conocer el CG del robot para el instante de tiempo actual; se calculó analíticamente
mediante el método de cuerpos compuestos, se programó y simulo en Matlab y  
los resultados fueron validados experimentalmente con ayuda del software CAD
SolidWorks.
Además, se evaluó la estabilidad estática para cada postura con ayuda del criterio SNE, dado que este se basa en un modelo geométrico al igual que el diseño
de las estrategias de adaptabilidad.
Se presenta la construcción del robot en materiales ligeros y la validación de 
las estrategias de adaptabilidad para el mismo. Se desarrolló una interfaz grafica 
en LabView de fácil manejo, donde el usuario tiene la posibilidad de manipular manualmente los motores de manera independiente y visualizar los resultados  de los cálculos requeridos en dichas estrategias, los cuales fueron realizados en  
Matlab. La comunicación entre Matlab y LabView se hizo con ayuda del módulo MathScript, ayudando a que se realicen rápidamente y se eviten los que no son  
necesarios para ese instante de tiempo. Por otro lado, la adquisición de las señales ˜
de los sensores de distancia se hicieron con la tarjeta ARDUINO MEGA, la cual
se encarga de leer cada sensor analógico y en caso de que la extremidad se encuentre tocando la superficie de apoyo se en la computadora como una señal digital, 1 o 0.
Para el diseño de las estrategias de adaptabilidad se tomaron en cuenta los parámetros geométricos del robot, así como el peso de cada uno de sus componentes;
tanto la cinemática directa como la inversa en posición fueron fundamentales  
para que el robot se pudiera reorientar; se presentaron dos estrategias, la primera
de ellas se dividió en dos: tórax constante y comparación entre las alturas de las  
extremidades y la última consistió en tomar un punto de apoyo m   as para aumentar  
el polígono de apoyo y as ı garantizar la estabilidad.
La estrategia de adaptabilidad tórax constante   resulto ser la m   as sencilla de  
todas pues el robot se adapta al terreno cambiando su orientación alrededor
de los ejes X y Y realizando los cálculos de cinemática inversa. Sin embargo,  
esta estrategia es la más limitada de todas; como está basada en los parámetros  
geométricos del robot puede adaptarse a cambios de inclinación menores a   50◦
,
cuando el robot se adapta con esta estrategia a superficies con una inclinación mayor a 50◦ ocurren colisiones con las extremidades que están en reposo, tal como se  
muestran en las simulaciones de la Figura 5.28, una de las extremidades pierde el
contacto, el polígono de apoyo es diferente y más reducido por lo que pierde la estabilidad si se quisiera utilizar esta estrategia para cambios mayores a ese ángulo  
sería necesario redisenar las extremidades o todo el robot. Por otra parte, la estrategia tórax constante   también funciona para llevar una carga a lugares remotos, ya  
que al mantenerse recto el tórax la posible carga no caería.
La estrategia comparación de altura de extremidades   consiste en calcular
las alturas de las extremidades de apoyo y la del tórax respecto a la superficie, se  
comparan los resultados y se elige una línea de soporte adecuada para que el robot
se reoriente como máximo la mitad de su orientación actual, lo que ayuda a que  
no se presente ninguna colisión con las extremidades y el terreno. Se realizaron  
simulaciones para inclinaciones desde 50◦
a 75◦
siendo todas ellas estables, y
quedando el tórax con una postura casi paralela al plano de apoyo aprovechando al máximo los parámetros geométricos.  
La estrategia B se usa solamente cuando ninguna de las anteriores funciona
o en situaciones críticas. Se presentaron simulaciones en Matlab que demuestran
como el robot es capaz de bajar una extremidad m   as para ampliar el polígono de
apoyo y así evitar que pierda el equilibrio y sufra una caída, cuando esto no es
suficiente se puede bajar una extremidad más para que no se pierda la estabilidad.  
De las estrategias presentadas en este documento se puede concluir:
Para llevar a cabo las estrategias de adaptabilidad al terreno, es necesaria
solamente la información de los sensores internos. Es decir, no es fundamental instrumentar con sensores ultrasónicos, una cámara, un GPS u otro  
tipo de sensor para lograrlo; ya que conociendo la posición actual tanto de  
la punta de las extremidades como del centro del tórax se puede reorientar  
al robot con el uso exclusivo del cálculo de la cinemática inversa, lo que  
permite tener un sistema más económico que los presentados en los robots  
desarrollados en Loc et al. (2010) y Taniwaki et al. (2008) y el robot DLR
clawer presentado en Gorner and Stelzer ¨ (2013).
Las estrategias fueron resueltas a través de un modelo geométrico, por lo  
que el consumo de recursos computacionales en cálculos es bajo, pues el  
más pesado de ellos es la cinemática directa. Resolver las estrategias mediante un modelo geométrico permite aprovechar el modelo cinemático directo e inverso del robot, las alturas de las extremidades se pueden conocer
teniendo como entrada únicamente la posición angular de cada motor.  
Se estudiaron varios criterios de estabilidad tales como el método de proyección del CG, el margen de estabilidad estático, el margen de estabilidad
longitudinal, el margen de estabilidad longitudinal y deriva, margen
energético de estabilidad   y el margen de estabilidad energético normalizado por el peso (SNE); siendo este último el seleccionado pues puede ser  
evaluado apropiadamente cuando ocurre una perturbación, además de que  
esta expresado en   mm y no en Jouls con el margen energético de estabilidad. Otro punto importante es que el SNE coincide con el método de proyección del CG cuando el robot se encuentra posicionado sobre un plano  
horizontal igual que las estrategias de adaptabilidad también trabaja bajo un  
modelo geométrico. El SNE como se mencionó en la sección correspondiente está basado en un modelo geométrico al igual que las estrategias de adaptabilidad; de esta manera se puede saber si la postura actual es o no estáticamente estable de una manera muy sencilla.  
Se desarrolló una interfaz gráfica de usuario en LabView para manipular  
al robot remotamente en la que este tiene cierta autonomía pues el usuario
tiene la capacidad de realizar paradas de emergencia o ayudar al robot a
llegar a su meta. Los cálculos necesarios para las estrategias se realizaron  
en Matlab, por lo que cada estrategia manda llamar solamente la función 
que necesita sin necesidad de ejecutarlas todas al mismo tiempo, lo que
hace que sea más rápida la repuesta de la estrategia ante la perturbación en  
el terreno.
Se garantizó que las estrategias de adaptabilidad fueran estáticamente estables, gracias a que cada una de las posturas se evaluó correctamente con el criterio de estabilidad SNE
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO   NACIONAL
CENTRO DE INVESTIGACIÓN EN   CIENCIA APLICADA Y
TECNOLOGÍA AVANZADA
UNIDAD QUERÉTARO  
POSGRADO EN TECNOLOGÍA AVANZADA
ESTRATEGIAS DE ADAPTABILIDAD
ESTÁTICAMENTE ESTABLES AL CAMBIO DE  
TERRENO PARA EL ROBOT CAMINANTE
HEXÁPODO HEX-PIDERIX  
TESIS
QUE PARA OBTENER EL GRADO DE
DOCTOR EN TECNOLOGÍA AVANZADA
PRESENTA
M.C. XOCHITL YAMILE SANDOVAL CASTRO
DIRIGIDA POR
DR. EDUARDO CASTILLO CASTAÑEDA ˜
SANTIAGO DE QUERÉTARO QRO. AGOSTO 2015
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Modelado, simulación y optimización de modelos de comportamiento con memoria de amplificadores de potencia para RF.
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El incremento en potencia es crucial para alcanzar la relación señal a ruido deseada en el
receptor, ya que la señal no pudiera ser detectable [42]. Para hacer posible la comunicación
inalámbrica con un amplio ancho de banda es necesario basarse en esquemas de multiplexación
complejos como Acceso Múltiple por División de Código de Banda Ancha (WCDMA) y
Multiplexación por División de Frecuencias Ortogonales (OFDM), y otras técnicas de modulación
como 16-, 32-, 64-, 128- Modulación de Amplitud en Cuadratura (QAM) o incluso 256-
Modulación por Desplazamiento de Fase (QPSK). Al momento de transmitir altas tasas de datos
se produce un recrecimiento espectral cuando se agregan efectos de memoria y no linealidades
debido a las características intrínsecas del PA.
El PA es el dispositivo en la etapa del transmisor que agrega la mayoría de las no linealidades y
efectos de memoria. Existen esfuerzos en la investigación basados en polinomios y modelos que
toman en cuenta la memoria y otros basados en redes neuronales. Sin embargo, no existe una
plataforma completa capaz de emular en hardware comportamientos adecuados de PAs donde
parta de las mediciones de las curvas de distorsión AM/AM y AM/PM y otorgue una integración
de modelos que permitan niveles de no linealidad y orden de memoria variables con modelos
polinomiales con memoria y de aprendizaje, además de algoritmos adaptativos que permitan
modelar PAs con comportamientos más complejos.
El PA es el componente principal en el consumo de energía en la etapa del transmisor y es
necesario establecer un compromiso entre linealidad y su eficiencia. Para lograr este propósito
es crucial contar con modelos de comportamiento donde se tenga un nivel variable de memoria
y de no linealidad.
La linealización de PAs se ha convertido en un área de gran interés en los sistemas de
comunicación inalámbrica modernos. Para ellos se requieren modelos de comportamiento que
describan fielmente el funcionamiento del PA. Cabe indicar que existen diversas técnicas para
lograr este proceso de linealización, lo cual contribuye a lograr una eficiencia en potencia del PA
al trabajarlo linealmente. Por mencionar algunos métodos comúnmente utilizados se encuentra
la prealimentación, la retroalimentación o la predistorsión analógica [43]. Los modelos
mencionados son eficientes para aplicaciones de baja frecuencia, pero a frecuencias altas, estas
técnicas pueden sufrir pérdidas de estabilidad [2].
Los efectos producidos por las no linealidades en los PAs repercuten de dos maneras, por un
lado causan recrecimiento espectral, dando lugar a interferencias en los canales de transmisión
adyacentes (distorsión fuera de banda de operación asignada), mientras que por otro lado,
causan distorsión dentro de la propia banda de transmisión, degradando por tanto la Tasa de
Error de Bits (BER) [9]. Los organismos reguladores fijan a través de los diferentes estándares de
comunicaciones, los niveles máximos permitidos de emisión fuera de banda, así como de los límites de distorsión en la banda, por ejemplo especificando porcentajes máximos de error en
las constelaciones. Las sanciones por violar cualquiera de las especificaciones como son
potencia de transmisión, emisión fuera de banda entre otras son multas elevadas.
Las series de Volterra pueden ser usadas para describir un sistema estable no lineal con pérdida
de memoria con un error arbitrario pequeño. Sin embargo, las desventajas principales son el
incremento dramático del número de parámetros con respecto al orden de no linealidad y
profundidad de memoria lo cual causa un incremento drástico de la complejidad de cálculo e
identificación de parámetros. Esta es la razón por la que resulta altamente impráctico el uso de
las series completas de Volterra para representar matemáticamente el comportamiento de
sistemas con alta no linealidad y profundidad de memoria por lo que se requiere el uso de
truncaciones como MPM, Wiener y Hammerstein donde el costo computacional y de cálculo de
los kernels de cada modelo sea reducido en comparación con las series completas.
Aunado al proceso de modelado de comportamiento del PA y de un sistema completo que
incluya diversos modelos de comportamiento, es necesario desarrollar técnicas de medición de
bajo costo, donde métodos tradicionales de medición a través de un VNA no sean
indispensables al realizar un proceso de linealización. La medición en fase del parámetro de
dispersión S21 provee la información de la distorsión en fase del PA. Sin embargo como se
menciona involucra un costoso uso de equipo de laboratorio, además de instalación y
calibración previa, donde involucra diversas consideraciones para un método tradicional de
medición [36].
Todo lo anterior nos lleva a la necesidad de contar con un sistema digital completo que pueda
modelar las curvas de distorsión de un PA, estimar su comportamiento si el PA es utilizado como
DUT, realizar el proceso de modelado y utilizar algoritmos de aprendizaje como ANN y ANFIS
dado que se obtuvo comportamiento complejos de PAs con una infinidad de datos a estimar. Y
además estimar la distorsión en fase del PA usando la teoría de transformación de amplitud a
fase.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Realizar contribuciones científicas al estado del arte en el modelado, simulación, medición e
implementación de PAs que optimicen los procesos de diseño, empleando técnicas
fundamentadas en las series de Volterra y que sean explotados eficientemente en ambientes de
simulación circuito y sistema.
Los objetivos particulares que se persiguen metódicamente en este trabajo de investigación,
son:
 Revisión bibliográfica de modelos de comportamiento de PA sin memoria polinomiales,
cuasi sin memoria como Saleh y Ghorbani; y con memoria como el MPM, Wiener y
Hammerstein y técnicas de aprendizaje como ANN y ANFIS.
 Síntesis de trabajos basados en modelos de PAs sin memoria, cuasi sin memoria y con
memoria como MPM, Wiener y Hammerstein y técnicas de aprendizaje como ANN y
ANFIS.
 Implementación en hardware del MPM y modelado a través de ANN con la finalidad de
predecir el comportamiento de un PA.
 Modelado de PAs reales basados en sus curvas de distorsión AM/AM y AM/PM además
del MPM así como implementación en hardware para enlazar el proceso de
Matlab/Simulink con la tarjeta FPGA.
 Optimización de modelos comportamiento de Pas basados técnicas de aprendizaje como
ANN y ANFIS.
 Desarrollo de una metodología para estimar la medición AM/PM de un PA basado en la
teoría de transformación amplitud a fase. </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Este trabajo de tesis se desarrolla debido a la necesidad de un sistema completo capaz de
predecir los efectos indeseables de memoria en un PA con profundidad de memoria y órdenes
de no linealidad variables. Para ello se desarrollan modelos polinomiales y basados en
truncaciones de las series de Volterra como Wiener, Hammerstein y MPM, además de utilizar
técnicas de aprendizaje como ANN y ANFIS dentro de una tarjeta de desarrollo FPGA/DSP, y una
técnica de medición de bajo costo de las curvas de distorsión en amplitud y en fase.
El sistema es controlado por una Interfaz Gráfica de Usuario (GUI) la cual permite de manera
simple abrir las mediciones reales de cualquier PA basado en sus curvas de distorsión AM/AM y
AM/PM y obtener los coeficientes respectivos del MPM obtenidos a través del método Error
Cuadrático Mínimo (LSE).
El sistema tiene niveles variables de memoria y órdenes de no linealidad variables para
adecuarse al comportamiento del PA. La parte principal del trabajo está basada en la
herramienta de diseño DSP Builder la cual permite traducir un programa de sistema como
Matlab/Simulink a VHDL ahorrando tiempo de diseño. En la parte final se realiza una
comparación con modelado a través de VHDL puro, y modelado obtenido a través de ANN y
ANFIS.
La ventaja de utilizar este sistema de simulación es el uso de algoritmos para etapas de
linealización posteriores. En trabajos desarrollados como autor se probó la eficiencia de los
modelos a través de FPGA [44-46].</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se demostró un modelado apropiado para PAs de RF basado en las curvas de distorsión AM/AM
y AM/PM, las cuales parten de un banco de medición. El sistema fue primeramente diseñado a
través de VHDL, sin embargo su complejidad mostrada motivó al uso de truncaciones especiales
de las series de Volterra para el modelado de altos órdenes de no linealidad y profundidad de
memoria. La etapa de modelado es implementada a través de la tarjeta de desarrollo, que
basado en la herramienta DSP Builder se mejora la precisión en los resultados obtenidos en
VHDL. Este trabajo de tesis realiza una comparación de resultados obtenidos en VHDL contra
DSP Builder, dando como resultado una novedosa técnica para la implementación basada en
FPGA para Pas de RF, reduciendo el tiempo de diseño.
El MPM como un caso especial de las series de Volterra permite incluir los efectos de memoria
de un PA y órdenes de no linealidad variables, dicho modelo permite un cálculo más rápido de
los kernels involucrados en el análisis. La herramienta de diseño inicia de las curvas de
distorsión reales de un PA de RF. El MPM se implementó en el Kit de desarrollo de DSP/FPA
Cyclone III Edición-ALTERA. El proceso involucrado durante el modelado es una aproximación
que nos guía a utilizar una herramienta flexible como el DSP Builder y comúnmente utilizada en
el estado del arte actual en relación al modelado de Pas de RF. Sin embargo, se encontró que la
resolución de 14 bits de la tarjeta de adquisición HSMC no es suficiente para representar un
resultado apropiado en un modelo complejo de PA lo que demuestra las ventajas del uso de
esta herramienta de diseño.
En la sección de técnicas de aprendizaje de modelos se abordó inicialmente la ANN, la cual se
basa en modelos establecidos como los datos de las curvas de distorsión y se entrena para
seguir dicho comportamiento, en este caso el ANN y ANFIS. En este trabajo se realizó simulación
de PAs y aprovechando las características de rapidez computacional de estos modelos de
aprendizaje se llevó a cabo la implementación de la misma forma que el MPM en la tarjeta de
desarrollo DSP/FPGA. Ambas arquitecturas tanto la ANN y ANFIS logran establecer una
metodología para el diseño y verificación de modelos y en todos los casos se tiene una GUI para
controlar totalmente las variables que interfieren en el análisis. Además fue posible realizar una
comparación de estas técnicas con las tradicionalmente utilizadas derivadas de las series de
Volterra.
En este trabajo de tesis se presentó también una propuesta de medición de AM/AM y AM/PM
de bajo costo, el sistema parte de una etapa digitalizada de un PA y queda lista la plataforma
para utilizar un PA como DUT. En la etapa inicial se abordó una técnica tradicional para medir las
curvas AM/AM y AM/PM basadas en un VNA, de igual forma se cita una técnica de bajo costo
basada en una PC. Este trabajo propone una técnica de medición basada en simulación y
emulación en hardware en la tarjeta de desarrollo FPGA. Se realizó una comparación de los resultados obtenidos en el proceso de amplificación y se obtuvo una representación muy
cercana a las curvas esperadas de diversos PAs de RF.
La integración del sistema desarrollado en este trabajo permite el uso de técnicas de
aprendizaje como ANN y ANFIS las cuales se pueden aplicar a modelos de comportamiento más
complejos o con una gran gama de datos.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN Y DESARROLLO DE TECNOLOGÍA
DIGITAL
DOCTORADO EN CIENCIAS EN
EN SISTEMAS DIGITALES
“MODELADO, SIMULACIÓN Y OPTIMIZACIÓN DE MODELOS DE
COMPORTAMIENTO CON MEMORIA DE AMPLIFICADORES DE POTENCIA
PARA RF”
TESIS
QUE PARA OBTENER EL GRADO DE
DOCTOR EN CIENCIAS
P R E S E N T A:
M.C. JOSÉ RICARDO CÁRDENAS VALDEZ
BAJO LA DIRECCIÓN DE:
DR. JOSÉ CRUZ NÚÑEZ PÉREZ
DR. CHRISTIAN GONTRAND
JULIO DE 2015 TIJUANA, B.C., MÉXICO</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Comportamiento a la fatiga en uniones de aluminio 6061-T6 con tratamiento térmico posterior a la soldadura	</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Evaluar la resistencia a la fatiga y crecimiento de grietas bajo la aplicación de una carga cíclica
(fatiga uniaxial) en una unión soldada de aluminio 6061-T6 por medio del proceso de soldadura
AEIM con tratamiento térmico post-soldadura</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible modificar la microestructura de una unión de aluminio 6061-T6 por medio del
tratamiento térmico, debido a que los elementos presentes en el electrodo ER4043 (material de
aporte), Al y Si se diluyen con el material base. Con lo anterior, es posible la generación de
intermetálicos de Si y Mg mediante la técnica AEIM y obtener un incremento en la resistencia
mecánica con un TTPS.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Existen elementos estructurales y mecánicos utilizados en Ingeniería como estructuras, chasis
de autos, componentes aéreos, estructuras marinas, válvulas, etc., son fabricados con la
aleación de aluminio 6061-T6.
Para la fabricación de dichos elementos se requieren procesos de unión, uno de ellos es la
soldadura. La soldadura es un proceso metalúrgico que requiere el entendimiento del
comportamiento del material durante su producción ya que la mayoría de los procesos de
soldadura, al igual que en la fundición de metales, requieren de altas temperaturas para hacer
posible la unión de los materiales involucrados.
En la aleación de aluminio 6061-T6 se presenta una degradación en las propiedades mecánicas
debido al efecto térmico del proceso de soldadura. Además, la alta solubilidad del hidrógeno en
el aluminio líquido que cuando ocurre el proceso de solidificación queda atrapado, generando
porosidad en el material.
También se sabe que es posible reducir el efecto térmico ya que con la técnica AEIM [23] al
utilizar un solo paso de soldadura reduce el efecto en la microestructura final. Para este caso la
aleación a unir se encuentra en un estado de solubilización y envejecido artificial (-T6) pero
cuando alcanza temperaturas muy elevadas se obtiene un sobreenvejecido en la aleación, lo
cual disminuye las propiedades mecánicas. Sin embargo, el tratamiento térmico T6 es un
proceso reversible, por lo tanto, se puede considerar como alternativa para mejorar las
propiedades mecánicas de las uniones de aluminio soldadas con un tratamiento térmico postsoldadura.
Una mejora en las propiedades mecánicas de uniones de aluminio soldadas se aprecia en un
incremento en la resistencia a la fatiga para elementos estructurales que son sometidos a cargas
cíclicas. Al incrementar la resistencia a la fatiga se retarda la aparición de grietas por fatiga. Por
lo anterior surge la necesidad de hacer uniones soldadas que permitan aumentar la resistencia a
la fatiga y retardar el crecimiento de grietas por fatiga en aleaciones de este tipo con la
implementación de la técnica AEIM y un tratamiento térmico post-soldadura</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El porcentaje de dilución obtenido por la técnica AEIM (45.7%) en la zona de fusión, favoreció
la composición química necesaria (Al-Si-Mg) para poder endurecer la zona de fusión con un
tratamiento térmico de solubilización y envejecido artificial logrando un incremento en la
resistencia a la tensión y la dureza. Además, en la ZAT, generada por el proceso de soldadura
fue posible recuperar las propiedades mecánicas que el material tenía antes del proceso de
soldadura.
Por medio de mapas y perfiles de dureza fue posible identificar observar la recuperación de las
propiedades mecánicas en la ZAT. Además, se observó el incremento de dureza en la zona de
fusión, logrando alcanzar valores cercanos a los 120 HV0.1, mientras que en el resto del
material la dureza tiene valores aproximados a 125 HV0.1. Esto se explica al evaluar el
contenido de Mg y Si en la zona de fusión a través de análisis por EDS. Este análisis reveló que
el contenido de Mg (~1.25% en peso), decrece conforme se acerca al centro de la zona de
soldadura (~0.8% en peso), mientras que el contenido de Si (~0.5% en peso), decrece conforme
se acerca al centro de la zona de soldadura (~4.5% en peso).
Las propiedades a la tensión de las uniones de aluminio con TTPS fueron recuperadas con el
tratamiento térmico logrando un módulo de elasticidad y un esfuerzo de fluencia prácticamente
iguales al material base (E = 68 MPa y 270 MPa). Sin embargo, prevalecen las
condiciones de fragilidad (baja ductilidad ~4.5%) debido al efecto de la porosidad.
En fatiga uniaxial de alto número de ciclos, se observó que la vida a la fatiga en uniones de
aluminio soldadas por AEIM + TTPS se reduce un 30% a
respecto al material base.
Además, la vida a la fatiga en uniones soldadas por AEIM sin refuerzo y uniones soldadas por
AEIM con refuerzo se reduce un 48.7% y un 64.1% a
respecto al material base. Esto
se debe a que la porosidad limita la vida a la fatiga, es decir, entre más defectos superficiales
existan, menor va a ser la vida a la fatiga.
Respecto al crecimiento de grietas por fatiga, se pudó observar una mejoría respecto al material
de uniones de aleación de aluminio con TTPS y uniones en condición de soldadura. Al hacer un
comparativo entre el exponentes m, se tiene un valor de 2.1 y 12.4 para uniones de aleación de
aluminio con TTPS y uniones de aluminio en condición de soldadura respectivamente. El valor de m = 2.1 indica que existen factores benéficos que retardan el crecimiento de grieta. Entre
esos factores se encuentran: a) endurecimiento de la zona de fusión por solubilización y
envejecido artificial, b) formación de defectos debido al proceso de soldadura (porosidad) que
actúan como disipadores de esfuerzo y hacen que la grieta recorra más distancia y c) El
acomodo irregular de la microestructura, ya que cuando la grieta avanza y encuentra granos con
un acomodo con un ángulo diferente la grieta deja de avanzar hasta que logra atravesar el plano
y continuar su propagación.
Finalmente, el análisis de las superficies de fractura revela que el espaciamiento de las estrías
mantiene una correcta correspondencia con la rapidez de propagación de grietas por fatiga. Con
las imágenes obtenidas por MEB se pudo cuantificar ese espaciamiento (~0.3-0.4 m), y al
relacionarlo con el exponente m = 2.1 para uniones de aleación de aluminio con TTPS se
observa que a menor velocidad de crecimiento, menor es el espaciamiento entre estrías.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
Centro de Investigación e Innovación Tecnológica
COMPORTAMIENTO A LA FATIGA EN UNIONES DE ALUMINIO
 6061-T6 CON TRATAMIENTO TÉRMICO POSTERIOR A
LA SOLDADURA
T E S I S
QUE PARA OBTENER EL
G R A D O D E
DOCTORADO EN TECNOLOGÍA AVANZADA
P R E S E N T A
M. en C. JAVIER SERRANO PÉREZ
ASESORES:
Dr. Ricardo Rafael Ambríz Rojas Dr. David Jaramillo Vigueras
JULIO 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Medición de deformación mecánica y torsión en fibras ópticas monomodo	</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Uno de los principales problemas que los sistemas normales de sensado en estructuras
presentan es que son puntuales y necesitan cableado y protección que resulta pesado,
incrementado el peso total a la estructura. Además se emplea cableado para alimentación de
energía o en un mejor caso se emplean baterías cuando se utiliza comunicación
inalámbrica. Aunado a esto, en ocasiones se necesita colocar sensores en lugares de difícil
acceso, así como formar una red de sensores para buscar una mayor exactitud. Lo anterior
involucra un sistema de adquisición limitado por el número de canales, que en caso de
requerir de una mayor cantidad de sensores, se necesitará de varios sistemas de adquisición.
Es necesario entonces, estudiar las técnicas actuales de sensado que emplean diferentes
tipos de fibras ópticas, tanto estándar como dopadas de tierras raras y el empleo de métodos
menos complejos que permitan un sensado confiable.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Modelar y diseñar sensores basados en fibras ópticas monomodo estándar (SMF-28) y
dopadas con Erbio para detectar deformación mecánica y torsión utilizando el efecto no
lineal de esparcimiento Brillouin y rejillas de periodo largo.
Objetivos particulares
a) Estudiar y analizar 3 tipos de sensores basados en fibra óptica para medir la
deformación mecánica y torsión en estructuras civiles, mecánicas o aeronáuticas. Se
propone el uso de fibras adelgazadas y fibras dopadas para observar si se mejora la
sensibilidad en sensado. Los tres tipos de sensores se basan en:
I. Esparcimiento Brillouin.
II. Esparcimiento Raman.
III. Rejillas de periodo largo.
b) Aplicación de los sensores desarrollados para el sensado de deformación mecánica y
presión en estructuras civiles, mecánicas o aeronáuticas.
c) Simulación de los sensores arriba mencionados mediante el software MATLAB.
d) Pruebas de tensión y torsión en el laboratorio que validen los modelos analizados.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Si se emplean los fenómenos que provocan pérdidas de señal en las fibras ópticas llamados
esparcimiento Brillouin, esparcimiento Raman e incluso otro dispositivo llamado Rejilla de
Periodo Largo en el modelado y diseño de sensores ópticos, mejorando su funcionamiento
mediante el empleo de diferentes tipos de fibras ópticas como por ejemplo fibras dopadas
y/o fibras adelgazadas (tapers), se obtendrá un incremento en la sensibilidad en la respuesta
de las lecturas de torsión.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los sensores basados en fibra óptica ofrecen algunas ventajas sobre los sensores más
tradicionales tales como inmunidad a interferencias electromagnéticas, son ultraligeros,
pueden ser empleados en ambientes altamente corrosivos o explosivos, y su uso en
aplicaciones para sensado remoto es factible debido a las bajas pérdidas de señal que las
fibras ópticas presentan.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Inicio
Estudio de los fenómenos no lineales Raman y Brillouin
Estudio de las rejillas de periodo largo mecánicas sobre fibras estándar y dopada de Erbio
Experimentación sobre los fenómenos no lineales y las rejillas
¿La experimentación corresponde a lo estudiado?
Estudio de las ecuaciones adecuadas para la realización de la simulación
Validación de la experimentación empleando el software Matlab
¿Se cumple la validación?
Sensor de FO
Final
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Son muchas las ventajas que los sensores basados en fibra óptica presentan tales como
inmunidad a interferencias electromagnéticas, ultraligeros, se pueden emplear en ambientes
altamente corrosivos, capaces de trabajar a altas temperaturas y seguros en ambientes
explosivos. Pueden ser empleados como sensores distribuidos o puntuales dependiendo de
la aplicación.
En este trabajo se explicaron los conceptos sobre los fenómenos de esparcimiento Brillouin
espontáneo y estimulado. Las ecuaciones presentadas describieron el comportamiento de la
velocidad acústica en función del módulo de Young y la razón de Poisson. Se comprobó
mediante simulación que ambos parámetros se modificaban con respecto a estímulos
exteriores y por tanto, la velocidad acústica también sufría cambios.
Por ejemplo se obtuvieron incrementos de la velocidad acústica de 5900 hasta 6600 m/s,
para variaciones de la razón de Poisson. Sin embargo, para variaciones del módulo de
Young, los valores de velocidad acústica sufrían cambios menores, de 5900 hasta 6000 m/s.
El comportamiento de la velocidad acústica es importante porque tiene una relación directa
con el corrimiento en frecuencia Brillouin y este es sensible a cambios externos. Estos
cambios significan que habrá un corrimiento detectable de la frecuencia Brillouin con lo
cual es posible relacionar tensión contra este corrimiento, siendo este el principio del
sensor. Así entonces, las variaciones en frecuencia Brillouin fueron de 11.9 a 12.35 GHz
para una longitud de onda λ = 1.55 µm. Se ilustró la manera de obtener sensado distribuido
empleando éste fenómeno, el cual se visualizó de manera experimental en el laboratorio. 
Otro de los fenómenos de esparcimiento estudiados que se presentan en las fibras ópticas es
el de Raman espontáneo y estimulado. Este fenómeno es sensible a cambios de temperatura
por lo que usualmente es empleado para fabricar sensores de temperatura. Se estudiaron las
ecuaciones involucradas para emplear este fenómeno y se visualizó de manera experimental
en el laboratorio empleando un laser de alta potencia. Para cuestiones de sensado se emplea
la ecuación que relaciona las señales de Stokes y anti-Stokes descritas anteriormente, y con
ésta relación se obtiene la temperatura absoluta. Se obtuvieron entonces para variaciones de
temperatura de 300 hasta 800 0K un incremento de esta razón dando como resultado una
temperatura absoluta de 0.7 hasta 2.7. Estos datos son importantes puesto que relacionan
incrementos de temperatura externos contra variaciones del fenómeno de esparcimiento
Raman presente en la fibra óptica.
Entre los distintos tipos de sensores basados en fibras ópticas se estudiaron también los
sensores de rejillas de periodo largo. La mayoría de los sensores de rejillas se fabrican con
radiación ultravioleta la cual es una técnica laboriosa y de alto costo. En esta tesis se
propone el uso de rejillas de periodo largo inducidas mecánicamente. Las rejillas mecánicas
son de fácil fabricación y bajo costo. Se mostró con trabajo de laboratorio cómo es posible
obtener una gran variedad de éstas con solo modificando el periodo de rejilla. Una ventaja
muy atractiva de este tipo de rejilla es que si se retira ésta, la fibra óptica adquiere sus
condiciones físicas y mecánicas originales. En este trabajo se comprobó también la
fabricación de rejillas mediante la técnica de arco eléctrico. Ambas técnicas han mostrado
ser más fáciles de implementar y de más bajo costo. 
Se realizaron experimentos de laboratorio empleando las rejillas mecánicas sobre fibras
monomodo estándar SMF-28 que ya han sido muy estudiadas y reportadas. Se propuso en
esta ocasión emplear fibras ópticas monomodo dopadas de Erbio para realizar pruebas de
tensión y torsión en el laboratorio empleando rejillas de periodo largo inducidas
mecánicamente con el fin de determinar si la sensibilidad se incrementaba. Se empleó una
rejilla mecánica con un periodo de 500 µm sobre 10 cm de longitud de fibra dopada de
Erbio. Los resultados confirmaron la hipótesis de una mayor sensibilidad, y estos resultados
fueron reportados en un artículo de revista. Se obtuvieron corrimientos de longitud de onda
de resonancia con pruebas de torsión de hasta 25 nm para una torsión de hasta 35 radianes.
Comparados estos datos contra los obtenidos con fibra estándar monomodo de hasta 9 nm
para un mismo número de radianes se comprueba una mayor sensibilidad empleando fibra
dopada de Erbio.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN E INNOVACIÓN TECNOLÓGICA
“Medición de deformación mecánica y torsión en fibras
ópticas monomodo”
Tesis para obtener el grado de:
Doctora en Tecnología Avanzada
Presenta
M. en C. María Guadalupe Pulido Navarro
Director de Tesis
Dr. José Alfredo Álvarez Chávez
Enero 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Doctorado</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SegmentaciÃ³n de imÃ¡genes digitales sobre la
base de la informaciÃ³n integral del color</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En visiÃ³n artificial, la segmentaciÃ³n de imÃ¡genes es el proceso de particiÃ³n de una imagen digital en mÃºltiples segmentos (conjuntos de pÃ­xeles, tambiÃ©n conocidos como
superpÃ­xeles). El objetivo de la segmentaciÃ³n es simplificar y / o cambiar la representaciÃ³n de una imagen en algo que es mÃ¡s significativo y fÃ¡cil de analizar. La segmentaciÃ³n de imÃ¡genes se utiliza normalmente para localizar objetos y bordes (regiones, lÃ­neas, curvas, etc.) en imÃ¡genes. MÃ¡s precisamente, la segmentaciÃ³n de imÃ¡genes es el proceso de asignar una etiqueta a cada pÃ­xel de una imagen de modo que los pÃ­xeles con la misma etiqueta comparten ciertas caracterÃ­sticas comunes [47].
El resultado de la segmentaciÃ³n es un conjunto de segmentos que cubren colectivamente toda la imagen o un conjunto de contornos extraÃ­dos de la imagen (ver detecciÃ³n de bordes). Cada uno de los pÃ­xeles de una regiÃ³n es similar con respecto a alguna caracterÃ­stica o propiedad calculada, como el color, la intensidad o la textura. Las regiones adyacentes son significativamente diferentes con respecto a la misma caracterÃ­stica (s) [47].
â€œLa segmentaciÃ³n de una imagen consiste en la particiÃ³n de dicha imagen en diferentes
regiones similares en alguna caracterÃ­stica predefinida (â€¦) Los seres humanos usamos
nuestro sentido visual para separar nuestro entorno inmediato en objetos distintos sin
esfuerzo alguno para reconocerlos, guiar nuestros movimientos y para casi todas las
actividades de nuestra vida.â€ (K.N., Plataniotis, y A.N.Venetsanopoulos, 2000, p. 237, traducido).
La segmentaciÃ³n es una caracterÃ­stica importante de la percepciÃ³n visual humana que se manifiesta de forma espontÃ¡nea y natural; aunque estÃ¡n involucrados procesos complejos
como el anÃ¡lisis de color, la forma, el movimiento, la textura, etc. [1][2][3].
El color es una caracterÃ­stica visual poderosa y robusta para diferenciar los diferentes objetos en una imagen. Es una importante fuente de informaciÃ³n en el proceso de segmentaciÃ³n y puede en muchos casos ser utilizado como Ãºnica caracterÃ­stica y Ãºnico descriptor para la segmentaciÃ³n de los objetos de interÃ©s [1][2][3].
En este trabajo de tesis se busca investigar sobre un mÃ©todo propio de segmentaciÃ³n del
color en imÃ¡genes de color. El mÃ©todo sÃ³lo utiliza la informaciÃ³n de color para realizar la segmentaciÃ³n, por lo que se considera como un indispensable paso previo al de
reconocimiento. En este trabajo no se pretende llegar al reconocimiento de objetos o clases de objetos, sino Ãºnicamente a la segmentaciÃ³n de la imagen en regiones homogÃ©neas de color que podrÃ­an ser utilizadas para alimentar a tareas subsecuentes.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>1.5 Objetivo general
Lograr la segmentaciÃ³n del color en imÃ¡genes digitales mediante el uso de la informaciÃ³n del color de forma integral, mediante una funciÃ³n de similitud en el espacio de color propuestos.
1.6 Objetivos particulares
a) Caracterizar el algoritmo de segmentaciÃ³n propuesto;
b) Evaluar el algoritmo de segmentaciÃ³n del color propuesto;
c) Obtener resultados cuantificables que permitan ubicarlo en el estado del arte.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Al procesar la informaciÃ³n de color como una unidad, mediante la funciÃ³n de similitud y el espacio de color propuestos, se evitarÃ¡ la pÃ©rdida de informaciÃ³n de color durante la segmentaciÃ³n.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En los Ãºltimos aÃ±os se ha dedicado considerable esfuerzo al problema de la segmentaciÃ³n del color en imÃ¡genes digitales dada su importancia y potencial. Hasta hace poco, la mayorÃ­a de los enfoques publicados para la segmentaciÃ³n del color se basaba en tÃ©cnicas monocromÃ¡ticas aplicadas a cada componente de color de la imagen, en diferentes espacios de color (RGB u otros) y en diferentes formas, para producir una composiciÃ³n. Estos enfoques tienen el problema inherente de una pÃ©rdida significativa de la informaciÃ³n de
color durante el proceso [3][4].</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El mÃ©todo de segmentaciÃ³n propuesto en esta tesis se basa en el cÃ¡lculo de una funciÃ³n de
similitud de color (tambiÃ©n propuesta) para cada pÃ­xel de la imagen de color (representado en el modelo de color RGB con 24 bits) para formar lo que llamamos una imagen de similitud de color (ISC), la cual es una imagen de escala de grises. Una imagen en colores suele contener millones de colores, y muchos miles de ellos representan el mismo color percibido de un objeto, pero con variaciones debido a la presencia de ruido aditivo, la falta de definiciÃ³n entre las fronteras de color y regiones, sombras en la escena, la resoluciÃ³n espacial del sistema de visiÃ³n humano, etcÃ©tera. La funciÃ³n de similitud de color propuesta permite la agrupaciÃ³n de los muchos miles de colores, en una sola imagen de salida en tonos de grises [3][5][31][32][34][35].
La ISC es entonces umbralada automÃ¡ticamente y la salida puede ser utilizada como una
capa de segmentaciÃ³n, o puede ser modificada posteriormente con operadores morfolÃ³gicos
para introducir caracterÃ­sticas geomÃ©tricas si Ã©stas son necesarias.
La generaciÃ³n de una ISC sÃ³lo requiere del cÃ¡lculo de la ecuaciÃ³n (1) (mÃ¡s adelante) para cada pÃ­xel de la imagen de entrada RGB. AsÃ­, la complejidad es lineal con respecto al
nÃºmero de pÃ­xeles en la imagen de origen y por esa razÃ³n el algoritmo es computacionalmente econÃ³mico.
En primer lugar, se calcula el centroide de color y la desviaciÃ³n estÃ¡ndar de color de una pequeÃ±a muestra que consiste en unos pocos pÃ­xeles (menos de 10 pÃ­xeles por color) del color que se desea segmentar. El centroide calculado representa el color deseado a ser segmentado mediante la tÃ©cnica propuesta para tal fin. Entonces, la funciÃ³n de similitud de color utiliza la desviaciÃ³n estÃ¡ndar de color calculada a partir de la muestra de pÃ­xeles para adaptar el nivel de dispersiÃ³n del color en las comparaciones [3][5][31][32][34][35].
El resultado del cÃ¡lculo particular de la funciÃ³n de similitud para cada pÃ­xel y el centroide de color (es decir, la medida de similitud entre el pÃ­xel y el valor representativo del color) genera la ISC. La generaciÃ³n de esta imagen es la base del mÃ©todo propuesto en esta tesis, la cual conserva la informaciÃ³n del color seleccionado de la imagen en colores original. La ISC es una representaciÃ³n discreta en el rango 0-255 de una funciÃ³n continua, cuyos valores estÃ¡n en el rango normalizado [0-1]. La ISC puede ser umbralada con cualquier mÃ©todo de umbralado automÃ¡tico. Para obtener los resultados presentados en esta investigaciÃ³n se utiliza el mÃ©todo de umbralado de Otsu [25][27].
Para generar una ISC se necesita como entrada: 1) Una imagen en colores representada en el modelo de color RGB de 24 bits (formato de color verdadero) y 2) Una muestra de pÃ­xeles seleccionados arbitrariamente que forman la muestra del color que se desea segmentar. De esta muestra de pÃ­xeles se calculan los indicadores estadÃ­sticos de acuerdo al modelo modificado de color HSI mediante los mÃ©todos propuestos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los resultados obtenidos durante la investigaciÃ³n demostraron que la funciÃ³n adaptativa de similitud de color y el mÃ©todo supervisado de segmentaciÃ³n de los colores propuesto, ofrecen una alternativa Ãºtil y eficaz para la segmentaciÃ³n de objetos (o regiones) con diferentes colores en imÃ¡genes de color relativamente complejas, con un buen rendimiento en presencia del inevitable ruido aditivo. El mÃ©todo propuesto discrimina de un modo muy sencillo cualquier tipo de objetos de diferente color independientemente de sus formas y tonalidades.
Por otro lado se presentÃ³ una evaluaciÃ³n cuantitativa y la caracterizaciÃ³n de la funciÃ³n adaptativa de similitud de color propuesta y del algoritmo semiautomÃ¡tico de segmentaciÃ³n de imÃ¡genes en colores obtenido directamente a partir de la funciÃ³n de similitud. Se llevÃ³ a cabo la caracterizaciÃ³n y evaluaciÃ³n mediante la generaciÃ³n de imÃ¡genes sintÃ©ticas con su correspondiente GT. Se calcularon la tasa de verdaderos positivos (VP) y la tasa de falsos
positivos (FP) para cada imagen, con el fin de obtener las grÃ¡ficas ROC de los resultados.
Este sistema es Ãºtil en general para evaluar la calidad del uso de la informaciÃ³n de color
dentro de algoritmos de segmentaciÃ³n.
El comportamiento (caracterizaciÃ³n) se evaluÃ³ mediante la variaciÃ³n del tamaÃ±o, la forma y
el contraste de color, la cantidad de ruido aditivo, el umbral y el nÃºmero de pÃ­xeles tomados en la muestra. DespuÃ©s de un anÃ¡lisis de las correspondientes grÃ¡ficas ROC, el algoritmo mostrÃ³ buen comportamiento en todos los casos de estudio con tasas de VP mayores que el 95% y tasas de FP inferiores al 2%. Se muestra el buen desempeÃ±o del algoritmo, incluso en casos en los que el contraste de color es tan bajo que un observador no encuentra diferencia de color.
TambiÃ©n se demuestra que la caracterizaciÃ³n de la regiÃ³n acromÃ¡tica presentada en este
trabajo de tesis mejora el rendimiento en comparaciÃ³n con los mÃ©todos previamente
publicados, debido a la afectaciÃ³n de la saturaciÃ³n por un factor exponencial en un esfuerzo para modelar mejor la respuesta visual humana en el caso de muy bajo o muy alto brillo. La mejora en la calidad de los resultados es significativa.
Se presentÃ³ un estudio comparativo entre el comportamiento de la tÃ©cnica propuesta que
utiliza una muestra de pÃ­xeles y umbralado automÃ¡tico, y dos tÃ©cnicas de segmentaciÃ³n
comparables en imÃ¡genes en color: 1) la mÃ©trica Euclidiana de los canales de color a* y b* rechazando L* en el espacio de color CIE L*a*b* y 2) un enfoque probabilÃ­stico en a* y
b*. 
Los resultados obtenidos en este trabajo muestran que los dos mÃ©todos que utilizan la
funciÃ³n de similitud de color propuesta obtuvieron buenos resultados en baja y alta
saturaciÃ³n en todos los casos de estudio siendo prÃ¡cticamente inmune a los cambios en el desvanecimiento de sombra, con tasas de Ã©xito superior al 95% de verdaderos positivos
(TP) y la tasa de falsos positivos (FP) (errores) menor que el 3%. El rendimiento del
mÃ©todo que utiliza una sola muestra de pÃ­xeles y que clasifica mediante umbralado automÃ¡tico es equivalente al mÃ©todo que usa muestras de pÃ­xeles tanto de la figura y el
fondo y que clasifica por mÃ¡xima similitud.
Como puede verse por los resultados, los dos mÃ©todos implementados que emplean la
funciÃ³n de similitud de colores en todos los casos superaron: 1) El mÃ©todo que utiliza la
distancia Euclidiana en el espacio de color CIE L*a*b*, pero descartando L*; 2) El mÃ©todo
que usa un enfoque probabilÃ­stico en a* y en b* y 3) La mÃ©trica Euclidiana de los canales
de color RGB usada como referencia.
En las regiones de alta saturaciÃ³n el algoritmo de segmentaciÃ³n que utiliza la distancia Euclidiana en el espacio de color CIE L*a*b* descartando L*, sufriÃ³ errores en todos los casos. Se manifestaron en diferentes grados y en diferentes niveles de sombra difuminada (de menos de 10% al 80%). Se pueden notar tres tipos de tendencias (como se muestra en la figura 33 izquierda) en sectores con 120 grados de diferencia: 1) Subida de la curva bruscamente (flores_0h, _2h _4h y que corresponden a los canales de color RGB) con una alta sensibilidad a la sombra difuminada (superior a la distancia Euclidiana en RGB); 2) Subida lenta (flores_1h y 5h) inferior al caso de la distancia Euclidiana en RGB y 3) Incremento insensible (flor_3h) hasta cerca de 90% de sombra.
TambiÃ©n en las regiones de baja saturaciÃ³n, la segmentaciÃ³n tuvo errores en todos los
casos. Los errores se manifestaron en diferentes grados y en diferentes niveles de sombra desvanecida (del 20% al 80%). Tres tipos de tendencias o simetrÃ­as recurrentes se pudieron notar en sectores con 180 grados de diferencia: 1) Subida de la curva gradualmente (flores_1h y _4h); 2) ElevaciÃ³n abrupta (flores_2h y_5h), y 3) Aumento en un Ã¡ngulo cercano a los 45Âº (flores_0h y _3h).
El mÃ©todo de segmentaciÃ³n que utiliza un enfoque probabilÃ­stico en a* y b* mejorÃ³ en
muchos casos los resultados al usar la mÃ©trica Euclidiana en a* y b*, pero en otros casos no se pudieron obtener mejoras debido a la corrupciÃ³n del mismo espacio L*a*b*.
La corrupciÃ³n de los componentes de color a* y b* debido a los cambios de luminancia ya
se nota en algunas regiones de baja saturaciÃ³n, pero aumenta rÃ¡pidamente en la mayorÃ­a de las regiones de alta saturaciÃ³n de forma irregular. En algunas regiones de alta saturaciÃ³n el espacio de color L*a*b* no es Ãºtil para distinguir cambios sutiles en el tono con la mÃ­nima cantidad de sombra aplicada.
La mayorÃ­a de las regiones de alta saturaciÃ³n son muy sensibles a los cambios de luminancia, ya que los mÃ¡s pequeÃ±os cambios provocan fallos durante la segmentaciÃ³n. De
estos resultados se puede observar que el CIE L*a*b* tiene un rendimiento peor que el
espacio de color RGB en varias regiones de color de alta saturaciÃ³n. Hemos demostrado
que la mejora en la calidad de la tÃ©cnica de segmentaciÃ³n propuesta con sus rÃ¡pidos
resultados es sustancialmente significativa.
Se puede observar que la no consideraciÃ³n del parÃ¡metro de luminancia L* en el cÃ¡lculo de
la distancia Euclidiana y en el enfoque probabilÃ­stico, no hizo a los mÃ©todos inmunes a cambios en la iluminaciÃ³n, de tal modo que una simple sombra podÃ­a alterar la calidad de
sus resultados. TambiÃ©n se pudo notar a partir de los resultados, que los parÃ¡metros a* b* del espacio de color L*a*b* no son independientes del parÃ¡metro L* como uno podrÃ­a suponer.
La segmentaciÃ³n usando la mÃ©trica Euclidiana de los canales RGB fallÃ³ en una forma mÃ¡s
regular empezando a tener problemas en baja saturaciÃ³n con menos del 10% de difuminado
y en alta saturaciÃ³n con menos del 20% de difuminado. Se pueden observar dos tipos de
comportamiento: 1) Incremento abrupto de los errores y 2) Incremento progresivo de los
errores.
El mÃ©todo propuesto de segmentaciÃ³n tambiÃ©n se puede utilizar directamente en imÃ¡genes
en escala de grises sin realizar ningÃºn cambio, brindando buenos resultados. Por el
contrario, los otros mÃ©todos de prueba que utilizan el espacio de color CIE L*a*b*
necesitan incluir el valor de la luminancia L* para realizar la segmentaciÃ³n en ese tipo de imÃ¡genes, ya que los valores a* y b* se mantienen sin cambios en el centro del plano a*b*.
Una tarea adicional por realizar es decidir cuÃ¡ndo el parÃ¡metro de luminancia L* debe ser
considerado en el cÃ¡lculo de la distancia.
El sistema de pruebas diseÃ±ado se puede utilizar ya sea para explorar el comportamiento de una funciÃ³n de similitud en diferentes espacios de color o para explorar diferentes distancias de color (o funciones de similitud) en el mismo espacio de color. En lugar de intercambiar espacios de color en los experimentos, sÃ³lo serÃ­a necesario un cambio de la mÃ©trica (o de la funciÃ³n de similitud) en el mismo espacio.
La corrupciÃ³n de los componentes de color a* y b* debido a los cambios de luminancia ya
se hace notable en algunas regiones de baja saturaciÃ³n, pero aumenta rÃ¡pidamente en la
mayorÃ­a de las regiones de alta saturaciÃ³n de forma irregular. En algunas regiones de alta
saturaciÃ³n el espacio de color L*a*b* no es Ãºtil para distinguir cambios sutiles en el tono con la mÃ­nima cantidad de sombra aplicada.
La mayorÃ­a de las regiones de alta saturaciÃ³n son muy sensibles a los cambios de
luminancia, ya que los mÃ¡s pequeÃ±os cambios provocan fallos durante la segmentaciÃ³n. De
estos resultados se puede observar que el CIE L*a*b* tiene un rendimiento peor que el
espacio de color RGB en varias regiones de color de alta saturaciÃ³n. Hemos demostrado
que la mejora en la calidad de la tÃ©cnica de segmentaciÃ³n propuesta y sus rÃ¡pidos
resultados son sustancialmente significativos.
Como se puede observar directamente a partir de los resultados de la segmentaciÃ³n y de las
grÃ¡ficas generadas, la funciÃ³n adaptativa de similitud de color propuesta superÃ³ en todos
los casos a la distancia Euclidiana en el espacio de color L*a*b*, sin tomar en cuenta L*.
El mÃ©todo de segmentaciÃ³n utilizando la funciÃ³n de similitud se desempeÃ±Ã³ bien en todos los casos con tasas superiores al 95% de verdaderos positivos (VP) y tasas menores que el 3% de falsos positivos (FP).
En cuanto a la evaluaciÃ³n del mÃ©todo de segmentaciÃ³n de color con condiciones muy
difÃ­ciles, podemos notar que la funciÃ³n adaptativa de similitud de color propuesta tuvo un buen desempeÃ±o en todas las pruebas y se mantuvo cerca de la zona de alta eficiencia de las grÃ¡ficas ROC (coordenadas 0, 1), sin cambios notables, cuando el nivel de sombra difuminada aumentaba.
Durante el curso de la presente investigaciÃ³n se pudieron observar las deficiencias del
espacio de color CIE L*a*b*para el proceso de la segmentaciÃ³n del color y se puede ver
que los resultados obtenidos son pobres y variables para cada regiÃ³n de color y nivel de saturaciÃ³n. Por lo tanto, todas las aplicaciones que utilizan ese espacio de color tendrÃ¡n malos e inesperados resultados, en funciÃ³n de las imÃ¡genes de entrada y la iluminaciÃ³n.
De los resultados se puede afirmar que algunos sistemas basados en el espacio de color CIE L*a*b* usan imÃ¡genes que se adaptan a sus pruebas y a sus resultados, como es el caso de las imÃ¡genes con baja saturaciÃ³n. Con este tipo de imÃ¡genes, la segmentaciÃ³n en el
espacio de color CIE L*a*b* se comporta bien con cambios de luminancia, incluso mejor
que en el espacio de color RGB. Sin embargo, con imÃ¡genes de alta saturaciÃ³n los sistemas
que utilizan el espacio de color CIE L*a*b* se comportan peor que en el espacio de color
RGB, por lo que forzosamente producen resultados muy pobres en la segmentaciÃ³n, incluso con la menor variaciÃ³n en la iluminaciÃ³n.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>Positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad Nacional AutÃ³noma de MÃ©xico
Posgrado en Ciencia e IngenierÃ­a de la ComputaciÃ³n
Facultad de Estudios Superiores CuautitlÃ¡n
SegmentaciÃ³n de imÃ¡genes digitales sobre la
base de la informaciÃ³n integral del color
T E S I S
QuÃ© para optar por el grado de:
Doctor en Ciencias
(ComputaciÃ³n)
Presenta:
M. en C. Rodolfo Alvarado Cervantes
Tutores:
Dr. Edgardo M. Felipe RiverÃ³n Dr. Vladislav Khartchenko
Centro de InvestigaciÃ³n en ComputaciÃ³n Facultad de Estudios Superiores CuautitlÃ¡n
Instituto PolitÃ©cnico Nacional
CuautitlÃ¡n Izcalli, Estado de MÃ©xico, febrero de 2017</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
</Informacion>
</body>
