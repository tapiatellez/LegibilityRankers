<?xml version="1.0" encoding="utf-8" ?>
<body>
<Informacion xmlns='http://www.w3.org/1999/xhtml' >

<style type='text/css'>
	
@charset 'utf-8';

table{
	border: 1px black solid;
	border-radius: 5px;
	min-width: 400px;
	font-family: Helvetica,Arial;
	margin-top: 10px;
	margin-right: 40px;
	margin-bottom: 10px;
	margin-left: 40px;
	} 

table td{
	padding:7px;
    text-align:justify;
	}

.tablas_corpus { 
	font-family: Arial, Helvetica, sans-serif;
	line-height:25px;
	float:left;
	}

.tablas_corpus table{
	font-family: 'Lucida Sans Unicode', 'Lucida Grande', Sans-Serif;
	font-size: 14px;
	text-align: left;
	border-top-style: none;
	border-right-style: none;
	border-bottom-style: none;
	border-left-style: none;
	}

.tablas_corpus th {
	font-size: 13px;
	font-weight: normal;
	padding: 8px;
	background: #b9c9fe;
	color: #039;
	border: 1px solid #FFFFFF;
	}

#campos{
    text-align:center;
    font-weight: bold;
    color: #4646C1;
	}

.tablas_corpus td {
	padding: 8px;
	background: #e8edff;
	color: #4E4E4E;
	border: 1px solid #fff;
	}

.tablas_corpus tr:hover td { 
	background: #d0dafd; 
 	}

h1 {
	color: #666666;
 	font-family: Helvetica Neue, Arial, Helvetica, sans-serif;
	letter-spacing: -1px;
	text-decoration: none; 
	text-shadow: 2px 2px #fff, 0 0 #0e0e0e, 3px 4px 2px #e3e3e3; 
	text-transform: none; 
	word-spacing: -2px;
	}

.titulo{
	width: 1000px;
	height: 50px;
	margin-left: auto;
	margin-right: auto;
	background-color: #BDDDF2;
	text-align: center;
	vertical-align: middle;
	line-height: 50px;
	border-radius: 26px 26px 26px 26px;
	}

resultados{
	float: right;
	padding-right: 100px;
    font-weight: bold;
	margin-top: 40px;
	margin-bottom: 10px;
	}

</style>
<div class='titulo'> <center><h1> INAOE CORPUS </h1></center> </div>
<resultados> Resultados = 269</resultados>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Sistema Multiagente para la Planificación y Supervisión del Aprendizaje"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
Desarrollar un sistema multiagente, que permita organizar, planificar, proporcionar, guiar y evaluar el aprendizaje en base al perfil del usuario, en un ambiente colaborativo soportado por computadora.
Objetivos Particulares
"Describir y modelar los componentes del Sistema Multiagente para la Planificación y Supervisión del Aprendizaje (SMPSA).
"Diseñar e implementar los agentes que intervendrán en el sistema SMPSA.
"Diseñar e implementar el modelo de usuario, para apoyar la construcción del plan de aprendizaje.
"Diseñar e implementar el modelo del dominio, para la construcción del plan de aprendizaje.
"Desarrollar un modelo de evaluación.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Son muchos los países que están utilizando las modernas tecnologías de información y comunicaciones aplicadas a la educación. Estas tecnologías de educación nos ofrecen grandes oportunidades para reacomodar el proceso de adquisición de conocimiento.
El autoaprendizaje, es una de las actividades que los seres humanos acudimos para poder desarrollar habilidades y adquirir conocimientos. Desde su aparición, las computadoras ha impactado el campo de la educación, desarrollándose una amplia variedad de aplicaciones y de proyectos de investigación, estos sistemas están enfocados a ser del tipo "tutores".
El desarrollo de los sistemas distribuidos y el avance de Internet, se ha desarrollado una explosión de información, haciendo muy importante el problema de la planificación, el guiar y supervisar el aprendizaje, así como la obtención de los materiales de interés para el usuario.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD AUTÓNOMA DEL ESTADO DE HIDALGO
INSTITUTO DE CIENCIAS BÁSICAS E INGENIERÍA
CENTRO DE INVESTIGACIÓN EN TECNOLOGÍAS DE
INFORMACIÓN Y SISTEMAS
"Sistema Multiagente para la Planificación y
Supervisión del Aprendizaje"
Propuesta de Tesis presentada en opción del título de Maestría
en Ciencias Computacionales
Autor: Euler Hernández Contreras
eulerconther@hotmail.com
Director: Dr. Gustavo Núñez Esquer
gnunez@uaeh.reduaeh.mx
Pachuca de Soto, Hgo., 15 Febrero del 2001.
México.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Algoritmo Handoff basado en Modelos Asociativos Alfa-Beta"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar e implementar con los modelos asociativos Alfa-Beta un algoritmo handoff que
permita la conectividad en sistemas celulares</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Diferentes han sido los avances en el estudio de este tema de tesis, la lógica difusa para
procesar los parámetros del algoritmo handoff, también se ha encontrado en la misma línea
de investigación resultados en el rendimiento mediante redes neuronales, los cuales se
han basado en sistemas de reconocimiento de patrones [4].
Dado que han sido diversas las herramientas de solución y modelado del algoritmo Por lo tanto, con base en los resultados que los modelos asociativos Alfa-Beta han logrado
en casos similares al de este tema de tesis, se pretende aplicar, observar y comparar su
desempeño en cómputo con los actuales algoritmos handoff diseñados.
handoff, en esta tesis se sugieren los modelos asociativos Alfa-Beta para alcanzar un ideal con resultados competitivos.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>1. Predicción de la celda de cobertura siguiente a la cual estará conectada la estación
móvil. Aunque esta conclusión no pensaba como tal, en las pruebas realizadas con
base de datos alternas (Tablas 5.4 y 5.5) de otros días se concreto este punto donde
se observa claramente que los modelos asociativos Alfa-Beta aprenden con un
historial de días transcurridos y se predice el comportamiento del próximo día.
2. Se asegura la conectividad anticipada de los teléfonos móviles en la red mediante el
vector y de destino (figura 4.3) y los resultados obtenidos en el capítulo 5.
Cumpliendo con el objetivo de esta tesis.
3. Recuperación de patrones entre el 90 y el 100% (tablas 5.2, 5.3, 5.4, 5.5).
Asegurándose el buen funcionamiento de los modelos asociativos Alfa-Beta que en
otras tesis también se ha comprobado
4. Una selección válida en información de más de un día de las bases de datos para las
fases de aprendizaje. Solo se considero la información de la trayectoria de la
estación móvil (tabla 5.1) y aunque lo planteado fue llevar al límite de los recursos
de MATLAB, con este punto podemos tener cierta norma de datos de aprendizaje
para poder hacer una predicción (tablas 5.4 y 5.5).
5. Uso de patrones de entrada más estables como son: el CGI, BSIC y el tiempo
(sección 4.1). Ya que el estándar internacional del algoritmo es originado por
niveles de potencia de señal, calidad de señal y distancia.
6. Paralelismo de trabajo respecto a otros métodos. La propuesta de esta tesis no
elimina o descarta los estándares manejados por los operadores si no se adjunta a
ellos siendo posible su aplicación tanto en la estación móvil como dentro de la
estructura que gestiona los recursos del sistema de telefonía móvil
7. Una implementación de la inteligencia en telefonía móvil. Gracias al uso de la
herramienta computacional de los modelos asociativos Alfa-Beta se puede hacer
una aportación más al área de telefonía inteligente.
8. Método no utilizado. Hasta el momento no se tiene registros de utilizar memorias
asociativas para la investigación de algoritmos handoff (sección 2.4), dándole al
método utilizado en este tema de tesis una mayor importancia y aportación, al
algoritmo y a la herramienta matemática.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Algoritmo Handoff basado en Modelos Asociativos Alfa-Beta
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRÍA EN CIENCIAS EN INGENIERÍA DE CÓMPUTO
CON OPCIÓN EN SISTEMAS DIGITALES
PRESENTA:
JOSÉ DEMETRIO RAMÍREZ AVELINO.
DIRECTORES DE TESIS:
DR. AMADEO JOSÉ ARGÜELLES CRUZ.
DR. OSCAR CAMACHO NIETO.
MÉXICO, D.F. DICIEMBRE DE 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"HERRAMIENTA INFORMÁTICA DE VIGILANCIA TECNOLOGICA PARA ANÁLISIS SOCIO-COGNITIVOS DE COMUNIDADES CIENTÍFICAS"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La información de la producción científico tecnológica (artículos en revistas
indexadas y patentes) se encuentra alojada en bases de datos no homogéneas.
"Volúmenes altos de información problemas en la búsqueda y navegación en estos
documentos.
"Supone una cierta especialización en el tópico tratado.
"Es posible construir una representación del conocimiento científico a través de
mapas tecnológicos de una área de conocimiento o mapas que contengan la
dimensión social, cognitiva del tópico tratado.
Desde un enfoque socio-cognitivo, se propone extraer las relaciones a partir de
documentos científicos y encontrar diferentes representaciones reticulares
tales como redes temáticas, redes de palabras clave, redes de autores, redes de
citas, redes de referencias bibliográficas y redes de documentos científicos,
teniendo como resultado no solo la representación del conocimiento, también
la posibilidad de desarrollar análisis reticulares y métricas que den cuenta de
cómo se construye dicho conocimiento.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
Desarrollar un sistema de extracción de relaciones socio-cognitivas a partir de documentos
científicos
Objetivos específicos:
Desarrollar un sistema para la obtención, preprocesamiento y extracción de
características de documentos científicos y patentes.
Construir un modelo para la construcción de redes sociales de las características
obtenidas de los documentos
Construir un modelo para la construcción de redes cognitivas de las características
obtenidas de los documentos
Desarrollo de un sistema para análisis descriptivos y dinámicos de las redes construidas
Desarrollar una herramienta prototipo que integre los modelos generados con una
interfaz de usuario, que permita la visualización, navegación y la construcción de
métricas de las redes obtenidas.
Evaluar la herramienta mediante expertos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En este trabajo se entiende la sociedad del conocimiento, como un sistema que está
en continua construcción, evolución y transformación, recurrentemente esta
obteniendo nuevos productos científicos, y sus actores modifican constantemente
sus capacidades. Éstas características se ven reflejadas en bases de datos de
artículos científicos y de patentes las cuales están distribuidas en diferentes
servicios donde la información es no homogénea, dado el alto volumen de
información; es un problema interesante para la ingeniería, debido a que existe la
necesidad de desarrollar herramientas informáticas que permitan a diferentes
actores y usuarios contar con información estructurada y con herramientas
computacionales que le permitan tener métricas y mapas de las dinámicas científico
tecnológicas.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>HERRAMIENTA INFORMÁTICA DE VIGILANCIA
TECNOLOGICA PARA ANÁLISIS SOCIO-COGNITIVOS DE
COMUNIDADES CIENTÍFICAS
Director: Fabio González
Autor:Víctor Andrés Bucheli Guerrero
Maestría en Ingeniería de Sistemas y Computación
Universidad Nacional de Colombia</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"DESARROLLO DE SOFTWARE, APLICADO A LA GEOMETRÍA ANALÍTICA, A NIVEL MEDIO SUPERIOR."</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>OBJETIVOS
Elaborar un software adecuado a los cursos de geometría analítica de la Universidad de Guadalajara y de la Universidad de Colima.
Desarrollar interfases en el software que sean consistentes y manejables durante toda la ejecución del sistema.
Ofrecer una alternativa didáctica a las matemáticas.
Propiciar que los cursos de matemáticas, sean más dinámicos.
Enfrentar al alumno a la realidad de su entorno y provocar que se manifieste activamente.
Desarrollar en el alumno sus habilidades aritméticas, algebraicas y geométricas, por medio del uso del software.
Lograr que el alumno aprenda a matematizar, apoyándose en el software.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La Elaboración de este Software se baso en la siguiente hipótesis:
Es posible lograr software donde, su contenido sea acorde a los programas
escolares, y al nivel que el grado de educación lo requiera Donde el Alumno Interactúe de forma dinámica con la computadora,
construyendo su propio conocimiento de una manera significativa y grupa ¡,
reforzando sus conocimientos básicos, y logrando aplicar los conocimientos en su
entorno.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La firma de las acuerdos comerciales internacionales como el Tratado de
Libre Comercio de América del Norte (TLCAN) y la corriente globalizadora que
impulsa el desarrollo económico en nuestra sociedad actual, ha traído como una de
múltiples consecuencias el que hoy en día sea muy común ver computadoras en
todas partes.
Debido a que la dinámica actual de esta sociedad de consumo es mucho más
rápida que la comparada con la que se tenia hace 20 años, se vuelve imprescindible
el incluir a la computadora como herramienta de trabajo en las aulas, ya que las
actividades nuevas exigen tiempo libre para dedicárselas a la deducción y la
inducción.
Los tiempos aquellos, en que se le dedicaba el mayor tiempo a los despejes
algebraicos, parece que están pasando de moda. Los alumnos, la mayoría de las
veces se muestran distraídos, cuando se abordan los temas de una manera
tradicional. Es muy probable que el tiempo de cambiar de estrategias en la
enseñanza de las matemáticas, ya este aquí.
Las herramientas actuales para la comprensión de la geometría analítica no
se ajustan del todo a los contenidos manejados.
Por lo que es muy importante, que se incluya a la computadora como una
herramienta elemental para su aprendizaje, y no como una simple y potencial
maquina de escribir, como desafortunadamente ha sido empleada hasta hoy.
Además, si se continúa utilizando los métodos tradicionales, con el cúmulo de
nuevos conocimientos que el alumno tiene que aprender, para estar al día, no se
alcanzaría a cumplir con todo el programa de estudio de un semestre.
De aquí la importancia de estar al día, con lo nuevo.
La utilización de este software plantea como un objetivo particular para el
estudiante, el investigar y hacer un análisis real de las ecuaciones y las familias de
curvas a las que pertenecen.
Se pretende que el uso de este Software; a diferencia de que cuando no se
contaba con esta herramienta, solo nos alcanzaba para deducir formulas,
algebraicamente y de una manera raquítica; nos haga disponer de tiempo, libre
disponible para analizar de manera formal los resultados obtenidos. Situación que
antes no se presentaba, por falta de tiempo.
Con este software prácticamente se han eliminado los problemas técnicos que
se tenían, con las aplicaciones de software existentes, para poderlos adecuar a la
currícula, que demandan la Universidad de Guadalajara y la Universidad de Colima.
Además de que se han implementado rutinas que no presentan las fallas de
funcionamiento, que se tenían en las otras aplicaciones.
La práctica que actualmente desarrollan los profesores en el aula y que cada
día toma una fuerza mayor es la de proporcionarle al alumno los medios para que
por sí mismo construya su propio conocimiento, a la vez que desarrolla este tipo de
construcción se le debe dirigir su avance de acuerdo al interés que muestre por la
materia en cuestión. Esto le va permitiendo que vaya formando sus juicios para
fundamentar su criterio, lo cual le permite a su vez reconstruir otros conceptos que
satisfacen sus necesidades. Quintero y Ursini (1988) comentan "Las confirmaciones
y suposiciones dan lugar a reformulaciones que permitan la elaboración de nuevos
marcos.".
Entendiendo como nuevos marcos, niveles de conocimiento más complejos
que son fruto de las formulaciones que nos facilitan abordar problemas y
conceptualizaciones de un nivel más complejo. Estas son actividades que en la
enseñanza tradicional no es posible lograr por la esquematización tan rígida a la que
el alumno se enfrenta; cuando lo conveniente debería ser que él, con base en su
avance cognitivo, valore la importancia que representa cada uno de los conceptos
que construye, a través de semejanzas que apropia de su realidad y a las cuales da
forma en su interpretación personal.
Se debe a esas interpretaciones, a que se le este dando un fuerte apoyo con
la ayuda de microcomputadoras, pero no con el objeto de tener esta herramienta
como algo tajante e inflexible, mucho menos para usarla en el desarrollo de una
labor mecánica; por el contrario, la microcomputadora, actualmente se está usando
como un medio para lograr que al alumno se le facilite la forma en cómo adquiere el
conocimiento que busca a su alrededor, facilitándole también que sus ideas
adquieran una objetividad más clara, mismas que le ayuden a plantearse
creativamente otras suposiciones con las cuales pueda confrontar sus resultados y
de esta forma objetivarlos en algo que corresponda a su entorno. Sin embargo, se
debe encaminar, por parte del maestro, el trabajo que el alumno quiere aprender, sin
que las microcomputadoras puedan sustituir su labor, lo cual nunca se podrá lograr.
La microcomputadora, como ya antes lo habíamos comentado sólo es una
herramienta que facilita el logro de los objetivos que se plantearon para lograr el
conocimiento de la materia.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La métodología empleada en la elaboración de esta investigación fue:
La investigación se inicio en el calendario 1999 B, con el uso de los Software
Funpol y Cónicas, como prototipos.
En el calendario 2002 B, se elaboro el nuevo Software, corrigiendo las fallas
observadas en los prototipos utilizados.
Se uso el Software en ese Semestre.
Se empleo la métodología de la INVESTIGACIÓN ACCIÓN[7].
Se recuperaron las clases por medio de videos, como herramienta para
levantar el REGISTRO DE LA PRÍCTICA DOCENTE[8], en caso de que fuera
necesaria una revisión a fondo.
En cada clase se tomaron notas de la interacción del software con los
alumnos, para detectar errores, o interfaces poco manejables.
También se tomaron notas de las innovaciones necesarias al programa.
Después de la aplicación del software, y con base a las notas obtenidas se
hicieron los cambios y correcciones necesarias al programa.
La nueva versión del Software, se utilizo en un semestre mas, de la misma
manera anterior.
Se corrigieron los errores, y se implementaron las mejoras, para obtener esta
última versión
Nota: Este proceso puede ser llevado iterativamente, hasta alcanzar el grado de
madurez deseado.
El calendario A, es el semestre que inicia en Marzo, y el B es el que inicia en
Septiembre</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Como se puede observar en las encuestas [26], La esperanza de los alumnos -
hayan tenido o no, la oportunidad de utilizar las computadoras en su aprendizaje- de
visualizar las matemáticas de una forma mas clara y concreta y lo menos abstracta
posible, esta depositada en la utilización de las computadoras en su aprendizaje.
Aunque también es cierto, porque así lo dicen, es necesario que se esfuercen
más para que realmente aprendan.
Otro hecho que se pudo observar, es que los alumnos piden formas de
enseñanza más atractivas, y acorde a su medio ambiente actual. Las utilizadas en la
forma tradicional, pudieran parecer poco motivadoras, y provocar el ausentismo en el
aula, o lo que es peor, la falta de compromiso con la materia.
Se observó en la 3a encuesta que, usando la computadora, el alumno
aprendió a analizar y se le facilita el reflexionar, importando poco que no posea los
conocimientos de álgebra necesarios.
El temor natural que el Alumno enfrenta ante la materia de Geometría
Analítica que aparentemente necesita de todos los niveles anteriores de
matemáticas, desaparece en parte, al sentir apoyo en la computadora, pero también
aquí el profesor debe poner mucho de su parte para simplificarle a los alumnos el
análisis, y no meter los en dependencias del álgebra que nada tienen que ver con
analizar, sino es confirmar de forma practica lo anteriormente aprendido.
De acuerdo a las encuestas, se puede decir que, es posible lograr software donde,
su contenido sea más acorde a los programas escolares, y al nivel que el grado lo
requiera. Donde el Alumno Interactúe de forma dinámica con la Computadora, construyendo su propio conocimiento de una manera
significativa y grupal, reforzando sus conocimientos básicos, y logrando aplicar los
conocimientos en su entorno.
Pero para lograrlo, el programador debe estar inmerso en el salón de clases,
ó mejor aun, que el profesor que imparte la materia, sea el mismo que elabore el
Software.
Solo que para que esto sea posible, los ingenieros en Software, deben dedicarse a crear estos ambientes gráficos de fácil programación, donde sea posible, "centrarse en el que y no el como".
Elaborar prototipos basados en software ya existentes, agilizan, el logro de objetivos.
Poner a prueba los prototipos, en sus ambientes reales, nos acerca más a lograr los objetivos esperados.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTADDE INGENIERÍA ELECTROMECÍNICA
DESARROLLO DE SOFTWARE, APLICADO A LA GEOMETRÍA ANALÍTICA, A NIVEL MEDIO SUPERIOR.
TESIS
PARA OBTENER EL GRADO DE:
MAESTRO EN COMPUTACIÓN.
Que presenta:
Ing. José de Jesús Sánchez Herrera
15 de Mayo de 2003 El Naranjo Col. México</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"USO DE INTERNET Y RENDIMIENTO ACADEMICO DE LOS ESTUDIANTES DE LA FCEH-UNIVERSIDAD NACIONAL DE LA AMAZONIA PERUANA, IQUITO-2008"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Existe relación entre el uso de Internet y el rendimiento académico de los
estudiantes de la Facultad de Ciencias de la Educación y Humanidades de la
Universidad Nacional de la Amazonia Peruana.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Tipo de Investigación
Correlacional simple, ya que permiten relacionar o asociar variables

Diseño de Investigación
Teniendo en cuenta la naturaleza del problema, objetivos e hipótesis es un diseño
no experimental, transversal.
Población y Muestra
- Población
La población de nuestra investigación estará conformada por 1471 estudiantes
matriculados en la Facultad de Ciencias de la Educación y Humanidades de la
Universidad Nacional de la Amazonía Peruana en el año 2007.
- Muestra
Tamaño de muestra
Para determinar el tamaño de muestra por los elementos del estudio se aplicará
la siguiente fórmula para población finita.
Procedimientos, Técnicas e Instrumento de Recolección de Datos.
1. Los procedimientos que se tuvo en cuenta para el recojo de la información
fue de la siguiente manera.
* De la Oficina General de Asuntos Académicos se recogió los datos
de los estudiantes matriculados en el semestre académico antes
mencionado.
* Se seleccionó una muestra aleatoria, del total de la población
teniendo en cuenta que todos los integrantes de la población tengan
las mismas oportunidades de formar parte de la muestra.
* Se solicitó el permiso a la Facultad de Ciencias de la Educación y
Humanidades, para programar una reunión con los integrantes de la
muestra, una vez coordinado con la facultad la reunión, se procedió
a convocarlos mediante una invitación formal, indicando en ella la
fecha, hora, y lugar.
* El día de la reunión, se explicó a los integrantes de la presente
muestra los motivos de su participación, los objetivos del trabajo,
además de absorber las dudas, sugerencias y comentarios que
tuvieron con relación a la ejecución del presente proyecto, además
de explicar y entregar un acta de compromiso en donde nos
comprometíamos los investigadores a no perjudicar la vida y salud
integral de cada uno de ellos.
En caso de que algún integrante de la muestra no quiso participar,
se procedió a escoger al azar a otro alumno de la relación total,
para completar la cantidad de estudiantes requeridos para nuestra
muestra.
* En esta reunión también se coordinó la fecha, hora y lugar para el
desarrollo del cuestionario de preguntas, teniendo en cuenta el
cronograma de actividades programado en el presente proyecto.
* El día, hora y lugar coordinado con anterioridad, se procedió a la
aplicación del cuestionario de preguntas a los integrantes de la
muestra, aplicando en todo momento el cumplimiento de las
normas estipuladas sobre la PROTECCION DE LOS DERECHOS
HUMANOS.
* Una vez terminada la aplicación de cuestionario de preguntas se
agradeció a los participantes por el apoyo a la recolección de datos
del presente proyecto de investigación.
La técnica que se utilizó en la investigación fue la encuesta porque
consideramos que esta técnica nos permitió recoger la información requerida
por los investigadores. Asimismo nos permitió contar con una relación directa
con los participantes. El instrumento que se utilizó fue un cuestionario,
orientada al uso de Internet.
Para la variable rendimiento se usó la técnica del análisis documental
utilizando como instrumento una Ficha de Consolidación de resultados de la
Facultad de Ciencias de la Educación y Humanidades, que sirvió para el acopio
de los promedios ponderados semestral de los estudiantes.
Procesamiento de la información
Para el procesamiento de la información se utilizó programas computarizados
como el SPSS 15.0 (Programa estadístico) y el Excel (Hoja de Cálculo), los
cuales nos permitieron una revisión y verificación de los datos obtenidos con los
instrumentos utilizados en la presente investigación.
Para el análisis e interpretación de los datos se utilizó las medidas de resumen,
para verificar nuestra hipótesis se hará uso de medidas de asociación teniendo en
cuenta la naturaleza de las variables de estudio. (Prueba estadística no
paramétrica Ji Cuadrada con un = 0.05)
ASPECTO ÉTICO
1. Participación de los sujetos de la muestra
La participación de los sujetos de la muestra se dio de manera voluntaria,
seleccionados al azar simple. Se informó a los estudiantes que la participación
y permanencia en el proceso de investigación es voluntaria, ningún alumno (a)
de la muestra fue retenido en el grupo contra su voluntad, ni fue perjudicado
y/o sancionado por la decisión que tomó.
2. Proceso de consentimiento informado (opcional)
Los participantes se informaron sobre los objetivos, procedimientos y
utilización de los resultados de la investigación.
3. Reclutamiento de los participantes al estudio (opcional)
4. Confidencialidad de la información obtenida
La información obtenida mediante la administración de las encuestas
realizadas a los estudiantes de la Facultad de la Educación y Humanidades de
la Universidad Nacional de la Amazonía Peruana, fue anónima y utilizadas
sólo con criterios estrictamente estadísticos. Al término del procesamiento de
la información (encuesta), estas fueron incineradas.
5. Consecuencias de la participación en la investigación (opcional)</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Luego de haber realizado el proceso de investigación sobre el Uso de Internet y el rendimiento
académico de los estudiantes de la Facultad de Ciencias de la Educación y Humanidades - UNAP -
2008 podemos concluir con lo siguiente:
- Los estudiantes matriculados en el I semestre académico de FCEH - UNAP tienen un
promedio 10.45 puntos con una desviación estándar 1.9 puntos respecto al uso de Internet,
como un medio en proceso de enseñanza aprendizaje. Cualitativamente se observa que el
59.9% de los estudiantes hacen e uso de internet en un nivel malo.
- Los estudiantes matriculados en el I semestre 2008 tienen un promedio ponderado de
rendimiento académico semestral de 12.92 puntos con una desviación estándar de 2.3
puntos. Asimismo concluimos que el 66.8% de los estudiantes pertenecen al nivel de
Rendimiento Académico Regular.
- En cuanto al grado de asociación afirmamos que no existe una relación estadísticamente
significativa entre el uso de Internet y el rendimiento académico de los estudiantes de la
Facultad de Ciencias de la Educación y Humanidades - UNAP -2008 con un 95% de
confianza.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD NACIONAL DE LA AMAZONÍA PERUANA
MAESTRÍA EN EDUCACIÓN CON MENCIÓN EN DOCENCIA
INFORME FINAL DE TESIS
"USO DE INTERNET Y RENDIMIENTO ACADEMICO DE LOS ESTUDIANTES DE LA FCEH-UNIVERSIDAD NACIONAL DE LA AMAZONIA PERUANA, IQUITO-2008"
Presentado por:
CHAVEZ RUIZ, Marlon
CHAVEZ RUIZ, Hanny.
Asesor : Lic. Juan de Dios Jara Ibarra Ms
PARA OPTAR EL GRADO ACADÉMICO DE MAGISTER EN
CON MENCIÓN EN
ESCUELA DE POSTGRADO
E INVESTIGACIÓN UNIVERSITARIA
RENDIMIENTO ACADÉMICO DE LOS
FCEH-UNIVERSIDAD NACIONAL DE LA
IQUITOS - 2008" 
Msc.
EDUCACIÓN
DOCENCIA E INVESTIGACIÓN UNIVERSITARIA
IQUITOS - PERÚ
2008
IDAD N</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"SOFTWARE PARA LA CONSTRUCCIÓN DE CADENAS PROTÉICAS"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La estructura química de los ácidos nucleicos es compleja, así como los mecanismos de
expresión genética. La ciéncia avanza cada día y se acumula el acervo teórico sobre este
tema, que cada vez alcanza dimensiones que no pueden ser asimiladas por las personas
comunes y corrientes. Por ello consideramos que es necesario crear un software que de manera sencilla explique la estructura quimica de los compuestos en los que reside el
mecanismo de transmision genetica, y que ademas el usuario pueda interactuar con ella
para crear moléculas simples proteicas, y comprender los principios b6sieos funcionales de
la expresión genética.
Consideramos que este software ser4 de gran aplicabilidad, ya que facilita a los
profesores la enseñíanza de estos tópicos de manera amena pero con un alto índice de
confiabilidad; ademas, sirve de base para la creación de un sofIware hecho en México que
se utilice en la creación de modelos tridimensionales de protelnas y su proceso de síntesis, con un amplio contenido técnico que puede ser utilizado tanto en docencia como en la investigación y la industria.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo principal de esta tesis se basa en el desarrollo de sojftwre multimedia que
incluye información teorica precisa referente a la biología molecular, en especial la
explicación de la estructura química de los ácidos nucleieos, parte de la genética humana,
donde se incluyen los siguientes rubros:
Teotia introductoria precisa acerea de Ia biología molecular explicada ademAs por
medio de imagenes y esquemas
Sofwaare demostrativo del contenido teórico acerca de la estructura qtimica de los
ácidos nucleicos y los principios de la expresion genética.
Sofiare interactivo donde el usuario pueda crear moléculas proteicas simples en
tercera dimensión.
Contenido del programa. Estos siete temas se encuentran englobados en un memí
principal de fácil manejo,
1 .- Historia de la evoluciOn de los ácidos nuoleicos y su reperwsión en la Genética.
2.- Estructura quimica de los acidos nucleicos.
3.- C6digo genetico.
4.- Organelos que participan en la síntesis de protemas.
5.- Mecanismos de síntesis de proteínas: transcripción, replicación, transducción.
6.- Glosario.
7.- Referencias</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Se pretende desarrollat un programa multimedia que englobe información tehica acerca
del código genético y los hcidos nucleicos, presentada en forma textual, asi como un4
seccih intemctiva de creación de moléculas. Debido a la gran cantidad de información
generada, el programa y los datos estarti almacenados en un CD-RCXvI.
El desarrollo de esta aplicación multimedia permitir6 una mejor y más rhpida
comprensión por parte de los alumnos de la carrera de Medicina acerca de la biología
molecular, en especial el código genético y los ácidos nucIeicos.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Primera etapa:
Acopio, compilación ykíición textual del material informativo proveniente de diversas
fuentes audiovisuales.
Porcentaje de tibdo: 20%
Segunda etapa:
Digitalización y retoque de las imágenes provenientes de libros, revistas e Internet.
Porcentaje de trabajo: 10%
AI final de esta etapa se contó con 40 imágenes referentes al proceso genético. Se
capturaron y editaron aproximadamente 10 cuartillas de información textual.
Tercera etapa:
Generación de gráficas en 3D de ácidos nucleicos y aminoácidos esenciales.
Porcentaje de trabajo: 20%
Al final de esta etapa se coartaron con 725 imhgenes que contienen g-rhficas en 3D.
Cuarta etapa:
Desarrollo de secuencias de animaciones acerca de ios ácidos nucleicos y los procesos
de replicwión, transcripción y transducción del ADN.
Cantidad de animaciones: 5
Porcentaje de trabajo: 20%
Quiuta tipa:
Desarrollo del programa interactivo donde el usuario podrá crear una molécula en 3D
a partir de una secuencia de ADN o viceversa (2 opciones), y el resto de los módulos.
Porcentaje de trabajo: 30%</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con respecto al programa de aplicación propuesto en el proyecto de tesis, éste. se realizó y
se completó tal como se había bisfiado con el compilador Delphi 3.0, de la empresa
Borland Este programa consta de ías siete opciones que se mostraron en la introduccián.
Tanto el software desarrollado como el programa de instalación de la aplicación POV-RAY
se encuentran incluídos en el CD-ROM que se editó para tal efecto, dada las magnitudes de
dichos programas, además de que el CD-ROM presenta cierta facilidad para la distribucibn
y el uso de programas multimedia. El tratar de realizar un programa de aplicación
multimedia de este tipo requiere de la inversión de mucho tiempo y esfuerzo.
El contenido de esta tesis puede en el futuro complementarse con aplicaciones
adicionales utilizando como base la informacion y programas de aplicacion obtenidos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>FACULTAD DE INGENIERÍA MECÍNICA Y ELÉCTRICA
SOFTWARE PARA LA CONSTRUCCIÓN DE
CADENAS PROTÉICAS
T E S I S
QUE PARA OBTENER EL TíTULO DE:
MAESTRO FiN CIENCIAS COMPUTACIONALES
PRESENTA:
MIGUEL ANGEL GARCÍA RUIZ
Coquimatlán, Col., 27 de Agosto de 1998</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"SIMULACIÓN DE UN ALGORITMO DE ENRUTAMIENTO PARA REDES DE SENSORES INALÍMBRICOS"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
Desarrollar y simular en OPNET, una red de sensores inalámbricos con LORA-CBF como protocolo de enrutamiento.

Objetivos Específicos
*	Diseñar el nodo sensor en el simulador OPNET.
*	Modelar el protocolo LORA-CBF dentro de un nodo sensor.
*	Publicación de resultados en revistas de prestigio</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El protocolo de enrutamiento LORA-CBF es aplicable en redes de sensores inalámbricos con adecuaciones mínimas</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Las redes de Sensores Inalámbricos ofrecen un medio eficaz para el análisis y monitoreo de los fenómenos físicos y químicos que ocurren a su alrededor. Las características que las distinguen, les permiten desplegarse en una gran variedad de terrenos, ya sea bajo condiciones hostiles o bien en situaciones de peligro, motivos por los que su uso ha continuado aumentando en diferentes sectores como la industria, medicina, seguridad, domótica, agricultura, etc.
A pesar de las ventajas que suponen las redes de sensores inalámbricos respecto a la infraestructura física o el costo de implementación sobre las redes de comunicación convencionales, también encuentran limitantes en aspectos como la energía disponible y las capacidades de memoria y procesamiento, lo que ha desembocado en un gran esfuerzo de investigación para desarrollar métodos o algoritmos que optimicen las características de los nodos y amplíen el alcance de las redes de sensores inalámbricos.
El desarrollo de nuevos algoritmos para las redes de sensores inalámbricos busca mejorar los protocolos existentes y optimizar las características de los nodos sensores, además de ampliar el campo de aplicación de los mismos.
La simulación es una herramienta de gran valor para el análisis del comportamiento de una red bajo situaciones controladas. OPNET Modeler ofrece una amplia gama de modelos y librerías para la evaluación de redes de comunicación.
Este trabajo de tesis desarrollo y simuló un modelo para redes de sensores inalámbricos con LORA-CBF como protocolo de enrutamiento, protocolo originalmente creado para redes en entornos vehiculares o VANETs, con el objetivo de demostrar que el protocolo es aplicable a redes de sensores inalámbricos con adecuaciones mínimas.
Para cumplir con el objetivo del trabajo de investigación, se pensó utilizar el modelo de nodo ZigBee incluido en el modelador, sin embargo, debido a que el conjunto de protocolos ZigBee un es de libre distribución, únicamente es posible modificar las características de la capa de acceso al medio, ya que se deseaba desarrollar el protocolo LORA-CBF a nivel de capa de red, dicho modelo fue descartado. Luego de una larga revisión de modelos existentes desarrollados por diversos grupos de investigación, se seleccionó un modelo de nodo desarrollado por el grupo de investigación Open ZigBee. Dicho modelo permitió desarrollar LORA-CBF y modificar al mismo tiempo las capas de aplicación y de acceso al medio para satisfacer las necesidades del protocolo propuesto en este trabajo de tesis.
Para desarrollar el protocolo LORA-CBF se analizaron detalladamente los documentos publicados por los autores del algoritmo y las características que definían a una red de sensores inalámbricos, debido a que las características de las redes vehiculares son en su mayoría similares a las de las redes de sensores inalámbricos, la adaptación del protocolo a éstas últimas, no representó mayor problema. Sin embargo, fueron necesarias algunas modificaciones para asegurar el correcto manejo de los recursos de los nodos y evitar duplicación de información.
El modelo desarrollado en OPNET Modeler, permitió realizar un análisis del comportamiento de una red de sensores inalámbricos con el protocolo LORA-CBF como protocolo de enrutamiento. Con las métricas seleccionadas para evaluación y los diferentes escenarios de simulación creados, fue posible el análisis de aspectos como la escalabilidad que el protocolo ofrece, robustez de la red y diferentes métodos de envío de datos Se demostró que LORA-CBF es funcional en redes de pequeña y gran escala al simular escenarios con 25, 50, 75, 100 y 200 nodos y arrojar resultados positivos respecto a la tasa de entrega de paquetes y a los tiempos reducidos de retardo punto a punto y de descubrimiento de ruta.
La robustez de la red se analizó a través del envío de datos por más de un nodo sensor al nodo "sink"  en escenarios con uno, dos y cuatro nodos emisores. A pesar de que la tasa de entrega de paquetes se redujo en el escenario con cuatro nodos emisores, el sobre-procesamiento de la red y sobre-procesamiento de enrutamiento se mantuvo en bajos niveles en los tres escenarios simulados.
LORA-CBF se adaptó para dar soporte a envío multidifusión, en el escenario desarrollado para evaluar esta característica, un nodo sensor fue el encargado de enviar tráfico de datos a más de un destino. Se obtuvieron excelentes resultados en la tasa de entrega de paquetes, alcanzando niveles de casi el 100%, mientras que los niveles de tráfico de enrutamiento así como los tiempos de retardo de transmisión y descubrimiento de ruta se mantuvieron en niveles bajos.
Se comprobó que es posible adaptar el protocolo de enrutamiento LORA-CBF a redes de sensores inalámbricos, que el algoritmo ofrece un rápido despliegue y convergencia a la red, garantiza altos niveles de entrega efectiva de datos y ofrece posibilidades de escalabilidad, robustez y diferentes formas de envío de datos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
"SIMULACIÓN DE UN ALGORITMO DE ENRUTAMIENTO PARA REDES DE SENSORES INALÍÁMBRICOS" 
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRA EN COMPUTACIÓN
PRESENTA:
ING. MAYTHÉ GONZÁLES GUTIÉRREZ
ASESORES:
D. EN C. RAÚL AQUINO SANTOS
M. EN C. OMAR ÁLVAREZ CÍÁRDENAS
COLIMA, COL., FEBRERO DE 2012</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Sistema Inteligente Conversacional para la Orientación Vocacional"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La problemática que se tiene en la Orientación vocacional son:
(Jaramillo, 2005)
*	La cantidad numerosa de estudiantes que necesitan orientación vocacional
y pocos los orientadores.
*	No se proporcionan los servicios de Orientación vocacional en las
instituciones educativas suburbanas por falta de recurso humano.
*	Los jóvenes de zonas rurales no buscan la orientación profesional por la
falta de información o el tener que ir a una oficina de orientación
vocacional.
*	Las consultas que tiene el estudiante con el orientador son esporádicas, y
por tanto en ocasiones el joven pierde interés y no recoge sus resultados,
esto hace deficiente el proceso de obtención de resultados.
*	Los test de orientación son individuales y como es una cantidad numerosa
de alumnos se obtienen demasiados test a calificar, esto produce
traspapeleo pérdida de información, atenciones, apoyo y servicios
inadecuados a los estudiantes
*	Los test de orientación que se realizan son mínimos de dos cuartillas o 30
preguntas y produce gastos de materiales elevados.
*	Para obtener resultados o calificar los test y proporcionar los resultados y
apoyo adecuado a los estudiantes debe pasar un tiempo mínimo de 3
semanas.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>OBJETIVO GENERAL:
Presentar un Agente Inteligente capaz de interactuar con una persona en forma
verbal y escrita en idioma español que auxilie el proceso de orientación
vocacional.
1.3.2.- OBJETIVOS ESPECÍFICOS:
*	Investigar el proceso de orientación vocacional
*	Realizar investigación sobre Robots Virtuales y Agentes Inteligentes.
*	Minimizar los tiempos de respuesta de análisis vocacional
*	Diseñar una interfaz conversacional con privacidad y confianza para el usuario.
*	Presentar un robot virtual que automatice el proceso de orientación vocacional.
*	Globalizar el acceso de un análisis vocacional</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El desarrollo de un chatbot es capaz de simular de manera efectiva a un experto humano en Orientación Vocacional.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La Orientación Vocacional proporciona información que permite a los jóvenes
seleccionar la carrera profesional de acuerdo a sus características y habilidades
personales para el desarrollo de su vida profesional, al automatizar este proceso
el análisis vocacional se puede realizar desde la comodidad y privacidad de su
casa.
El "chatbot"  que desarrollamos apoya la orientación vocacional, el cual puede ser
accesado desde Internet por lo tanto puede estar al alcance de la mayoría de los
jóvenes que tienen interés por conocer su vocación personal y es un sistema con
interfaz tipo Chat. La aplicación del sistema impacta con mayor incidencia en los
jóvenes ya que es una conversación amigable con otra persona virtual. Asimismo
nuestro sistema repercute en la disminución de la deserción y reprobación escolar
ya que más alumnos pueden realizar su análisis vocacional y por lo tanto
seleccionar la carrera de acuerdo a sus capacidades e intereses personales.
Esta investigación se basa en las investigaciones de la ultima decada, ya que
han surgido nuevos tipos de sistemas, basados en robots virtuales que han
demostrado ser de gran apoyo en la capacitación de alumnos y de gran ayuda en
el trabajo de los profesores (Villareal, 2004).
En la actualidad las escuelas de nivel medio superior en zonas suburbanas no
cuentan con un proceso de orientación vocacional formal, no se utiliza tecnología
para estos análisis, lo cual es un obstáculo para que los jóvenes dispongan de
un apoyo para su orientación vocaciónal.
Hay ocasiones en el que algunos jóvenes realizan este análisis pero los
resultados obtenidos no son los esperados de acuerdo a sus expectativas o
gustos personales y se guardan la intención de volver hacer este estudio. Por el
hecho de ser un trámite largo, el involucrar personal y el invertir más tiempo, por
lo cual deciden continuar con sus dudas y muchas veces estudiando una carrera
que no es su vocación. Con este sistema podrán realizar este análisis el número
de veces que consideren necesarias. Ya que se accesa desde Internet y puede
ser desde la privacidad de su casa.
En esta área, la falta de una adecuada orientación, el manejo de un gran volúmen
de datos y por otro lado, la facilidad de utilizar interfaces de sistemas y el hecho
de que cada día más personas están acostumbradas al uso de sistemas
informáticos hace que este modelo de sistema sea una herramienta para ayudar
a que los jóvenes se interesen en ella y la utilicen.
El impacto social de esta tésis se proyecta en apoyar a los jóvenes de zonas
rurales para que tengan la posibilidad de un análisis vocacional. La proyección
económica de este sistema es brindar a las escuelas suburbanas una
herramienta que disminuya las consecuencias de la falta de personal
especializado en el aspecto vocacional.
La orientación vocacional se concibe como un servicio educativo que se debe
proporcionar a los alumnos para apoyar su desarrollo armónico, funcional,
laboral y profesional que les permitá tener una vida profesional competitiva
apoyandose en las áreas Psicológicas, Pedagógicas y Vocacionales.
El desarrollo de herramientas computacionales permite apoyar esta actividad. Ya
que mediante la automatización de este tipo de análisis, los jóvenes ya que no
tendran que hacer camino para ir a una oficina de orientación vocacional por el
contario lo pueden hacer desde la privacidad de su casa, ahorrando tiempo y
dinero. No se involucra recurso humano para que realice el estudio.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para diseñar el robot virtual se tomó de base la metodología de Russell (2002).
Ya que es uno de los autores que más a escrito e investigado sobre Agentes
Inteligentes y es seguidor de los avances e investigaciones de A.L.I.C.E. (Artificial
Linguistic Internet Computer Entity), quien es uno de los mejores "chatbot" , fáciles
de utilizar e implementar, su arquitectura se basa en dos módulos principales el
intérprete del lenguaje y almacen de datos en AIML. Para nuestro proyecto
A.L.I.C.E. es la base de la cual partimos para desarrollar nuestro sistema.
Para desarrollar el agente inteligente conversacional tomaremos como base la
metodología planteada por Russell (2002). Dicha metodología define los
siguientes pasos:
1.- Recopilar la información del tema a tratar. En esta etapa se investiga la
manera en que actualmente se lleva acabo el análisis vocacional, las teorías que
existen actualmente y la bibliografía que soporta el tema de orientación
vocacional.
2.- Diseñar el diálogo. En base a la información que se obtuvo en la recopilación
de la información se establecen las preguntas y expresiones que el usuario puede
hacerle al robot virtual, así como las respuestas que el sistema debe proporcionar.
3.- Crear el código AIML. En esta se utilizan las instrucciones y sintaxis del
lenguaje de programación que permite diseñar robots virtuales que en este caso
es el lenguaje AIML.
4.- Probar la interfaz con el diálogo. En este paso se realizan las pruebas del
robot el cual entabla diálogo con los usuarios.
5.- Terminar la interfaz. Se termina la interfaz después de realizar las pruebas y
mejoras en cada diálogo.
6.- Mantenimiento de acuerdo al análisis de los resultados de las conversaciones.
La base de conocimiento del robot se tiene que estar alimentando de información
como parte importante del desarrollo del robot. Entre más información almacene
mayor será su desempeño.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En nuestro trabajo el objeto principal fue presentar como producto terminado un
agente inteligente que simula a un orientador vocacional en una entrevista capaz
de interactuar en forma verbal y escrita en el idioma español.
El objeto de estudio es un agente inteligente. Al hablar de un agente estamos
hablando de un programa, un software que se ejecuta en una computadora.
Entonces decimos que nuestro objeto de estudio en primer lugar es un programa,
un sistema inteligente.
Cuando hablamos de un agente inteligente entramos a un área que resulta
sumamente compleja en su concepción, la Inteligenicia Artificial. Entrar en los
dominios de la Inteligenicia Artificial representa encontrarse con una gran cantidad
de definiciones diferentes de lo que significá esta área de la computación, ideas
tan diversas que nos pueden llevar a la confusión y a pensar que tratar el tema
es una aventura muy compleja. En primer lugar descubrimos que en realidad no
hay una definición para Inteligenicia Artificial, sólo aproximaciones y aunque
teóricamente lo anterior resulta desconcertante al inicio, se transforma en una
interesante ventaja cuando tenemos que definir el alcance de describir inteligencia
a la hora de afirmar que vamos a trabajar con un agente inteligente. La primera
definición con que nos encontramos y que basta con leerla una sola vez para
tratar de huir es la de Minsky (1984) donde nos dice que la Inteligenicia Artificial
es "el estudio de cómo programar computadoras que posean la facultad de hacer
aquello que la mente humana puede realizar" . Hacer que una computadora
realice los procesos que lleva a cabo el cerebro humano no es la clase de
actividad que nosotros deseamos realizar, por lo tanto, pensar en los alcances de
esa definición nos llevaría a un nivel de complejidad extremo.
Investigando un poco más en la teoría de la Inteligenicia Artificial nos
encontramos con el trabajo de Barber et al. (2002) el cual nos dice que la
Inteligenicia Artificial al ser tan amplia como lo vimos en la definición anterior, es
posible definirla de manera separada en dos grupos: inteligencia relacionada al
procesamiento de la información y la inteligencia que trata de la representación de
la información. Lo anterior nos reduce notablemente los alcances de lo que es la
Inteligencia Artificial y ahora lo que corresponde es situar al agente inteligente
dentro de una de esas dos clasificaciones. La representación de la información
tiene que ver con los sentidos, el lenguaje y la comunicación, de tal manera que
cuando decimos que el agente inteligente interactua de manera verbal y escrita es
sencillo pasar de lado la representación de la información. Ahora que sabemos
en que clasificación dentro de la Inteligencia Artificial estamos indagando un poco
más y descubrimos que Russell y Norvig (1995) profundizan más y ponen sobre
la mesa dos aproximaciones más a lo que es la Inteligenicia Artificial; esta vez las
clasificaciones son en base al pensamiento y al comportamiento humano. El
enfoque del pensamiento tiene que ver muy estrechamente con la razón mientras
que el comportamiento nos habla de conductas. Nuestro agente inteligente
sabemos que se comunica de manera escrita y verbal lo cual nos dejó de lado la
representación de la información en la Inteligencia Artificial, sin embargo todavía
es muy amplio el espectro de posibilidades dentro de la inteligencia, esto debido a
que se puede pensar que el agente tiene la capacidad de razonar y tiene un
comportamiento. Entonces hay que reducir un poco más el alcance y situar al
agente del lado del comportamiento. La Inteligenicia Artificial basada en el
comportamiento, la definen Russell y Norvig (1995) como sigue: "un campo de
estudio que busca explicar y emular la conducta inteligente en términos de
procesos computacionales" . Con esta última definición nos queda claro que la
inteligencia a la que nos referimos cuando decimos agente inteligente se refiere a
la de comunicarse e interpretar una conducta inteligente. Nuestro agente
entonces se comunica con el usuario de manera verbal y escrita, a partir del uso
de ese medio de comunicación simula un comportamiento.
Se puede concluir que con esta investigación se establecen los elementos
necesarios para desarrollar un "chatbot"  el cual es una herramienta de apoyo
para los usuarios y especialistas en orientación vocacional.
La ventaja de este proyecto es que la orientación vocacional ya no se podrá
realizar con extensos test, se ahorra tiempo al no tener que ir a una oficina de
orientación vocacional, el joven podrá realizar este análisis desde la comodidad y
privacidad de su casa, este proceso será más agradable ya que se establece un
diálogo simulado y amigable con otra persona.
La aplicación de este software impacta con mayor incidencia en los jóvenes, los
cuales podrán desempeñarse con mayor eficiencia en su actividad profesional.
Asimismo este sistema repercutirá en una disminución de la deserción y
reprobación escolar.
Para finalizar explicaré brevemente las actividades que realizamos para el
desarrollo y puesta en marcha del "chatbot" , el primer paso que realizamos fue
obtener información sobre orientación vocacional como son los datos que
contienen los test vocacionales, determinar las posibles respuestas a cada
expresión por parte de los usuarios. La cual se almacena en la base de
conocimiento del robot. Posteriormente procedimiento a diseñar el diálogo del
robot que consistió en implementar las preguntas que se realizan en un análisis
vocacional para de esta forma determinar la vocación de cada persona. Para
posteriormente implementarlo en código AIML. Se realizaron las pruebas del
diálogo y se agrego una imagen interactiva a la interfaz del agente inteligente.
El desarrollo de este agente en orientación vocacional se realizó en base al
modelo de Russell (2002) que fundamenta el desarrollo de un agente inteligente
capaz de proporcionar información acerca de celulares en venta desde un sitio
Web.
La metodología para la puesta en marcha del sistema fue en primer lugar una
entrevista personal con cada uno de la usuarios que participaron en las pruebas,
lo primero que hicieron fue llenar un test de texto para determinar la orientación
vocaciona anexo A, posteriormente se realizaron pruebas con el agente,
siguiendo la entrevista, llenaron el cuestionario de usabilida anexo B, para
proceder a graficar los resultados. Y después se comparó el resultado del test
con el resultado que proporcionó el agente. En el caso de las pruebas de
congruencia. En el test de usabilidad se graficaron los datos resultado de las
respuestas de los usuarios.
Para finalizar se aplicó el concepto de usabilidad con la finalidad de lograr que la
interfaz proporcione funcionalidad y satisfacción a los usuarios, por lo que se
aplicó una encuesta donde se incluyeron diez preguntas referentes a la
usabilidad, basadas en el cuestionario SUS (Brooke, 1996), las cuales nos dieron
resultados favorable ya que los usuarios del agente mostraron interés en utilizar
el sistema, no lo encontraron innecesariamente complejo, lo consideraron sencillo
de utilizar y opinaron que se puede aprender a utilizar fácilmente, se han sentido
muy confiados al momento de interactuar con él, por lo que no necesitan el apoyo
de una persona experta para utilizar el sistema. No necesitan información previa
sobre el agente conversacional, ya que es muy sencillo de usar, comentaron.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
Sistema Inteligente Conversacional para la
Orientación Vocacional
Tésis que para obtener el grado de
Maestra en Computación
Presenta:
Ana Claudia Ruiz Tadeo
Asesores:
Dr. Miguel García Ruiz
Dr. Nicandro Farías Mendoza
Julio/ 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Recuperación rápida de imágenes mediante memorias asociativas"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Dentro del reconocimiento de patrones, existen muchos trabajos que respaldan a las
memorias asociativas, para resolver de manera eficaz la recuperación de las imágenes.
Sin embargo, las memorias asociativas tienen una debilidad: que por ser matrices, las
operaciones consumen demasiado tiempo de procesamiento.
Por lo tanto, para procesar una imagen muy grande (una imagen espacial por ejemplo)
el tiempo de cómputo que se consume al usar una memoria asociativa aumenta
considerablemente.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo
General
Dada una imagen contaminada con ruido (gaussiano o sal y pimienta) recuperar la
imagen original utilizando memorias asociativas.

Particulares
*	Desarrollar un esquema de reducción de tiempo al aplicar las memorias
asociativas.
*	Integrar a este esquema el reconocimiento por sub-patrones.
*	Realizar pruebas con diferentes tipos de ruido.
*	Desarrollar un sistema computacional que realice la recuperación rápida de
imágenes digitales mediante memorias asociativas.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El problema puede resolverse con un esquema de procesamiento en paralelo, pero la
mayoría de los modelos de memorias asociativas existentes no lo contemplan.
En este trabajo se propone un esquema de reducción de tiempo para memorias
asociativas, en el que se utilizan partes específicas de la imagen para recuperar la
imagen completa. El esquema funciona aún cuando la imagen se encuentra afectada por
ruido.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En este capítulo se proponemos una metodología para la recuperación de imágenes mediante memorias asociativas ab extendida.
La metodología que empleamos en esta tesis, hace uso de la característica local del ruido, por medio de la cual se aísla la parte afectada y reconoce por medio de las áreas no afectadas. 
Para resolver el problema con esta metodología lo dividimos en dos fases de tres etapas cada una. En la primera fase o fase de aprendizaje preprocesamos en la primera etapa,
según sea la imagen en niveles de gris o color; en la segunda etapa aplicamos el esquema de reducción para obtener vectores de rasgos que la identifiquen y por último
la tercera etapa que consta del entrenamiento de las memorias asociativas, este procedimiento lo repetimos tantas veces como patrones queramos almacenar, tal como se muestra en la Figura. 3.1
Para la segunda fase o fase de recuperación preprocesamos la imagen en niveles de gris
o color según sea el caso; en la segunda etapa aplicamos el esquema de reducción para
obtener vectores de rasgos que la identifiquen , en la tercera etapa recuperamos por
medio de la memoria asociativa y por ultimo utilizamos un sistema de votación, como
se ilustra en la Figura 3.2.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este capítulo se presentan las conclusiones finales de la investigación a partir de los
resultados teóricos y prácticos presentados sobre la recuperación rápida de imágenes
mediante memorias asociativas.
En los últimos años, las redes neuronales y en particular las memorias asociativas han
solucionado estos problemas. Una memoria asociativa puede almacenar información y
poder recuperarla cuando sea necesario. Pero para su funcionamiento requiere gran
cantidad de tiempo de cómputo. El procesamiento de una imagen suele tomar más
tiempo del que nos lleva adquirirla. Y más aún si la imagen adquirida contiene ruido, lo
que usualmente ocurre. Dentro del reconocimiento de patrones, existen muchos trabajos
que respaldan a las memorias asociativas, para resolver de manera eficaz la
recuperación de las imágenes. Sin embargo, las memorias asociativas tienen una
debilidad: que por ser matrices, las operaciones consumen demasiado tiempo de
procesamiento.
Por lo tanto, para procesar una imagen muy grande (una imagen espacial por ejemplo) el tiempo de cómputo que se consume al usar una memoria asociativa aumenta
considerablemente.
En este trabajo se presentó un esquema de procesamiento en paralelo para memorias
asociativas. En él se utilizan partes específicas de la imagen para recuperar la imagen
completa. El esquema funciona aún cuando la imagen se encuentra afectada por ruido.
Para ello se integró a este esquema el reconocimiento por sub-patrones. El cual divide
una imagen en partes llamadas subpatrones y a cada parte le asigna una memoria
asociativa.
El sistema realizado recupera una imagen contaminada con ruido (gaussiano o sal y
pimienta) utilizando memorias asociativas. Se realizaron pruebas con diferentes tipos de
ruido. El desempeño de la recuperación aumenta al trabajar con subimágenes ya que se
aísla la región afectada y las partes no afectadas por ruido ayudan a una mejor
recuperación.
La metodología que se presentó en esta tesis si requiere menor tiempo de procesamiento
y menos recursos que la forma tradicional.
Una de las ventajas de la metodología basada en subpatrones es que nos permite el uso
de imágenes de diversos tamaños para una sola memoria, ya que el esquema utilizado
reduce las imágenes a patrones de entrada de un mismo tamaño. También nos soporta
mayores cantidades de ruido en la recuperación.
Una de las aplicaciones en las que se podría utilizar esta metodología es la búsqueda de
imágenes usando la imagen misma como parámetro de búsqueda. Es decir, dada una
imagen en una base de datos de imágenes encontrar las imágenes que se más se le
parezcan. En este caso se utilizarán las subimágenes en color para determinar qué tanto
se parecen.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Recuperación rápida de imágenes mediante memorias asociativas
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN
PRESENTA
MIRIAM BALBUENA SÍNCHEZ
DIRECTOR: Dr. JUAN HUMBERTO SOSSA AZUELA
México D.F., Diciembre 200</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Resolución automática de la homonimia morfológica para el español"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Nuestro objetivo general es encontrar un método no supervisado para la
etiquetación de clases gramaticales, que dependa de propiedades distribucionales
generales del texto, estas propiedades deben ser invariantes a través de los lenguajes.
Implementamos un sistema de etiquetación, que solamente utiliza información
correspondiente a las clases gramaticales de las palabras, esta información es
representada en un formato de etiquetas.
Se propone una solución empírica para la etiquetación de categorías
gramaticales, únicamente con información pertinente a las clases gramaticales a las que
corresponden las palabras en un corpus y se propone continuar, con algunos métodos
para mejorar el trabajo.
Objetivos específicos
En este proyecto consideramos que existe una manera de atacar el problema de
etiquetación automática usando distintas redes neuronales sugiriendo restricciones en
las clases gramaticales de palabras posibles y desconocidas, obtenidas de su contexto.
El objetivo específico es desarrollar el modelo, el método y el software para un
sistema computacional, idóneo para resolver el problema de la etiquetación de clases
gramaticales, únicamente asignando una etiqueta que corresponde a la categoría
gramatical a la cual pertenece una palabra, esta asignación se debe efectuar de manera
correcta. Con la metodología de las redes neuronales y usando el algoritmo de
retropropagación, tratamos de encontrar una solución al problema de la resolución
automática de la homonimia morfológica para el idioma español.
Para desarrollar el modelo de este sistema, es necesario implementar distintos
tipos de redes neuronales, realizar diferentes experimentos en cada una de las redes
modificando algunos de sus parámetros. También es necesario analizar y comparar los
resultados obtenidos en las redes. Este proceso es imprescindible para desarrollar el
método de resolución, necesario para resolver el problema planteado.
No sólo hay que separar la homonimia, sino también tomar en consideración
enteramente el contenido comunicativo de la oración.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El uso explícito de etiquetas de contextos antecedentes y subsecuentes,
representados en redes neuronales, con un amplio uso de características gramaticales
léxicas, obtiene una solución de la homonimia morfológica, con probabilidades en los
contextos de cada una de las diferentes redes empleadas.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Este trabajo de investigación permitió desarrollar un método de resolución en la
etiquetación de clases gramaticales como una aplicación de redes neuronales, este método
trabaja con un buen nivel de precisión, cuando existe información completa en los
contextos anteriores y posteriores de una determinada frase.
Aplicamos un etiquetador de clases gramaticales para el español y probamos una
mejora con un costo computacional pequeño.
La tarea involucraba la clasificación de las salidas de cada arquitectura de la red, en
alguna de las 15 categorías posibles.
La solución al problema mas general usando retropropagación es una aproximación
a la función muchas entradas, muchas salidas, donde cada nodo de salida en la red
corresponde a una dimensión de salida del vector de salida y cada nodo de entrada en la red corresponde a una dimensión del vector de entrada. El número de capas y nodos ocultos
son dependientes del problema.
El tiempo requerido para el entrenamiento es proporcional al producto del número
de iteraciones, el número de pesos usados en la red y el número de muestras. Entonces los
requerimientos computacionales para el entrenamiento pueden ser constantemente largos
para algunas redes de tamaño razonable.
En problemas multiclase como el de esta investigación, cada nodo de salida
corresponde a una clase separada y la clase a la cual el patrón es asignado debe ser indicada por el nodo con la salida más alta, para ese patrón.
Se obtuvieron buenos resultados en la evaluación de los experimentos, al separar
palabras que tenían más de una etiqueta, con lo que resolvimos de alguna manera el
problema de ambigüedad en la etiquetación. Para resolver este problema y obtener una
etiqueta correcta, se verificaron los vectores de salida en cada una de las redes más los
pesos que contenían cierta información, dependiendo de los contextos y la etiqueta que
obteníamos como salida, era aquella localidad con el valor máximo en nuestro vector de
salida.
Una vez implementado el sistema es posible experimentar con diversas estrategias
de etiquetación esto con el fin de sacar el mayor provecho de él.
En este trabajo se describe una aportación para la etiquetación de clases
gramaticales sin usar información léxica.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN
COMPUTACIÓN
Resolución automática de la homonimia morfológica para el español
TESIS
Que para obtener el grado de:
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN
P R E S E N T A
José Ernesto Gómez Balderas
Director:
Dr. Grigori Sidorov
Codirector:
Dr. Héctor Jiménez Salazar
México D.F. Abril de 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Desarrollo de una Suite BPM para el modelado, ejecución y monitoreo de los procesos de un Modelo de Mejora de Procesos de Desarrollo de Software"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los procesos para desarrollar productos de software "complejos" ,
frecuentemente lo son también; debido a esta complejidad, su modelado,
documentación y monitoreo se dificulta. En particular, estas dificultades se les
presentan a las organizaciones que trabajan de acuerdo a los Modelos de
Mejora de Procesos. También, como los procesos son continuamente
estudiados y rectificados, mantener la documentación actualizada representa
un reto adicional. En general, las dificultades a las que se enfrentan las
organizaciones con respecto a los procesos son: modelar, documentar, ejecutar y monitorear los procesos; así mismo, mantener actualizada la documentación
donde se encuentren descritos.
Existen organizaciones en México con menos de cincuenta participantes, conocidas como PyMES, que tienen que enfrentar las dificultades antes mencionadas con escasos recursos económicos y de personal. Dar solución a estas dificultades con bajo costo, permitiría a las PyMES dedicadas al desarrollo de software, implementar, sin estas dificultades específicas, los Modelos de Mejora de Procesos. Al lograr implementar estos modelos, trabajarían con procesos que ayudarían a producir software de calidad.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Estudiar la factibilidad de construir una Suite BPM de bajo costo que permita
soportar el ciclo de vida de los procesos de un Modelo de Mejora de Procesos
de acuerdo a las etapas del enfoque BPM de Modelado, Ejecución y Monitoreo.
Objetivos específicos
Objetivo 1. Soportar dentro de la Suite BPM el modelado de un número
variable de procesos de un Modelo de Mejora de Procesos.
Objetivo 2. Soportar la etapa de Ejecución de procesos operativos, en
particular en lo que se refiere al intercambio de artefactos y el control de
versiones de los mismos.
Objetivo 3. Soportar mecanismos flexibles de colecta de métricas que
permitan el monitoreo durante la etapa de Ejecución del proceso.
Objetivo 4. Proveer información a los participantes del proceso sobre la
etapa en la cual se encuentra la instancia del proceso durante la etapa de
Ejecución del proceso y sobre las tareas que deben realizar.
Para alcanzar estos objetivos, en este trabajo de investigación se siguió la
metodología que se presenta a continuación (ver Figura 1).</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Una Suite BPM puede ser adaptada para soportar el ciclo de vida de los
procesos de un Modelo de Mejora de Procesos en las etapas de Modelado,
Ejecución y Monitoreo.
En base a esta hipótesis, se plantean el objetivo general y los objetivos
específicos que guiaron la investigación.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Metodología
1. Investigación y redacción del Estado del arte.
2. Definición de los requerimientos de la Suite BPM
2.1 Identificación de Requerimientos Funcionales.
2.2 Identificación de Requerimientos No Funcionales.
6
3. Diseño y Construcción de la Suite BPM.
2.3 Diseño de la arquitectura.
2.4 Desarrollo de componentes.
4. Evaluación de los objetivos de la investigación
4.1 Selección de procesos a modelar.
4.2 Modelado de procesos seleccionados.
4.3 Revisión del cumplimiento de los Requerimientos Funcionales.
5. Análisis e interpretación de resultados y redacción de conclusiones.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Conclusión del objetivo uno.
En esta sección se presenta el objetivo 1, su evaluación, resultado, justificación
del resultado y limitaciones en el alcance del objetivo.
Objetivo 1. "Soportar dentro de la Suite BPM el modelado de un número
variable de procesos de un Modelo de Mejora de Procesos" .
Evaluación. "La evaluación del alcance de este objetivo se realiza mediante el
modelado de procesos seleccionados de un Modelo de Mejora de Procesos.
Los procesos son seleccionados porque incorporan elementos que se
encuentran en todos los demás procesos estudiados" .
Resultado. Parcialmente se alcanzo el objetivo 1.
Justificación. La Suite BPM permite modelar 6 de 10 elementos de la notación
EPF (ver sección 5.2.1.5). Esto se debió principalmente a que no se modelaron
las descripciones de Procesos o Tareas (notación EPF). La interpretación de
este resultado es que las Suites BPM estudiadas, se han especializado en la
ejecución del proceso, más no así en la capacitación simultanea al Participante
del Proceso, es decir, le indican al participante que hacer, pero sin detalladas
explicaciones. En este trabajo, con la Suite BPM que se construyó, se demostró
que es posible adaptar las Suites BPM para que presenten las descripciones
contenidas en los Modelos de Mejora de Procesos.
Limitaciones. La Suite BPM que se construyó no modela subprocesos.
Conclusión particular. Se concluye que la Suite BPM soporta parcialmente el
modelado de un número variable de procesos de un Modelo de Mejora de
Procesos.
De acuerdo al estado del arte, los Modelos de Mejora de Procesos tienen como
propósito capacitar y guiar a los participantes de los procesos. Esto hace que
cuenten con descripciones amplias sobre los elementos con los que se definen
los procesos, además de otros aspectos como el intercambio y control de
versiones de artefactos. Las Suites BPM estudiadas cuentan con los elementos
para modelar la ejecución de los procesos de negocios, pero no modelan las
descripciones mencionadas y los otros aspectos de los Modelos de Mejora de
Procesos. Por esta razón existe la necesidad de hacer adaptaciones a la Suites
BPM estudiadas si se desea modelar este tipo de modelos.

Conclusión del objetivo dos
En esta sección se presenta el objetivo 2, su evaluación, resultado, justificación
del resultado y limitaciones en el alcance del objetivo.
Objetivo 2. "Soportar la etapa de Ejecución de procesos operativos, en
particular en lo que se refiere al intercambio de artefactos y el control de
versiones de los mismos" .
Evaluación. "La evaluación del alcance de este objetivo se realiza mediante un
estudio que permite verificar que la Suite BPM satisface los requerimientos de
los procesos operativos de los Modelos de Mejora de Procesos (sección 2.2.6),
en particular el intercambio y el control de versiones de artefactos" .
Resultado. Se alcanzó el objetivo 2.
Justificación. La Suite BPM soporta:
El acceso restringido a participar en un proceso es una ventaja de seguridad
que ofrece ocupar una Suite BPM.
Crear varias instancias de un mismo proceso de forma tal que con cada
instancia, por ejemplo, se puede estar desarrollando un producto de
software diferente.
Que en la etapa de Ejecución de un proceso, al mandar las tareas al monitor
de los participantes del proceso, que se adhieren al orden de las tareas.
También los participantes del proceso pueden intercambiar artefactos sin
tener que enviarlos por correo o de mano en mano. Es posible que el
Administrador del Proceso recupere versiones anteriores de artefactos.
"Cerrar"  una tarea sin marcha atrás de forma tal que el Participante del
Proceso sólo tiene la opción de proseguir con la siguiente tarea.
La actualización e instalación de una versión de un modelo de una definición
de un proceso, sólo se hace al inicio, o terminación de la Instancia del
Proceso, pero no durante la instancia.
Limitaciones. Cuando se instanciaron los procesos seleccionados se
observaron las siguientes limitaciones que no se implementaron porque no se
identificaron como requerimientos de la Suite BPM en el inicio:
- Cuando un proceso ocupa las salidas (artefactos) de otros procesos, no se
implementó que la Suite BPM automatice pasar los artefactos completados
por un proceso al proceso que los requiere como entradas.
- La Suite BPM carece de un mecanismo que avise a los participantes del
proceso que ya pueden realizar su tarea, porque ya se llegó a la actividad
que incluye su tarea.
Conclusión particular. Se concluye que la Suite BPM soporta la etapa de
ejecución de los procesos operativos, en particular en lo que se refiere al
intercambio de artefactos y el control de versiones de los mismos.

Conclusión del objetivo tres
En esta sección se presenta el objetivo 3, su evaluación, resultado, justificación
del resultado.
Objetivo 3. "Soportar mecanismos flexibles de colecta de métricas que
permitan el monitoreo durante la etapa de Ejecución del proceso" .
Evaluación. "La evaluación del alcance de este objetivo se realiza mediante
una revisión del Diseño Arquitectural que demuestre que se han considerado
mecanismos que permitan una adición flexible de componentes para la
recolecta de métricas" .
Resultado. Se alcanzó el objetivo 3.
Justificación. El diseño que se realizó permite agregar un nuevo componente
de colecta de métricas y el impacto de su introducción sólo afecta a un
componente. Un nuevo componente de colecta de métricas permite ampliar el
monitoreo de la Instancia del Proceso. Los mecanismos fueron presentados en
la sección 3.3.5.2.2.
Conclusión particular. Se concluye que la Suite BPM soporta mecanismos
flexibles de colecta de métricas que permitan el monitoreo durante la etapa de
Ejecución del proceso.

Conclusión del objetivo cuatro
Objetivo 4. "Proveer información a los participantes del proceso sobre la etapa
en la cual se encuentra la Instancia del Proceso durante la etapa de Ejecución
del proceso" .
Evaluación. "La evaluación del alcance de este objetivo se realiza mediante un estudio que permite verificar que la Suite BPM incluye la funcionalidad de proveer a los participantes del proceso la información sobre la fase y tarea en la cual se encuentra la Instancia del Proceso" .
Resultado. Se alcanzó el objetivo 4.
Justificación. Una de las ventajas que ofrece ocupar la Suite BPM, que proporciona tanto a los participantes de los procesos como al Administrador del Proceso información sobre la fase y tarea en donde se encuentra la Instancia
del Proceso a través de la Tabla de monitoreo de la Instancia del Proceso.
Limitaciones. La Suite BPM que se construyó incluye diagramas donde el Participante del Proceso puede observar las actividades del proceso completo, pero no existe un señalamiento que le indique en el diagrama la actividad
exacta en la que se encuentra el proceso.
Conclusión particular. Se concluye que la Suite BPM soporta proveer información a los participantes del proceso sobre la etapa en la cual se encuentra la Instancia del Proceso durante la etapa de Ejecución del proceso.

Conclusión general
Se concluye que es posible construir una Suite BPM de bajo costo que permita
soportar el ciclo de vida de los procesos de un Modelo de Mejora de Procesos
de acuerdo a las etapas del enfoque BPM de Modelado, Ejecución y Monitoreo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD AUTÓNOMA METROPOLITANA
UNIDAD IZTAPALAPA División de Ciencias Básicas e Ingeniería
"Desarrollo de una Suite BPM para el modelado,
ejecución y monitoreo de los procesos de un Modelo
de Mejora de Procesos de Desarrollo de Software" 
Tesis que presenta:
C.P. y Lic. Silvia Nagheli Márquez Solís
Para obtener el grado de:
MAESTRA EN CIENCIAS
DEL
POSGRADO EN CIENCIAS Y TECNOLOGÍAS
DE LA INFORMACIÓN
Asesores:
Dr. Humberto Cervantes Maceda
Dr. Carlos Montes de Oca Vázquez
Jurado calificador:
Presidente: M. en C. Alfonso Martínez Martínez
Secretario: Dr. Humberto Cervantes Maceda
Vocal: M. en C. Mery Helen Pesantes Espinoza
MEXICO, D.F. JULIO 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"CONSTRUCCIÓN AUTOMÍTICA DE DICCIONARIOS SEMÍNTICOS USANDO LA SIMILITUD DISTRIBUCIONAL"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La generación de ontologías para un dominio, representa un desafío, ya que para generarlas se requiere de los servicios de uno o varios expertos en el dominio, con experiencia en el tema, y experiencia en la construcción de diccionarios semánticos.
El método que se presenta en esta tesis, no requiere la intervención del usuario y es mucho más rápido que hacer un diccionario semántico manualmente.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Se pretende crear un sistema que genere diccionarios semánticos automáticamente en base a textos de un dominio en específico.
Objetivos específicos
Crear los módulos pertinentes al sistema automático de generación de diccionarios o Implementar método del coseno del ángulo. Módulo para clusterización de términos.
o Implementar el método K-Means.
Hacer los Siguientes Experimentos: Experimentos con un corpus de Informática, descargados de www.wikipedia.com, comparándolo con un corpus general que serán las noticias del periódico Excélsior.
Al final de la tesis se esperan obtener los siguientes productos:
o Metodología de construcción de diccionarios semánticos a partir de los textos especializados, utilizando un conjunto de métodos estadísticos.
o Extracción de términos. Utilizando el algoritmo Log-Likelihood.
o Extracción de ciertas relaciones semánticas. Utilizando el método del coseno del ángulo.
o Construcción del diccionario semántico.
o Prototipo de software que toma como entrada un conjunto de textos y construye un diccionario semántico tipo ontología en modo automático (sin alguna intervención del usuario). 
semánticos. Módulo para análisis de frecuencias de palabras. Módulo para extracción de conceptos.
o Implementar método para cálculo de Log-Likelihood.
o Implementar método para cálculo de Tf.Idf. Modulo para cálculo de Similaridad entre 2 palabras.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se presenta un sistema para la construcción automática de diccionarios semánticos usando la similitud distribucional observada en los textos de un dominio.
Este sistema construye diccionarios semánticos de un dominio en específico con las siguientes características: Contiene los términos más utilizados dentro de ese dominio. Pondera la semejanza entre los términos encontrados. Estos términos están agrupados en grupos con al menos un elemento, cuyos elementos tienen un alto grado de similitud. Estos términos tienen baja similaridad con los términos de otros clústeres.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
LABORATORIO DE LENGUAJE NATURAL Y PROCESAMIENTO DE TEXTO CONSTRUCCIÓN AUTOMÍTICA DE DICCIONARIOS SEMÍNTICOS USANDO LA SIMILITUD DISTRIBUCIONAL
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN
PRESENTA
L. I. MOISES EDUARDO LAVÍN VILLA
DIRECTORES DE TESIS:
DR. GRIGORI SIDOROV
DR. ALEXANDER GELBUKH
MÉXICO, D. F. 08 de Enero de 2010</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Análisis de protocolos de encaminamiento para redes inalámbricas tipo malla en modo infraestructura"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>En este trabajo de investigación se busca evidencia que respalde o refute la adopción de
protocolos de encaminamiento diseñados específicamente para redes ad hoc en iWMN.
El objetivo principal de este proyecto es evaluar los protocolos de encaminamiento
más populares y representativos del encaminamiento reactivo y proactivo en redes ad
hoc, AAd hoc On Demand Distance Vector (AODV) y Destination Sequenced Distance
Vector (DSDV). Partiendo de esta evaluación, se enumeran las características, de cada
esquema de encaminamiento, que pueden ser adaptadas para iWMN.
Los protocolos de encaminamiento diseñados para redes ad hoc fueron diseñados para
ser implementados y ejecutados de manera distribuida sobre una arquitectura de red
plana", donde todos los nodos que conforman la red tienen el mismo conjunto de responsabilidades.
Es así que la organización jerárquica que existe en las iWMN, demanda
modificaciones significativas al funcionamiento de estos protocolos. Como parte de los
objetivos de este proyecto, se realizan un conjunto de adecuaciones para cada esquema
de encaminamiento que permita su ejecución distribuida en una arquitectura de red
jerárquica tal y como la presente en iWMN.
A través de simulaciones computacionales se evaluará un conjunto de medidas de desempe~
no (caudal de datos (throughput), retardo de extremo a extremo, tasa de entrega,
señalización del protocolo, latencia de adquisición de ruta, la distribución del tráfico
y las pérdidas dentro del backbone de la red) que permitirán evidenciar los efectos
inducidos por estos esquemas de encaminamiento en iWMN.
Los resultados obtenidos por este trabajo de investigación nos dan la pauta para enumerar
las ventajas e inconvenientes que se presentan al utilizar estos esquemas de
encaminamiento, y nos auxilian en la identificación de las direcciones a seguir para
futuras propuestas de encaminamiento específicas para iWMN.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Para el desarrollo de este trabajo de investigación se utilizó la siguiente metodología:
Estudio sobre arquitecturas de red inalámbricas y particularmente iWMN.
Revisión de la literatura sobre protocolos de encaminamiento diseñados para redes
inalámbricas.
Revisión del estado del arte sobre evaluación de protocolos de encaminamiento en diversas
arquitecturas de red inalámbrica, particularmente en redes ad hoc e iWMN.
Diseño de las propuestas de modificación a los protocolos de encaminamiento seleccionados
para este proyecto.
Determinación de los parámetros de desempeño así como del escenario de simulación.
Codificación del escenario de simulación propuesto e implementación de las modificaciones
a los protocolos de encaminamiento seleccionados.
Evaluación de desempeño de los protocolos de encaminamiento a través del simulador
de eventos discretos ns2.
Análisis e interpretación de resultados.
Redacción de la idónea comunicación de resultados.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Las redes inalámbricas en malla en modo infraestructura (iWMNs) han surgido como
una de las propuestas más viables para atender la demanda de conectividad total de una
manera económica, eficiente y fácil de implementar. En este trabajo de investigación, hemos
presentado un estudio detallado sobre las características y particularidades de esta novedosa arquitectura de red. Debido a que las iWMNs combinan algunas propiedades de la arquitectura de redes ad hoc y redes en modo infraestructura (WLAN), éstas heredan un conjuto de ventajas y dificultades de cada una de estas arquitecturas. Uno de los problemas más significativos en iWMNs es el encaminamiento de la información, y comúnmente, la solución más inmediata a este problema consiste en utilizar como base a los protocolos de encaminamiento diseñados específicamente para redes ad hoc. Por esta razón, en este trabajo hemos realizado un estudio detallado acerca del problema del encaminamiento en redes inalámbricas y hemos explorado el estado del arte de la evaluación de protocolos de encaminamiento en iWMNs. Los resultados obtenidos tras la revisión del estado del arte, justifican los objetivos planteados para este proyecto y de manera conjunta nos han permitido diseñar un escenario de simulación representativo y un conjunto integral de medidas de desempeño.
En este trabajo de tesis evaluamos el desempeño de dos protocolos de encaminamiento
que son representativos de los esquemas de encaminamiento reactivo y proactivo para redes
ad hoc. Se han seleccionado los protocolos de encaminamiento AODV y DSDV como los
principales representantes del encaminamiento en redes ad hoc, debido a la gran popularidad de éstos. Estudiamos los efectos que cada esquema de encaminamiento induce en la red como función de la densidad de tráfico en la misma. Uno de los aportes más significativos de nuestra investigación, ha sido la propuesta de mejora al funcionamiento clásico de los protocolos de encaminamiento para generar un comportamiento basado en jerarquías tal y como lo demanda la arquitectura de las iWMNs; conservando en todo momento la ejecución distribuida de estos protocolos. Los resultados obtenidos muestran que cada uno de los protocolos ofrece ventajas bajo ciertas condiciones en la red. Ambos protocolos han ofrecido un desempeño similar para diversos parámetros, y las diferencias entre estos resultados han sido mínimas, e.g. throughtput y tasa de entrega. Sin embargo, existen parámetros en donde cada protocolo
muestra ventajas muy superiores a las de su contraparte. éstas se describen a continuación.
AODV ha presentado un mejor desempeño con niveles de carga baja-media. Este protocolo
de encaminamiento ofrece un menor retardo comparado con el que se obtiene al utilizar
DSDV. La tasa de entrega de AODV también es ligeramente superior a la que se obtiene
con DSDV. Para niveles de carga baja, media y alta, AODV supera la tasa de entrega que
ofrece DSDV en aproximadamente un 5% en todos los experimentos realizados. Por otro
lado, AODV supera por un amplio margen los resultados de DSDV, en el mejor de los casos,
AODV ofrece un retardo promedio 50% menor al que se obtiene con DSDV. DSDV por su
parte, ha optimizado sustancialente la carga de señalización, pues el porcentaje de paquetes
de señalización se decrementa rápidamente a medida que la tasa de transmisión de las STAs
se incrementa gradualmente. La tasa de decremento de este parámetro en DSDV, es mayor
a la que se presenta con el protocolo AODV. DSDV logra disminuir su carga de señalización
y permite enviar un mayor número de paquetes de datos en relación a lo que se obtiene con
AODV. En el mejor de los casos, DSDV puede enviar hasta 20 paquetes de datos promedio
por cada mensaje de señalización mientras que AODV logra enviar solo 10 paquetes de datos.
Por otro lado, AODV ha logrado distribuir de manera más uniforme el tráfico en la red,
mientrasque los resultados obtenidos para el protocolo DSDV demuestran que tanto las
pérdidas así como la distribución del tráfico se ha concentrado muy específicamente en la
zona central del backbone y principalmente en los MAPs. Una de las desventajas más importantes que enfrenta un protocolo de encaminamiento reactivo es el tiempo necesario para adquirir una ruta. En nuestro escenario de simulación, AODV requiere, para todos los casos, un tiempo de descubrimiento de rutas menor a 2:5 segundos, lo cual le representa una muy ligera desventaja respecto a DSDV cuyo tiempo promedio siempre resulto 0 segundos, pues el tiempo en el que comenzarón las transmisiones de datos fue superior a 20 segundos, lo cual le da el tiempo suficiente de convergencia al protocolo.
Dadas las ventajas que cada protocolo de encaminamiento presenta ante las diversas propiedades de la red, sugerimos como trabajo futuro, el diseño de un protocolo híbrido que tome como base los resultados que se han obtenido en esta investigación. Dada la naturaleza estática y estable del backbone de la red, DSDV puede ser el candidato óptimo para funcionar dentro de este nivel de la jerarquía de las iMWNs. La baja carga de señalización y su comportamiento proactivo se adapta de mejor modo a la conducta del backbone. Si consideramos que este nivel de la red cuenta con un alto grado de estabilidad, se podría disminuir la carga de señalización del protocolo DSDV, si la frecuencia con la que se intercambian los mensajes de señalización se ajusta dinámicamente a los eventos en el backbone. Es decir, si la topología
y conectividad del backbone se mantienen estables, la frecuencia de señalización puede ser disminuida gradualmente, y solo en caso de detectarse algún evento ajeno al comportamiento del backbone se podría aumentar la frecuencia de estos mensajes hasta que la totalidad del sistema detecte el evento y se alcance de nueva cuenta un estado de estabilidad. La movilidad y la constante pérdida de conectividad entre las STAs y el backbone de la red limitan el uso de DSDV en la parte inferior de la jerarquía, es decir, en los usuarios móviles. En este caso proponemos utilizar AODV para ser ejecutado por los usuarios móviles. Las frecuentes pérdidas de conectividad en la parte inferior de la jerarquía requieren de un protocolo reactivo que solo en el momento de transmitir información genere un mecanismo reactivo de asociación con el backbone de la red y AODV es una muy buena opción para este fin.
Como se ha mostrado en los resultados de esta investigación, los MAPs de la red son
los elementos que sufren una mayor cantidad de pérdidas y presentan una alta densidad de
tráfico. En el trabajo futuro es importante evaluar el efecto de la topología del backbone en
el desempeño de la red. La creación de regiones de coberura superpuestas o bien la inclusión de un mayor número de portales en la malla podrían incrementar significativamente
las prestaciones de la red. La utilización conjunta de métricas de encaminamiento que contemplan otras propiedades de la red, podría incrementar el desempeño de los protocolos de encaminamiento.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad Autonoma Metropolitana
Análisis de protocolos de encaminamiento para redes inalámbricas tipo malla en modo infraestructura Idónea Comunicación de Resultados
para obtener el grado de
Maestro en Ciencias
en Tecnologías de la Información
presentada por
Carlos Ernesto Carrillo Arellano
Asesor:
Dr. Víctor Manuel Ramos Ramos
Defendida públicamente en la UAM-Iztapalapa el 12 de septiembre de 2011 a las 12:00 hrs
frente al jurado integrado por:
Presidente : Dr. Raúl Teodoro Aquino Santos, Universidad de Colima, Facultad de Telemática
Secretario : Dr. Víctor Manuel Ramos Ramos, UAM - Iztapalapa, Redes y Telecomunicaciones
Vocal : Dr. Enrique Rodríguez de la Colina, UAM - Iztapalapa, Redes y Telecomunicaciones</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"DISEÑO E IMPLEMENTACIÓN EN HARDWARE DE UN ALGORITMO BIO-INSPIRADO"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
El objetivo general de este trabajo de tesis es realizar el diseño e implementación de un
Algoritmo Genético Compacto estándar y su versión con elitismo y mutación en VHDL
(Lenguaje de Descripción de Hardware de Circuitos Integrados de Muy Alta Velocidad, donde
la V es de Very High Speed Integrated Circuit (VHSIC) y HDL de Hardware Description Language). La implementación se realiza sobre un FPGA (Field Programmable Gate Array) de la marca Altera, el Cyclone II EP2C70F896C6.

Objetivos particulares
Los objetivos particulares de este trabajo son:
a) Diseñar e implementar en hardware un Algoritmo Genético Compacto (AGc) y la
versión con elitismo y mutación (AGcEM).
b) Diseñar en VHDL un conjunto de funciones de prueba clásicas para algoritmos
evolutivos.
c) Realizar pruebas de desempeño de los algoritmos sobre un FPGA.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>De acuerdo a los resultados obtenidos, podemos observar que un AGc implementado
en un FPGA es extremadamente rápido, del orden los microsegundos para el caso del
problema max-one. Sin embargo, esta velocidad puede ser afectada directamente por el tipo de función objetivo que se vaya a implementar, debido a que el cálculo de la función objetivo es la parte más costosa computacionalmente hablando, de estos tipos de algoritmos. Una forma de compensar un poco esta desventaja, es tratar de paralelizar lo más que se pueda la función objetivo, en la medida que sea posible, ya que esto incrementará la velocidad de ejecución del algoritmo genético compacto.
Otro aspecto interesante es que esta propuesta no está diseñada para sustituir a los
AG's, sino como una alternativa en aquellas aplicaciones donde existan restricciones de
hardware, por ejemplo, en el consumo de recursos.
Debido a las velocidades que se alcanzan con el algoritmo genético compacto, es
posible su implementación en aplicaciones de tiempo real y además por su mínimo consumo
de recursos, es adecuado para formar parte de aplicaciones de mayor escala, por ejemplo, la incorporación de un módulo o componente de este algoritmo en alguna aplicación de
hardware evolutivo, ya nuestro diseño es modular y está capacitado para formar parte de otro modulo en un nivel superior.
Los módulos diseñados que forman parte del algoritmo genético compacto pueden ser
de utilidad en el diseño de otros tipos de algoritmos bio-inspirados, incrementando la cantidad de alternativas para implementar motores de búsqueda en hardware.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
"DISEÑO E IMPLEMENTACIÓN EN HARDWARE DE UN ALGORITMO BIO-INSPIRADO" 
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN
P R E S E N T A
ALEJANDRO LEÓN JAVIER
DIRECTORES DE TESIS
DRA. NARELI CRUZ CORTÉS
DR. MARCO A. MORENO ARMENDÍRIZ
MÉXICO, D. F. 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Extracción de Información con Algoritmos de Clasificación"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La cantidad excesiva de documentos en lenguaje natural disponibles en formato electrónico hace imposible su análisis, los sistemas de extracción de información permiten estructurar esa información para un dominio especifico, lo que convierte el problema de analizar una colección de documentos en consultar una base de datos, siendo esto último más rápido de realizar además de hacer más factible encontrar una relación entre los datos. Sin embargo, la construcción de estos sistemas es una tarea costosa en tiempo y recursos tanto computacionales como humanos. Además, si tomamos en cuenta que actualmente se emplean una variedad de recursos lingüísticos para su construcción, el resultado son sistemas con un alto costo de portabilidad a nuevos dominios e idiomas.
Parece ser que la hipótesis actual en el desarrollo de sistemas de extracción de información es que se necesita entender el texto en el mayor grado posible para lograr extraer piezas de información específicas dentro del mismo. En contraste, la pregunta de investigación de la que parte esta tesis es:  ¿cómo construir sistemas de extracción de información sin utilizar un análisis lingüístico sofisticado? En otras palabras,  ¿cómo extraer información sin tratar de entender el texto? La hipótesis que tenemos y que pretendemos probar en el contenido del documento es que para ciertos fragmentos del texto que representan información factual de forma explicita, conocidos como entidades, es posible realizar su extracción sin tratar de entender el contenido del documento. Esto es, partimos de la idea que las palabras que rodean a las diferentes entidades (i.e. su contexto) presentan patrones léxicos que las caracterizan, llamados patrones de extracción, y hacen posible discriminar entre las entidades aquellas que representan información relevante o irrelevante para el estudio en caso. De modo tal que cuando ocurre una entidad no vista anteriormente, por medio de su contexto y los patrones de extracción existentes sea posible decidir si esta entidad representa información que debe ser extraída.
Consecuentemente, al pretender utilizar únicamente un análisis en el ámbito de palabras para extraer la información, dos preguntas necesitan ser contestadas:  ¿cómo obtener los patrones de extracción? y  ¿cómo detectar las entidades para analizar sus contextos? La respuesta que proponemos a la primera pregunta es utilizar técnicas de aprendizaje automático para así obtener los patrones requeridos, además esto coincide con las tendencias empíricas actuales que permiten al sistema una mayor portabilidad entre dominios. Con respecto a la segunda pregunta, nuestro objetivo es hacer el mínimo uso de recursos lingüísticos y así obtener una mayor portabilidad entre idiomas, por tal motivo la propuesta que tenemos es recurrir a un análisis mediante expresiones regulares para detectar las entidades; de manera que, enfocamos la extracción únicamente a entidades que presentan una forma regular, tal como ocurre con ciertos formatos de fechas, nombres propios y cantidades.
Con el objetivo de mostrar lo útil de la arquitectura planteada, se presenta una aplicación real de la extracción de información para el escenario de noticias en español que reportan desastres naturales. Su elección se debe a que es un dominio rico en datos como los que deseamos extraer, además del interés de colaborar en la creación de un inventario con información de este tipo para nuestro país. Por lo tanto, una vez establecida la tarea surgen varias preguntas técnicas. En primer lugar las relacionadas a los patrones de extracción, éstas son:  ¿cómo adecuar los contextos para ser analizados por los esquemas de aprendizaje automático?, i.e.  ¿cuál es la representación y tamaño de contexto que se tienen que utilizar?; además de contestar  ¿cuál de los algoritmos de aprendizaje evaluados se adapta mejor a la tarea? Posteriormente, con respecto a las entidades,  ¿es suficiente un análisis con expresiones regulares para detectar entidades? Y finalmente,  ¿qué posibilidades tiene esta propuesta de contribuir en el conocimiento del caso de estudio?
En el contenido del documento se tratará de dar respuesta mediante nuestros experimentos a las preguntas antes planteadas, además de responder a la otra pregunta general de  ¿cómo se comporta esta propuesta en la extracción de información?, i.e.,  ¿cuáles son sus alcances y limitaciones?.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>La extracción de información se ocupa de estructurar información contenida en textos que son relevantes para el estudio de un dominio (o escenario) particular, llamado dominio de extracción. En otras palabras, el objetivo de un sistema de extracción de información es encontrar y enlazar la información relevante mientras ignora la extraña e irrelevante [11]. A manera de ejemplo, se presenta el siguiente fragmento de una noticia relacionada a ataques terroristas (reportada el 3 de abril de 1990 en la cadena de televisión Inravisión de Bogotá, Colombia [26]):
El senador liberal Federico Estrada Vélez fue secuestrado el tres de abril en la esquina de las calles 60 y 48 oeste en Medellín... Horas después, por medio de una llamada anónima a la policía metropolitana y a los medios, los Extraditables se atribuyeron la responsabilidad del secuestro... La semana pasada Federico Estrada Vélez había rechazado pláticas entre el gobierno y traficantes de drogas.
Un sistema de extracción de información debe ser capaz de ubicar como información relevante los siguientes fragmentos: secuestro como el tipo de incidente, los Extraditables como el grupo autor, Federico Estrada Vélez como el objetivo humano, además de 3 de abril y Medellín como la fecha y lugar, respectivamente, del incidente. Entonces, la salida de un sistema de extracción son los registros que sirven para llenar lo que se conoce como plantilla de extracción. Además, los registros extraídos pueden ser complementados con información del dominio (e.g. en la tabla 3.1 se muestra la plantilla de extracción para la noticia antes expuesta). Por lo tanto, los elementos que componen la plantilla de extracción deben ser definidos desde las primeras etapas de desarrollo del sistema y tienen dependencia directa con el dominio de extracción.
Para finalizar la sección, cabe destacar que algunos autores consideran a la extracción de información como una etapa posterior de la recuperación de información [11], donde la principal diferencia entre ambas radica en que la primera proporciona la información que exclusivamente interesa, mientras que la segunda proporciona los textos en los que aparece dicha información. Algunas de las nuevas tecnologías tratan de superar las diferencias y tomar ventaja de ambas técnicas, e.g. la generación de "wrappers"  para Internet (i.e. extracción de información desde documentos HTML), y la búsqueda de respuestas (i.e. contestar automáticamente a preguntas puntuales por medio del contenido de una colección de documentos) [13].</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se presentó una arquitectura para desarrollar sistemas de extracción de información, así como una aplicación real implementada bajo este planteamiento, con esto creemos haber proporcionado las ideas para contestar a las dos preguntas de investigación generales planteadas en el capítulo 1 (i.e.,  ¿cómo construir sistemas de extracción de información sin utilizar un análisis lingüístico sofisticado? y  ¿cómo se comporta esta propuesta en la extracción de información?). Además, también se realizó una comparación del sistema con un trabajo de extracción realizado de forma manual, donde se comprobó que la propuesta puede ser útil para incrementar el conocimiento de un dominio.
En resumen, la arquitectura aquí expuesta presenta una fuerte carencia de entendimiento del lenguaje reemplazado por métodos de aprendizaje supervisado, las ventajas y desventajas que esto implica son:
"El uso de métodos empíricos mejoran la portabilidad entre dominios. Los algoritmos de aprendizaje supervisado como base de la arquitectura proporcionan un ahorro de esfuerzo en mano de obra del experto para generar el conocimiento necesario, lo cual se traduce en una portabilidad entre dominios más eficiente.
"Utilizar sólo un análisis a nivel léxico hace más factible la portabilidad entre idiomas. Debido a que las técnicas empleadas en la arquitectura emplean únicamente características a nivel léxico de los textos para lograr su objetivo, la portabilidad de la misma a nuevos idiomas resulta más simple, esto por no requerir de herramientas que realicen un análisis lingüístico sofisticado.
"Desventajas de la falta de entendimiento del lenguaje. Estas desventajas tienen que ver principalmente con la falta de información del dominio y el nulo análisis del discurso, i.e., la información implícita no puede ser extraída y si se tienen registros con múltiples valores la información no se puede interpretar fácilmente
Además, durante el trabajo de investigación se obtuvieron algunas conclusiones importantes, éstas son:
"La inflexión de las palabras y los términos independientes del tema son importantes para la extracción de información. Como se comprobó en los experimentos, las palabras vacías y sufijos son importantes para discriminar entre información relevante e irrelevante, caso contrarío ocurre cuando el objetivo es descubrir el tema de un documento.
"La extracción de información es un proceso de largo plazo. Como ya se había demostrado en las competencias del MUC, la extracción de información no es una tarea fácil, ni para las personas. En nuestros experimentos, además de la dificultad de la tarea, también comprobamos que para algunos casos se requiere que la fuente de información sea monitoreada por un largo periodo de tiempo, esto con el objetivo de obtener la información correcta. Por lo tanto, esta conclusión reafirma la importancia de automatizar la tarea.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Extracción de Información con Algoritmos de Clasificación
Por
ALBERTO TÉLLEZ VALERO
Tesis sometida como requisito parcial para obtener el grado de Maestro en Ciencias en la especialidad de Ciencias Computacionalesen el Instituto Nacional de Astrofísica, Óptica y Electrónica.
Supervisada por:
DR. MANUEL MONTES Y GÓMEZ
DR. LUIS VILLASEÑOR PINEDA
INAOE
Tonantzintla, Pue.
2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"DESARROLLO DE UNA HERRAMIENTA PARA EL DISEÑO DE SISTEMAS RECONFIGURABLES"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Diseñar un sistema que facilité el diseño de sistemas digitales a partir de sistemas basados en lógica reconfigurable, pudiendo diseñar sistemas digitales que no varíen en el tiempo a partir de sistemas reconfigurables.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
Diseñar un sistema de control de reconfiguración parcial dinámica sobre sistemas basados en componentes que permita una modificación segura y eficiente de los mismos, además de proveer un medio para diseñar sistemas digitales basados en esta tecnología.

Objetivos Particulares
*	Diseñar circuitos de pruebas basados en la reconfiguración dinámica parcial.
*	Construir un medio para controlar la configuración del FPGA.
*	Analizar y diseñar las principales opciones que deberá de tener un sistema que permita la administración de proyectos basados en reconfiguración dinámica parcial.
*	Desarrollar un sistema que permita la administración de los sistemas basados en reconfiguración dinámica parcial.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El diseño de este tipo de sistemas representa un nuevo reto para la investigación en ésta área: El poder controlar la aplicación de la reconfiguración parcial dinámica sobre un sistema compuesto por módulos de distinta naturaleza teniendo en cuenta que debe garantizar su funcionamiento mientras se modifica una única sección.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con la aparición de la familia de Virtex II Pro Xilinx dotó a sus dispositivos la habilidad de ser reconfigurados dinámicamente, esta reconfiguración, aunque un poco pobre, debido a que el dispositivo debía ser seccionado en columnas para formar las áreas reconfigurables por lo que se perdía el concepto de utilizar al máximo el área útil del dispositivo, mostro una interesante tendencia en el diseño de sistemas digitales. Tiempo después con la aparición de las familias Virtex 4 y Virtex 5, Xilinx da mayor flexibilidad hacia este tipo de características en sus dispositivos, lo que ha motivado al surgimiento de técnicas y herramientas que ayuden al diseño de sistemas reconfigurables.
La principal aportación de la presente tesis fue desarrollar una herramienta que facilite el diseño de sistemas a partir de sistemas reconfigurables, por ejemplo, si se tuviera un diseño de 10 regiones reconfigurables, y estas a su vez tuvieran asociadas 10 funciones, al final de la metodología de diseño descrita en el capítulo 4 se tendrían 100 archivos .bit para la reconfiguración, 10 archivos _blank generados adicionalmente y el full.bit, lo que arroja un total de 111 archivos para el control del sistema. El controlar tal cantidad de archivos puede ser una labor tediosa, esta labor es controlada con la herramienta desarrollada, una vez cargado el proyecto las funciones pueden ser usadas sin necesidad de recordar que archivo genera x función.
Adicionalmente, debido al control que se lleva de los proyecto, estos pueden ser compartidos por varias personas o grupos de trabajo, lo que facilitaría el intercambio de modelos o sistemas así como sus funciones.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
"DESARROLLO DE UNA HERRAMIENTA PARA EL DISEÑO DE SISTEMAS RECONFIGURABLES" 
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN
PRESENTA:
ING. ISRAEL ROMAN PALACIOS
DIRECTOR DE TESIS: DR. LUIS ALFONSO VILLA VARGAS
CODIRECTOR DE TESIS: DR. MARCO ANTONIO RAMIREZ SALINAS
México D.F. Diciembre de 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"MODELO COGNITIVO DIFUSO DEL ALUMNO COMO SOPORTE EN EL APRENDIZAJE"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la actualidad una de las tareas más difíciles a la cual se enfrentan los profesores en el aula es la evaluación del alumno. En un entorno virtual dicha tarea se vuelve más complicada, como determinar el nivel de aprendizaje del alumno en un determinado tema es el tópico central del presente trabajo. Los sistemas inteligentes de tutorías dividen el curso en objetivos de aprendizaje, en cada uno de dichos objetivos se tienen diferentes conceptos, ejercicios y cuestionarios que registran el grado de comprensión del alumno pero no realizan una análisis especial para cada alumno.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
"Obtener un modelo cognitivo computacional del alumno en el cual se recopile y
clasifique la información significativa de su evaluación y retroalimentación.
I. INTRODUCCION

Objetivos Específicos
"Analizar las diferentes modelos de evaluación utilizadas en sistemas de tutoría
inteligente en México.
"Construir un prototipo multi-agentes con soporte de análisis neuro-difuso para la
experimentación de variables del estado cognitivo del alumno.
"Realizar investigación de campo con un grupo de alumnos en la materia de informática
2 en la Universidad de la Sierra usando el prototipo desarrollado.
"Comparar los resultados de evaluación del prototipo con los de una clase tradicional de
la Universidad de la Sierra en la materia de informática 2.
"Conocer el grado de aceptación de los sistemas de evaluación y retroalimentación
electrónica por medio de la aplicación de una encuesta de satisfacción para los
estudiantes.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿La tutoría soportada por un modelo cognitivo difuso ayudará en el aprendizaje del estudiante?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿La retroalimentación inmediata al alumno a través de modelo cognitivo difuso incrementa la recuperación del alumno?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Las explicaciones proporcionadas por el modelo cognitivo difuso satisfacen al alumno?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿El uso de aplicaciones distribuidas mejora la comunicación de los resultados tanto para los
alumnos como para los profesores?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿El uso de tecnologías de inteligencia artificial como los sistemas neuro-difusos presenta una descripción satisfactoria de las reglas utilizadas para la evaluación del alumno?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Actualmente en muchas instituciones de educación superior en el país, incluyendo la
Universidad de la Sierra, diversas materias son soportadas por alguna plataforma de
educación, en nuestro caso se tiene implementada la herramienta MOODLE para el soporte a
la docencia.
Esta herramienta tiene funcionalidades muy interesantes, pero desafortunadamente no permite
realizar una buena evaluación del proceso de aprendizaje de manera automática e integral. Si
se considera que a este tipo de herramientas se le pueda agregar un sistema de evaluación más
adecuado, entonces se puede obtener una evaluación más personalizada. Con la presente
investigación se obtiene un sistema multi-agentes que realiza el almacenamiento de las
mediciones de la adquisición del conocimiento por parte del alumno. Dicho sistema
proporciona un diagnóstico actualizado al maestro. Además de enviar una retroalimentación
personalizada al alumno antes y después de presentar su evaluación. Con esto se pretende
mejorar tanto el proceso de aprendizaje del alumno como la evaluación del maestro.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En nuestro país, se han comenzado a utilizar los sistemas de manejo de cursos electrónicos
como apoyo al alumno en universidad, de igual forma se han desarrollado investigaciones en
varias áreas relacionadas con los sistemas de tutoría inteligente con los cuales se puede brindar una atención personalizada al alumno. Dichas investigaciones utilizan diferentes maneras de almacenar el conocimiento del alumno (modelo del alumno), algunas utilizan redes bayesianas, lógica difusa, redes neuronales y sistemas neuro-difusos. El modelo cognitivo difuso propuesto utiliza un sistema neuro-difuso para brindar una respuesta rápida y textual al alumno ayudando a los maestro en la evaluación de grupos grandes. El sistema aprende del maestro su forma de evaluar para cada materia en cada tema en particular con la ayuda del sistema neuro-difuso.
Para comprobar la efectividad del modelo cognitivo difuso se construyó un prototipo con el
cual se realizó un experimento con un grupo de alumnos de la materia de informática 2. Los
resultados obtenidos con el grupo experimental se compararon con un grupo de control en
donde se encontró que dichos grupos eran diferentes con un 95% de confianza y con una
diferencia de medias de 13.1 puntos, obteniendo el mayor promedio el grupo experimental.
Además, se considera importante la opinión del alumnado. Al aplicar la encuesta de
satisfacción al grupo experimental se pudo observar que el 84% de los alumnos demostraron
un alto grado de aceptación al sistema y a seguir utilizando dicho sistema en otras materias.
También cabe destacar que la retroalimentación inmediata y textual del sistema motivó al 85% del alumnado a estudiar un poco más.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO TECNOLÓGICO DE HERMOSILLO
DIVISIÓN DE ESTUDIOS DE POSGRADO E INVESTIGACIÓN
MODELO COGNITIVO DIFUSO DEL ALUMNO COMO SOPORTE EN EL
APRENDIZAJE
T E S I S
PRESENTADA COMO REQUISITO PARCIAL
PARA OBTENER EL GRADO DE:
MAESTRO EN CIENCIAS
EN CIENCIAS DE LA COMPUTACIÓN
PRESENTA:
ING. JESÚS MIGUEL GARCÍA GORROSTIETA
DIRECTOR DE TESIS:
M.C. CÉSAR ENRIQUE ROSE GÓMEZ
HERMOSILLO, SONORA, MEXICO. FEBRERO DE 2008</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Interfaz Cerebro - Computadora para el Control de un Cursor Basada en Ondas Cerebrales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Lo que se propone realizar en este proyecto de investigación, es una interfaz que 
permita la comunicación entre el usuario y una computadora, haciendo uso de sus ondas 
cerebrales, más en específico, es el desarrollar una interfaz BCI para el control de un 
cursor en pantalla mediante comandos obtenidos de las lecturas EEG (Figura 2.1-1). 
Dentro de esta área de investigación existen muchos puntos que aun hay que 
estudiar para así resolver algunos problemas que aun siguen vigentes, como lo es la 
optimización del tiempo de entrenamiento y la precisión para interpretar los comandos 
del usuario. 
No obstante, este trabajo centrara su investigación dentro de puntos específicos, 
los cuales son: 
La caracterización de las señales EEG 
La clasificación de las ondas EEG 
La optimización del tiempo de entrenamiento 
La caracterización de las señales se puede hacer mediante 3 componentes de esta, 
que entregan información de diferente tipo pero referente a una misma intención del 
individuo. Las componentes de la señal que se plantean usar son: beta, mu y P300 
(Figura 2.1-2), de las cuales las primeras dos están relacionadas con los movimientos del 
cuerpo y la ultima esta relacionada con el nivel de concentración del individuo con respecto a lo que esta viendo. Con lo que se propone hacer una combinación lineal de 
estas tres componentes para poder obtener información mas precisa acerca de lo que el 
individuo pretende hacer. (Sus intenciones).</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>"Análisis, diseño e implementación de una interfaz BCI que sea capaz de 
controlar un cursor por medio de la clasificación de lecturas EEG de una 
persona.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Como se mencionó anteriormente, los equipos de cómputo actuales poseen una 
capacidad de procesamiento muy superior a la que existía cuando se desarrollaron sus 
interfaces de comunicación (principalmente ratón y teclado) y que con el paso del tiempo 
han demostrado ser muy eficientes, aun con el acelerado avance que ha tenido el área de 
las computadoras desde su creación. Un estudio en el cual se modela los movimientos del 
Mouse [1] se demuestra que una interfaz de comunicación que supere la eficiencia del 
Mouse será difícilmente implementada. 
No obstante de esta eficiencia, en la actualidad se han encontrado ya aplicaciones 
para este dispositivo en el cual resulta ineficiente, más en específico podemos hablar de 
un artículo en el cual se propone una interfaz de usuario que soporte una visualización tridimensional neuroquirurgica [1] en la cual la manipulación de dicha visualización resulta 
poco manejable con un Mouse por lo que se propone una interfaz de 2 manos para su 
control. Esta solución satisface las necesidades de la aplicación en específico, sin 
embargo en el artículo se menciona que un candidato a la solución de crear una interfaz 
más eficiente no puede ser la solución a todos los problemas, ya que esta forma de atacar 
el problema obtiene su eficiencia de hacer una adaptación especifica de las interfaces ya 
disponibles (joystick y Mouse). 
Este es uno de los problemas que muestra la necesidad de un medio de 
comunicación más efectivo, haciendo un mejor uso de las capacidades computacionales 
de los equipos con los que ya se disponen y por otro lado, y que al mismo tiempo permita 
aprovechar de mejor manera las habilidades intelectuales de los usuarios. 
Es aquí donde surge la propuesta de una interfaz de comunicación que permita un 
control más rápido y acertado de las tareas que queremos realizar, incluso que funcione 
con personas que tienen discapacidades motoras, desde personas con Parkinson hasta 
personas que se encuentren totalmente paralizadas pero que aun disponen de su 
capacidad intelectual intacta.[2][3][4] 
Esa es la razón por las que las BCI prometen ser una solución eficiente ya que la 
base del funcionamiento de estos dispositivos no incluye movimientos físicos, si no las 
ondas cerebrales del usuario. Por lo cual cumple de entrada con las necesidades de 
personas con discapacidad motora (pero con capacidades intelectuales intactas). Por otro 
lado, también puede ser un dispositivo que permita múltiples canales de comunicación 
con la PC (parte que aun sigue en desarrollo) lo cual puede eliminar la barrera de 2 
dimensiones impuesta por el Mouse. 
Lo que falta por desarrollar de estas interfaces son los múltiples canales de 
comunicación (ya que hasta ahora se han alcanzado resultados aceptables solo en 2 
dimensiones) y la velocidad de comunicación (existen otros problemas a resolver, sin embargo estos son los mas buscados) con la PC, ya que en un futuro posible, tal vez 
podamos hacer un uso más eficiente de los equipos de cómputo si esta interfaz se sigue 
desarrollando de manera exitosa, dando así como resultado un dispositivo de 
comunicación adecuado a la nueva generación tecnológica con la que contamos. 
Los trabajos que hasta ahora se han desarrollado han incursionado en áreas como 
las redes neuronales [3] y en algoritmos adaptables (teoría de control) [4] en los cuales 
se han obtenido buenos resultados en la clasificación de las ondas cerebrales, sin 
embargo, no hay un trabajo que recopile diferentes técnicas que permitan evaluar cual de 
ellas puede ser el mejor camino hacia encontrar una BCI eficiente. Y es aquí donde nos 
podemos enfocar al desarrollo de una BCI en la que se puedan probar y proponer 
diferentes técnicas de reconocimiento de patrones para poder así, realizar una 
comparativa entre ellas.
Con lo que respecta a las dificultades que se encontraron a lo largo del desarrollo 
de esta tesis, hubo varios problemas a solucionar, uno de ellos, que es de los que mas 
tiempo requirió al comienzo fue el desarrollo del software para la adquisición de las 
señales, ya que este se tuvo que desarrollar desde cero, principalmente, hablando de la 
interfaz de comunicación con el Mindset MS-1000, la cual se desarrollo en C como una 
biblioteca compartida (DLL) y que se puede contar como un producto resultado de éste 
trabajo. 
 También se encontraron dificultades al momento de poner a prueba los protocolos 
de experimentación, ya que estos comenzaron a arrojar información confusa con 
respecto a los diferentes movimientos, en específico, estos formaban 4 cumulos 
perfectamente separables, sin embargo, cada cúmulo contenia elementos de las 4 
direcciones, lo cual fue desconcertante. Para solucionar esto se tuvo que regresar al 
protocolo experimental, al cual se le agregaron restricciones que evitaran que los mismos 
musculos oculares inducieran artefactos dentro de las lecturas. 
 Finalmente, podemos agregar que éste trabajo contuvo, a parte del alto grado de 
dificultad del tema, varios desafios técnicos, como lo fue el desarrollo del software de 
adquisición del sistema y la vinculación de software desarrollado en VC++ con Matlab.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Durante el desarrollo de la presente tesis me permitió estar en estrecho contacto 
con señales fisiológicas. La meta principal fue el control de un cursor de PC por medio de 
señales Electroencefalográficas. En un primer trabajo se analizaron y preocesaron señales 
provenientes de los electrodos C2, C4, T3 y T4, de la cual se extrajeron tres 
componentes: a) Beta, b) Mu y c) P300. A dichas se les hicieron los siguientes procesos: 
 Con transformada wavelets se realizaron analisis sobre las descomposiciones en 
diferentes niveles, con transformada Wigner-Ville se hizo análisis de frecuencia a lo 
largo del tiempo, análisis de cambio de fase y análisis de texturas, obteniendo como 
resultado, gran dificultad para su clasificación, de acuerdo con los diferentes análisis, los 
movimientos se confundían sistemáticamente y las tasas de reconocimiento fueron bajas. 
 Un factor que pudo haber contribuido fue la metodología planteada para la 
adquisción de las señales (protocolo de adquisición), siendo ésta un área dónde se puede 
seguir investigando. 
 Debido a que no se tenían buenos resultados con señales EEG, se pasó a explorar 
señales de tipo Electro-Oculográficas, que son señales provenientes de los movimientos 
oculares. Se planteó una metodología de adquisición para estas, un tanto diferente a la 
metodología usada para EEG. Con las señales obtenidas, se pudo realmente clasificar las 
cuatro direcciónes de movimiento propuestas en esta tesis, arriba, abajo, derecha e 
izquierda. 
 La clasificación se llevó a cabo con dos clasificadores estadísticos: KNN y 
Gaussiano; y uno neuronal: perceptrón multicapa. Los métodos de evaluacion de 
performance utilizados fueron: deja uno fuera, resustitucion y validación cruzada. Los 
resultados significativos son los proporcionados por validación cruzada. 
 Las tasas de clasificación de los clasificadores aislados oscilan entre el 88 y 95 por 
ciento. Con el fin de obtener una clasificación mas robusta, se realizó la fusión de los tres 
clasificadores en las siguientes configuraciones: Voto Mayoritario y Unanimidad. Por 
Voto Mayoritario se obtuvieron mejores resultados, alcanzando tasas de buen 
reconocimiento del 95%.
Con lo que respecta a las dificultades que se encontraron a lo largo del desarrollo 
de esta tesis, hubo varios problemas a solucionar, uno de ellos, que es de los que mas 
tiempo requirió al comienzo fue el desarrollo del software para la adquisición de las 
señales, ya que este se tuvo que desarrollar desde cero, principalmente, hablando de la 
interfaz de comunicación con el Mindset MS-1000, la cual se desarrollo en C como una 
biblioteca compartida (DLL) y que se puede contar como un producto resultado de éste 
trabajo. 
 También se encontraron dificultades al momento de poner a prueba los protocolos 
de experimentación, ya que estos comenzaron a arrojar información confusa con 
respecto a los diferentes movimientos, en específico, estos formaban 4 cumulos 
perfectamente separables, sin embargo, cada cúmulo contenia elementos de las 4 
direcciones, lo cual fue desconcertante. Para solucionar esto se tuvo que regresar al 
protocolo experimental, al cual se le agregaron restricciones que evitaran que los mismos 
musculos oculares inducieran artefactos dentro de las lecturas. 
 Finalmente, podemos agregar que éste trabajo contuvo, a parte del alto grado de 
dificultad del tema, varios desafios técnicos, como lo fue el desarrollo del software de 
adquisición del sistema y la vinculación de software desarrollado en VC++ con Matlab.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Maestría en Ciencias de la Computación 
Tesis 
Interfaz Cerebro - Computadora para el 
Control de un Cursor Basada en Ondas 
Cerebrales 
por: 
Job Ramón de la O Chávez 
Asesor: Dr. Carlos Avilés Cruz 
Írea de Concentración: Procesamiento de Señales y 
Reconocimiento de Patrones</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Generador de los grafos conceptuales a partir del texto en español</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Existen diversas formas de representar el conocimiento contenido en textos, entre otros se puede 
mencionar la representación basada en marcos, reglas, lógica de primer orden y redes semánticas. Los grafos conceptuales son un tipo de red semántica, donde sólo existen dos tipos de nodos: los nodos concepto y los nodos relación. 
Los grafos conceptuales tienen el potencial de representar de forma simple y directa los detalles 
finos del lenguaje. Por ejemplo, permiten representar dependencias contextuales que no pueden 
se representadas por predicados lógicos. 
El proceso de generación de los grafos conceptuales a partir del texto en español está dirigido 
por la sintaxis, teniendo como entrada las estructuras de representación sintáctica proporcionadas 
por un analizador sintáctico (parser), identificando los roles de las palabras y establecer así el 
tipo de relación conceptual entre ellas.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un convertidor de árboles sintácticos a grafos conceptuales, de tal manera que puedan ser aprovechados por herramientas que utilicen grafos conceptuales como entrada.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se desarrolló un programa que genera grafos conceptuales tomando como entrada árboles sintácticos. Esto permite aplicarlos a tareas que aprovechen la riqueza expresiva de los grafos conceptuales como representación del contenido de textos. Aplicaciones como la minería de texto y la 
recuperación de información pueden hacer uso de esta representación. Asimismo este trabajo es 
un paso a procesamiento semántico del texto en español y probablemente permitirá un avance 
en muchas otras aplicaciones que requieren de un tratamiento semántico y no puramente estadístico. 
Las aportaciones específicas de este trabajo incluyen: 
- El desarrollo del método para obtener la representación semántica (grafos conceptuales) del 
texto en español. 
- Demostración de la utilidad de este método para las aplicaciones, usando como ejemplo la 
minería de texto. 
- Uso de gramáticas de dependencia como la base sintáctica para el método, lo que permitirá 
alcanzar mejor calidad de los resultados según el desarrollo de los analizadores sintácticos de 
dependencia (tales como por ejemplo DILUCT). 
- Desarrollo de una arquitectura desacoplada del convertidor semántico compatible, en principio, con diferentes analizadores sintácticos, existentes o futuros.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Generador de los grafos conceptuales 
a partir del texto en español 
T E S I S 
QUE PARA OBTENER EL GRADO DE 
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN 
PRESENTA 
MACARIO HERNÍNDEZ CRUZ 
Director: 
Dr. Alexander Gelbukh 
México, D.F. 
Mayo, 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>MODELADO DE SISTEMAS SOFTWARE  BASADO EN MDE (Caso: SISTEMAS EXPERTOS DE DIAGNÓSTICO)</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El desarrollo de sistemas software es cada vez más complejo debido a una 
serie de factores, como la aparición de nuevas tecnologías, la interconexión 
de varios sistemas y plataformas, la necesidad de integrar viejos sistemas 
aun válidos, la adaptación personalizada del software a cada tipo de usuario, 
las necesidades específicas de un sistema y las diferentes plataformas de 
implementación. Esteescenario lleva a necesitar, en cortos periodos, múltiples versiones de la misma o parecida aplicación. Por ello, la Ingeniería del 
Software debe proporcionar herramientas y métodos que permitan desarrollar una familia de productos con distintas capacidades y adaptables a situaciones variables, y no sólo un único producto. Esta situación provocó la creación de un diseño que puede ser compartido por todos los miembros de una 
familia de programas. De esta manera, un diseño hecho explícitamente para 
un producto, beneficia al software común y puede ser usado en diferentes 
productos, reduciendo los gastos generales y el tiempo para construir nuevos 
productos. 
Ante esta situación, surge el concepto de Líneas de Productos Software (del 
inglés Software Product Lines y siglas SPL),con la finalidad de controlar y 
minimizar los altos costos del desarrollo de software, así como los largos 
tiempos de producción. Uno de los elementos clave para una SPL es la representación y gestión de la variabilidad.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo de esta tesis, derivada de la hipótesis es: Generar modelos de 
sistemas software (Sistemas Expertos de Diagnóstico), aplicando estrategias 
de la Ingeniería Dirigida por Modelos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Con base en lo anteirormente descrito, la hipótesis planteada en esta tesis 
expresa que: "Los sistemas software pueden modelarse aplicando estrategias 
de la Ingeniería Dirigida por Modelos"</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Para captar la semántica de la variabilidad de un dominio específico, i.e. para 
plasmar las características comunes y variantes de una SPL mediante modelos que representen familias de sistemas software, se utiliza el denominado 
Modelo de Características. Este modelo es la representación clásica de la variabilidad en una SPLel cualpuede definirse como "una jerarquía de características con variabilidad" (K. Czarnecki , et al., 2006). 
Asimismo, la Ingeniería Dirigida por Modelos (del inglés Model Driven Engineering y siglas MDE) (Kent, 2002)y el enfoque de la Programación Generativa (Czarnecki y Eisenecker, 2000) proporcionan una base adecuada para 
apoyar el desarrollo de las SPL, facilitando el desarrollo de productos de 
software para diferentes plataformas y permitir su utilización en diferentes 
tecnologías. Pero este desarrollo de productos de software deberá hacerse 
bajo las técnicas ampliamente utilizadas durante el proceso de desarrollo de 
productos de toda la SPL, lo cual permite que el proceso sea correcto y adecuado.
Además, para enfrentar la complejidad de los sistemas software, la Ingeniería de Software promueve la automatización del desarrollo del software y 
otros mecanismos. Sin embargo, la automatización de estos sistemas sólo es 
posible mediante un framework que soporte este proceso. Ante ésto, el Grupo Gestionador de Objetos (del inglés Object Management Group y siglas 
OMG) (http://www.omg.org) propone para la MDE: i) La Arquitectura Dirigida por Modelos (del inglés Model Driven Architecturey siglas MDA) 
(http://www.omg.org/mda), la cual aboga el uso de estándares e independencia de plataforma en el proceso de desarrollo de software, como un nuevo método de producir aplicaciones; y ii) La Facilidad de Meta-Objetos (del 
inglés Meta Object Facilityy siglasMOF) (http://www.omg.org/mof), que identifica conceptos básicos como modelo, metamodelo y meta-metamodelo, y 
las relaciones entre ellos.
Por otro lado, una clase de sistemas que está cobrando interés en los últimos 
tiempos son los Sistemas Expertos (del inglés Expert Systems y siglas 
ES)(Giarratano y Riley, 2004). Pero el desarrollo de este tipo de sistemas es 
complejo, dado que los elementos básicos que conforman su arquitectura 
varían tanto en su comportamiento como en su estructura (Cabello,2008). 
Por consiguiente, existe la necesidad de soportarlos adecuadamente. Las metodologías y aplicaciones desarrolladas de los SEforman una amplia categoría de productos de investigación, ofreciendo ideas y soluciones a dichos 
sistemas en dominios específicos. Estos sistemas emulan el razonamiento 
humano para la solución ante un problema. Cabe señalar que es notable la 
importancia que han cobrado en los últimos años los SE que realizan tareas 
de diagnóstico(Cabello, 2008). Por ello, para ilustrar la tesis, se ha elegido 
como dominio a los Sistemas Expertos de Diagnóstico (del inglés Diagnostic 
Expert Systems y siglas DES). 
Por todo lo anterior, en esta tesis se sigue el enfoque MDA, para poder generar código desde modelos (que capturan la variabilidad), utilizando programación generativa y técnicas de SPL. Explícitamente, se propone capturar la 
variabilidad de la SPLde los DES, en términos de un modelo de características.Con la configuración de dicho modelo y la información del comportamiento de un sistema específico (mediante un diagrama de secuencias), se 
transforma un modelo modular en modelos de componentes y conectores. La 
transformación entre estos modelos es definida mediante mapeos, que se 
caracterizan por tener una correspondencia sintáctica y semántica entre sus 
respectivos metamodelos.Para realizar dicha transformación de modelos será 
utilizado el lenguaje estándar de la OMG para la transformación de modelos, 
denominado "Query/View/Transformations Relational (QVT-Relational) (OMG, 
2001).</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Esta tesis básicamente consistió en modelar sistemas software mediante técnicas de transformación de modelos. Para ilustrar esta estrategia de modelado, se eligió a los Sistemas Expertos de Diagnóstico. Específicamente esta 
estrategia consistió en transformar el modelo modular de los Sistemas Expertos de Diagnósticoen modelos componente-conector "ad-hoc"  al tipo de 
diagnóstico. Para llevar a cabo dicha tarea, se plasmó la variabilidad de la 
Línea de Productos Software de los Sistemas Expertos de Diagnóstico, mediante un modelo conceptual de la variabilidad (el modelo de características), 
se representó el comportamiento de dichos sistemas mediante diagramas de 
secuencia de UML (el modelo de interacción), para generar las arquitecturas 
de los Sistemas Expertos de Diagnóstico medinate modelos de componentesconectores con comportamiento. 
El trabajo realizado en la tesis tiene como fundamento el plan de producción 
contemplado en la aproximación BOM-Lazy: un marco para desarrollar aplicaciones basado en Línea de Productos Software y técnicas de transformación de modelos, la cual ha sido publicada en diferentes artículos internacionales. Específicamente la investigación realizada en esta tesis contempla sólo 
la primera transformación (i.e. del modelo modular al modelo de C-C esqueletos), pero con una mejora: la incorporación del comportamiento de todos 
los elementos arquitectónicos de los sistemas mediante la especificación de 
modelos de componentes y conectores con comportamiento.
Con ello se propone una solución posible a la problemática del desarrollo de 
software, permitiendo:
* Utilizar diferentes espacios tecnológicos, que están en la vanguardia 
internacionalmente y que son estándares de la OMG (MDA, MOF, QVT, 
UML, XMI, SPEM).
* Trabajar en altos niveles de abstracción (i.e. con modelos no con programas) y de esta manera trabajar en el espacio del problema y no de 
la solución.
* Aplicar la técnica de la transformación de modelos, especificando relaciones de mapeo que identificaron patrones de los metamodelos que 
conforman los modelos utilizados en la transformación.
* Desarrollar familias de productos que comparten partes comunes (reutilización del software), mediante técnicas de Línea de Productos 
Software, bajando los costes y reducir los tiempos de mercado así 
como los múltiples esfuerzos que se llevan a cabo en el desarrollo del 
software.
* Realizar tareas de diagnóstico, emulando el comportamiento de un 
experto para resolver un problema, ilustrando nuestra propuesta con 
el dominio específico de los Sistemas Expertos de Diagnóstico, dado el 
interés de los últimos tiempos en desarrollar sistemas inteligentes y 
dada la importancia de la tipología de dichos sistemas al resolver tareas de diagnóstico en diversas áreas.
En esta tesis se tuvieron en cuenta las siguientes consideraciones:
* La especificación de la variabilidad de una SPL y la funcionalidad de 
dichos sistemas software, es manejada en modelos independientes y 
separados. La funcionalidad de una SPL se plasma en modelos 
arquitectónicos, mientras que la variabilidad de la SPL se plasma en 
modelos de las características del dominio. Por ello se definen una serie 
de metamodelos utilizando MOF para su especificación. 
* La aproximación BOM-Lazy (Cabello, 2008) esutilizada como una 
aproximación MDA basada en SPL que utiliza técnicas de transformación 
de modelos, para transformar un modelo modular en varios modelos de 
componente-conector. 
* El modelo de características es utilizado para representar la variabilidad. 
El modelado de las características de una SPL es una de las técnicas más 
utilizada en el desarrollo de las SPL. Esta técnica tiene una sintaxis clara 
utilizada para construir un modelo conceptual. 
* La transformación de modelos aplica el principio de la transformación de 
modelos dirigido por metamodelos.
* La transformación entre los modelos se define por medio de mapeos, que 
se caracterizan por las relaciones sintácticas y semánticas de 
correspondencias entre sus metamodelos.
* La identificación de diferentes patrones de mapeo entre el modelo 
modular y el modelo C-C, es especificado usando el estándar QVTRelations. 
* El comportamiento de los modelos C-C es especificado mediante un 
metamodelo híbrido denominado metamodelo C-C-C (metamodelo de 
componentes, conectores y comportamiento) que consiste en unir el 
metamodelo C-C con el metamodelo de interacción (metamodelo de 
secuencias de UML).
* El comportamiento es introducido a través de un refinamiento de los 
modelos C-C resultantes de la transformación (i.e. estructura de los 
modelos C-C) instanciando mensajes, argumentos y secuencias, 
obteniendo así los modelos C-C con comportamiento (i.e. estructura y 
comportamiento) también denominadas arquitecturas esqueleto.
Se ha establecido un marco de trabajo para el diseño y análisis de arquitecturas software (en particular de los Sistemas Expertos de Diagnóstico). Este 
marco consiste en la especificación de los modelos de las vistas arquitectónicas (Modelo Modular, Modelo de Componentes-Conectores, Modelo de Interacción), de la vista de variabilidad (Modelo de Características), así como de 
la propuesta de un proceso del modelado de estas vistas.
El proceso de modelado está basado en la propuesta MDA de la OMG, el cual 
incorpora dos principales ventajas del metamodelado: i) poder crear los metamodelos de las vistas para generar modelos, y ii) mantener consistencia 
entre los modelos.
De esta manera, el proceso propuesto en esta tesis, contempla las entradas, 
actividades, secuencia y productos en cada uno de estos dos niveles de abstracción (metamodelos y modelos). Dicho proceso ha sido aplicado al caso 
particular de los Sistemas Expertos de Diagnóstico. Pero puede ser usado 
para otros casos donde se requiera modelar una arquitectura software.
En un metamodelo se representan elementos de los modelos que conforman 
dicho metamodelo y cómo están relacionados entre sí. La formalización de 
las diferentes relaciones entre los metamodelos sirven de guía para la implementación de las reglas de transformación, que han sido realizadas usando el lenguaje QVT-Relational, pero son útiles también para guiar otro tipo 
de implementación, por ejemplo con QVT-Operational y ATL. 
En la implementación, algunos de las conclusiones obtenidas al momento de 
transformar modelos son los siguientes:
*	El lenguaje QVT-Relational tiene poco uso en el medio de las transformaciones siendo QVT-Operational el estándar más utilizado: Prueba 
de esto son las tres herramientas que implementan QVT-Relational, 
dejando de lados varios aspectos funcionales de este lenguaje y obteniendo sólo cierto porcentaje de fiabilidad con implementaciones parciales.
*	El plug-in elegido, en este caso Medini, presenta diferentes dependencias que deben ser resueltas al momento de instalar en la plataforma 
Eclipse.
*	Aunque formalmente existen cuatro niveles de abstracción en el modelado, en EMF sólo es posible trabajar en dos niveles: metamodelo y 
modelo (como una instancia del metamodelo).
La estrategia de transformación que se ha seguido aquí, apegada a la propuesta MDA de la OMG, ha permitido llevar acabo la generación de modelos 
de una vista en función de otro modelo, específicamente de la vista modular 
a la vista de componentes-conectores, con lo cual se logra una transitividad 
entre modelos vía transformaciones.
Además se ha establecido un vínculo entre la estructura y el comportamiento 
que existe entre algunos elemento arquitectónicos, mediante la incorporación 
de un modelo de interacciones que captura la interrelaciones entre los componentes de la vista de componentes-conectores, de ahí que dicho modelo 
sirva de puente entre esta vista y la vista modular.
Con la estrategia de la transformación de modelos se mantiene consistencia 
entre los modelos de las dos vistas aquí consideradas, cuando alguna de 
ellas cambie por razones demantenimiento o evolución.
Los metamodelos sirven como plantillas para crear los modelos, así que al 
crear un modelo, esto contiene sólo lo que está especificado en su plantilla, 
es decir no hay elementos ni relaciones sorpresas.
La vista modular se fue conformando al ir aplicando el proceso de descomposición iterativa, obteniéndose al final del proceso, el modelo correspondiente de esta vista (especificación de módulos y relaciones). Luego, al usar 
este modelo como fuente, y al aplicar la transformación, se obtuvo la vista 
de componentes-y-conectores, la cual gracias al modelo de interacciones que 
se incorporó, se llegó a refinar la segunda vista a través del refinamiento de 
esta transformación, llegando a obtenerla en forma precisa.
Existen otras estrategias de modelado de los sistemas software, como la que 
modela cada uno de los sistemas de forma individual e independiente. Pero 
si utilizamos estrategias basadas en Línea de Productos Software, podemos 
aprovechar para generar variasmodelos bajando costos y reduciendo tiempos 
de producción. Asimismo existen otras técnicas de Línea de Productos Software para generar modelos, como la aproximación BOM-Eager la cual es conveniente aplicar cuando existe poca variabilidad de la Línea de Productos 
Software, y que utiliza técnicas de árbol de decisión. Pero si existe mucha 
variabilidad, es más conveniente utilizar la estrategia utilizada en esta tesis, 
que se corresponde con la aproximación BOM-Lazyy que utiliza técnicas de 
transformación de modelos, la cual permite generar modelos a partir de 
otros modelos. 
Como trabajo futuro se plantea crear un prototipo, el cual capture las instancias a través de la configuración del Modelo de Características en tiempo de 
ejecución, y con ello poder transformar automáticamente el Modelo Modular 
en un Modelo de Componentes-Conectores. 
Cabe hacer énfasis en que en esta tesis se obtienen los modelos de los sistemas software, quedando de trabajo fututo obtener los sistemas ejecutables, i.e. implementar completamente la aproximación BOM-Lazy, pero con 
las mejoras implementadas en esta tesis, lo cual representa una aportación a 
las técnicas de modelado de software.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
MODELADO DE SISTEMAS SOFTWARE 
BASADO EN MDE
(Caso: SISTEMAS EXPERTOS DE DIAGNÓSTICO)
TESIS
PARA OBTENER EL GRADO DE 
MAESTRO EN COMPUTACIÓN 
PRESENTA:
Saúl Iván Beristain Petriz
ASESORES:
D. en C. María Eugenia Cabello Espinosa
D. en C. Jorge Rafael Gutiérrez Pulido
COLIMA, COL., JUNIO 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ABCSIS: ARQUITECTURA BASADA EN COMPONENTES DE SOFTWARE PARA LA INTEGRACIÓN DE SERVICIOS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En el ámbito de sistemas de información, particularmente en el desarrollo de 
sistemas de automatización bibliotecaria, existen en el mercado sistemas 
bibliotecarios que ofrecen desde el punto de vista de interoperabilidad, enlace a sus 
módulos mediante interfaces denominadas Application Programming Interface (API). 
Es precisamente aquí donde se ha detectado un área de oportunidad muy fuerte en 
el software SIABUC, ya que los usuarios que lo utilizan han externado a través del 
departamento de soporte técnico la necesidad de realizar desarrollos 
complementarios para integrarlos al sistema, de manera particular aquellos servicios 
que se podrían realizar de manera remota o a distancia para aprovechar el uso de 
Internet, como por ejemplo: la reservación de libros, verificación de status, 
retroalimentación de novedades. Así mismo, se ha identificado que varias 
instituciones cuentan con la infraestructura necesaria y recursos humanos 
capacitados que cuentan con sistemas propios complementarios y tienen la 
necesidad de enlazarlos con SIABUC, por ejemplo: el desarrollo de una aplicación 
para la consulta/reservación de libros que interactúe con un sistema propietario de 
control escolar, el cual puede estar basado en un entorno Web, en un dispositivo 
móvil. 
La solución a esta área de oportunidad fue el desarrollo de una arquitectura 
que ofrece servicios Web de manera interoperable, dicha arquitectura es 
denominada: Arquitectura Basada en Componentes de Software para la Integración 
de Servicios (ABCSIS). La razón de crear esta arquitectura fue para enriquecer el 
software SIABUC y proveer un medio que permite conectarlo con desarrollos 
propietarios. Específicamente, se busca proveer a los desarrolladores de software una herramienta que les permitan crear e implementar nuevos componentes que 
puedan trabajar de manera transparente con SIABUC. 
Una de las principales aportaciones de ABCSIS es que será el programador 
quien decida el lenguaje y plataforma a utilizar, ya que al utilizar los servicios Web 
estos ofrecen la ventaja de ser neutrales en cuanto al lenguaje de programación, 
sistema operativo, protocolos de red y mecanismo de almacenamiento utilizado 
(Newcomer, 2002). Además, con la utilización de SOA se permite la utilización de un 
rango más amplio de interacciones de una manera más flexible que una integración 
basada en API's (Chen y Huang, 2006).</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Crear una metodología de desarrollo de software basado en SOA para el software 
SIABUC, con la finalidad de extender los servicios que actualmente se ofrecen.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La arquitectura ABCSIS permitirá, a las instituciones que hacen uso de SIABUC y 
que cuenten con personal de perfil informático o áreas afines, poder implementar 
mecanismos interoperables que permitan la comunicación con otras aplicaciones.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para la realización de este proyecto se siguieron una serie de pasos, los cuales se 
describen a continuación: 
"Investigación documental 
Consiste en buscar información acerca de las tecnologías relacionadas con el 
desarrollo de la arquitectura propuesta, principalmente artículos, así como 
libros de actualidad, en el caso de los artículos la mayor fuente de consulta fue 
la biblioteca digital ACM, así como artículos creados por empresas de 
renombre como IBM, Microsoft y organismos independientes como Apache 
Group, OASIS, entre otros. 
"Diseño de la arquitectura 
Elaboración del modelo conceptual de la arquitectura propuesta, básicamente 
se genero un esquema de la arquitectura ABCSIS con el funcionamiento 
propuesto. 
"Desarrollo del prototipo funcional 
Consistió en la elaboración de una aplicación que consume el servicio de 
reservación de libros para demostrar su funcionalidad e interoperabilidad. 
"Evaluación del prototipo funcional 
Esta etapa consistió en realizar pruebas de operación y pruebas de 
rendimiento. 10
"Documentación de la investigación 
Consiste en redactar el documento de la tesis. 
"Análisis de los resultados obtenidos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La información obtenida en la prueba de operación muestra que a pesar de que un 
poco más de la mitad (62%) de los participantes tiene experiencia en el uso de las 
nuevas tecnologías, y con ciertas carencias de actualización profesional, cumplen 
con sus funciones encomendadas de apoyo técnico a las bibliotecas. 
Esta realidad del perfil informático identificada en la prueba, puede ser incluso 
más deficiente en otras instituciones. Para sostener esta aseveración se cuenta con 
la experiencia que se ha obtenido con más de 5 años brindando soporte técnico a 
infinidad de usuarios bibliotecarios e informáticos de instituciones a nivel 
Latinoamérica. Estas condiciones prevalecen en las instituciones que usan SIABUC, 
donde son pocas las que tienen el apoyo continuo del personal de informática, y por 
eso es de suma importancia ofrecer un mecanismo tecnológico que facilite las tareas 
de instalación, configuración y puesta en marcha, donde la plataforma ABCSIS viene 
a cubrir esta carencia. 
Con los resultados obtenidos en las pruebas, se puede concluir que se ha 
cumplido el objetivo de este trabajo, encapsular una gran funcionalidad y ofrecerla 
para que los usuarios finales puedan incorporarla a sus desarrollos, incluso de 
manera más fácil y sencilla como se hacia con la plataforma anterior basada en CGI. 
También, en base al cuestionario realizado, la hipótesis ha quedado comprobada, ya 
que el 88% de los encuestados consideran factible la interconexión de ABCSIS con 
otros sistemas. 
Actualmente ya se encuentra disponible una aplicación funcional denominada 
iSIABUC. El encargado de crear dicha aplicación también participó en la prueba y le 
resultó muy fácil realizar el acoplamiento de ABCSIS con la aplicación front-end. La 
aplicación consiste en una interfaz Web para SIABUC que consume todos los 
servicios que ABCSIS provee, desarrollando así un mecanismo que ofrece distintos 
servicios vía Web a la comunidad universitaria (Guedea, 2009). La dirección 
electrónica para acceder a ésta aplicación es la siguiente: 
http://siabuc.ucol.mx/s9mashup. 
Las expectativas de este proyecto se han cumplido satisfactoriamente, 
creando una metodología de desarrollo de software basado en SOA para el software SIABUC, con la finalidad de extender los servicios que actualmente se ofrecen en la 
biblioteca a partir de la funcionalidad básica existente en SIABUC.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima 
Facultad de Telemática 
ABCSIS: ARQUITECTURA BASADA EN COMPONENTES 
DE SOFTWARE PARA LA INTEGRACIÓN DE SERVICIOS 
TESIS 
Que para obtener el grado de 
MAESTRO EN COMPUTACIÓN 
PRESENTA: 
ING. HUGO CÉSAR PONCE SUÍREZ 
ASESORES: 
M. en C. JOSÉ ROMÍN HERRERA MORALES 
D. en C. PEDRO DAMIÍN REYES 
COLIMA, COLIMA. NOVIEMBRE DE 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Implantación del sistema SAP R/3 en Peña Colorada a partir del 2000</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El problema es el punto de partida de la investigación, todo problema aparece a raíz de una 
dificultad, y esta, se origina a partir de una necesidad. 17
En el planteamiento de un problema se deben considerar tres aspectos, los cuales son: 
1. Descripción del problema, es la ambientación de la realidad del problema, en relación con 
el medio dentro del cual opera. 
2. Elementos del problema, son aquellas características de la situación problemática 
imprescindibles para el enunciado del problema. 
3. Formulación del problema, consiste en la estructuración de toda la investigación en su 
conjunto, de tal modo que cada una de sus piezas resulte de un todo y que ese todo forme un 
cuerpo lógico de la investigación; es decir, reducir el problema a términos concretos y explícitos. 
Cuando un problema esta bien formulado se tiene ganado la mitad del camino hacia su solución.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Presentar una propuesta para que Peña Colorada implante el sistema SAP R/3 y con 
ello estaría en capacidad de incrementar la productividad y ser más competitiva en el 
mercado global.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La hipótesis es la respuesta tentativa a la pregunta del problema planteado, la cual se 
ratificará o desechará con los resultados de la investigación. 34
El sistema SAP R/3 es un sistema integral, completo y en tiempo real, que si se 
implanta en la empresa Peña Colorada incrementaría la productividad y sería una ventaja 
competitiva. 
Variable independiente = Sistema de información SAP R/3
Variable dependiente = Productividad
Indicadores que le permitiría a la empresa observar los beneficios tangibles de la 
implantación del sistema SAP R/3: 
1. Reducción de costos y gastos :
- Reducción de gastos administrativos. 
- Reducción de costos de mantenimiento. 
- Reducción de costos de materiales. 
2. Optimizar utilización de activos
- Reducir niveles de inventario.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Con la justificación se plantean argumentos que dan respuestas a interrogantes tales como: 
 ¿En qué medida es importante el problema planteado?,  ¿Vale la pena estudiarlo?,  ¿Puede aportar 
algo al conocimiento o ayuda a resolver una dificultad?,  ¿Cuáles son las razones que llevan a 
abordarlo?,  ¿Justifica la tesis de maestría? y sobre todo,  ¿Justifica los recursos a invertir?.24</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>ESQUEMA METODOLOGIA ESQUEMA M ETODOLOGIA
HIPOTESIS TIPO DE INVESTIGACION METODO
Alcanzar el objetivo
Peña Colorada
implante el
SAP R/3</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El aprendizaje adquirido al desarrollar este trabajo es invaluable y muy alentador en el 
desarrollo profesional, ya que invita a continuar realizando proyectos basados en una metodología 
que es muy comentada en el ámbito académico y profesional, pero poco utilizada en su justa 
medida para resolver los problemas que viven las empresas hoy en día. 
 Las metas estimadas en este trabajo se agrupan en tres aspectos: cuantitativos, 
cualitativos y de negocio. 
Aspectos cuantitativos. 
El proyecto SAP R/3, es altamente rentable, de acuerdo a los siguientes indicadores 
financieros: 
- Inversión inicial de $19 millones de pesos para el año 2000, de $5.6 millones de 
pesos para el año 2001 y a partir del 2002 una inversión recurrente de $0.7 millones 
de pesos. 
- Valor presente neto (VPN) a 5 años en $79 millones de pesos. 
- Tasa interna de retorno (TIR) del 164%. 
- Período de recuperación en 0.7 años. 
- Período de implantación del SAP R/3 en 8 meses. 
Los siguientes indicadores avalan el cumplimiento de la hipótesis planteada, ya que se 
observa que el proyecto SAP R/3, ofrece reducciones globales en: 
- 10% de reducción de gastos administrativos, reportando un beneficio acumulado 
de $ 43.8 millones de pesos en 5 años. 
- 10% de reducción de costos de mantenimiento, reportando un beneficio 
acumulado de $ 38.2 millones de pesos en 5 años. 
- 2% de reducción de costos de materiales, reportando un beneficio acumulado de 
$ 41.3 millones de pesos en 5 años. 
- 20% y 40% de reducción de niveles de inventario, reportando un beneficio 
acumulado de $ 51.9 millones de pesos en 5 años. 
Aspectos cualitativos 
 Los principales indicadores que se alcanzan en una implantación de SAP son: 
- Los usuario sólo capturan sólo el 10% de los datos que utiliza el sistema. 
- Los analistas dedican un 30% en tareas de planeación y análisis. 
- Los directivos, ejecutivos, analistas dedican hasta un 60% en tareas de desarrollo 
de estrategias, planeación funcional y de acción. 
Aspectos de negocio 
 En la implantación de SAP R/3 Peña Colorada alcanzaría los siguientes beneficios: 
- Estandarización de tareas. 
- Mejorar sus sistema de información y comunicación. 
- Obtener información en tiempo real, y así mejorar la toma de decisiones. 
- Desarrollar su personal, al proporcionarles capacitaciones, equipo de cómputo y una mejor 
manera de realizar su trabajo. 
- Finalmente aprovechar una ventaja competitiva que es su gente, a través de aplicar 
tecnologías de información, y con ello ser mas rentable y productiva.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
Facultad de Contabilidad y Administración deManzanillo
Programa
Maestría en Ciencias,, Írea Administración,, con opción terminal en
Alta Dirección
Trabajo
Implantación del sistema SAP R/3 en Peña Colorada a partir del 2000
Tesis para obtener el grado de 
Maestro en Ciencias 
Presenta 
Pedro González Mojica 
Coasesores 
Dr. Oscar Rebolledo Domínguez 
M. C. Mario Rito Moreno Martínez
Manzanillo, Colima, Noviembre del 2000</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>MODELACIÓN Y SIMULACIÓN DE UNA VÍA RÍPIDA CON RAMPAS USANDO AUTÓMATAS CELULARES.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la década de los 90s, gracias al desarrollo de los sistemas de cómputo, los 
autómatas celulares (AC) fueron adoptados como una herramienta para el análisis de 
fenómenos complejos, entre los que se encuentran: turbulencias, avalanchas, 
incendios forestales, problemas sociales, problemas económicos, modelación del 
tráfico vehicular, etc.
Un AC está formado por un conjunto de células, llamado espacio celular. 
Una célula o sitio es una región del espacio que puede tomar diferentes valores 
llamados estados. Los estados evolucionan dependiendo del valor de un cierto 
número de vecinos (la vecindad), siguiendo una regla de evolución. 
La regla de evolución o transición nos indica a qué estado pasa una célula, 
de una generación (iteración) a la siguiente dependiendo de su estado actual y de su 
vecindad. Todas las células tienen el mismo número de estados. Si toman un valor 
entre dos posibles se llaman binarias; entre tres posibles se denominan ternarias, etc. 
En todos los casos las reglas de evolución son las mismas para todas las células.
En 1992, Nagel y Schreckenberg (NS) propusieron [10] un modelo con 
Autómatas Celulares probabilístico para la modelación del tráfico vehicular en una 
vía rápida de un solo carril. Usando reglas simples este modelo es capaz de 
reproducir los fenómenos básicos encontrados en el tráfico real.
El incremento en el número de vehículos dentro de las ciudades, hace que las 
vialidades e infraestructura que se tienen sean insuficientes, ocasionando problemas 
de tráfico vehicular, entre los que podemos mencionar:
a) Accidentes automovilísticos.
b) Contaminación atmosférica.
c) Pérdida de tiempo en los recorridos.
d) Grandes recorridos,
e) Problemas de estrés, etc. 
Varios son los esfuerzos por encontrar soluciones a estos problemas. Por 
ejemplo, incrementar el número de vialidades y la infraestructura, pero ésta es una 
opción de alto costo económico y presenta un alto impacto en el medio ambiente, 
por lo que la optimización de la infraestructura actual resulta más viable.
La optimización de la infraestructura vial requiere reproducir situaciones de 
tráfico adecuadas que permitan realizar pruebas de investigación, lo cual es 
complicado, pues produciría situaciones difíciles como congestionamientos, 
accidentes viales, etc. A través de los estudios realizados por varios investigadores
[11, 12, 13], se han obtenido propiedades del tráfico y modelos que resultan útiles 
para realizar simulaciones. Estas simulaciones permiten aplicar las características de 
tráfico encontradas en diferentes ciudades y vialidades, para poder definir el 
comportamiento del tráfico y de esta manera poder tener un mejor control sobre el 
mismo. 
En este trabajo, se presenta un modelo que considera una vía rápida con 
rampas de entrada y salida. La mayoría de las vías rápidas en las ciudades se 
encuentran constituidas por carriles centrales y laterales. Para nuestro caso se tiene 
una vía rápida de tres carriles (uno central y dos laterales), una rampa por la cual 
ingresan vehículos y otra en la que los vehículos pueden salir de la vía rápida.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>* Proponer un modelo de tránsito vehicular basado en Autómatas Celulares, de 
tal forma que dé información de la dinámica del comportamiento del tránsito 
de una vía rápida multi-carril con rampas.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Para el funcionamiento adecuado de las sociedades modernas 
industrializadas, es necesario contar con sistemas de transporte eficientes, que nos 
permitan llegar a los lugares deseados a tiempo, y de manera segura. Es por ello que 
el estudio del tránsito de vehículos ha cobrado gran importancia en la actualidad. 
Aunado a esto, algunos caminos tienen una capacidad reducida, y por tanto es 
necesario contar con ciertas políticas que permitan reducir la congestión de tráfico, y 
en lo posible, reducirla hasta un valor mínimo, de tal manera que podamos llegar a 
tiempo a nuestros lugares de destino.
Un camino para poder lograr un uso eficiente de las redes de transporte, 
implica el conocimiento y la reproducción de ciertos fenómenos que están presentes 
en el tránsito de vehículos.
Es posible observar, en el marco del tráfico real, ciertos fenómenos que están 
presentes. Sin embargo resulta un tanto complicado reproducirlos en este ámbito, 
pues esto implicaría que tendríamos que hacer experimentos con los autos en los 
caminos, lo que provocaría bastantes inconvenientes. Por esta y otras razones resulta 
importante contar con modelos capaces de reproducir el fenómeno del tránsito de 
vehículos, ya que a través de la simulación de dichos modelos es posible manipular 
ciertas variables y conocer el efecto que esto produciría. De aquí que se dedique 
actualmente gran importancia al modelado y simulación de tránsito de vehículos, 
puesto que permite obtener datos importantes sobre su dinámica, sin necesidad de 
molestar a los conductores y como consecuencia provocar un caos vial. 
En este trabajo se presenta un modelo de tránsito vehicular a partir del 
modelo de NaSch que reproduce algunas de los fenómenos que se presentan en la 
dinámica del tránsito vehicular en una vía rápida con rampas usando autómatas 
celulares. 
Al final se presentan los diagramas correspondientes a la forma en que el 
modelo propuesto reproduce la dinámica del tránsito de vehículos.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se obtuvieron los diagramas fundamentales para el modelo propuesto, en la versión 
que incluye:
* Multi-carril.
* Diferentes tipos de vehículos.
* Diferentes velocidades máxima para cada tipo de vehículo.
* Con rampa de entrada.
* Con rampa de salida.
En cuanto a la modificación de la etapa de aceleración que fue sustituida por una 
ecuación de aceleración de un modelo car-following, esto permite representa la 
etapa de aceleración de manera más aproximada al tráfico real comparada con el 
modelo básico de NaSch, sin embargo el tiempo de cómputo requerido para la 
simulación incrementa debido al cálculo que debe hacerse para conocer el valor de 
la aceleración.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
MODELACIÓN Y SIMULACIÓN DE UNA VÍA
RÍPIDA CON RAMPAS USANDO
AUTÓMATAS CELULARES.
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN CIENCIAS DE LA
C O M P U T A C I Ó N
P R E S E N T A
ING. ALECXIS MORALES FERNÍNDEZ
DIRECTOR: M. EN C. GERMÍN TÉLLEZ CASTILLO
MÉXICO D.F. DICIEMBRE DE 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Desarrollo de Desarrollo de una Suite BPM para el modelado, ejecución y monitoreo de los procesos de un Modelo de Mejora de Procesos de Desarrollo de Software"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los procesos para desarrollar productos de software "complejos" , 
frecuentemente lo son también; debido a esta complejidad, su modelado, 
documentación y monitoreo se dificulta. En particular, estas dificultades se les 
presentan a las organizaciones que trabajan de acuerdo a los Modelos de 
Mejora de Procesos. También, como los procesos son continuamente 
estudiados y rectificados, mantener la documentación actualizada representa 
un reto adicional. En general, las dificultades a las que se enfrentan las 
organizaciones con respecto a los procesos son: modelar, documentar, ejecutar y monitorear los procesos; así mismo, mantener actualizada la documentación 
donde se encuentren descritos. 
Existen organizaciones en México con menos de cincuenta participantes, 
conocidas como PyMES, que tienen que enfrentar las dificultades antes 
mencionadas con escasos recursos económicos y de personal. Dar solución a 
estas dificultades con bajo costo, permitiría a las PyMES dedicadas al 
desarrollo de software, implementar, sin estas dificultades específicas, los 
Modelos de Mejora de Procesos. Al lograr implementar estos modelos, 
trabajarían con procesos que ayudarían a producir software de calidad.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Estudiar la factibilidad de construir una Suite BPM de bajo costo que permita 
soportar el ciclo de vida de los procesos de un Modelo de Mejora de Procesos 
de acuerdo a las etapas del enfoque BPM de Modelado, Ejecución y Monitoreo.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Una Suite BPM puede ser adaptada para soportar el ciclo de vida de los 
procesos de un Modelo de Mejora de Procesos en las etapas de Modelado, 
Ejecución y Monitoreo.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. Investigación y redacción del Estado del arte. 
2. Definición de los requerimientos de la Suite BPM
2.1 Identificación de Requerimientos Funcionales. 
2.2 Identificación de Requerimientos No Funcionales.6 
3. Diseño y Construcción de la Suite BPM.
2.3 Diseño de la arquitectura. 
2.4 Desarrollo de componentes. 
4. Evaluación de los objetivos de la investigación 
4.1 Selección de procesos a modelar. 
4.2 Modelado de procesos seleccionados. 
4.3 Revisión del cumplimiento de los Requerimientos Funcionales. 
5. Análisis e interpretación de resultados y redacción de conclusiones.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se concluye que es posible construir una Suite BPM de bajo costo que permita 
soportar el ciclo de vida de los procesos de un Modelo de Mejora de Procesos 
de acuerdo a las etapas del enfoque BPM de Modelado, Ejecución y Monitoreo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD AUTÓNOMA METROPOLITANA 
UNIDAD IZTAPALAPA División de Ciencias Básicas e Ingeniería 
"Desarrollo de Desarrollo de una Suite BPM para el modelado,
ejecución y monitoreo de los procesos de un Modelo 
de Mejora de Procesos de Desarrollo de Software"
Tesis que presenta: 
C.P. y Lic. Silvia Nagheli Márquez Solís C.P. y Lic. Silvia Nagheli Márquez Solís Silvia Nagheli Márquez Solís 
Para obtener el grado de: 
MAESTRA EN CIENCIAS MAESTRA EN CIENCIAS 
DEL 
POSGRADO EN CIENCIAS Y TECNOLOGÍAS 
DE LA INFORMACIÓN DE LA INFORMACIÓN
Asesores: 
Dr. Humberto Cervantes Maceda Dr. Humberto Cervantes Maceda
Dr. Carlos Montes de Oca Vázquez Dr. Carlos Montes de Oca Vázquez
Jurado calificador: 
Presidente: M. en C. Alfonso Martínez Martínez Presidente: M. en C. Alfonso Martínez Martínez
Secretario: Dr. Humberto Cervantes Maceda Secretario: Dr. Humberto Cervantes Maceda
Vocal: M. en C. Mery Helen Pesantes Espinoza Vocal: M. en C. Mery Helen Pesantes Espinoza
MEXICO, D.F. JULIO 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Detección automática de humor en textos cortos en español</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Hoy en día existen ya diversas herramientas que intentan detectar el humor escrito, 
e inclusive generarlo; sin embargo, estas herramientas son muy poco eficientes 
dado que se especializan en un solo tipo de humor (cabe hacer mención, que el 
humor escrito se presenta con diversas variantes); además de que no existe una 
herramienta que detecte textos humorísticos para el idioma español, y por 
consiguiente, que detecte una de las expresiones humorísticas características de 
los mexicanos, el albur. 
El presente trabajo de tesis pretende subsanar la falta de una herramienta para la 
detección del humor en el idioma español; así como para la expresión humorística 
característica de los mexicanos, el albur. Trabajo que no será una tarea sencilla, 
dado que el albur, como todas las expresiones humorísticas, es una expresión 
bastante rica - lingüísticamente hablando - pero que será un buen intento por 
avanzar en materia de estudio y comprensión de la cualidad humana del humor.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Detectar algunos atributos característicos del humor como: rima, aliteración, albur y
contenido adulto, en los textos cortos en español: chistes y dichos, para utilizar los 
algoritmos de la aplicación WEKA a fin de determinar cuál o cuáles son los mejores 
para la detección de humor en textos cortos de dicho idioma.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible detectar textos humorísticos en español con los atributos de rima, 
aliteración y contenido adulto, propuestos por Rada Mihalcea[20] para el idioma 
inglés, y por el atributo palabraalbureable para el albur, utilizando como herramienta 
el programa WEKA.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se pudo demostrar la hipótesis planteada al principio que sugiere que es posible 
utilizar para la detección de textos humorísticos en español los atributos de rima, 
aliteración y contenido adulto, que propone Rada para el idioma inglés, utilizando 
los algoritmos del programa WEKA. 
Son importantes los esfuerzos que se hacen para detectar el albur; sin embargo, 
aún dista mucho de poderse detectar gran parte del mismo, dado que muchas 
veces el albur se presenta como juegos de palabras muy complejos que todavía no 
está dentro de los alcances de la Lingüística poder detectar eficientemente.
Además se descubrió que para algunas situaciones en conjuntos de datos en el 
idioma español los algoritmos NaÍ¯ve Bayes y SVM utilizados por Rada, no son los 
más eficientes a la hora de hacer una clasificación. 
Por último, y con base en los resultados obtenidos en este trabajo de tesis, 
podemos proponer a los algoritmos bayes.AODE, lazy.LBR y misc.VFI como los 
mejores algoritmos para la detección de humor en textos cortos en español.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
LABORATORIO DE PROCESAMIENTO DE LENGUAJE NATURAL
Tesis
Detección automática de humor 
en textos cortos en español 
que presenta el
Ing. Rigoberto Ocampo Pólito
Para obtener el grado de:
Maestro en Ciencias de la Computación
Director de Tesis:
Dr. Alexander Gelbukh
México, D.F., Junio de 2010</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Recuperación personalizada de información de una Biblioteca Digital</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Para el usuario de una computadora conectada a Internet, una Biblioteca Digital [López Guzmán &amp; 
Estrada Corona, Bibliotecas Digitales, 2006] es un servicio a distancia que tiene algunas semejanzas con 
la biblioteca tradicional, entre ellas, el acceso a libros y publicaciones periódicas mediante la consulta de 
un catálogo. 
En una Biblioteca Digital existen diferencias positivas, que representan ventajas respecto a una biblioteca 
tradicional como el manejo de documentos multimedia, el uso de búsquedas automatizadas y la 
independencia de la ubicación física de la persona que acceda a la biblioteca.
Hasta ahora, la mayoría de las Bibliotecas Digitales requieren que el usuario acceda en línea, a la 
información que requiere. Una vez conectado a la biblioteca, la información que se obtiene 
generalmente se encuentra en documentos con formato PDF (Portable Document Format, Formato de 
Documento Portable), limitando la obtención de partes especificas del documento, como lo son: titulo, 
autores, resumen, etc. Las bibliotecas digitales generalmente presentan en pantalla los documentos 
completos dejando atrás la posibilidad de obtener dicha información de acuerdo a las necesidades y 
preferencias del usuario.
Por otro lado, se ha detectado que existen documentos que tiene más de 30 MB en espacio en disco 
duro lo cual provoca que en conexiones de internet por debajo del ancho de banda el tiempo de 
descarga del mismo sea de aproximadamente 30 min, por el contrario el lugares especializados como 
universidades con anchos de banda grandes aun el tiempo de descarga puede ser de aproximadamente 
10 segundos lo que implica que el volumen del contenido de un documento, puede ocasionar 
afectaciones en el tiempo de descarga además de limitar el tráfico en la red, así mismo un documento 
con bastante contenido incrementa el tiempo para su análisis y revisión al no conocer si el documento en 
sí cuenta con la información que se ha solicitado.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar una propuesta en la que se adicione la funcionalidad de búsquedas mediante el uso de XPath 
en documentos con formato XML, contribuyendo a la personalización de la que información que se 
recupera acorde a las preferencias y necesidades de un usuario</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La información almacenada en formatos digitales ha ido incrementándose y las Bibliotecas Digitales se 
han vuelto los espacios de almacenamiento de esta información. Los catálogos de las bibliotecas digitales 
se encuentran divididos en diferentes categorías, las cuales no tienen la misma importancia para los 
usuarios que tiene acceso a ella.
Actualmente tanto las Bibliotecas Digitales como los diferentes servicios de Internet buscan de 
estándares eficientes para tratar el contenido de los documentos digitales en Internet, la iniciativa dublin 
Core [The Dublin Core, 1998] es una de las principales propuestas para el manejo de la información en la 
Web para estandarizar los datos de los documentos digitales por medio de etiquetas o descriptores. XML 
debido a que usa etiquetas para la representación de sus datos y a su acoplamiento con la Web 
semántica se sitúa entre uno de los formatos de mayor demanda en la explotación de información digital 
en Internet, es así que por ello la información digital se convertirá de su formato original a un formato XML donde se ubicaran los datos que generalmente presentan mayor relevancia considerando XML 
como el estándar para la representación del texto, siendo este una tendencia mundial, ya que sus 
características y peculiaridades para la estructuración de texto lo hacen un buen aliado de las Bibliotecas 
Digitales además de proporcionar interoperabilidad facilitando su uso en Internet y reduciendo el 
volumen de información sobre el tráfico de red.
La Recuperación de Información basada en las preferencias de un perfil de usuario permite entregar al 
usuario lo que necesita y nada más lo que necesita realizando un proceso de abstracción sobre las cosas 
que no sean de su preferencia acotando la cantidad de información que percibe el usuario ya sea en 
línea o en su correo electrónico.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Debido a los diferentes formatos de escritura de los documentos del catálogo se debe estandarizar la 
construcción y diseño de las Tesis e Informes Técnicos con el fin de homologar la información contenida 
en ellos y facilitar el mapeado de los documentos a XML (Extensible Markup Language, Lenguaje de 
Marcado Extensible), la propuesta de meta esquema para trabajar con Tesis e Informes Técnicos puede 
ser ampliada o modificada para su uso con otro tipo de documentos indistintamente del área de 
conocimiento al que puedan pertenecer.
Al ser XML un estándar para el intercambio de información y el cual ha sido aceptado en forma 
importante por los desarrolladores de la tecnología de la información se vuelve importante el contar con 
herramientas que permitan su explotación y debido al súbito uso de este estándar en Internet se vuelve 
importante el también proporcionarle a las Bibliotecas Digitales la opción de interoperabilidad entre la 
información de los documentos para así poder reutilizar el contenido de ellos independientemente de la 
aplicación a usar, y extender las posibles búsquedas de documentos, en el contenido de los mismos y no 
solo en los datos que usualmente poseen los catálogos de las bibliotecas tradicionales.
En esta tesis se desarrolló el soporte de expresiones lógicas usando XPath (XML Path Language, 
Lenguaje de Ruta XML), el filtrado por palabras frecuentes, la presentación y Recuperación de 
Información de acuerdo con las preferencias del usuario contenidas en su perfil.
Por otro lado los dispositivos móviles actualmente se encuentran en una constante modificación tanto 
en sus requerimientos estéticos como en su poder de computo debido a la constante necesidad de los 
usuarios por sentirse comunicados a cualquier hora y en cualquier lugar, sin embargo, actualmente no 
poseen la suficiente memoria para poder permitir el uso de DOM limitando el uso del sistema 
directamente sobre la plataforma en la que estos se encuentran diseñados, sin embargo, la opción del 
sistema de recuperación de información resultante en formato HTML de tamaño pequeño por correo
electrónico facilita que cualquier dispositivo con acceso a internet pueda revisar dicho resultado.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL 
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN 
DFDFDF
Recuperación personalizada de información de 
una Biblioteca Digital
T E S I S 
QUE PARA OBTENER EL GRADO DE MAESTRÍA EN CIENCIAS DE 
LA COMPUTACIÓN 
PRESENTA: 
ING. EDUARDO MARTÍNEZ CÍZARES 
DIRECTOR DE TESIS: DR. JESÚS MANUEL OLIVARES CEJA
MÉXICO, D.F. NOVIEMBRE, 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Adaptación de una Metodología de Desarrollo Arquitectónico al Contexto de Equipos de Desarrollo Pequeños"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los métodos que integran a la metodología de desarrollo arquitectónico propuesta por el
SEI buscan ayudar a los arquitectos a realizar diseños arquitectónicos de forma sistemática, a
documentar estos diseños y a comprender las consecuencias de las decisiones arquitectónicas
que se toman con respecto de los atributos de calidad y objetivos de negocio del sistema
[Nord2004]. Uno de los primeros métodos relacionados con el desarrollo arquitectónico es
el método de evaluación arquitectónica llamado ATAM, el cual con el paso del tiempo ha
permitido a los investigadores del SEI crear otros métodos derivados del mismo, ejemplos de
estos métodos son QAW, ADD y VaB [Kazman2004].
Los métodos arquitectónicos del SEI antes mencionados tienen 3 características que difi-
cultan su uso por equipos de desarrollo pequeños, estas características son:
1. No están integrados dentro de un proceso de desarrollo de software particular y para
obtener mayor beneficio estos métodos se deben utilizar juntos, lo cual requiere de
ajustes importantes (alguien en la organización de desarrollo debe tener mucho conocimien-
to en su uso para poder ajustarlos en un proceso cohesivo) [Lattanze2005]. Está car-
acterística es esencial ya que con los ajustes necesarios los métodos arquitectónicos del
SEI se pueden usar dentro de cualquier proceso de desarrollo.
2. Los métodos arquitectónicos del SEI están pensados para que sean usados en el diseño
de sistemas complejos de alto riesgo. Está característica los hace ser pesados y costosos
en términos de documentación, recursos financieros, de calendario, capacitación, etc.
[Lattanze2005]
3. Están orientados hacia grandes equipos tanto de desarrollo como de involucrados. Está
característica se podría ver como un derivado de la anterior debido a que sólo equipos
de desarrollo grandes pueden desarrollar sistemas complejos con alto riesgo.
Adicionalmente, se ha identificado que la secuencia con la cual el SEI propone que los
métodos sean realizados tiene potenciales riesgos de re-trabajo en la documentación arqui-
tectónica, esto se debe a que una vez que el diseño ha sido documentado se evalúa y en caso
de que se necesiten correcciones la documentación se tiene que actualizar.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de este proyecto de investigación es:
Adaptar la metodología de desarrollo arquitectónico del SEI para que pueda ser em-
pleada por equipos de desarrollo pequeños.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El desarrollo de este proyecto se basa en las siguientes hipótesis:
Cada uno de los métodos que integran la metodología propuesta por el SEI se pueden
adaptar al contexto de equipos de desarrollo pequeños.
* Es posible agilizar los pasos que cada uno de los métodos comprende.
* Es posible reducir el número de artefactos o reducir el contenido de algunos de
estos.
* Es posible que arquitectos con poca experiencia mejoren sus diseños arquitectóni-
cos siguiendo las adaptaciones propuestas.
Los métodos propuestos por el SEI pueden trabajar de forma coordinada.
La metodología propuesta por el SEI se puede integrar con TSPi.
Es posible determinar los elementos arquitectónicos específicos de ASOA con la liter-
atura y aplicaciones disponibles.
Es posible establecer un proceso general que permita identificar los elementos arquitec-
tónicos para patrones distintos a ASOA.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La realización de este proyecto de investigación permitirá a las organizaciones de desarrollo
reducir los costos de:
Adaptación de los métodos arquitectónicos del SEI: Para realizar la adaptación de los
métodos que integran a la metodología de desarrollo arquitectónico del SEI las organi-
zaciones de desarrollo tendrían que contratar a expertos en la misma o asignar a parte
de su personal en la realización de esta tarea, lo que involucra un gasto en recursos
financieros, humanos y de calendario (costos que pocas organizaciones pueden pagar).
Integración de la metodología del SEI en un proceso de desarrollo: De igual forma que
en el punto anterior, las organizaciones deberían invertir recursos para determinar en
qué etapas de su proceso de desarrollo se puede integrar a la metodología de desarrollo
arquitectónico del SEI y realizar las actividades necesarias para adoptar estos cambios
en su proceso.
Entrenamiento en la metodología: Para realizar entrenamiento se requiere que expertos
en la metodología propuesta por el SEI creen el material e impartan las sesiones de
entrenamiento.
El uso de algún método de desarrollo arquitectónico, como el propuesto por el SEI, per-
mite incrementar la posibilidad de éxito (entendido como entregar sistemas de calidad, a
tiempo y con los recursos acordados) en el desarrollo de sistemas de cómputo, por lo cual es
necesario dotar a los equipos de desarrollo pequeños con alguno de estos métodos adaptado
a sus características y necesidades propias (posibilidad de comunicación persona a persona,
pocos roles involucrados, desarrollan sistemas de complejidad media a baja, se considera que
desarrollan sistemas no críticos1
, etc.). Lo anterior le permitirá a estos equipos realizar diseños
arquitectónicos de calidad de forma repetible.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología propuesta en este proyecto de investigación consiste de las siguientes fases:
Investigación: Considera la investigación de los siguientes elementos:
* Conceptos relevantes de arquitectura de software.
* La relación de la arquitectura de software con la calidad del sistema.
* Las diferentes opciones de metodologías de desarrollo arquitectónico.
* La identificación de los niveles de aplicación de la Arquitectura Orientada a Ser-
vicios (SOA) disponibles en la literatura.
Estudio: Comprende el estudio de las actividades que integran cada método de la
metodología de desarrollo arquitectónico propuesta por el SEI.
Adaptación: Se adaptan los métodos que comprende la metodología de desarrollo ar-
quitectónico propuesta por el SEI para que se puedan aplicar en el contexto de equipos
de desarrollo pequeños.
Realización: Se selecciona un caso de estudio de la vida real que considere el uso de
ASOA. Por último, se realiza, con ayuda de un grupo de estudiantes de maestría, el
diseño arquitectónico del caso de estudio siguiendo las adaptaciones propuestas a la
metodología de desarrollo arquitectónico propuesta por el SEI. El resultado de esta
fase es un diseño validado y documentado que permita implementar al sistema.
Recopilación y reporte de resultados: Se recopilan los resultados obtenidos y se realiza
el reporte de los mismos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este proyecto se definió un conjunto de adaptaciones a los métodos de desarrollo
arquitectónico del SEI para que estos funcionen de forma coordinada y, de forma breve, se
presentó como estas adaptaciones se pueden incorporar en un proceso de desarrollo orientado a
equipos de desarrollo pequeños (TSPi). En estas adaptaciones se logró mantener el espíritu de
cada uno de los métodos aligerando los artefactos que produce y las actividades que cada uno
comprende. Estas adaptaciones se realizaron considerando que equipos de desarrollo pequeños
realizan sistemas de complejidad media a baja y sin riesgos que involucren pérdidas financieras
o de vidas humanas (en caso contrario es recomendable realizar análisis minuciosos).
Para realizar la evaluación de las adaptaciones propuestas se eligió un caso de estudio
real cuyos objetivos de negocio se satisfacían con el uso de ASOA. Como apoyo para la
evaluación de las adaptaciones se creó un conjunto de elementos arquitectónicos (catálogo
de tácticas y patrones arquitectónicos y tablas de generación de escenarios de atributos de
calidad) que son necesarios para soportar la ejecución de los métodos en el contexto de la
creación de aplicaciones basadas en el enfoque ASOA, además, se definió un proceso genérico
que permite definir estos elementos arquitectónicos para otros contextos diferentes de ASOA.
Finalmente, se describió una evaluación preliminar con estudiantes de maestría, está eval-
uación arrojó resultados que permiten pensar que es altamente factible que gente con poca
experiencia se beneficie de realizar las adaptaciones propuestas en este trabajo dado que les
permite crear diseños arquitectónicos de calidad aceptable desde sus primeros intentos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Maestría en Ciencias y Tecnologías de la Información
Adaptación de una Metodología de Desarrollo
Arquitectónico al Contexto de Equipos de
Desarrollo Pequeños"
Idónea Comunicación de Resultados que para obtener el grado de
MAESTRO EN CIENCIAS
(Ciencias y Tecnologías de la Información)
PRESENTA:
Lic. José Ismael Nuñez Reyna
Asesor:
Dr. Humberto Cervantes Maceda
Sinodales:
Dr. Cuauhtémoc Lemus Olalde
M. en C. Eduardo Rodríguez Flores
Dr. Humberto Cervantes Maceda
23 de octubre de 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>COMPONENTES MULTIMEDIA   ORIENTADOS A OBJETOS PARA WBE  UTILIZANDO ABP </Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>A través de la historia de la educación se han desarrollado diferentes formas de transmitir el 
conocimiento a quien no lo tiene; estas formas de acercamiento han utilizado diferentes medios para 
llegar a ese objetivo: distribuir el conocimiento de una manera eficaz y oportuna. Actualmente las 
infraestructuras más populares que se utilizan para distribuir el conocimiento son la Internet e 
Intranets, así también existe una serie de software que permite facilitar al docente o instructor la 
generación de cursos que deba impartir, el problema es que no existen a nivel básico software 
educativo basado en alguna metodología que se haya comprobado resulte exitosa para el proceso de 
enseñanza / aprendizaje. 
En algunas instituciones educativas como el caso del IPN, con la transición que se está 
llevando a cabo del modelo tradicional al nuevo modelo educativo [2] que tiende a una enseñanza 
constructivista, base del modelo ABP, los profesores que se ven inmiscuidos en este campo necesitan 
herramientas para complementar su cátedra</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Construir una herramienta bajo la arquitectura Cliente / Servidor que a través de componentes 
multimedia orientados a objetos permita a los profesores crear escenarios basados en la metodología 
propuesta y que permita también, a los estudiantes acceder a esos materiales vía Web.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Esta tesis propone un procedimiento en base a la metodología ABP y genera un sistema que 
permite implementar este procedimiento; esta herramienta permitirá que los profesores que no tengan 
conocimientos en informática ni conocimientos de alguna metodología de enseñanza / aprendizaje 
sean capaces de diseñar sus materiales educativos con facilidad. El primer paso es diseñar el 
escenario, plantear los objetivos de aprendizaje, planear el tiempo que se invertirá en las actividades 
desarrolladas, formar equipos de trabajo asignando roles a cada miembro del equipo, paralelamente a 
esto se debe comunicar a los alumnos la forma en que se llevará a cabo la evaluación (se proponen 2 
esquemas, el profesor elegirá con cual desea trabajar). 
Una vez realizada la creación del escenario, el tutor planteará éste a los estudiantes de tal 
forma que a partir de ese momento el estudiante será el que lleve el manejo del escenario, 
identificarán los puntos clave del problema, asignarán las tareas de investigación y el tiempo a cada 
una de ellas, una vez digerido el problema e investigado los conceptos que no se saben, cada equipo 
planteará su solución. Cabe hacer la mención que en cada etapa que los alumnos desarrollen, el tutor 
se verá involucrado guiando y encausando a los equipos; para ello el sistema cuenta con un módulo de 3
comunicación y envió de tareas, preguntas y sesiones programadas para que todos los miembros del 
equipo puedan estar en línea comunicándose con el tutor. 
De esta forma la propuesta se engloba de una manera muy sencilla en proporcionar a los 
profesores o tutores la metodología basada en ABP y el sistema que logre captar esta metodología 
para poder generar un curso de Educación Basada en Web.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El uso de la computadora en educación ha sido un punto de inflexión en la educación, La 
Internet es otro punto fundamental que ha permitido a diferentes tecnologías ser utilizadas para 
educación. A través del desarrollo de este trabajo se ha podido hacer uso de esas tecnologías para 
implementar un sistema llamado SABPOO basado en WBE utilizando como metodología pedagógica la 
técnica de Aprendizaje Basada en Problemas (ABP). 
Con el desarrollo de este proyecto se logra hacer uso de una metodología de enseñanza / 
aprendizaje dentro del sistema SABPOO que le permite a los profesores aplicarla de una manera muy 
sencilla y a través de elementos multimedia que apoyan la generación de contenidos, y permitiendo a los 
estudiantes aprovechar las ventajas del paradigma WBE. Además el sistema SABPOO simplifica la 
forma de desarrollo de materiales de aprendizaje con esta metodología, reduciendo la complejidad de los 
materiales educativos para el profesor, permitiéndole reducir la brecha digital que sobre algunos 
profesores se ha abierto debido a las nuevas tecnologías. 
A continuación se puntualizan las ventajas que el sistema SABPOO proporciona: 
1. Al profesor le permitirá aplicar una metodología basada en ABP tomando en cuenta que 
posiblemente la desconozca, de tal forma que el sistema proporciona un procedimiento 
sencillo y práctico para que se puedan generar los materiales educativos sin demasiado 
esfuerzo. 
2. A través de chats, foros de discusión, messeger, etc, permitirá que el profesor 
programe las sesiones con sus alumnos para aclarar dudas, apoyarlos y guiarlos en las 
actividades que ellos estén desarrollando. 
3. Al profesor le permitirá construir equipos de trabajo basados en la metodología 
presentada en esta tesis. El sistema sugerirá al profesor algunos roles que deben ser 
tomados en cuenta para la construcción de equipos dependiendo del número de alumnos 
que se tengan. 
4. Al profesor le permitirá ingresar elementos multimedia que apoyen la exposición del 
escenario o problema base que permitirá a los estudiantes recolectar el conocimiento 
necesario para llegar a los objetivos fijados por él. 
5. Al alumno le permitirá de una forma fácil trabajar en equipo con las ventajas que se 
tienen actualmente con la Internet, herramientas como el chat, los foros de discusión, 
messenger, etc. El sistema le proporcionará las citas programadas que se tenga con su 
tutor para aclarar dudas o pedir que los guíe en algún concepto que no tengan bien 
puntualizado. 
6. Al alumno le permitirá trabajar a su propio ritmo debido que la presentación del 
escenario y los elementos multimedia que en el hubiere, pueden ser presentados en el 
momento que el alumno lo solicite y las veces que lo requiera. 
7. Al equipo le permitirá estar informado de las actividades que se les haya asignado a 
cada miembro, podrá verificar el porcentaje de avance que se tenga en cada actividad. 
8. Por último, al equipo le permitirá descargar la solución a la que haya llegado dentro de 
la resolución del escenario. Puntualizando que el objetivo de esta metodología no es 
realmente el resultado al que lleguen sino los conceptos, herramientas e información 
que hayan recolectado y utilizado a través de la búsqueda de la solución.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACION
COMPONENTES MULTIMEDIA 
 ORIENTADOS A OBJETOS PARA WBE 
UTILIZANDO ABP 
 T E S I S 
 QUE PARA OBTENER EL GRADO DE 
MAESTRO EN CIENCIAS DE LA 
COMPUTACIÓN 
PRESENTA 
LIC. LILIA GONZALEZ ARROYO 
DIRECTOR: 
M. EN C. RUBÉN PEREDO VALDERRAMA 
 México, D.F. Mayo de 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>HERRAMIENTA DE AUTORÍA PARA TUTORES  INTELIGENTES BASADOS EN MODELOS  PROBABILISTAS RELACIONALES</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El diseño de un STI requiere, entre otras cosas, una representación 
fidedigna del conocimiento que posee cada estudiante que lo utiliza, 
conocida como modelado del estudiante. La definición de este modelo es 
otra tarea que sigue en constante investigación; Murray (1999) presenta
diversos enfoques que se han abordado para el modelado del estudiante. La 
implementación correcta del modelo del estudiante radica en la necesidad de 
obtener una estructura que permita una representación flexible y que 
produzca inferencias rápidas y correctas acerca de las acciones a tomar por 
el STI.
Un laboratorio virtual es un ambiente heterogéneo donde un 
estudiante interactúa para resolver ejercicios propuestos por algún docente y 
la retroalimentación que recibe la evalúa el propio estudiante cuando analiza 
los resultados que obtuvo de algún experimento. En algunos casos un 
estudiante no puede identificar fallas en sus resultados; por tal motivo es 
necesario incorporar un software que permita dar esta retroalimentación y 
guie al estudiante durante el proceso enseñanza-aprendizaje. Este software 
es un STI que permita monitorear los resultados del experimento y pueda 
administrar lecciones según sea necesario.
Otro aspecto importante sobre los STI para laboratorios virtuales es el 
manejo de la incertidumbre, debido a que el STI sólo interactúa con el 
estudiante mediante los experimentos que ha resuelto. Es decir, se tiene que
inferir lo que sabe el estudiante a partir de su interacción con el laboratorio 
virtual. Para esto se requiere un modelo que permita el manejo de 
incertidumbre como lo son las redes bayesianas y los modelos probabilistas 
relacionales. La construcción de STIs con estas características es una labor compleja debido a la falta de una herramienta de autoría para este tipo de 
modelos.
Los problemas descritos anteriormente son afrontados en esta
investigación a través del desarrollo de una herramienta de autoría basada 
en Modelos Probabilistas Relacionales (PRMs por sus siglas en inglés)
(Getoor, et al. 2007). Dicha herramienta facilita la construcción automática 
del STI, de forma que un docente no tiene que ser experto en tutores 
inteligentes ni en modelos probabilistas. Se utiliza en el diseño de la 
herramienta una arquitectura orientada a objetos a fin de independizar cada 
parte de las tareas, y definiendo las interfaces requeridas para el mejor 
aprovechamiento de las relaciones entre los objetos que componen el STI, la 
herramienta y el laboratorio virtual que apoya. La herramienta se enlaza con 
el laboratorio mediante el uso de las variables que el laboratorio puede 
monitorear; las cuales son utilizadas para evaluar el resultado de 
experimentos que un estudiante realice, y actualizar el modelo del estudiante.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de este trabajo de investigación es desarrollar una 
herramienta de autoría para tutores inteligentes en laboratorios virtuales, 
basada en modelos probabilistas relacionales.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En el presente trabajo se desarrolló una herramienta de autoría capaz 
de construir STI basados en modelos probabilistas para ser utilizados en 
laboratorios virtuales. La herramienta proporciona una interfaz para capturar 
el contenido de un curso y dar un peso a los conceptos que se van a 
enseñar. De manera automática se genera un modelo del estudiante que 
utiliza una red bayesiana dinámica, que unido a las reglas de tutoría permite 
al STI tomar decisiones sobre las acciones pedagógicas que se tomarán. 
Para poder utilizar esta herramienta es necesario que el docente cuente con 
un laboratorio virtual donde realizar los experimentos. La Figura 1-1 muestra 
de manera general la construcción de un tutor inteligente con la herramienta 
de autoría.
La herramienta se desarrolló utilizando un enfoque orientado a objetos y 
se programó en JAVA 2.0. Su funcionalidad se evaluó utilizando un pequeño 
laboratorio virtual de redes neuronales artificiales, este laboratorio virtual fue 
construido para apoyar un taller de enseñanza de redes neuronales (GómezGil 2009). La herramienta permite generar el STI y es consistente en cuanto 
a las inferencias realizadas para escenarios de prueba. La usabilidad se 
evaluó por el docente a través de un cuestionario.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El uso de sistemas tutores inteligentes se ha popularizado en los 
últimos años; sin embargo, su construcción requiere de una gran cantidad de 
tiempo y personal capacitado. Para agilizar la construcción de STIs se han 
desarrollado herramientas de autoría. Las herramientas desarrolladas a la 
fecha están restringidas a ciertas clase de tutores y no se aplican 
directamente a laboratorios virtuales.
La herramienta de autoría que se ha desarrollado en esta 
investigación permite a los profesores generar sistemas tutores inteligentes 
basados en un modelo probabilístico; los cuales son obtenidos de una 
jerarquía de temas, subtemas y conceptos (temario del curso). El STI 
generado por la herramienta es utilizado como apoyo a un laboratorio virtual 
existente. La herramienta facilita a un docente la generación de un STI, sin 
requerir que sea experto en tutores o modelos probabilistas. El aspecto 
central de la herramienta es la generación automática de un MPR a partir de 
la información dada por el profesor. El MPR permite posteriormente modelar 
el estado de conocimiento del estudiante y proveerle asesoría personalizada.
Se ataca la facilidad de uso haciendo transparente para el docente 
que el modelo de la red bayesiana obtiene sus valores de probabilidad 
basado en el porcentaje del contenido temático. El tipo de STI generado 
puede ser utilizado en cualquier situación donde se tenga un laboratorio 
virtual funcionando. El sistema tutor inteligente se comunica al laboratorio 
virtual mediante las variables que se desean evaluar en cada experimento. 
Con lo anterior la herramienta es de uso general para cualquier laboratorio 
virtual que incluya variables a monitorear en los aspectos a ser evaluados.
La herramienta permite que cualquier profesor que utiliza un 
laboratorio virtual y desea incorporar un STI, pueda hacerlo sin necesidad de 
manejar las técnicas de inteligencia artificial necesarias ni el dominio de un 
lenguaje de programación.
La herramienta de autoría fue evaluada mediante la generación de un 
STI para un laboratorio virtual de redes neuronales y dos pruebas de 
usabilidad a personas voluntarias en Base de datos y Robótica. Se evaluaron 
tres aspectos: Usabilidad, inferencias en el modelo del estudiante y 
evaluación del tutor. Mediante las pruebas que se realizaron se considera 
que la herramienta tiene una usabilidad aceptable y permite generar al 
modelo de forma transparente al docente; sin embargo, falta por mejorar las 
instrucciones y la tolerancia a errores de captura. En cuanto al modelo del 
estudiante se hicieron pruebas que demuestran que los valores obtenidos 
mediante propagación probabilística en el modelo generado es coherente 
con los resultados esperados. La evaluación de tutor inteligente queda como 
trabajo futuro, pero de manera indirecta gracias, a las pruebas realizadas 
para este tipo de tutores en laboratorios virtuales se esperan buenos 
resultados.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>HERRAMIENTA DE AUTORÍA PARA TUTORES 
INTELIGENTES BASADOS EN MODELOS 
PROBABILISTAS RELACIONALES
Por
Mario Alberto Romero Inzunza
Tesis
Sometida como requisito parcial
para obtener el grado de
MAESTRO EN CIENCIAS EN EL ÍREA DE 
CIENCIAS
COMPUTACIONALES
en el
Instituto Nacional de Astrofísica, Óptica y 
Electrónica
Tonantzintla, Puebla
Supervisada por:
Dr. Luis Enrique Sucar Succar
Dra. María del Pilar Gómez Gil
 ©INAOE 2009
Derechos reservados
El autor otorga al INAOE el permiso de
reproducir y distribuir copias de esta tesis
en su totalidad o en partes.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"RECONOCIMIENTO DE ROSTROS UTILIZANDO ANÍLISIS DE COMPONENTES PRINCIPALES: LIMITACIONES DEL  ALGORITMO"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Para el reconocimiento de rostros se han utilizado diversos métodos, entre éstos: elementos geométricos del rostro, análisis estadístico, redes neuronales, componentes principales, etc. 
La técnica más utilizada en los últimos años, ha sido el de componentes principales. Uno de los paquetes de software aplicado al reconocimiento de rostros y que ha presentado resultados exitosos, utiliza el algoritmo denominado Eigenfaces (Sirovich y Kirby,1987; Kirby y Sirovich,1990; Turk y Pentland,1991), basado en la técnica de análisis de componentes principales. Sin embargo, aunque dicho software se considera uno de los 
mejores, presenta ciertos porcentajes de error. Otros sistemas de reconocimiento de rostros, que utilizan otras técnicas derivadas del análisis de componentes principales (LFA, propuesto por Penev y Atick, 1996), también han presentando resultados inciertos. 
La gran mayoría de los sistemas computacionales para reconocer rostros, comparten algo en común, sus respectivos algoritmos matemáticos trabajan en un espacio en . Lo anterior, presupone que las características de los datos deben cumplir con ciertos supuestos estadísticos y matemáticos al aplicar las diversas técnicas de análisis multivariado que se utilizan.
El software basado en la técnica de análisis de componentes principales y algunos otros desarrollos basados en técnicas similares, presentan diversas limitaciones y por tanto, deficiencias en los resultados. Tales limitaciones se considera que se presentan por dos motivos principales: el primero, debido al propio algoritmo; en segundo lugar, al tipo de características estadísticas que presentan los valores que se obtienen de los pixeles de una imagen digital de un rostro. Se considera que los datos numéricos de los rostros utilizados en el sistema de Eigenfaces, viola el supuesto de normalidad multivariada, elemento importante para utilizar las técnicas de análisis con componentes principales.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un sistema computacional para el reconocimiento de rostros mediante
aprendizaje supervisado basado en el análisis de componentes principales, con el propósito 
de realizar diversos experimentos para obtener las ventajas y limitaciones del 
procedimiento y proponer de manera conceptual una nueva alternativa de solución.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Se tiene posibilidad de realizar un reconocimiento de rostros utilizando la técnica de
análisis de componentes principales?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Las características estadísticas que presentan los pixeles de imágenes digitales de rostros 
cumplen con los requisitos para realizar un análisis por medio de componentes principales?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Cuántas fotografías por cada sujeto en la "base de entrenamiento"  de rostros son 
requeridas para llevar a cabo un reconocimiento adecuado utilizando la técnica de 
componentes principales?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Cuáles serían las principales desventajas que presenta el análisis de componentes 
principales para llevar a cabo un adecuado reconocimiento de rostros?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Qué otra técnica sería susceptible de emplearse para llevar a cabo un adecuado 
reconocimiento de rostros?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Utilizando una base de datos conteniendo una sola fotografía digital por persona, frontal y 
con elementos controlados de luz será posible llevar a cabo un adecuado reconocimiento 
del rostro de una persona utilizando otra fotografía digital del sujeto.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En conclusión, el método de componentes principales; se considera una técnica que permite
realizar reconocimientos de rostros sobre todo, cuando se tienen diversas imágenes por 
persona. Sin embargo, a pesar de lo anterior, la confiabilidad no es completamente
adecuada. 
Se considera que al tener datos (los rostros), que no cumplen con el supuesto de normalidad  multivariada, inciden en las diversas fallas y errores de tales tipos de sistemas. 
Por tanto, se plantea la opción de trabajar en un espacio L el cual no requiere los
supuestos de normalidad, linealidad y homoscedasticidad. Este nuevo método para el 
reconocimiento de rostros, utilizará un enfoque de aproximación multivariada, utilizando el
denominado Algoritmo Genético Ecléctico para obtener bajo un paradigma de optimización 
combinatoria, la forma y orden del polinomio de aproximación que caracterizará a los
rostros. 
Asimismo, dicho método requerirá únicamente, de una muestra de los píxeles que configura 
cada rostro, lo cual redituará en una menor cantidad de almacenamiento y tiempo de 
procesamiento computacional. Además, al utilizar un enfoque holístico, no requiere de la 
utilización de características geométricas o "plantillas"  del rostro.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD IBEROAMERICANA 
"RECONOCIMIENTO DE ROSTROS UTILIZANDO ANÍLISIS DE 
COMPONENTES PRINCIPALES: LIMITACIONES DEL 
ALGORITMO"  
TESIS 
Que para obtener el grado de 
MAESTRO EN SISTEMAS Y PLANEACION 
P r e s e n t a: 
CARLOS VILLEGAS QUEZADA 
Director 
MTRO. JORGE RIVERA ALBARRAN 
Asesores: 
MTRO. PEDRO FERNANDO SOLARES SOTO
MTRO. FELIPE ANTONIO TRUJILLO FERNANDEZ 
MÉXICO, D.F. 2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>APRENDIZAJE MAQUINAL  MULTIVALORES
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El concepto de aprendizaje puede ser visto como la búsqueda de una hipótesis que satisfaga
ciertos criterios de calidad [19]. Dicha búsqueda es realizada por un sistema computacional
llamado aprendiz, el cual puede ser descrito a partir de la estructura de su espacio de búsqueda,
de su heurística de búsqueda y de su estrategia de búsqueda [12].
El espacio de búsqueda es el conjunto de hipótesis donde el aprendiz realiza la búsqueda,
en ILP está determinado por el lenguaje de los programas lógicos, los cuales, están formados
por cláusulas de programa de la forma T   Q1, donde T es un átomo p (X1,...,Xn) y Q
es una conjunción de literales L1,...,Lm (en el apéndice A se describen con más detalle las
principales definiciones de la lógica de primer orden). Por otro lado la forma de las hipótesis del
espacio de búsqueda, está restringida sintácticamente por el prejuicio de lenguaje. Este prejuicio
determina las cláusulas, que pueden formar parte de cada hipótesis, a partir del vocabulario de
los predicados, de los símbolos de función y de las constantes declaradas en el conocimiento
previo2 [12].
Cuando el prejuicio de lenguaje es muy fuerte, lenguaje poco expresivo, el espacio de
búsqueda se vuelve más pequeño, y aunque la búsqueda es más eficiente, es más probable que
la hipótesis encontrada no represente una solución adecuada para el problema que se intenta resolver.
Por ejemplo si el prejuicio de lenguaje impide el uso de la literal de la cabeza en el cuerpo
de las cláusulas, entonces ninguna hipótesis del espacio de búsqueda representará la solución a
un problema cuya naturaleza es recursiva.
En cada algoritmo ILP se define un prejuicio de lenguaje, con el objetivo de que las hipótesis
se construyan con la mayor expresividad posible, y de que la hipótesis encontrada represente
una mejor generalización. Sin embargo, para construir las hipótesis, los algoritmos ILP actuales
prueban un solo valor por cada aparición del atributo que representa, por lo que es probable que
las hipótesis se construyan con una gran cantidad de cláusulas. Entre más grande sea la base de
conocimientos, es más probable que la hipótesis final esté construida con más cláusulas, por lo
tanto es más difícil de interpretar. Para describir este comportamiento presentamos un problema
de clasificación planteado por Stephen Muggleton [21]. Este problema consiste en clasificar un
conjunto de animales en las siguientes clases: ave, reptil, mamífero y pez. La literal objetivo es:
clase (Animal;Clase)  
El aprendiz creará cada hipótesis (conjunto de reglas) con la información de cada animal
acerca de su habitat, su cubierta, su número de patas, si toma leche, si es homeotérmico, si
pone huevos o si tiene branquias. Las literales que representan la información mencionada
anteriormente, conocimiento previo, se muestran en la lista siguiente:
* habitat (Animal;Habitat). Hay cuatro: ftierra; agua; aire; cuevag :
* tiene cubierta (Animal;Cubierta). Hay cuatro fpelo; plumas; escamas; ningunag :
* tiene patas (Animal; Patas) : Los números de patas son: f0; 2; 4g :
* toma leche (Animal) :
* homeotermico (Animal) :
* pone huevos (Animal) :
* tiene branquias (Animal) :
En este caso el algoritmo Progol, uno de los algoritmos ILP creado e implementado por
Stephen Muggleton [22], devuelve la siguiente hipótesis con cinco reglas:
1. clase (serpiente; reptil) :
2. clase (A; mamifero)   toma leche (A) :
3. clase (A; pez)   tiene branquias (A) :
4. clase (A; ave)   tiene cubierta (A; plumas) :
5. clase (A; reptil)   tiene cubierta (A; escamas) ; tiene patas (A; 4) :
Para crear una hipótesis como la anterior, los algoritmos ILP utilizan cláusulas cuyas literales
declaran un valor por cada atributo. Por ejemplo, en la cláusula cuatro de la hipótesis
anterior, el segundo argumento de la literal tiene cubierta, que llamaremos Cubierta, es un
atributo categórico con cuatro posibles valores: escamas, pelo, plumas y ninguna. Cada vez que
aparece la literal tiene cubierta en alguna de las cláusulas que forman las hipótesis del espacio
de búsqueda, el atributo Cubierta presenta sólo uno de sus valores. A este tipo de cláusulas las
llamaremos univalor. Es decir, si todos los argumentos de cada literal presente en el cuerpo de
una cláusula, declaran un sólo valor, llamaremos a esta cláusula univalor. Si algún argumento
presenta dos o más valores, entonces la llamaremos cláusula multivalores.
Notemos que la cláusula univalor cuatro de la hipótesis mostrada anteriormente, nos indica
que un animal A es un ave si su cubierta es plumas. Sin embargo es evidente que si las aves
tuvieran dos o más tipos de cubierta, como característica propia, la hipótesis final tendría que
declarar una cláusula por cada tipo de cubierta como en las reglas de la ecuación 1.1.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Las respuestas a las preguntas formuladas en la sección 1.1 forman la base de la presente
tesis. Nuestro objetivo principal es crear cláusulas multivalores que mejoren la expresividad
del lenguaje en ILP. Con ello se espera que los algoritmos ILP construyan hipótesis con menos
cláusulas y por lo tanto más fáciles de interpretar</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Como hemos visto en el capítulo 7, nuestra propuesta ha logrado reducir el número de reglas
con que representamos cada hipótesis. Además en la mayoría de los ejemplos analizados la
precisión se mejora.
Esta reducción de cláusulas es consecuencia de la partición que realizamos sobre el conjunto
de reglas sobre las cuales realizamos el aprendizaje multivalores. Esta partición es binaria debido
al uso del algoritmo k-means con k = 2, por lo que por cada predicado del conocimiento previo
analizado, sólo podemos obtener dos cláusulas que pueden formar parte de la hipótesis.
A diferencia del análisis multivalor, los algoritmos ILP sin este tipo de aprendizaje comparan
un mayor número de cláusulas, ya que utilizan uno por uno los valores, ya sean numéricos o
categóricos. Como consecuencia el conjunto de cláusulas crece y de este conjunto una, dos o
más pueden formar parte de la hipótesis final.
Podemos añadir que nuestra propuesta no mejora la eficacia con la que se buscan las hipótesis
en el espacio de estas, sino que amplía este conjunto con mejores hipótesis. Es decir, los algoritmos
ILP a los que se les permitió realizar aprendizaje multivalores siguen realizando la
búsqueda de las hipótesis de la misma manera, pero el espacio de búsqueda contiene además
hipótesis multivalores.
Vale la pena resumir los aspectos de los algoritmos de ILP sobre los cuales se trabajó en esta
tesis:
* Prejuicio de lenguaje: Se permitió la inclusión de nuevas literales para formar nuevas
reglas.
* Prejuicio de búsqueda: Las nuevas reglas (multivalores) hacen crecer el espacio de búsqueda.
En este aspecto debemos mencionar que a pesar de haber aumentado el espacio de búsqueda,
el tiempo de ejecución no aumentó significativamente por lo que es necesario estudiar este
fenómeno a futuro.
* Prejuicio declarativo: El usuario determina las cláusulas que son apropiadas para realizar
el aprendizaje multivalores.
Para hacer uso de las ventajas del aprendizaje multivalores, es necesario identificar dos tipos
de predicados.
* El primero de ellos es aquel predicado que tiene al menos una entrada cuyos valores son
categóricos. Ejemplo de estos predicados los vimos en los primeros ejemplos: número
de lados de figuras geométricas, número de patas en animales, tipo de pelaje en animales,
dirección hacia donde apuntan los triángulos, etc.
* El otro tipo de predicado que debemos identificar es aquel con al menos una entrada cuyos
valores son numéricos. La base de datos japanese credit tiene cinco predicados con este
tipo de entradas.
Además de identificar el tipo de atributo, también es necesario determinar si dicha entrada
puede generalizar mejor con un valor categórico, numérico o con una variable.
Si la entrada puede generalizar el conjunto de ejemplos positivos con una variable, entonces
no debe ser tomada para realizar aprendizaje multivalores. Por ejemplo si queremos generalizar
una relación familiar con una hipótesis, como la relación tío, entonces en el conocimiento previo
podemos tener la literal de la relación 8.1. Si realizamos aprendizaje multivalores con este
predicado obtendríamos una cláusula como la de la ecuación 8.2, ya que los valores que toma
la segunda entrada de la relación hermano de es tipo categórica, sin embargo no generaliza de
manera adecuada, ya que no todo el mundo es hermano de paco o de manuel.
hermano de (A;B) : (8.1)
tio de (A;B)   hermano de (A;C) ; member (C; [paco; manuel]) ; progenitor de (C;B) : (8.2)
Si la entrada de un predicado, ya sea numérico o categórico, puede generalizar a todo el
universo de ejemplos posible, entonces podemos utilizar aprendizaje multivalores. Por ejemplo,
si tenemos el predicado de la ecuación 8.3, podemos observar que su segunda entrada es
numérica. Por otro lado si dicho predicado es utilizado como conocimiento previo para generalizar
el concepto mayor de edad, entonces, es posible utilizar aprendizaje multivalores, ya que
el split encontrado es 18, y es aplicable a cualquier persona, es decir, a cualquier elemento del
universo de ejemplos que se puede construir. La hipótesis correspondiente la mostramos en la
ecuación 8.4 
mayor de edad (Persona)   edad (Persona;Edad) ;Edad 18: (8.4)
Resumiento y contestando a las preguntas planteadas en la sección 6.2.3 concluimos:
* El método efectivamente permite reducir el número de reglas que componen las hipótesis.
Esto sucede cuando los problemas son de clasificación, y los atributos elegidos para crear
las cláusulas multivalores son importantes para realizar la clasificación.
* No todos los atributos pueden elegirse para crear las cláusulas multivalores. Además de
ser categórico o numérico, un atributo puede elegirse únicamente si la información que
aporta dicho atributo puede generalizar a todo el universo de ejemplos posible.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD AUTONOMA METROPOLITANA
APRENDIZAJE MAQUINAL MULTIVALORES
PARA OBTENER EL GRADO DE 
MAESTRO EN CIENCIAS 
(CIENCIAS Y TECNOLOGIAS DE LA INFORMACION)
PRESENTA:
LIC. ORLANDO MUÑOZ TEXZOCOTETLA
ASESOR:
DR. RENE MAC KINNEY ROMERO
SINODALES:
PRESIDENTE: DR. MIGUEL ANGEL GUTIERREZ ANDRADE
SECRETARIO: DR. RENE MAC KINNEY ROMERO 
VOCAL: DR. EDUARDO MORALES MANZANARES
VOCAL: DR. JOHN HENRY GODDARD CLOSE
ABRIL 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Desarrollo de un software de monitoreo y seguridad para redes malladas inalámbricas 802.11s</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El número de ataques que reciben las corporaciones privadas o instituciones públicas por parte de grupos del crimen organizado para robar información, números de seguridad social e información personal, por ejemplo, sigue en constante aumento. Lo anterior incluye a todo tipo de redes incluyendo las que son recientes y en desarrollo como lo son las redes inalámbricas malladas. Se requiere entonces del desarrollo de un software que incremente la capacidad de seguridad y protección de este nuevo tipo de redes.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un sistema de seguridad implantable en redes malladas inalámbricas, que contribuya a incrementar el nivel de seguridad del canal entre los puntos de acceso y los clientes terminales y que provea de la información necesaria a un administrador de red de los eventos de la misma y con ello pueda conocer el desempeño de seguridad de la red y el registro de ataques maliciosos o no autorizados.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Como se comentó en los antecedentes, las pérdidas que representan un ataque exitoso de la red pueden ser cuantiosas en términos de tiempo, dinero y privacidad. Lo que incluso puede poner en peligro la integridad y seguridad física no solo de los equipos sino de las personas. Por otro lado, las redes inalámbricas de datos tipo Wi-Fi están evolucionando de manera sorprendente y pasando de áreas restringidas o “hot spots"  a zonas de cobertura amplia en la ciudad y cuyas aplicaciones pueden ser de misión crítica como en patrullas de policía, ambulancias u otros servicios municipales como bomberos, por citar algunas. Es muy importante un desarrollo de software que incremente los niveles de seguridad e idealmente no permitan el uso autorizado e intrusiones en este nuevo tipo de redes.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El desarrollo del proyecto incluye los siguientes pasos:
*	Revisión bibliográfica (Durante todos los puntos siguientes)
*	Análisis del problema
*	Generación de alternativas de solución
*	Definición de requerimientos y selección de la mejor alternativa
*	Selección de Hardware y Software a utilizar
*	Configuración del punto de acceso y del servidor de monitoreo(HW y SW)
*	Desarrollo de módulos de obtención de datos de red
*	Desarrollo del software de monitoreo del tráfico de la red
*	Análisis del tráfico de red
*	Programación de la interfaz gráfica
*	Desarrollo de módulos de control de usuarios y equipo
*	Generación de gráficas en base a datos obtenidos
*	Validación y pruebas de seguridad del SW desarrollado
*	Pruebas en campo del protocolo resultado
*	Elaboración de manuales del software
*	Redacción de la tesis</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Tras el desarrollo y culminación del presente proyecto se obtuvieron las siguientes conclusiones:
*	El monitoreo sobre un nodo de red mallada inalámbrica mediante el protocolo SNMP, resulta ser eficiente y rápido si la cantidad y el tipo de datos a censar son pocos y no todo los que proporciona por defecto, con lo que reduce en gran medida el tráfico de datos de la red.
*	El uso de SNMP en el desarrollo de un software de monitoreo, hace independiente el software de la arquitectura sobre la que se implemente siempre que los datos de acceso a un nodo se almacenen en forma de tabla en una base de datos.
*	La herramienta para generación de gráficos RRDTool ha demostrado ser muy eficiente tanto para el almacenamiento histórico de datos como para evitar la saturación y llenado de la base de datos.
*	El control de usuarios es un mecanismo de seguridad que resulta ser muy eficiente en redes privadas de empresas o instituciones y es conveniente que esté basado en las direcciones MAC de los equipos pues es un valor único para cada dispositivo de red.
*	El uso de una estructura de datos Tabla Hash basada en diccionario, en lugar de un arreglo sencillo (estructura usada comúnmente) para módulos de obtención de datos, reduce en gran medida el tiempo de procesamiento del software en que se implementen. Aplicado a algunos módulos de este desarrolló, demostró ser una solución eficiente tomando en cuenta que existen en una red N cantidad de puntos de acceso y N clientes conectados a cada uno.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN E
INNOVACIÓN TECNOLÓGICA
Desarrollo de un software de monitoreo y seguridad para redes malladas inalámbricas 802.11s
TESIS
para obtener el grado de
Maestro en Tecnología Avanzada
Presenta:
L.I. Israel Martínez Santiago
Director de tesis:
Dr. Fernando Martínez Piñón
Dic. 2010</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DESARROLLO DE UN AMPLIFICADOR DE BAJO NIVEL DE RUIDO PARA REDES INALÍMBRICAS W-LAN</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La señal de un receptor de radio, necesita tener un bajo ruido a la entrada, ancho de banda y ganancia aceptables. La amplificación en el receptor se distribuye entre las etapas del sistema, un amplificador ideal, incrementa la señal deseada sin agregar distorsión o ruido. Desafortunadamente, se sabe que los amplificadores agregan ruido y distorsión a la señal deseada, por lo que en la cadena de recepción, la primera etapa después de la antena y el filtro RF es casi siempre un LNA. Se sabe que cada etapa en el receptor agrega ruido a la señal, las señales que son muy débiles pueden perderse entre todo este ruido. La principal función del amplificador de bajo nivel de ruido es proveer una ganancia lo suficientemente alta para sobreponerse al ruido de las etapas subsecuentes (mezclador, preamplificador, etc.), mientras se agrega el menor ruido posible.
La figura de ruido (NF) en un receptor especifica cuanto ruido es agregado a la señal que pasa por éste. Para un circuito en cascada, el impacto de cada etapa en la figura de ruido total puede calcularse con la fórmula de Friis [45].
(1.1)
En donde Fi y Gi son el factor de ruido y la ganancia de la i-esima etapa respectivamente, esta expresión demuestra que la primera etapa desempeña un papel fundamental y se le debe brindar especial atención en el diseño de un sistema.
Agregando suficiente ganancia en la primera etapa G1 de un receptor, la ec. (1.1) se reduce a F1. La contribución de ruido total está determinada principalmente por el primer amplificador. Para tal propósito el amplificador de bajo ruido, por sus características y la naturaleza de su estructura, cumple con las expectativas deseadas (F1 pequeño y G1 lo más elevada posible).
Partiendo de la teoría clásica para amplificadores de bajo ruido y tomando en consideración las diferentes alternativas que surgen al realizar el análisis, se podrá efectuar el diseño. A lo largo de todo el desarrollo, el diseño al que se llega al final no es el único, sino únicamente la consecuencia de haber tomado múltiples caminos posibles. Lo que se ha perseguido en todo momento es la simplicidad a la hora de trabajar, tomando las opciones, igualmente válidas, que presentan menos complicaciones tanto para el estudio matemático como circuital.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los sistemas de comunicaciones actuales para redes inalámbricas se han expandido ampliamente alrededor del mundo, debido a que ofrecen las bondades de cualquier sistema de radiocomunicación. Estos sistemas muestran a la vez altas tasas de transmisión, por lo que su operación en altas frecuencias se hace necesaria. Lo anterior demanda una excelente sensibilidad en el equipo receptor, por lo que es necesario contar con un amplificador a la entrada que agregue al sistema el mínimo ruido posible con una ganancia aceptable. Este proyecto está enfocado al desarrollo de dicha etapa, se ha decidido trabajar en la frecuencia de 2.4 GHz ya que una gran cantidad de sistemas inalámbricos trabajan en esta banda, uno de estos sistemas es la tecnología WLAN.
El desarrollo de esta tesis es posible desde la investigación, la construcción y caracterización del amplificador debido a las herramientas de programación, simulación e instrumentos de medición con las que cuenta la Maestría en Ciencias en Ingeniería de Telecomunicaciones de la ESIME Zacatenco.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En esta tesis se logró llevar a cabo el diseño, construcción y caracterización de un amplificador de bajo nivel de ruido a 2.4 GHz obteniendo una figura de ruido de 4.5 dB con una ganancia »10 dB, este trabajo cumple con el objetivo que se planteó al momento del diseño para su desempeño dentro de un sistema completo de comunicaciones inalámbrico, por lo que los pasos aquí presentados cumplen con su objetivo fundamental. En la tabla 5.1, se pueden comparar nuestros resultados con los de otros fabricantes comerciales, se observa que otros competidores ofrecen una mayor ganancia en sus dispositivos, porque estamos hablando de que estos amplificadores contienen múltiples etapas de pre-amplificación, además de ofrecer una tecnología de construcción MMIC, logrando mejoras importantes en discretización. No es de interés primordial en este trabajo reducir costos de fabricación, sin embargo aquí presento un costo aproximado de mi prototipo, considerando únicamente los materiales utilizados.
Es importante aclarar que en el mercado de las telecomunicaciones estos dispositivos existen y con mejores propiedades que las alcanzadas en este estudio.
Sin embargo, se tenía la incertidumbre si era posible llevar a cabo este proyecto con las herramientas tecnológicas e infraestructura con las que se cuenta en la Maestría en Ingeniería de Telecomunicaciones. Y ahora que hemos acotado, sabemos con más certeza, cuáles son nuestros límites.
Ahora podemos afirmar que podemos mejorar aún más las características del diseño, reduciendo costos, tamaño y prestaciones tanto en frecuencia, ganancia y propiedades en ruido.
El estudio realizado a partir de la teoría clásica de amplificadores de bajo ruido y la construcción de uno de estos amplificadores lo confirman. Ya que se obtuvieron resultados de análisis de estructuras de microcinta coherentes con los resultados obtenidos mediante el análisis de modelos proporcionados por el programa de microondas ADS. Mostrando una elevada compatibilidad pero con menor flexibilidad a la hora de optimizar en lo que a la teoría clásica concierne. Por esta razón para tener mayor seguridad se utilizó el programa de simulación ADS, el cual trabaja a una velocidad aceptable y logra calcular estructuras geométricas complicadas en los rangos de frecuencia que se desean. Además en ADS, se puede llevar a cabo la simulación electromagnética, la cual resulta ser otra alternativa tecnológica para el análisis de gran precisión y diseño de complicados circuitos de microondas, antenas y circuitos digitales de alta velocidad.
Se incursionó en la utilización de líneas de microcinta para la construcción del diseño sobre substratos de FR4. Para un análisis más seguro se tendrá que hacer uso del estudio de la propagación electromagnética en las líneas de microcinta.
Se utilizó un transistor TBH de la tecnología SiGe:C, con excelentes propiedades en ganancia y frecuencia, con nivel de ruido aceptable, comparándolo con los amplificadores de bajo ruido existentes. El circuito ha sido dibujado de acuerdo a los resultados obtenidos en las simulaciones de ADS. Aparecen dificultades físicas a la hora de la implementación, que no habían sido consideradas hasta el momento, como son las soldaduras de los componentes de montaje superficial, las longitudes mínimas a mantener de las líneas de acceso a los puertos de entrada y salida.
Debido a esto se tuvieron que recortar las longitudes de los tramos de línea y por consecuencia la atenuación en las líneas de reduce.
La construcción del prototipo fue muy económica, considerando los materiales y procedimientos utilizados, lo que sugiere que puede reducirse el costo en una producción en serie del LNA diseñado.
Este amplificador es apto para sistemas de comunicación inalámbricas tales como teléfonos celulares, radios móviles y teléfonos inalámbricos para uso doméstico. La utilización del circuito es muy variada dentro de las comunicaciones ya que representa una de las partes más importantes del bloque completo de recepción o transmisión. Es indispensable la cuidadosa selección de los componentes y materiales correctos a utilizar para la óptima realización del diseño, de lo contrario se reflejaría en la respuesta.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
SEPI E S I M E -ZACATENCO Maestría en ciencias en Ingeniería de Telecomunicaciones
DESARROLLO DE UN AMPLIFICADOR DE BAJO NIVEL DE RUIDO PARA REDES INALÍMBRICAS W-LAN
TESIS
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN CIENCIAS EN INGENIERÍA DE TELECOMUNICACIONES.
ING. GRETHELL GEORGINA PÉREZ SÍNCHEZ
DIRECTORES DE TESIS:
DR.LUIS ANUEL RODRIGUEZ MENDEZ.
M. EN C. SERGIO VIDAL BELTRÍN
MÉXICO D.F. ENERO 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“GESTIÓN DE CONFLICTOS ENTRE POLÍTICAS EN UN SISTEMA DE APROVISIONAMIENTO Y ACTIVACIÓN DE SERVICIOS EN REDES DE BANDA ANCHA"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Cuando la gestión de redes basada en políticas (PBNM) entró por primera vez al mundo de
las redes, las expectativas, así como la publicidad exagerada, abrumaban [25]. Las
compañías rápidamente se lanzaron a construir los productos que utilizaron mecanismos
para la gestión de redes usando políticas, conjuntos de estándares para su impacto en
Internet en general y en la gestión de redes en particular, y originaron conferencias
dedicadas a los sistemas PBNM. Las personas que realizaron los primeros sistemas PBNM
demostraron que fue difícil. No había un método simple y fácil para solucionar problemas
complejos de gestión de redes. Después mas personas utilizaron aplicaciones mas
sofisticadas, usadas en la red. Estas aplicaciones con frecuencia tenían requisitos que
entraban en conflicto en la red. Esto condujo a que las políticas se consideraran sinónimo
de aplicación de calidad de servicio (QoS).
Desafortunadamente, esto significó que las capacidades de PBNM no fueran desarrolladas
completamente y condujo a que se considerara a PBNM como un proyecto de investigación
académico y no pudo ser utilizado en la gestión de redes en el mundo real. El verdadero
potencial de PBNM no es gestionar QoS, no es un camino para determinar quién puede
utilizar tal función. Más bien, PBNM es un camino para definir necesidades de negocios y
asegurar que la red proporcione los servicios que sus clientes requieren [25].
Esta es de hecho una declaración de gran alcance. La reciente atención a la computación
bajo demanda, la cual adopta la teoría simple pero urgente de que un negocio bajo demanda
es el que puede responder inmediatamente a las necesidades de mercado y a las condiciones
en tiempo real, no podría ser completada sin las técnicas de gestión basadas en políticas.
En los sistemas PBNM pueden existir conflictos entre las políticas y es precisamente esto
que se pretende resolver en el sistema propuesto en esta tesis, de entre la clasificación de conflictos se detectarán los de modalidad y el análisis de esto es a nivel de elemento de red,es decir las políticas contendrán información que respecta a este nivel, por ejemplo,direcciones IP fuente y destino, número de puerto fuente y destino y otros. Se tomará como base una propuesta que utiliza tablas en las cuales se almacenan las políticas de tal forma que se separa la parte de la condición y la parte de la acción para identificar de una manera sencilla cuando exista un conflicto de modalidad utilizando espacios topológicos, es decir cuando ocurra un traslape entre targets (elemento de red sobre el que se realiza una acción),subjects (elemento o entidad que realiza la acción) y/o actions (acciones).</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>La detección y la resolución de conflictos en redes es un tópico complejo. El ambiente
descrito en la arquitectura de IETF [27] no explica la detección de conflictos y mucho
menos la resolución. El TMF (TeleManagement Forum) y otros foros presentan una
arquitectura para sistemas de gestión basada en políticas incluyendo la detección y
resolución de conflictos entre políticas, y explícitamente identifica cinco niveles de
detección y resolución de conflictos [25]: globalmente entre servidores de políticas,
globalmente a través de los diferentes componentes de un servidor de políticas en
particular, localmente para todos los puntos de decisión de políticas en un dominio de un
servidor de políticas, local a un punto de decisión de políticas particular con un dominio
especifico de un servidor de políticas y local por dispositivo en un dominio (por ejemplo un punto de aplicación de políticas).
El objetivo de este trabajo de tesis es detectar los conflictos de modalidad que se presentan entre políticas en un sistema de aprovisionamiento utilizando un sistema de gestión basado en políticas a nivel de elemento. Para llegar a cumplir con este objetivo, se desarrollan algoritmos que detectan los conflictos de modalidad que se generan entre políticas tomando como base el modelo conceptual del grupo de trabajo de IETF y espacios topológicos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo de tesis se han presentado los fundamentos de los formalismos empleados y
las características que se identificaron como relevantes para la detección de conflictos de
modalidad en un sistema de gestión basado en políticas, tomando como base la teoría de
espacios topológicos.
El conjunto de algoritmos desarrollados contribuyen a la solución del problema abierto de
detección de conflictos entre políticas. Estos algoritmos constituyen una herramienta para
facilitar la gestión de conflictos al administrador de red, automatizando los procesos de
detección e identificación de conflictos, sugiriendo una posible solución en los casos donde
el conflicto detectado se produzca entre políticas con atributos distintos.
Para la planeación de la solución propuesta se realizó una extensión al modelo básico PCIM
para adecuarlo a los requerimientos necesarios de la especificación de políticas a nivel de
elemento de red.
Las restricciones para la implementación de estos algoritmos están relacionadas
principalmente con la complejidad computacional requerida. Los algoritmos propuestos
representan una solución viable dado que son de orden lineal. El esquema de
almacenamiento de las políticas en el repositorio afecta el tiempo de ejecución de los
algoritmos; el ordenar las políticas de acuerdo a su signo puede disminuir el número de
comparaciones necesarias para la obtención de un resultado válido.
Se verificó la confiabilidad de los algoritmos realizando pruebas en tres escenarios de
aprovisionamiento representativos a los conflictos de modalidad sobre una muestra mínima
requerida, el tamaño de esta muestra se calculó mediante un análisis estadístico en base a
una prueba de confianza.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN Y DESARROLLO
DE TECNOLOGÍA DIGITAL
MAESTRÍA EN CIENCIAS CON
ESPECIALIDAD EN SISTEMAS DIGITALES
“GESTIÓN DE CONFLICTOS ENTRE POLÍTICAS EN UN SISTEMA DE
APROVISIONAMIENTO Y ACTIVACIÓN DE SERVICIOS EN REDES DE
BANDA ANCHA" 
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS
P R E S E N T A:
DORA LUZ FLORES GUTIÉRREZ
BAJO LA DIRECCIÓN DE:
M.C. JAVIER RUBIO LOYOLA
Junio de 2005 Tijuana, B.C., México</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Posicionamiento Basado en Redes Inalámbricas para la Determinación de Funcionalidades en un GIS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar e implantar un sistema de cómputo sensible al contexto geográfico, utilizando las señales de radio frecuencia que emiten los puntos de acceso de redes inalámbricas, con el propósito de identificar la posición geográfica de dispositivos móviles sobre un área definida.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Fundamentalmente, la hipótesis de este trabajo es la siguiente:
Es posible realizar el posicionamiento local de dispositivos de cómputo móviles utilizando
mediciones de la intensidad en las señales de radio frecuencia que provienen de diversos puntos de acceso en una red inalámbrica. Adicionalmente, es posible realizar el posicionamiento local utilizando conceptos, los cuales describen una área en particular. Las memorias asociativas, nos permiten construir un modelo capaz de identificar un cierto número de ubicaciones definidas.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Esta tesis se enfoco en la investigación de los sistemas sensibles al contexto, en la cuales se emplearon diversas ramas de las Ciencias de la Computación. Este trabajo tuvo el tiempo aproximado de 1 año de desarrollo, presentándose una serie de obstáculos los cuales
mencionamos a continuación: El primer obstáculo fue el de obtener la lectura de la tarjeta de WiFi, la cual nos proporcionara las intensidades de todos los puntos de acceso que se encontraban a nuestro alrededor, ya que esta lectura es de suma importancia debido a que es el vector de entrada para nuestro algoritmo de aprendizaje. Cabe mencionar que se tuvieron algunos aciertos, como digamos elegir las memorias asociativas como el algoritmo para el cálculo de la ubicación geográfica, debido a su sencillez y rapidez de aprender nos facilito mucho el desarrollo.
Tomando en cuenta las observaciones anteriores, podemos comentar que se desarrolló una
Biblioteca para el uso de lecturas del protocolo WiFi, adicionalmente se utilizó una API desarrollada por Intel denominada Placelab. La cual nos permite de manera sencilla la lectura no únicamente de la tarjeta de WiFi, sino de otro tipo de dispositivos o periféricos como pueden ser los GPS o tarjetas de Bluetooth, así como tiene una variación esta API que puede ejecutarse en un PDA como el iPac, con esto nos da la extensibilidad de migrar el sistema a dispositivos móviles del tipo handheld.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
LABORATORIO DE GEOPROCESAMIENTO
Posicionamiento Basado en Redes Inalámbricas para la
Determinación de Funcionalidades en un GIS
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN
PRESENTA:
NOE MAXIHELLEEN MONTIEL HERNÍNDEZ
DIRECTOR DE TESIS: M. en C. Marco Antonio Moreno Ibarra
MÉXICO, D.F. JUNIO 2006</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>MODELADO DE SOFTWARE CON LOGICA DIFUSA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Para llevar a cabo estas tareas la Dirección de Investigación tales como se encarga de
llevar el registro, dar seguimiento y asignar recursos a los más de 1200 proyectos de
investigación (científicos y tecnológicos, propuestas de estudio y educativos de manera
individual y en programa) que se realizan en las diferentes escuelas de nivel medio
superior, superior y centros de investigación del IPN, se apoya en un sistema de
información que se encuentra en funcionamiento desde el 2003 denominado: Sistema
de Administración de Programas y Proyectos de Investigación (SAPPI): Un sistema de
información que está basado en tecnologías Web y en una base de datos centralizada
Uno de los criterios que son considerados para asignar recursos a los proyectos de
investigación es la calificación de productividad obtenida en la ficha de productividad
del investigador. La etapa de registro de la ficha de productividad que contempla
SAPPI no aplico a partir del 2005 debido a que la base del conocimiento de SAPPI es
estática y en consecuencia no pudo adecuarse a los cambios y nuevos requerimientos,
lo cual implico:
- La recepción en papel de fichas de productividad.
- Evaluación de alrededor de mil quinientas fichas de productividad
- Captura de los diez criterios de evaluación de las fichas de productividad,
para un polinomio que dicta una evaluación.
Una de los requisitos para que un sistema de información se pueda adaptar a los
cambios es que su base de conocimiento sea dinámica dentro de intervalos difusos.
Con lo anterior se llego a la conclusión de que era necesario realizar el rediseño del
sistema que permitiera el registro de la ficha de productividad en línea y la idea de
aprovechar esta información registrada para poder diseñar un sistema de validación y
calificación de la ficha de productividad. Con lo cual llegamos a la siguiente pregunta
 ¿Cómo realizar un sistema que responda a las variables lingüísticas y a la base del
conocimiento generada por los evaluadores y asignadotes de recursos de la SIP que
han dado resultados en la mayoría de las veces de manera confiable sin perder las
propiedades del SAPPI? Considerando para ello el requerimiento de menos tiempo de
evaluación y asignación, usando a las tecnologías de información, permitiendo:
*	Registrar en línea la ficha de productividad.
*	El personal de la SIP podrá revisar las fichas de productividad de cualquier año
con solo capturar la clave del proyecto o seleccionando la escuela.
*	El analista validará en línea contra documentos aprobatorios lo que registro el
investigador y registrara sus observaciones.
*	Una vez validada la ficha por los analistas de la DI será calificada en
automático, evitando cualquier error y eliminado el tiempo que se requiere para
llevar a cabo la calificación manual y captura de puntajes por rubro de las más
de 1500 fichas q son registradas año con año que se lleva más de un mes.
*	Mantener un histórico de la productividad del investigador.
*	Permitir la generación de los siguientes reportes: tesitas, patentes, libros,
etc.,realizados en el instituto, investigadores con becas COFFA y EDI entre
otras.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un modelo de software utilizando lógica difusa de acuerdo a la base de
conocimiento en base a la información de la Secretaría de Investigación y Posgrado
(SIP) para implantar un modelo de evaluación a través de variables lingüísticas para
medir la productividad de los investigadores del instituto y generar una base de
conocimiento dinámica para la asignación presupuestal a los proyectos de investigación.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible aplicar los principios de la lógica difusa para automatizar los procesos de
evaluación de la ficha de productividad y asignación presupuestal, que se realizan en la
Dirección de Investigación del IPN debido a que son procesos con métricas difusas</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los sistemas basados en lógica difusa imitan la forma en que toman decisiones los
humanos, con la ventaja de ser operativamente más rápidos y ajustables dinámicamente
.Estos sistemas son generalmente robustos y tolerantes a imprecisiones y ruidos en los
datos de entrada [1]. Es por ello que aplicaremos la lógica difusa en el diseño un modelo
de software que en su base fue desarrollado a través de las experiencias de los analistas,
autoridades y demás personal de la SIP, requiriendo que el sistema de información
tenga un dinamismo de acuerdo a la evolución de los parámetros que permiten observar
una productividad y asignación presupuestal.
Rediseñar la etapa del registro de la ficha y agregar la validación y evaluación en
automático de la ficha de productividad permite validar al proyecto y determinar el
presupuesto que le será asignado: SAPPI en su estructura, automatiza procesos,
conserva la integridad de la información y evita su duplicidad. Requiriendo integrar a su
diseño una base de datos dinámica que permita al SAPPI adaptarse de acuerdo a la base
de conocimientos evolutiva.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para llevar a cabo el desarrollo de este sistema se realizaron los siguientes pasos:
1. Desarrollo de la base del conocimiento de la ficha de productividad, en base a la
experiencia del personal de la SIP.
2. Determinar las variables lingüísticas y las propiedades del polinomio difuso
considerando las propiedades marcovianas.
3. Construir los módulos de software para evaluar la productividad de acuerdo a la
base del conocimiento y variables lingüísticas previamente establecidas.
4. Desarrollar la base del conocimiento dinámica para la asignación presupuestal,
considerado a las variables lingüísticas.
5. Determinar las variables lingüísticas y las propiedades del polinomio difuso sin
perder sus cualidades de estabilidad.
6. Construir los módulos de software para la asignación presupuestal a proyectos
de investigación para proporcionar una interfaz amigable entre el sistema y los
evaluadores así como a los asignadotes de presupuesto.
7. Enlazar los módulos de software a través del sistema de información SAPPI.
8. Poner en operación el modelo de software y mejorar su operación a través de las
evaluaciones que realizan los usuarios del mismo, en su desempeño,
considerando a funciones de membresía resultantes de las propiedades
estadísticas de las evaluaciones.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Actualmente existen en el mercado numerosos sistemas como SAPPI sin embargo
ninguno se ajusta a las necesidades específicas de la Secretaría de Investigación y
Posgrado. Lo interesante de realizar un proyecto de este tipo implica dar solución a un
problema real, asimilación de tecnología, desarrollo de software a la medida y lo
principal servir de modelo para poder identificar características similares de este tipo de sistemas y desarrollar un generador de sistemas de información que con pocas
adaptaciones se convierta en software a la medida aprovechando los principio de la
lógica difusa para modelar matemáticamente el sistema, identificar su base de
conocimiento, variables lingüísticas y funciones de membresía .
En este capitulo se llevo a cabo el planteamiento del problema en el cual se cuestiona si
existe una relación entre la lógica difusa y el diseño de sistemas de información. Se hizo
una descripción de la metodología para llevar a cabo esta relación, estableciendo que el
principal objetivo de la tesis es desarrollar un modelo de software utilizando lógica
difusa de acuerdo a la base de conocimiento en base a la información de la Secretaría de
Investigación y Posgrado (SIP) para implantar un modelo de evaluación a través de
variables lingüísticas para medir la productividad de los investigadores del instituto y
generar una base de conocimiento dinámica para la asignación presupuestal a los
proyectos de investigación de acuerdo a la hipótesis de que es posible aplicar los
principios de la lógica difusa para automatizar los procesos de evaluación de la ficha de
productividad y asignación presupuestal, que se realizan en la Dirección de
Investigación del IPN debido a que son procesos con métricas difusas.
En este capítulo se desarrollo de la base del conocimiento de la ficha de productividad
concentrada en la Tabla 1, se determino que las variables ligüísticas están definidas por
los rubros que se evalúan en la ficha de productividad del investigador (Tabla 1) como
son: número de patentes, libros, tesitas, etc. Así mismo las propiedades del polinomio
difuso. Se diseñaron y construyeron los de los módulos de software para evaluar la
productividad misma que sirvió para el desarrollo de la base del conocimiento dinámica
para la asignación presupuestal. Se determinaron las variables lingüísticas y las propiedades del polinomio difuso para el módulo de asignación presupuestal y se diseño
y construyó el módulo de asignación para llevar a cabo esta asignación.
En este capítulo se discutió y se llevo a cabo el análisis de los resultados con las
preguntas:  ¿por qué no utilizar un software de uso específico?,  ¿qué desventajas tiene
este desarrollo?, ¿cuáles son sus principales avances tecnológicos respecto a su
operación tradicional?,  ¿qué tipo de tecnología de software se uso y por qué esa
tecnología?, ¿qué criterios hicieron que este proyecto fuera viable para la SIP y otros?
,  ¿qué tipo de plataforma es necesaria para usarlo?, ¿qué tipo de experiencia técnica para
usarlo? . Finalmente se platearon cuales fueron los resultados básicos tangibles.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Ciencia Aplicada y Tecnología Avanzada
CICATA
Unidad Legaría
MODELADO DE SOFTWARE CON LOGICA DIFUSA
TESIS
Que para obtener el titulo de:
Maestro en Tecnología Avanzada
Presenta:
Ing. Consuelo Varinia García Mendoza
Director de Tesis:
César Saúl Guzmán Rentería
Codirector de Tesis:
Dr. José de Jesús Medel Juárez
México Distrito Federal, Noviembre de 2006</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema Administrador de números de guía de paquetería en forma distribuida</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El sistema pretende tener toda la información de los números de guía
disponible al momento en que son capturados en las diferentes plazas para su
consulta. Para hacer esto, utiliza el concepto de bases de datos distribuidas.
También permite a los encargados de los diferentes departamentos de
sistemas hacer actualizaciones a la información si es necesario.
Características generales del sistema
- El prototipo trabaja bajo el concepto de bases de datos distribuidas.
- Existen al menos tres programas en cada plaza: un servidor, un programa
por medio del cual se van a capturar los números de guía de los paquetes,
y uno o varios clientes los cuales van a acceder a la información
almacenada y administrada por los servidores.
- La comunicación entre los clientes y los servidores es a través de sockets.
- Se trabaja bajo la plataforma de Windows, en cualquiera de sus versiones.
- Se utiliza Visual Basic 6 como herramienta de desarrollo, utilizando el
control winsock para la comunicación.
- Se manejan dos tipos de usuarios, el administrador o supervisor, el cual
puede hacer actualizaciones sobre la información ya almacenada, y los
usuarios que solo pueden hacer consultas o rastreos de los números de
guía.
- Se maneja lo que se llama bloqueos de registros, esto evita la
inconsistencia de la información.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un prototipo capaz de automatizar eficientemente los números de guía de la paquetería que se envía por medio de la empresa Estafeta, manejando el concepto de bases de datos distribuidas.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La necesidad de tener la información disponible para el personal operativo de
la empresa Estafeta al momento en que se genera es de suma importancia
para una compañía con enlaces mundiales, particularmente si la información se
genera desde diferentes puntos del país.
Este prototipo evitará que se realicen procesos innecesarios (separación de
información, envió por parte de los encargados a la central, formateo de
información, etc.), lo cual reducirá tiempos en la disponibilidad de la misma
para una mejor atención hacia los clientes de la empresa, así como también
para una mejor administración por parte de los coordinadores de los diferentes departamentos de sistemas en las diferentes regiones del país.
También permitirá que cada plaza sea la encargada de administrar la información que se genera, evitando que se centralice el control y administración de los procesos para el manejo de la misma.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>I. Investigación bibliográfica y recopilación de información
disponible en Internet.
II. Descripción de la tecnología de software para el desarrollo de
sistemas de bases de datos distribuidas.
III. Seleccionar las herramientas de desarrollo de software para la
elaboración de los programas que conformaran el sistema de
bases de datos distribuidas.
IV. Descripción de los procesos actuales que realiza la empresa
Estafeta, y en base a ellos, diseñar la base de datos, identificar
los procesos que se modifican, seleccionar un método para el
diseño del sistema de base de datos distribuidas.
V. Elaboración del prototipo.
VI. Conclusión y presentación de resultados.
VII. Elaboración de material para presentación de los resultados de la
investigación.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El tema principal de esta tesis son los sistemas de bases de datos
distribuidas, y es satisfactorio mencionar que se lograron implementar los
siguientes objetivos de estos sistemas; tener autonomía local, no depender de un
sitio central, ofrecer operación continua, transparencia de localización,
transparencia de fragmentación, transparencia de replica, transparencia de red;
así como también se logra hacer un trabajo en un tema de interés personal.
Los objetivos de la tesis se cumplieron, ya que se logro desarrollar el
prototipo cumpliendo con los conceptos de los sistemas de bases de datos
distribuidas y se lograría mejorar los procesos de la empresa, dando así una mejor
respuesta en la actualización de la información y lo más importante, se lograría
tener la información disponible al momento en que está se genera, sin importar el
lugar en el que se esta generando.
Para que se llegara a instalar este sistema se tendría que proponer,
primero, al gerente general de la empresa en Colima para que éste lo propusiera
con sus superiores y después vendría una evaluación por parte del grupo de
desarrollo de la empresa, lo cual en mi particular punto de vista sería muy difícil
que se aprobara, por eso es que esta tesis propone el sistema como un prototipo,
aunque sería importante que lo tomaran en cuenta ya que les ofrece muchas
facilidades y muchas ventajas en comparación son su sistema actual.
Existe un pequeño inconveniente con este prototipo, trabaja bajo el sistema
operativo Windows, la empresa cuenta en cada plaza con servidores UNIX lo cual
tendría como consecuencia que se cambiara de sistema operativo, servidores
Windows NT y terminales con Windows 95, 98, 2000 o ME, aunque no sería
necesaria la adquisición de un servidor muy sofisticado ya que el equipo que
tienen, cuenta con la capacidad suficiente para que soporte este sistema.
Con este prototipo, se mejoraría el sistema actual y los procesos que están
implementados en la empresa Estafeta para el control de los números de guía de
paquetes. Así también se lograría optimizar su infraestructura ya establecida (red
privada), y no sería necesario implementar algún otro sistema de comunicación
entre las distintas plazas de la empresa.
La implementación de éste prototipo permitiría a los administradores y
personas encargadas de atender a los clientes, saber en cualquier momento y en
cualquier plaza, donde está un paquete en especial, lo cual dará lugar a que la
información que soliciten los clientes sea la más precisa referente a la ubicación
de su paquete.
El prototipo reduciría tiempos de espera en la actualización de la
información, los encargados de los departamentos de sistemas de las diferentes
plazas ya no tendrían que esperar a que la central les envié la información
actualizada de la ubicación de los paquetes.
El desarrollo de este prototipo dio lugar a que se aplicara un gran número
de los temas vistos en clase (redes, comunicaciones, programación, etc),
abarcando en un gran porcentaje el propósito de la maestría.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Sistema Administrador de
números de guía de paquetería
en forma distribuida
Ing. Mario Eduardo Figueroa Verduzco</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>MEDICIÓN DISTRIBUIDA DE DESCARGAS PARCIALES EN REDES SUBTERRÍNEAS UTILIZANDO TÉCNICAS DE BANDA ULTRA ANCHA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La utilización de redes subterráneas de energía se considera más segura y confiable que la distribución aérea, pero conlleva diversos riesgos para la continuidad del servicio eléctrico. Los problemas comienzan desde el tendido de la red, puesto que los conductores aislados se ven expuestos a esfuerzos mecánicos tales como la separación de sus capas concéntricas debido a operaciones deficientes que favorecen excesos en su radio de curvatura. Estos esfuerzos ocasionarán defectos que serán los precursores de las DP's.
Una vez instalados los conductores, las condiciones de operación de la red son la principal causa de problemas relacionados con su aislamiento, además del envejecimiento natural que sufren en función del tiempo. Dos son los principales factores que repercuten en el deterioro del aislamiento de los cables de energía: el primer factor y tal vez el más adverso, es la temperatura de operación. Una alta temperatura de operación ocasionará cambios en la composición físico-química del material aislante, ya sea tipo sólido (polietileno de cadena cruzada XLPE, o goma de etileno propileno EPR) o bien, en aceite papel (aislado en aceite, cubierto con plomo PILC), debilitándolo, favoreciendo la aparición de defectos. Existen condiciones o eventos que incrementan la temperatura de operación de un cable. Entre otros se pueden mencionar la profundidad de instalación, el número de conductores instalados por trinchera, el tipo de recubrimiento mecánico, sobrecargas de la red, etc.
El segundo factor es el tipo de carga que alimentan los conductores. Debido a los avances de la tecnología de semiconductores de potencia y al auge de la computación, se ha incrementado la utilización de cargas no lineales, con el consecuente efecto térmico de las corrientes armónicas producidas por estas cargas. El efecto de las armónicas sobre los conductores es principalmente de carácter térmico, pero también, debido a la conmutación de dispositivos semiconductores de potencia, los armónicos de alta frecuencia pueden ser vistos como una fuente de transitorios de alta energía recurrentes, capaces de perjudicar a largo plazo las características del aislamiento.
Como se mencionó, uno de los métodos más eficientes para la detección de problemas en el aislamiento de cables de energía es la detección del nivel de descargas parciales. Esta técnica ha sido utilizada en fábrica como prueba de aceptación desde principios de los 60's [32], pero debido a los inconvenientes que presenta su aplicación en sitio, su interpretación y a su susceptibilidad a interferencias, no ha tenido gran aceptación por parte de los grupos de diagnóstico. Los grandes inconvenientes de la aplicación de la técnica en sitio son el requerimiento de alta potencia en el circuito de medición para excitar la gran capacitancia de los cables, la elevada interferencia electromagnética presente en la mayoría de las instalaciones industriales y la atenuación de las señales de descargas parciales durante su camino al equipo detector, esto debido a que la medición se realiza en un extremo del circuito y si ocurre una DP cerca del extremo opuesto, deberá viajar toda la longitud del circuito del cable probado. Sin embargo, desde el punto de vista del propietario de la instalación, la principal desventaja radica en el hecho de que es necesario retirar de operación los circuitos bajo estudio y prepararlos para la realización de la medición. Este inconveniente redunda entonces en pérdidas económicas y problemas de logística, dado que el propietario de la instalación estima un costo mayor en la aplicación de la prueba que en la sustitución de algún circuito bajo sospecha de deterioro. Bajo estos prejuicios, en muchas ocasiones no se toma en cuenta el riesgo de daños en equipos principales, originados por simples fallas en los cables, considerados como equipo auxiliar barato.
Como resultado del análisis de estos inconvenientes, numerosas investigaciones han sido realizadas para determinar nuevas estrategias de detección de descargas parciales. Entre las principales está el empleo de nuevas bandas de frecuencia de medición, lo cual favorece la discriminación de la interferencia electromagnética y en teoría brinda información adicional sobre la física de la descarga. Desgraciadamente la utilización de esta tecnología está limitada por la longitud de onda de las descargas, que minimiza la región efectiva de análisis y por el grado de especialización requerido en control de circuitos de alta frecuencia por parte del ejecutante de la prueba.
Debido a lo anterior, la problemática asociada a la detección de descargas parciales en UWB, radica en el hecho de qué, como para toda metodología de desarrollo reciente, aún se requieren establecer compromisos, criterios de diagnóstico y mejorar los equipos de detección. Estos aspectos de aplicación de la técnica están relacionados con las características constructivas y de instalación de cables subterráneos de energía, cuya longitud promedio es extremadamente grande en comparación con la longitud de onda de las DP's, favoreciendo la atenuación y los problemas de distorsión de las señales debido a reflexiones.
Un problema adicional que debe minimizarse o solucionarse es el impacto del ejecutante de la medición en los resultados obtenidos, por lo que es conveniente disminuir la complejidad de sensores y mejorar o automatizar los sistemas de detección. El obtener información suficiente o mejorar sustancialmente las condiciones de ejecución de la medición en línea de descargas parciales en UWB en cables de energía, contribuirá con la disminución de accidentes personales y con la mejora del proceso de producción, minimizando los problemas socio-económicos asociados.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo de este trabajo de tesis es el aplicar técnicas distribuidas de diagnóstico, basadas en la detección de descargas parciales en banda ultra ancha, que permitan predecir o determinar la existencia de fallas incipientes en el aislamiento de cables de distribución subterráneos de energía.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En estadísticas recientes se observa que las fallas en los equipos auxiliares de Subestaciones eléctricas (transformadores de instrumento, boquillas con graduación capacitiva, dispositivos de potencial, terminales y empalmes de cables de alta tensión, entre otros), son una de las principales causas de falla catastrófica y accidentes en el sector eléctrico [33].
Este problema se agrava debido a tres factores principales:
1. Alta probabilidad de ocurrencia de fallas debido a la gran población de estos equipos en subestaciones eléctricas e instalaciones industriales.
2. Características operacionales adversas en la mayoría de las instalaciones (humedad, contaminación, etc.)
3. Escaso diagnóstico y mantenimiento de estos equipos debido a que presentan un costo comparativamente bajo con respecto al equipo principal.
Un cambio en el paradigma que se tiene a la fecha con respecto al último punto en subestaciones e instalaciones industriales en operación, sería una alternativa eficaz y económica de reducir accidentes y fallas en estos equipos, pero para la ejecución de este cambio se requieren superar las siguientes limitaciones:
1. Las metodologías de diagnóstico a utilizarse deberán ser atractivas en costo.
2. Las técnicas a implantarse deberán ser realizadas en línea, debido a que, por factores económicos y de disponibilidad del servicio, no es factible retirar de operación estos equipos para su evaluación.
3. Las técnicas empleadas tendrán que ser robustas, confiables e inmunes a la alta interferencia electromagnética prevaleciente en las S.E.'s.
Se ha demostrado que una de las técnicas más sensibles para la cuantificación del grado de afectación de un aislamiento frente a un defecto o falla incipiente es la medición de descargas parciales.
Desgraciadamente, además de que para su ejecución, se requiere que el objeto bajo estudio esté fuera de línea, las técnicas tradicionales de medición de descargas parciales utilizan por norma frecuencias de medición de hasta 600 kHz, por lo que pierden información sobre el fenómeno de la descarga y además, son susceptibles a interferencias electromagnéticas que impiden su detección cuando los equipos se encuentran en ambientes de campo.
De acuerdo con lo anterior, la implantación de un esquema de medición que permita evaluar en línea los diversos equipos de alta tensión, sin interferencia y con una mejor interpretación de la física de la descarga, sería de gran utilidad para el sector eléctrico nacional.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los resultados presentados en el Capítulo 5 muestran claramente que la técnica de medición distribuida de DP's en UWB es suficientemente sensible y selectiva, ya que con base en los resultados obtenidos, permite la obtención de diagnósticos precisos y la clasificación gruesa de objetos bajo prueba. Como aportación de este trabajo de tesis se propuso y utilizó una escala con tres niveles. Esta escala, sin referencia publicada en la literatura especializada, comprende los niveles alto, medio y bajo de acuerdo con el grado de atención requerida por cada cable y accesorio medido. Para lograr esta clasificación, básicamente se aprovecharon dos tendencias en los resultados: detección de fallas puntuales y detección de deterioro generalizado en la longitud de los objetos bajo prueba [72,73]. A continuación se presentan las conclusiones individuales de los resultados obtenidos al analizar ambas tendencias:
6.1 Fallas incipientes puntuales
Se encontró que la gran incidencia de fallas incipientes detectadas en los cables subterráneos evaluados está asociada a puntos débiles ocasionados por trabajos de reparación inadecuados, por fugas del compuesto aislante o por condiciones de operación e instalación deficientes que se presentan en la longitud del circuito subterráneo analizado. La técnica propuesta permitió la detección y reparación de al menos 95 problemas puntuales con valores considerados como altos.
El impacto económico y logístico de la técnica en UWB es excelente, ya que reduce el tiempo de reparación al evitar sustituciones completas de tramos de cable aprovechando que para este tipo de imperfecciones sólo se requieren trabajos menores de mantenimiento para reacondicionar el cable afectado. Una vez efectuado este trabajo, el cable podrá permanecer operando dentro de sus especificaciones nominales, independientemente del tiempo que tenga en servicio.
6.2 Fallas incipientes por deterioro uniforme en la longitud total del cable o en segmentos localizados
Durante el análisis de los resultados obtenidos con la técnica en UWB, se observaron cables en los que el deterioro es uniforme en toda su longitud o en un segmento considerable de la misma. Se encontraron 4 muestras con deterioro en toda la longitud del cable, aún en segmentos donde no hay ningún accesorio instalado, mientras que para el caso de deterioro en una parte considerable de la longitud se encontraron 2 muestras.
En todos los puntos de medición evaluados en estos cables, se presentaron valores muy elevados de descargas parciales. Correlacionando las mediciones de carga con los valores de descargas parciales obtenidos, estos cables presentan un deterioro generalizado originado principalmente por sobrecargas sostenidas y por condiciones adversas de instalación tales como excesivo número de circuitos instalados en la misma trinchera e inundación de la misma. En conjunto con estos resultados, el tiempo que tienen en operación estos cables, más de 25 años y el bajo grado de polimerización que presenta su papel aislante, son factores que corroboran los resultados obtenidos con las técnicas de UWB.
En casos como estos, se recomendó efectuar la medición fuera de línea de estos cables con la finalidad de verificar el resultado obtenido y planear la sustitución completa e inmediata del cable bajo estudio. Como verificación de las cualidades de la técnica en UWB utilizada, uno de los alimentadores cuya libranza estaba programada para verificar el estado del cable por métodos convencionales, falló catastróficamente antes de que fuera posible su salida de operación.
6.3 Conclusiones acerca del comparativo entre las técnicas de medición de DP´s por método normalizado y las técnicas de medición distribuida de DP's en UWB
En relación con los resultados comparativos entre la técnicas en UWB y los métodos normalizados según IEC 60270, como se muestra en el capítulo 5, se presentan las siguientes conclusiones que justifican el empleo de la técnica propuesta sobre las metodologías convencionales.
La interferencia electromagnética presente en los pozos de visita de los alimentadores, no permite evaluar el estado de los cables utilizando técnicas normalizadas. Estas técnicas operan en un ancho de banda que coincide con el espectro comercial de radiofrecuencia y con señales electromagnéticas diversas.
Las técnicas convencionales requieren que el cable bajo estudio salga de operación para colocar el capacitor de acoplamiento y la impedancia de medición necesarios para la medición normalizada y para el procedimiento de calibración. En instalaciones críticas esto no es posible por aspectos económicos o logísticos, por lo que las operaciones de diagnóstico y mantenimiento pueden sufrir postergaciones peligrosas para la integridad del cable.
En el caso de utilizar el esquema normalizado de medición, una vez conectados los accesorios de prueba, es necesario energizar el espécimen de prueba al menos a su tensión nominal. Para tal fin puede optarse por energizarlo con el sistema del cual forma parte o bien, energizarlo mediante una fuente alterna.
En el caso de que se energice el objeto bajo prueba utilizando el propio sistema, las mediciones se verán afectadas por el ruido e interferencia conducida proveniente del sistema sin que se tenga capacidad de discriminar la fuente de posibles descargas.
En el caso de que se opte por utilizar una fuente alterna de energía, en su elección se establece un compromiso entre la portabilidad necesaria en campo y las características de potencia requeridas por la naturaleza capacitiva de los cables. En ambos casos, portabilidad y potencia requerida, se debe considerar los aspectos económicos y de operación del arreglo completo de prueba.
Los métodos convencionales o normalizados de medición de descargas parciales no brindan información completa de la física del fenómeno de la descarga debido a que están limitados en banda por el empleo de impedancias de medición con características RC o RLC y por dispositivos internos que reducen la frecuencia de los pulsos de descargas medidos.
Las técnicas convencionales, al operar a frecuencias de radio, presentan un ancho de banda suficiente para analizar grandes distancias de cable, pero no brindan información alguna de la ubicación de la posible fuente de descargas.
Las técnicas convencionales de medición están fundamentadas por el concepto de carga aparente, por lo que su enfoque puede considerarse como circuital, aún y cuando presentan condiciones poissonianas debido a la carga espacial.
A diferencia del enfoque circuital de las técnicas convencionales, las técnicas en UWB, están íntimamente ligadas a la propagación de ondas electromagnéticas transversales y en como influyen éstas en la carga inducida en el sensor de campo cercano. En otras palabras, su enfoque es puramente electromagnético.
La técnica en UWB propuesta en este trabajo, no presentan ninguno de los problemas mencionados anteriormente para las técnicas convencionales. Sin embargo presenta 3 características que deben tomarse en cuenta para su adecuada aplicación:
"La primera de estas características es la susceptibilidad a distorsión de las señales por blindajes inadecuados de la instrumentación y por reflexiones de la señal debidas a la alta frecuencia de operación de la técnica y a acoplamientos de impedancia deficientes.
"La segunda característica a tomarse en cuenta es que la ganancia obtenida en la medición depende de la posición relativa del sensor de campo cercano con respecto a la fuente de descargas. El aumento en magnitud de la señal es proporcional al cubo de la distancia cuando el sensor se ubica en una vecindad de la fuente de DP's.
"Por último, la tercer característica de un sistema de medición en UWB es la relativamente corta longitud de análisis. Esta característica se basa en la consideración de que el cable bajo estudio, recorrido de manera axial, es un filtro pasabajas natural, que evita la propagación de señales de alta frecuencia debido a la inductancia de sus componentes. Por lo anterior, cualquier señal de alta frecuencia (>100MHz) experimentará un acelerado decaimiento en función de la distancia.
Cualquiera de las características mencionadas anteriormente podría ser considerada como un inconveniente de la técnica de UWB. Sin embargo, un blindaje adecuado, el empleo de geometrías coaxiales en la transmisión de las señales y la terminación de cada componente del sistema con una impedancia adecuada, puede ser la solución a los problemas inherentes a la primer característica.
Los inconvenientes debidos a la segunda característica son fáciles de eliminar si se diseña una geometría del sensor de campo cercano, que permita mantener condiciones de repetitividad en su colocación. Esto se logra fácilmente adecuando el diámetro interno del sensor tipo Rogowski al diámetro del cable y embebiendo sus componentes en algún material sólido para evitar variaciones en su posición.
En cuanto a la tercer característica, nada se puede hacer para incrementar la longitud de análisis sin afectar el ancho de banda y por lo consiguiente la relación señal a ruido. Sin embargo, esta característica no debe considerarse como una desventaja, sino más bien como una gran ventaja para la localización física de las fuentes de descargas y para la discriminación de ruido.
El lograr configurar los componentes del circuito de prueba para satisfacer los requerimientos mencionados en los párrafos anteriores y evitar las desventajas de las técnicas en UWB no es tarea fácil, por lo que la configuración sugerida en este trabajo de tesis muestra su valor facilitando las mediciones a usuarios no experimentados.
En cuanto al aspecto económico, las técnicas en UWB son mucho más atractivas que las técnicas convencionales: Las mediciones se hacen con los cables en operación, no se requieren equipos de acoplamiento en alta tensión y los sensores de campo cercano utilizados son muy económicos y relativamente fáciles de construir. Incluso, si se tiene la suficiente experiencia en el manejo de señales de alta frecuencia, se puede utilizar un osciloscopio o analizador de espectro de un ancho de banda adecuado como detector, con la ventaja de que el costo de estos equipos es de una fracción del costo de un equipo especializado para la medición de descargas parciales.
Un factor importante a tomarse en cuenta y que se verificó durante el desarrollo del Capítulo 4 de este documento, es que en los sensores de campo cercano, el compromiso entre ancho de banda y ganancia es inevitable. A mayor ganancia, menor ancho de banda y a mayor ancho de banda, menor ganancia.
Para el sistema de medición configurado en este trabajo, el ancho de banda y la ganancia proporcionada por el sensor de campo cercano son adecuados para la detección de descargas en cables con aislamientos PILC y poliméricos.
A pesar de que ambos métodos, el de UWB y el normalizado, cuantifican una descarga de manera indirecta, el método en UWB es relativamente más preciso en el aspecto de que las corrientes generadas por inducción de carga son digitalizadas directamente y la carga asociada al fenómeno es la simple integral de esta corriente afectada por el valor de la impedancia de acoplamiento.
Para finalizar con las conclusiones, como aportación principal de este trabajo está el hecho de utilizar simultáneamente las bondades de ambas técnicas: esto es, se efectúan las mediciones en frecuencias muy elevadas, lo que permite incrementar la relación señal a ruido y se despliegan las señales así obtenidas y acondicionadas en diagramas  y N- tradicionales. Estos diagramas son familiares a los usuarios de las técnicas convencionales, y dado el gran acervo existente y la experiencia obtenida en el análisis de este tipo de mapas, es posible caracterizar diferentes tipos de imperfecciones.
Entre los resultados que excedieron las expectativas, se tiene la sensibilidad lograda, la cual siempre será referida a una calibración indirecta adecuada para cada muestra. Asimismo, una vez configurado el sistema de medición, se verificó la facilidad de su uso en cables de 23kV, correlacionando sus resultados con datos de carga y condiciones de operación. Esta correlación permitió el diagnóstico integral del aislamiento de los cables instalados y en operación normal.
En adición, durante el desarrollo del trabajo se obtuvieron resultados interesantes que permitieron identificar los factores que dan origen a la problemática de fallas al menos en el sector eléctrico mexicano. A continuación se listan estos factores:
6.4 Factores térmicos que afectan la operación normal de los cables subterráneos
En virtud de los resultados obtenidos, se verificó la hipótesis de envejecimiento acelerado de los cables motivado por sobrecargas que prevalecen aún fuera de las horas de demanda máxima. Los alimentadores que experimentan la mayor actividad de descargas parciales utilizando la técnica propuesta son aquellos que están o han sido sometidos a excesos de su ampacidad nominal durante un gran periodo de tiempo, sin importar la edad que tengan en operación.
El análisis de la distribución térmica dentro de la geometría del cable se efectuó mediante técnicas numéricas, utilizando como fronteras y condiciones de solución la generación interna de calor por efecto Joule y la temperatura medida mediante termografía en el exterior del cable, tal y como se presenta en el Anexo A.
La razón por la cual se utilizó este esquema de cálculo y no una medición directa de la temperatura en el aislamiento, radica en el hecho de que no es posible utilizar técnicas invasivas de medición durante la operación de los cables evaluados. La inclusión de termopares, RTD's u otros dispositivos similares sólo es posible en mediciones de laboratorio sin potencial aplicado, ya que afectan la distribución del potencial en el cable.
El utilizar un modelo numérico para la estimación de la temperatura interna de los cables, entraña un cierto grado de incertidumbre, ya que las constantes térmicas de conducción de los materiales del cable son obtenidos de tablas y pueden variar con respecto a los valores reales. Asimismo, el coeficiente convectivo del aire en el interior de las trincheras, sólo es estimado, ya que un valor preciso requiere contar con propiedades del aire, cuya obtención está fuera del alcance de esta tesis.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE INGENIERÍA
MECÍNICA Y ELÉCTRICA
SECCIÓN DE ESTUDIOS
DE POSGRADO E INVESTIGACIÓN
MEDICIÓN DISTRIBUIDA DE DESCARGAS PARCIALES EN REDES SUBTERRÍNEAS UTILIZANDO TÉCNICAS DE BANDA ULTRA ANCHA
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN CIENCIAS
CON ESPECIALIDAD EN INGENIERÍA ELÉCTRICA
PRESENTA
CARLOS GUSTAVO AZCÍRRAGA RAMOS
MÉXICO D. F. 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>software para comprensión y descompresión de vídeo utilizando la norma MPEG</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar y crear un programa computacional que permita comprimir y descomprimir
vídeo digital en el estándar MPEG.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Dada la gran cantidad de espacio que ocupa un archivo de video digital y debido a la
creciente demanda de transmisión de archivos de video y al gran ancho de banda
que se necesita para ello, es necesario reducir el tamaño de estos archivos sin que
se pierda la calidad del video y se obtenga un buen grado de reducción de los datos,
por lo que su compresión resulta indispensable.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Durante el desarrollo de este trabajo de tesis se logró abundar en el conocimiento y
estudio de las técnicas de compresión de imágenes que son base para los algoritmos
de compresión de video más importantes de la actualidad, a tal grado que se diseñó
y creó un programa computacional que permite utilizar dichas técnicas de forma
conjunta bajo el estándar de compresión de video MPEG-1.
Como se planteó en el objetivo de este trabajo, se logró que el software desarrollado
permita comprimir y descomprimir secuencias de video bajo el estándar MPEG-1
siendo 100 % compatible con las aplicaciones comerciales existentes, por lo que las
salidas de los procesos de codificación y decodificación pueden ser leídas por
cualquier programa que soporte archivos .mpg y .avi.
Otro objetivo importante que se buscaba en este trabajo era el de sentar las bases
para el estudio y desarrollo de técnicas de compresión de imágenes y video en
México, objetivo que se alcanzó al desarrollar una aplicación que no utiliza
controladores ni rutinas de código que sean propiedad de terceros ya que se
desarrolló el 100% del código fuente del que se compone este software, lo cual nos
brinda la libertad de poder manipular el código a conveniencia, con lo que es posible
utilizar rutinas y/o módulos del programa para desarrollar otros programas que
permitan implementar otros estándares de compresión de video tales como el
MPEG-2 o el MPEG-4 o bien nos da la posibilidad de poder experimentar de forma
simple con el uso de algún tipo alternativo de algoritmo de transformación diferente al
de la transformada discreta coseno (DCT) y así poder innovar en el desarrollo de
técnicas de transformación.
En lo que respecta al desarrollo del software se pudo comprobar que el uso del
estándar VESA es de suma importancia para mejorar el rendimiento del manejo de
gráficos en la PC, de tal manera que no sólo se acelera el despliegue de gráficos
sino que se evita la necesidad de tener que crear código para diferentes tarjetas
gráficas. Durante el desarrollo de este software también se tuvo la necesidad de
realizar la programación de la memoria expandida, para que al igual que en el caso del manejo de gráficos, la aplicación final tenga un mejor rendimiento. El desarrollo
de las rutinas de código que permiten realizar las tareas antes mencionadas
implicaron un esfuerzo adicional al desarrollo del codificador-decodificador de video;
sin embargo, el tener una aplicación final que trabaje de forma aceptable bajo una
plataforma de bajos recursos como lo es MSDOS es debido al esfuerzo hecho para
realizar la programación de los procesos en cuestión, por lo que el trabajo
desarrollado en dichos esfuerzos puede ser considerado una excelente inversión.
En lo que respecta al codificador, como se muestra en el capitulo de resultados, se
logró crear un programa que es lo suficientemente flexible como para alcanzar
diferentes niveles de compresión con diferentes niveles de calidad de imágenes, por
lo que dicho software puede competir en ese rubro tan importante con programas
existentes; sin embargo, tanto el codificador como el decodificador están limitados en
respuesta en el tiempo como para trabajar en tiempo real. Este bajo rendimiento con
respecto al tiempo es debido a la gran cantidad de operaciones requeridas para
realizar la operaciones correspondientes a la DCT y a la conversión de formatos de
color.
La diferencia más significativa entre los programas comerciales que se encontraron
con respecto al que se desarrolló, es que estos tienen soporte para una gran
cantidad de formatos adicionales al .avi y .mpg. Por todo lo expuesto, las principales
ventajas del trabajo realizado con respecto de aplicaciones comerciales son la
posibilidad de poder mejorar, aumentar o bien reutilizar dicha aplicación; aunado al
no menos importante hecho de que se está creando tecnología propia en nuestro
país.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
COORDINACIÓN GENERAL DE POSGRADO E INVESTIGACIÓN
ACTA DE REVISIÓN DE TESIS
EN LA CIUDAD DE MÉXICO D.F. SIENDO LAS 18:00 HORAS DEL DÍA 6 DEL MES DE JUNIO DEL 2005 SE REUNIERON 
LOS MIEMBROS DE PROFESORES DE ESTUDIOS DE POSGRADO E INVESTIGACIÓN DE LA E.S.I.M.E. PARA EXAMINAR 
LA TESIS DE GRADO TITULADA:
software para comprensión y descompresión de vídeo utilizando la norma MPEG
PRESENTADA POR EL ALUMNO:
SÍNCHEZ ALMARAZ ARTURO
CON REGISTRO A020501
ASPIRANTE EL GRADO DE MAESTRO EN CIENCIAS
DESPUES DE INTERCAMBIAR OPCIONES LOS MIEMBROS DE LA COMISIÓN MANIFESTARON SU APROBACIÓN 
DE LA TESIS, EN VIRTUD DE QUE SATISFACE LOS REQUISITOS  SEÑALADOS POR LAS DISPOSICIONES 
REGLAMENTARIAS VIGENTES.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>TÉCNICAS DE CODIFICACIÓN PARA EL CONTROL DE ERROR EN RADIO DEFINIDO POR SOFTWARE</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Realizar una revisión de la evolución de las comunicaciones móviles e inalámbricas,
haciendo énfasis en las técnicas de procesamiento digital de señales que son utilizadas.
Realizar una revisión de la tecnología del radio definido por software, su
arquitectura, ventajas y los elementos necesarios para implementarlo.
Diseñar módulos de software con diferentes técnicas de control de error e
implementarlas en un dispositivo lógico programable (FPGA).</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Tradicionalmente los equipos receptores y transmisores de radiocomunicaciones son
equipos constituidos por multitud de componentes electrónicos, los cuales forman circuitos
sintonizadores, etapas de frecuencia intermedia, detectores, amplificadores de baja
frecuencia, etc., es decir, están constituidos por “hardware" . Posteriormente, en los años
1980's y 1990's se introdujeron microprocesadores en estos equipos para el control de
funciones internas y para añadir nuevas prestaciones, así como también la posibilidad de
controlar los equipos de radio desde una computadora, añadiendo al equipo de radio puertos
de comunicación o interfaces para la conexión a la computadora.
En estos casos, y usando el software adecuado, es posible controlar desde la
computadora numerosas funciones del equipo de radio, igual o mejor que desde los
controles del propio equipo. También en la década de los 1990's comenzó la introducción
en los modernos equipos de radio de los chips DSP o “Procesadores Digitales de Señal" , los
cuales permiten mediante técnicas digitales realizar filtros de paso de banda y de supresión de ruidos, entre otras posibilidades, muy eficaces, mejor que los realizados
tradicionalmente con circuitos analógicos.
En cualquier caso, siempre se trata de equipos realizados enteramente con
componentes electrónicos, o sea, en términos informáticos se definirían como “radios
hardware" . Durante las últimas cuatro décadas los sistemas de radio tanto para servicio
militar como comercial han ido sufriendo una gran transición de sistemas de comunicación
análoga a digital. Esta transición ha traído como consecuencia la evolución de todo el
conjunto de formas de onda en comunicaciones digitales y equipos de transmisión de datos
asociados a ellos que soportan esta creciente necesidad.
Durante el mismo periodo de tiempo, las plataformas de radio han soportado esas
formas de onda las cuales también han ido evolucionando. Hace no más de 15 años la gran
mayoría de las plataformas de radio usaban hardware y software dedicado, en la última
década ha venido creciendo un cambio hacia un nuevo paradigma muy fuerte que son los
radios definidos por software, en los que la parte hardware es mínima, y la mayor parte de
las funciones que definen un equipo de radio se definen por software en una computadora o
algún otro dispositivo lógico programable y sus correspondientes implementaciones en
forma de onda.
Este nuevo paradigma se conoce como Radio Definido por Software (SDR,
“software defined by radio" ) el cual propone una plataforma de hardware común que pueda
ser reconfigurada en software.
Comparando los radios análogos basados en hardware de los años pasados, el
moderno concepto del radio definido por software provee un grado incomparable de
compatibilidad y versatilidad, permitiendo formas de onda complicadas que eran
inconcebibles de hacer en el pasado y actualmente son implementadas con facilidad. Por lo
tanto el radio definido por software es un concepto que está revolucionando la tecnología
de las comunicaciones rumbo a la integración de múltiples arquitecturas de redes de
telecomunicaciones inalámbricas y alámbricas.
El SDR supone realizar la mayor parte de las funciones de un equipo de radio,
incluso las más importantes, mediante el software implementado en algún dispositivo. Los
radios definidos por software pudieran reconfigurarse a sí mismos automáticamente para
reconocer y comunicarse con otros. Esto podría imponer el orden en el actual caos de los
estándares competitivos inalámbricos (CDMA, GSM, TDMA, etc.), esto transformaría las
redes rígidas en sistemas abiertos. La principal ganancia: desempeño del sistema mejorado,
costos de servicios más bajos, roaming transparente. Con reprogramación instantánea, se
podría cargar un mismo aparato para múltiples usos.
El SDR frente a los estándares de comunicación actuales tiene una gran ventaja, ya
que todos ellos convergerían en un solo dispositivo capaz de reconfigurarse
automáticamente dependiendo las necesidades del usuario, esto nos daría como resultado
un sistema abierto de comunicaciones, en donde no importaría el estándar que se utilice
sino simplemente el “software"  que se implementaría para cambiar el funcionamiento de
nuestro dispositivo SDR.
Sin embargo, la reconfiguración del hardware a partir del software también requiere
un comportamiento similar del software destinado a regular el intercambio de información.
En otras palabras, es importante también tener algoritmos encargados en cada fase del
proceso de comunicación, este trabajo se centra en el desarrollo de módulos de software
que implementan técnicas de control de error.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo se logró la implementación de diferentes técnicas de control de error,
utilizando códigos de bloque lineal, como son los códigos Hamming, códigos cíclicos y
códigos BCH.
Los códigos más adecuados para su implementación en hardware son los códigos
cíclicos, debido a esto la programación en el lenguaje de descripción de hardware resultó
ser más sencilla que la de los códigos Hamming o BCH, aunque en este trabajo el último
de los códigos que se implementaron fue un código BCH (15-11) en el que se logró un
programa general, ya que con solo unos cambios en parámetros de la función del
codificador se pueden implementar códigos de diferente longitud, por lo que no solo
llegamos al diseño de diferentes módulos de software, sino que llegamos a un solo módulo
de software general el cual puede modificarse para la implementación de códigos de
diferente longitud.
En la parte de la decodificación de los códigos, la forma más viable de implementar
un decodificador es utilizando la decodificación Meggit, ya que emplea registros de
corrimiento como los códigos cíclicos, una vez implementado el módulo encargado de
detectar el síndrome, sólo resta implementar el circuito del decodificador Meggit para
poder corregir el error, pero este depende de la longitud del código, ya que el tamaño del
registro de corrimiento cambiará así como el circuito de corrección de la palabra código.
Los programas que resultaron de este trabajo para códigos cíclicos y BCH utilizan
decodificador Meggit, estos programas están construidos de tal forma que sólo con hacer el
cambio en el tamaño del arreglo que representa el registro de corrimiento y de cambiar las
condiciones necesarias para hacer la corrección de error se tiene un módulo que nos
representa otro decodificador diferente, por lo que puede servir de base para la construcción de más módulos de software que implementen distintos decodificadores.
Los resultados de este trabajo son importantes en el estudio del procesamiento de
señales en el radio definido por software, ya que como se mencionó en el Capítulo 2 no
todo el proceso de comunicación puede implementarse en software, pero una de las etapas
importantes en este proceso es el de detección y control de error, ya que los codificadores y decodificadores nos dan un porcentaje mayor de eficiencia en la transmisión del mensaje.
Lo más importante de implementar un diseño en un dispositivo lógico programable
es que utilizando el mismo hardware, en este caso la misma tarjeta FPGA, se pueden tener
implementadas diferentes técnicas de control de error, así con un mismo dispositivo
tenemos una gran variedad de codificadores y decodificadores. Esta es la idea principal del radio definido por software.
De este modo se trata de hacer conciencia en la necesidad de un radio definido por
software, ya que cada día que pasa la tecnología en comunicaciones móviles e inalámbricas
avanza y no siempre se utilizarán los mismos dispositivos de transmisión y recepción, por lo cual para evitar estos cambios el tener un dispositivo reconfigurable sería de gran
eficiencia y utilidad.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
SECRETARIA DE INVESTIGACIÓN Y POSGRADO
ACTA DE REVISIÓN DE TESIS
EN LA CIUDAD DE MÉXICO D. F. SIENDO LAS 9:00 HORAS DEL DIA 16 DEL MES DE OCTUBRE DEL 2007
SE  REUNIERON LOS MIEMBROS DE LA COMISIÓN REVISORA DE TESIS DESIGNADA POR EL COLEGIO DE PROFESORES DE ESTUDIOS DE POSGRADO E INVESTIGACIÓN DE LA E.S.I.M.E. PARA EXAMINAR LA TESIS DE GRADO TITULADA
TÉCNICAS DE CODIFICACIÓN PARA EL CONTROL DE ERROR EN RADIO DEFINIDO POR SOFTWARE
PRESENTADA POR EL ALUMNO
VÍZQUEZ ÍLVAREZ DAVID
CON REGISTRO B021446
ASPIRANTE AL GRADO DE 
MAESTRO EN CIENCIAS
DESPUÉS DE INTERCAMBIAR OPINIONES LOS MIEMBROS DE LA COMISIÓN MANIFESTARON SU APROBACIÓN DE LA
TESIS, EN VIRTUD DE QUE SATISFACE LOS REQUISITOS SEÑALADOS POR LAS DISPOSICIONES REGLAMENTARIAS VIGENTES.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema de Acceso Seguro a Recursos de Información para Redes Inalámbricas 802.11</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar un sistema seguro que permita a un usuario móvil ingresar a contenidos de
información a través de un punto de acceso de una red inalámbrica, sin hacer pública
su información personal de acceso.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El funcionamiento de las redes inalámbricas se basa en la emisión y recepción de datos
a través de frecuencias de radio. Una de las principales razones para instalarlas es
poder compartir servicios y recursos entre los dispositivos móviles, sin embargo, la
conexión a Internet es la funcionalidad que resulta más apreciada por los usuarios.
Las principales ventajas que presentan las redes de este tipo son su libertad de
movimiento, la sencillez en la reubicación de equipos y la rapidez de instalación. Pero el
punto más débil de este tipo de redes está asociado a la seguridad que ofrecen.
Solamente se necesita colocar un receptor dentro del área de cobertura de una red
inalámbrica para detectarla, una vez detectada es susceptible a sufrir ataques, por
ejemplo, la intercepción de la señal transmitida con el propósito de recopilar información
confidencial, como contraseñas, datos de autenticación o cualquier dato sensible.
Existe una gran variedad de técnicas de ataques a las redes inalámbricas, los cuales
son cada vez más populares debido a su rápida difusión.
Para proteger el acceso a redes inalámbricas se utiliza métodos de autentificación y
cifrado, los cuales impide el acceso a personas no autorizadas y que algún intruso que
intercepte una comunicación pueda descifrarla. Uno de los inconvenientes de la
utilización de estos mecanismos es la necesidad de configurar o instalar algún software
específico en el equipo móvil del usuario.
Muchos de los puntos de acceso utilizados en la construcción de redes inalámbricas no
traen ningún tipo de seguridad activa, para que el montaje de la red sea simple.
Además, los administradores de redes inalámbricas no activan los mecanismos de
seguridad disponibles y cuando lo hacen utilizan mecanismos vulnerables como el
algoritmo WEP.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Las redes inalámbricas comenzaron como una ventaja competitiva en el ámbito
empresarial, actualmente están cada vez más presentes en nuestra vida diaria, ya sea
en empresas, domicilios particulares, bibliotecas, restaurantes o aeropuertos. A pesar
de su rápido desarrollo, las redes inalámbricas actualmente no son totalmente seguras,
por tanto no conviene tratarlas como tales y más aún cuando hay información
confidencial involucrada.
Existen muchos trabajos de investigación para aumentar la seguridad de las redes
inalámbricas, uno de los más recientes es el estándar 802.11i que está siendo
implementado por los fabricantes en sus nuevos productos. Pero, mientras este
estándar sea aceptado por los consumidores o exista la necesidad de mantener el
hardware de redes inalámbricas ya existente se deberán considerar algunas
precauciones de seguridad extras al momento de diseñar una red inalámbrica.
En este trabajo se diseñó un sistema de acceso seguro a recursos de información
para redes inalámbricas 802.11, el cual permite a un usuario móvil ingresar a
contenidos de información confidencial a través de un punto de acceso, sin hacer
pública su información personal. Para lograr los objetivos planteados se realizaron los
siguientes pasos:
"Se evaluó la seguridad en redes inalámbricas 802.11, donde se mostró las
debilidades de los algoritmos WEP y WAP. Además, se describió el
funcionamiento del nuevo estándar 802.11i haciendo énfasis en los métodos
que utiliza para proveer seguridad, entre ellos los estándares 802.1x y AES.
"Se realizó un análisis del sistema propuesto, logrando una independencia entre
sus tres subsistemas:
o El Portal Cautivo ofrece al usuario un mecanismo de acceso seguro,
centralizado y completamente transparente.
o El Servidor de Autenticación cumple con la estructura del estándar
802.1x y proteger la información personal del usuario al cifrarla.
o El Punto de Acceso protege los contenidos de información para que
únicamente los usuarios previamente autenticados puedan acceder, para
lograrlo utiliza claves temporales.
"Se utilizó los diagramas UML para desarrollar el diseño del sistema, estos
diagramas proporcionan una sencilla interpretación del funcionamiento del
sistema propuesto.
"Se construyó un prototipo basado en el análisis y el diseño del sistema
propuesto, para lo cual se utilizó únicamente software libre.
Luego de implementar el prototipo se determinó que el sistema de acceso seguro a
recursos de información para redes inalámbricas 802.11, ofrece las siguientes ventajas:
* Estandarización. En el análisis y el diseño del sistema se consideraron las
recomendaciones del estándar IEEE 802.11i y los artículos que pusieron en
evidencia la vulnerabilidad de sus predecesores.
* Independencia entre subsistemas. La arquitectura del sistema está diseñada
para garantizar la independencia entre los tres subsistemas. Este aspecto ayuda
a realizar modificaciones y actualizaciones a cada subsistema sin afectar el
funcionamiento de los demás.
* Procedimientos transparentes. Para proporcionar accesos seguros, el sistema
propuesto utiliza procedimientos completamente transparentes a los usuarios,
de manera que no se requiere de una capacitación adicional para la utilización
del sistema.
* Flexibilidad. El administrador de una red inalámbrica que implemente el sistema
propuesto tendrá la opción de emplear esquemas de autenticación propios,
según sus políticas de seguridad.
* Compatibilidad. El sistema propuesto ofrece compatibilidad con cualquier otro
procedimiento adicional de seguridad, es decir, que puede trabajar en forma
paralela con otros sistemas de control de acceso.
* Utilización de Software Libre. Esta ventaja ofrece a los administradores de la red
inalámbrica la posibilidad de realizar cambios según las políticas de seguridad o
los requerimientos de la red. Se deberá tener conocimientos de programación y
del funcionamiento del sistema para realizar cambios en el código fuente del
software utilizado.
* Fácil implementación. Los requisitos necesarios para implementar el sistema de
acceso seguro a recursos de información para redes inalámbricas son mínimos,
solo se necesita el hardware para la puesta en marcha de una red inalámbrica y
la obtención del Software Libre.
Después de autenticar a un usuario el sistema propuesto determinará sus permisos
utilizando su base de datos. Cada usuario tendrá un perfil único, es decir, podrá
acceder a determinados contenidos de información restringida dentro de la red local, a
Internet o a ambos, dependiendo de su perfil de usuario.
El usuario móvil simplemente debe autenticarse con el sistema para acceder a la
información confidencial, la única herramienta que utilizará para lograrlo será su
navegador web preferido. En ningún momento el usuario necesita instalar programas
específicos, realizar configuraciones complicadas o informar la identidad de su equipo
móvil, mediante la dirección IP o la MAC de su tarjeta de red.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE INGENIERÍA
MECÍNICA Y ELÉCTRICA
SECCIÓN DE ESTUDIOS DE POSGRADO E INVESTIGACIÓN
Sistema de Acceso Seguro
a Recursos de Información
para Redes Inalámbricas 802.11
TESIS QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS EN INGENIERÍA DE
TELECOMUNICACIONES
Presenta:
Ing. Jose Luis Mejia Nogales
Director de Tesis:
M. en C. Sergio Vidal Beltrán
México D. F. 2006</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Identificación de la Estabilidad a Pequeños Disturbios de la Máquina Síncrona Bus-Infinito por Redes Neuronales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La estabilidad ante disturbios pequeños depende del punto de operación [7]; si
se tiene un conjunto de mediciones, que corresponden a una condición de operación
del sistema, se puede determinar si el sistema es estable o inestable en esa
condición; de tal manera que si se tienen varios conjuntos de mediciones, para cada conjunto corresponde una condición estable o inestable; es decir, cada conjunto de
mediciones pertenece a una de dos condiciones.
Visto de esa manera, la determinación de la estabilidad es una clasificación de
objetos, cada objeto es un punto de operación, y cada uno se puede clasificar como
“estable"  o “inestable" ; así la identificación de la estabilidad puede verse como un
problema de reconocimiento de patrones.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Proponer un método sencillo para la evaluación de la estabilidad ante
disturbios pequeños en el SEP, tomando como base el reconocimiento de
patrones con redes neuronales (RN).
* Evaluar el desempeño del sistema de identificación de la estabilidad con
RN para diferentes condiciones.
* Investigar los efectos de la incertumbre en el desempeño del sistema de
identificación.
* Investigar y proponer la mejor estructura de la RN conforme a su
desemepeño.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El proceso de linealización y cálculo de eigenvalores no es fácil, además,
tratándose de aplicaciones de tiempo real, el tiempo que toma realizarlo puede ser
insuficiente para responder adecuadamente ante la presencia de la inestabilidad en
el sistema, mas aún porque las condiciones del sistema cambian constantemente.
Las técnicas de control clásico son dependientes completamente de un buen
modelado, el cual no siempre es fácil de desarrollar, aunque para sistemas eléctricos
de potencia existen modelos prácticos que han sido desarrollados y utilizados
satisfactoriamente, por lo que no es realmente necesario investigar en nuevos
modelos. Los resultados de la aplicación de estas técnicas tradicionales de control se
ven afectados por otro tipo de causas, como pueden ser malas interpretaciones de
datos, errores en mediciones, estimaciones equivocadas, etc., las cuales son parte
de la realidad; por lo que es necesario lidiar con estas incertidumbres y entender sus
efectos para que los resultados no se vean afectados de manera importante.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La metodología tradicional, para determinar la estabilidad del SEP ante disturbios
pequeños, ha sido el análisis basado en los valores propios del modelo linealizado
del sistema, pero el cálculo de los eigenvalores de cualquier sistema real no es una
tarea fácil; aunque no es problema para cualquier computadora contemporánea, el
tiempo que se lleva puede ser esencial para realizar otras acciones de control, sobre
todo en sistemas dinámicos como el SEP.
La determinación de la estabilidad ante disturbios pequeños del SEP se ha
planteado como una identificación de patrones, donde los objetos a clasificar son las
condiciones de operación del sistema, y las clases a las cuales puede pertenecer
cada objeto son: “estable"  e “inestable" . Visto de esta manera, se permite proponer
soluciones alternativas al análisis de eigenvalores, ampliando los recursos
disponibles el análisis de la estabilidad.
La identificación de los patrones de estabilidad se realizó con Redes
Neuronales, y se logró obtener un sistema de identificación más rápido que el
análisis de eigenvalores y con buena precisión.
La importancia que tiene la selección de la topología adecuada para la RN es
innegable; un problema en el diseño de sistemas con RN es la escasez de
metodologías eficientes para encontrar la mejor estructura, en la bibliografía,
solamente se proporcionan ideas generales, que en el mejor de los casos solo sirven
para encontrar un punto de partida en la búsqueda de la solución satisfactoria.
Actualmente existe una gran variedad de software disponible para el
entrenamiento de las RN, pero no todos comparten las mismas características y
están diseñados para trabajar con problemas muy específicos, algunos se han
desarrollado para dar solución a problemas financieros, otros a para realizar
diagnósticos médicos, etc., por eso mismo no son muy útiles para estudiar el
problema de este trabajo. MATLAB ® en cambio, con su grupo de herramientas para
el manejo de redes neuronales (Neural Networks toolbox), provee una buena opción
para el desarrollo y análisis del entrenamiento de las RN en general.
Es de esperarse que si la RN se entrena con datos obtenidos de simulación
ésta se adapte bien, sin embrago, para una aplicación real se debe considerar que
en muchos casos no es posible disponer de buenos modelos, y los datos para
entrenar la RN no provienen de simulaciones, sino de mediciones directas o estimaciones, que son susceptibles a errores o ruido, esta incertidumbre puede
causar, como se comprobó en este trabajo, que los resultados cambien
completamente.
La adición de ruido en los datos provoca que los patrones se dispersen, y
entonces los límites que separan las dos clases se distorsionan y se vuelven más
complejos; es difícil realmente visualizar estas distorsiones, ya que se trabaja en un
espacio de 4 dimensiones que describen a los objetos: potencia, voltaje en
terminales, voltaje en bus infinito y número de líneas de enlace, e incluso el número
de líneas de enlace no es continuo.
Lo que se pretendía era que el sistema fuera sencillo, e identificará los nuevos
objetos presentados con base en los límites “reales"  y no los distorsionados por el
ruido, entonces era deseable que, al entrenar con los datos contaminados con ruido,
los parámetros de la RN se ajustarán para identificar los límites “reales" , es decir,
que la RN no aprendiera del ruido de los datos. Para resolver este tipo de problemas
hay algunas opciones, un ejemplo es el tipo de RN ARTMAP, pero el sistema se
vuelve complejo, por lo que se optó por reducir la capacidad de aprendizaje de la RN
reduciendo el número de neuronas, así se puede decir que la RN tiende a aprender
solamente los rasgos importantes que distinguen las clases.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE INGENIERÍA
MECÍNICA Y ELÉCTRICA
SECCIÓN DE ESTUDIOS DE POSGRADO E INVESTIGACIÓN
DEPARTAMENTO DE INGENIERÍA ELÉCTRICA
Identificación de la Estabilidad a Pequeños
Disturbios de la Máquina Síncrona Bus-Infinito
por Redes Neuronales
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS CON ESPECIALIDAD
E N I N G E N I E R Í A E L É C T R I C A
P R E S E N T A :
OSCAR MORENO REYES
México, D. F., Diciembre 2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ANÍLISIS DE VARIACIONES RÍPIDAS DE VOLTAJE EN SISTEMAS DE SUB-TRANSMISIÓN Y REDES DE DISTRIBUCIÓN</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar una metodología que permita predecir las características y el número de eventos de variaciones de voltaje de corta duración, así como evaluar su impacto en las cargas electrónicas de las redes de distribución.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Antes de que la electrónica invadiera los equipos de iluminación, calefacción y máquinas de proceso; la relación con el sistema de suministro consistía en la verificación de los datos de placa de voltaje y frecuencia. Desafortunadamente, los equipos electrónicos requieren de una mayor atención debido a los problemas de calidad de la energía. En los últimos años los resultados de los informes de monitoreo de las redes de distribución permiten establecer que las interrupciones de procesos industriales provocadas por variaciones de voltaje representan un serio problema económico, por lo que antes de concluir que métodos de mitigación deben ser implementados es necesario predecir las características de estos eventos[3].
Al predecir las características de los disturbios de voltaje se tiene la oportunidad de
evaluar configuraciones alternas de la red de distribución y prevenir problemas en las
especificaciones del voltaje de suministro. Los problemas pueden ser evitados reduciendo la magnitud, duración ó el número de eventos. Modestos cambios en las especificaciones de las cargas sensibles también pueden reducir significativamente el número de interrupciones [10].
Los estándares de calidad de la energía establecen que las variaciones de voltaje son
eventos aleatorios tratados como problemas técnicos - económicos de compatibilidad entre los equipos industriales y el sistema de potencia. Cuando se instala un equipo de proceso
electrónico, un usuario necesita comparar la sensibilidad del equipo con el comportamiento de la alimentación de la energía eléctrica. Un estudio para estimar el impacto y la frecuencia con que se presentarán las salidas de estos equipos consiste básicamente de los siguientes 2 pasos:
1. Obtener información de la tolerancia de voltaje de los equipos por parte de fabricantes, bases de datos recopiladas por estándares de prueba [13] ó simplemente tomar valores típicos.
2. Determinar las características y el número esperado de variaciones de voltaje en un sitio de interés, a través de programas de monitoreo ó métodos estocásticos.
Los programas de monitoreo que se han desarrollado en diferentes países proporcionan
un promedio de la calidad del suministro, pero no proveen información de un lugar en
especifico, dado que se requieren periodos largos de monitoreo para obtener resultados
confiables [9].
La comunidad internacional de calidad de la energía ha reportado en general dos
técnicas para predecir eventos de variaciones de voltaje de corta duración: 1.- Analíticas que obtienen el número de eventos a través de la tasa de falla de los elementos del sistema y 2.- Simulación Monte Carlo que determina una función de distribución de probabilidad de los eventos.
El análisis probabilístico de corto circuito por el método de simulación Monte Carlo de
un sistema de potencia permite simular las condiciones aleatorias de las fallas que
experimentan los elementos del sistema y predecir las características de los disturbios a través de histogramas de voltaje de falla en una región de interés ó un bus en particular.
En el estudio de corto circuito probabilístico únicamente se consideran los componentes de
la red que pueden afectar los voltajes resultantes, por ejemplo [21]:
I. Las unidades de generación que estarán en servicio en base al pronostico de la carga.
II. El sistema de transmisión representado por datos históricos de salidas de líneas.
Uno de los argumentos por el renovado interés de la calidad de energía en los niveles de
distribución es la época de la desregulación que ha traído interrogantes acerca de la
reestructuración del servicio eléctrico y la forma en que puede ser comparado un proveedor a otro. La mayor parte de las compañías distribuidoras podrán basar su competencia en el nivel de calidad de la energía entregada y los índices de calidad proporcionarán una forma de medir el servicio eléctrico y los beneficios de mejorar los circuitos de suministro.
Dentro de los efectos de la desregulación está la dispersión de responsabilidad por la
calidad de la energía. Inicialmente la compañía suministradora fue responsable por los niveles de calidad del usuario, después de la reestructuración organizacional de las empresas eléctricas habrá confusión sobre esta responsabilidad y posiblemente debates sobre el costo de las acciones tomadas para mitigar los problemas de calidad [2].
Las empresas distribuidoras de electricidad por años han utilizado índices de
interrupción como indicadores de la confiabilidad del servicio proporcionado por su sistema, como SAIFI y CAIDI [12,34]. La evolución y expansión de dispositivos electrónicos han alterado la realidad de lo que realmente es un servicio confiable. Debido a la sensibilidad de cargas industriales y comerciales, la confiabilidad ya no es indicada por la frecuencia y duración de las interrupciones permanentes que ocurren en el sistema de distribución. En la actualidad, los usuarios son afectados por disturbios de voltaje, lo que hace necesario implementar índices de variaciones r.m.s para evaluar la calidad del servicio en un lugar especifico [12].</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>* La magnitud y duración de los disturbios de voltaje son características que definen
principalmente los problemas en el voltaje de suministro a los usuarios.
* El análisis de los informes de los programas de monitoreo de calidad de la energía
establecen que las variaciones de voltaje de corta duración representan básicamente
conflictos de tipo económico entre los usuarios industriales que utilizan equipos
electrónicos en sus procesos de producción y las compañías suministradoras.
* Predecir las características de los disturbios de voltaje permite evaluar el Riesgo de falla de los equipos electrónicos e identificar las posibles soluciones en las especificaciones del voltaje.
* El método de Simulación Monte Carlo modela los aspectos aleatorios de los factores que
originan incertidumbre al presentarse una falla de corto circuito en el sistema de
potencia con funciones densidades de probabilidad, especificadas de registros
estadísticos de las compañías suministradoras. La veracidad de los resultados estará en
función de la información disponible.
* Las funciones de distribución de probabilidad de magnitud (p.u) y duración (seg) de los
eventos de variaciones de voltaje de corta duración utilizadas para evaluar el Riesgo de
falla de los equipos electrónicos, también podrían ser utilizadas para justificar técnica y económicamente los proyectos de solución propuestos por los usuarios, la compañía
suministradora o los fabricantes.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE INGENIERÍA
MECÍNICA Y ELÉCTRICA
SECCIÓN DE ESTUDIOS DE POSGRADO E INVESTIGACIÓN
ANÍLISIS DE VARIACIONES RÁPIDAS DE
VOLTAJE EN SISTEMAS DE SUB-TRANSMISIÓN Y
REDES DE DISTRIBUCIÓN
TESIS
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN CIENCIAS CON ESPECIALIDAD
EN INGENIERÍA ELÉCTRICA
PRESENTA
FABIÁN GARCÍA PERALTA
MÉXICO D.F. MARZO 2006</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Guía para el Desarrollo de Software en las PyMEs</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>La Guía para el Desarrollo de Software en las PyMEs se enfoca en diseñar un modelo que
cubra todas las necesidades que se deben tomar en cuenta para el desarrollo de software
en las PyMEs, brindando un control y administración del proyecto de la manera mas
elemental, pero cubriendo los aspectos necesarios de éxito que manejan otras
metodologías y que concluyen con un producto de software con calidad.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El presente trabajo fue desarrollado bajo la siguiente metodología:
1. Observación y experiencia profesional.
2. Investigación teórica.
3. Síntesis y propuestas.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La Guía para el Desarrollo de Software en las Pequeñas y Medianas Empresas es un
documento que asesora a una organización que cuenta con recursos limitados a ejecutar
las tareas de generación de software de manera ordenada, y con calidad.
También puede ser utilizada por organizaciones que cuentan con un departamento de
desarrollo de software pequeño que quieran controlar sus proyectos de desarrollo.
Actualmente en nuestro país existen diversas PyMEs, que han sido analizadas y se han
determinado sus características, las cuales diferencian entre una pequeña y una mediana
empresa, también vemos que hay factores que afectan a la competitividad de las PyMEs,
es decir, cuales factores afectan o benefician a las PyMEs en nuestro país y se describen.
La guía fue basada en metodologías mayores que son utilizadas actualmente por grandes
organizaciones, de las cuales se analizaron las etapas que se componen esas grandes
metodologías, se tomaron las más importantes y se sintetizaron de manera que se cuente
con una guía practica, y de fácil uso para desarrollos de software.
Las PyMEs que se dedican al desarrollo de software pueden tomar diversas metodologías y
estándares que se encuentran en el mercado, desafortunadamente son herramientas de
altos costos y que están enfocadas a empresas que tienen ejércitos de personal en sus
áreas de Tecnologías de Información, esto hace que sean difíciles de alcanzar, aun
contando con los recursos económicos pueden llegar a encontrarse con problemas dado
la falta de recursos humanos para desempeñar las tareas que marcan esas metodologías.
Esperamos que este sea de utilidad para todos aquellos proyectos de desarrollo de
software que requieran de un control, seguimiento, orden, claridad y calidad en el cual la
metodología o guía sea entendible y gratuita, considerando los puntos o etapas
principales en el desarrollo de software y marcando claramente los limites y objetivos a
alcanzar, logrando finalmente un producto de software que cumpla con las necesidades
del usuario, con calidad y en los tiempos planeados, y que cuente con la documentación
necesaria para posteriores referencias.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
UNIDAD PROFESIONAL INTERDISCIPLINARIA DE INGENIERIA Y
CIENCIAS SOCIALES Y ADMINISTRATIVAS
SECCION DE ESTUDIOS DE POSGRADO E INVESTIGACIÓN
Guía para el Desarrollo de Software en las PyMEs
T E S I S
PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS EN INFORMÍTICA
P R E S E N T A
ROBERTO CRUZ DOMÍNGUEZ
DIRECTOR: GUILLERMO PÉREZ VÍZQUEZ
MÉXICO, D.F. 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Ambiente Visual de Simulación y Análisis de Redes de Colas</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Hoy en día en muchos lados se ven colas de espera por atencion, es decir se pueden encontrar en muchos lugares que se tiene que esperar un determinado tiempo para finalmente obtener un servicio.
Este fenomeno se puede observar en el banco, en una tortillería, en el supermercado, restaurantes, parques de diversiones tambien se ve en salas de espera del seguro social, dentista, etc, lugares muy comunes en la vida cotidana de toda persona. Asimismo se puede observar que algunas veces la manera de espera no es una línea de personas, inclusive podría no ser en un lugar físico como lo son en los centro de atencion telefonicos en donde se tiene que esperar a que un ejecutivo atienda la llamada. Algunas
de las compañías invierten mucho dinero en el estudio del comportamiento de su sistema de atencion, esto va desde que llega un cliente, en cuanto tiempo es atendido, cuanto tiempo se toma atenderlo y sale. Esto es por que las empresas saben que parte de su negocio final (vender, realizar operaciones bancarias, prestar un servicio) involucra una cola de atencion y para ellos es muy importante que el cliente espere a que se le atienda.
Así, existen muchos factores que se estudian desde que el cliente no tenga que esperar tanto a que se le atienda, hasta que como hacer que tenga una espera placentera. Estas empresas estudian todos estos factores de tiempo de atencion, cantidad de clientes en espera, cuantos clientes se retiran antes de ser atendidos, etc. Usualmente estos estudios se realizan en simuladores computacionales, en los cuales se modelan estos sistemas de atencion y se simula el flujo de clientes que se tienen.
En la actualidad estos simuladores son muy costosos, ademas los modelos y el analisis de resultados los tiene que hacer un experto en la materia. Existen algunos simuladores gratuitos, sin embargo necesitan que los programe un experto en el simulador y en materia de teoría de redes de colas, ya que no presentan algun tipo de interfaz grafica y el lenguaje de programacion es otra limitante. Es por eso,
que se presenta un simulador con las capacidades de llevar a cabo la simulacion de muchos de estos modelos de redes de colas, presentar y facilitar el analisis de resultados precisos al usuario y facilitar el modelado de estos sistemas al proveer una interfaz de usuario grafica facil de usar.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Programar un area de trabajo que provea de un ambiente grafico para que el usuario pueda modelar un sistema de colas y despues pueda simularlo. Para así obtener resultados que se puedan analizar (ya sea por medios visuales o de cifras numericas) y valorar para generar un analisis de desempeño del sistema modelado.
Para lo cual es necesario:
Comprender la teoría de desempeño de sistemas, Analizar la relacion y la equivalencia entre un sistema real y la modelacion y simulacion del
mismo.
Programar un nucleo para la simulacion basada en eventos discretos.
Realizar una interfaz grafica de usuario sencilla de usar.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Para llevar a cabo una simulacion de un modelo real, es muy importante tomar en cuenta todo lo mencionado en esta tesis, acerca de los resultados que se pueden obtener de un modelo no conocido y las suposiciones que se hacen inicialmente. Es tambien importante tomar en cuenta la desviacion que se tiene numericamente debido a la generacion de numeros aleatorios y la precision por naturaleza de la plataforma de hardware que se utilice. Finalmente el mejor punto de vista de resultados parciales de una simulacion de un modelo real, la tendra un experto que pueda saber el comportamiento aproximado de un sistema de redes de colas, o de algunos segmentos del sistema.
En este trabajo de tesis se presento un ambiente de desarrollo para la modelacion y simulacion de redes de colas que cuenta con una interfaz de usuario amigable en el lenguaje de programacion Java lo cual permite que el programa se pueda ejecutar en diferentes plataformas y sistemas operativos. El nucleo programado lleva a cabo las simulaciones, es adaptable y se puede usar sin la necesidad de la interfaz grafica. Asimismo permite la modelacion de sistemas por medio de archivos XML. La simulacion se basa en un simulador de eventos discretos los cuales son administrados por clases internas propias. Por  último tambien se proveen las clases bases para la generacion de numeros aleatorios y algunos generadores de numeros aleatorios con ciertas
distribuciones probabilísticas.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Ambiente Visual de Simulación y Análisis de
Redes de Colas
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN
P R E S E N T A
ING. JUAN MANUEL HORTA MENDOZA
DIRECTOR DE TESIS
M. EN C. JOSÉ GIOVANNI GUZMÍN LUGO
CODIRECTOR DE TESIS
M. EN C. ROLANDO MENCHACA MÉNDEZ
MÉXICO D.F. MAYO 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“Análisis de las Redes de Energía Eléctrica como Canal de Comunicaciones de Banda Ancha por Algoritmos Genéticos"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar de una metodología para analizar el comportamiento de las redes de energía
eléctrica como canal de comunicaciones de banda ancha. La metodología debe de presentar
resultados mejores respecto a las metodologías existentes. Para esto se propone aplicar las
técnicas de algoritmos genéticos por su eficiencia para modelar las funciones de
transferencia no lineales de canales de comunicación.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los avances de la tecnología han permitido que las líneas de distribución de energía
eléctrica sean empleadas como canal de comunicaciones para la transmisión de información
de banda ancha. Sin embargo, al no haber sido diseñadas para este propósito son un medio
altamente hostil con muchos inconvenientes que dificultan la transmisión de información.
Para poder superar estos inconvenientes se requiere conocer el comportamiento que
presentan como medio de comunicación de banda ancha; para esto, se han desarrollado
algunos modelos matemáticos y circuitos equivalentes con el fin obtener la función de
transferencia que permita caracterizar las líneas de distribución como canal de
comunicaciones. Los modelos desarrollados con mayor aplicación para este análisis son
híbridos, ya que combinan una parte analítica y una parte experimental por lo que su
procesamiento es complejo [21-25]. Para la parte experimental se requiere medir la función
de transferencia que da información de los niveles de atenuación para una cierta localidad
física de red. Para la parte analítica se propone un modelo de multitrayectorias, cuyas
componentes se suponen y se obtiene la respuesta del canal de comunicación por métodos
de optimización determinísticos. Estos modelos tienen varias imprecisiones, las más
importantes son: (1) la metodología de medición de la red de distribución de energía
eléctrica aún no está normalizada y no se especifica como se realiza; (2) los parámetros que
caracterizan a la red de distribución de energía eléctrica son de carácter aleatorio; (3) Los
modelos solo se han aplicado a redes eléctricas internas donde se tiene un mayor control de
la geometría.
La obtención de un modelo que permita analizar las redes de distribución de energía
eléctrica en media y baja tensión para lugares abiertos y cerrados y con mucha mayor
precisión que los modelos presentado en la literatura es el reto de este trabajo de
4
investigación. Para ello se propone usar un modelo híbrido con parte experimental
describiendo la metodología de medición y usando un método de optimización de mayor
precisión como es el de algoritmos genéticos, ya que proporciona un balance de
explotación y exploración entre las posibles soluciones sobre todo a problemas no lineales
como es el caso del modelo de la función de transferencia de una línea de distribución de
energía eléctrica.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Respecto a los Algoritmos Genéticos:
Los Algoritmos Genéticos (AG) son una técnica de optimización numérica que emplean un
conjunto de operadores estocásticos para generar una solución que satisfaga la problemática
en cuestión. Cada uno de los operadores del AG debe ser seleccionado de acuerdo al
conocimiento que se tenga del espacio de la solución o en caso de que no se conozca este
espacio, los operadores deben ser seleccionados de acuerdo al desempeño mostrado por el
algoritmo genético diseñado. Esta técnica comúnmente requiere probar varias
configuraciones de los operadores para alcanzar un buen desempeño con un mínimo de
tiempo empleado.
El tiempo que emplee el AG para encontrar una solución adecuada dependerá mucho de la
complejidad del problema, es decir: número de variables a optimizar y el intervalo de
valores posibles para cada variable. La población inicial determinara el dominio de trabajo
del algoritmo, si esta población se encuentra en el dominio de la solución, los tiempos de
trabajo se reducen en gran medida, en cambio si la población se encuentra dispersa en un
dominio extremadamente amplio puede tomar demasiado tiempo lograr la convergencia.
La técnica de optimización por LSQ proporciona una solución cercana al optimo, que
permite al AG tener una referencia de la región o regiones donde debe encaminar el trabajo
de optimización, esta referencia LSQ permite reducir los tiempos del algoritmo para
alcanzar la convergencia, así lo demuestran los resultados obtenidos en este trabajo de tesis,
donde el AG empleado requiere un tiempo menor a 15 minutos para encontrar la solución
optima del problema propuesto con una PC de 1.4 GHz cuyas caracteristicas se especifican
en el anexo 4.
Respecto al canal de comunicaciones y la optimización por AG:
El modelo matemático empleado para obtener el comportamiento de la función de
transferencia de las líneas de distribución de energía eléctrica es de tipo hibrido (teórico -
experimental). Se emplea la teoría clásica de líneas de transmisión en conjunto con una
serie de mediciones realizadas directamente sobre las líneas que constituyen el canal de
comunicaciones. Estas mediciones deben realizarse con la mayor precisión posible para
lograr que el modelo represente adecuadamente la función de transferencia.
Inicialmente el proceso de optimización se realiza a través de la técnica de Mínimos
Cuadrados (LSQ) que proporciona una aproximación deterministica sobre los valores de
atenuación del canal, sin embargo el comportamiento de las líneas de distribución de
energía eléctrica es aleatorio y esta técnica de optimización resulta insuficiente debido a que es empleada para ajuste de curvas y si los valores de prueba presentan un
comportamiento muy disperso la representación de la curva resulta imprecisa, por lo que en
este caso corresponde a una primera aproximación en la optimización del problema a
resolver. Esta es la razón por la que se recurre al empleo de otro tipo de optimización que
no sea deterministica ya que el problema objeto de esta tesis es de carácter aleatorio.
La solución proporcionada por LSQ es entonces empleada como referencia para indicar al
AG la región o regiones de trabajo más viables para encontrar al óptimo. En conjunto
ambas técnicas de optimización constituyen una metodología mixta para encontrar la
solución mas adecuada del comportamiento del canal de comunicaciones de las líneas de
energía eléctrica.
El proceso de optimización por AG en problemas complejos requiere de una referencia, la
cual si no se selecciona adecuadamente lo pone en desventaja respecto a otras técnicas de
optimización. En este trabajo de investigación se recurrió a la referencia proporcionada por
LSQ que permitió reducir los tiempos de operación para analizar las líneas de energía
eléctrica como canal de comunicación.
El desempeño de la metodología empleada puede observarse en la figura 5.2, en la que se
muestra la función de transferencia medida sin señal PLC; los resultados son muy precisos
(color verde) y el modelo LSQ (color azul) resulta una buena referencia para la
optimización por AG, así lo demuestra el modelo resultante (color rojo), el cual coincide
con la función de transferencia para el intervalo de frecuencia de 1 MHz a 50 MHz.
En la figura 5.4, se muestra la respuesta experimental de una linea de transmisión con señal
PLC (color verde), asi como tambien los resultados simulados por LSQ (color azul) y AG
(color rojo). En el intervalo de frecuencia de 7 MHz - 12 MHz y de 18 MHz - 23 MHz se
tiene la señal de la tecnología PLC, en estos intervalos no se presentan nulos por lo que el
analisis presentado nos ayuda a seleccionar las bandas de frecuencia por donde se puede
transmitir señal PLC. Sin embargo es claro que se tiene una gran discrepancia en el
intervalo de 35 MHz - 50 MHz, lo cual se debe a que la referencia LSQ no es la mas
adecuada, porque la inestabilidad del canal se manifiesta mas conforme aumenta la
frecuencia.
De las observaciones anteriores se concluye:
*	Es importante que las mediciones de las líneas de distribución de energía eléctrica
se realicen sin señal PLC para lograr una optimización adecuada como la de la
figura 5.2
*	La optimización mixta proporciona una mejoría de 30 dB en comparación con la
técnica LSQ.
*	El AG permite hacer un análisis en tiempo real del comportamiento del canal de
comunicaciones BPL con esquema de modulación OFDM, con el fin de hacer los
ajustes necesarios que permitan obtener tasas de transferencia lo más altas posibles,
con un mínimo de errores. Se dice que el análisis es en tiempo real porque para que
el algoritmo genético converja hacia una solución y proporcione los valores de
atenuación óptimos, tan solo se requiere de algunos minutos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE INGIENERIA MECANICA Y ELECTRICA
UNIDAD PROFESIONAL ADOLFO LOPEZ MATEOS
Sección De Estudios De Posgrado E Investigación
Maestría en Ingeniería Electrónica
“Análisis de las Redes de Energía Eléctrica como Canal de
Comunicaciones de Banda Ancha por Algoritmos Genéticos" 
TESIS
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN CIENCIAS
PRESENTA:
ING. MANUEL GASTON SALINAS CARPIO
ASESOR:
DR. ROBERTO LINARES Y MIRANDA
México, D. F. NOVIEMBRE 2006</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ENRUTAMIENTO SEGURO Y ESCALABLE EN REDES MOVILES AD HOC</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En una red móvil de tipo Ad Hoc, para llevar a cabo una comunicación entre un nodo origen y un nodo destino, si el nodo destino está fuera del alcance del radio de propagación del nodo origen, se debe establecer una ruta de comunicación a través de otros nodos, los cuales actuarán como enlace entre el nodo origen y el nodo destino. Cada uno de los nodos que participan en la ruta que conecta al origen con el destino reenvían la información generada por el origen, de forma tal que ésta es recibida por el destino. Por la naturaleza de este tipo de redes, donde las comunicaciones son inalámbricas, y donde cualquier nodo puede formar parte de rutas que conectan a los orígenes con el destino, pueden ser vulnerables a diversos tipos de ataques, por ejemplo:
*	Cuando un nodo atacante miente acerca de su identidad (e.g., pretende ser el destino).
*	Cuando un nodo miente acerca de su distancia al destino (e.g., afirma tener una ruta más corta hacia el destino de la que en realidad tiene).
*	Cuando un nodo atacante sólo reenvía información de control y la información de datos la desecha.
*	Cuando un nodo atacante envía o desecha información de datos de manera aleatoria.
*	Cuando un nodo atacante envía o desecha información de control de manera aleatoria.
Además de esos tipos de ataques se deben solucionar los problemas inherentes a la movilidad de los nodos de una MANET. Por ejemplo, las rutas se pueden romper continuamente debido a que un nodo intermedio ha dejado de estar activo o se ha movido a otra posición y por lo tanto será necesario calcular otra ruta hacia el destino. Por otro lado, debido a que los recursos disponibles para este tipo de redes son escasos (ancho de banda, energía de las baterías, etc.) es necesario implementar protocolos eficientes, que sean capaces de soportar múltiples flujos de comunicación de manera concurrente, es decir, que sean escalables.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Implementar un protocolo de enrutamiento para redes móviles ad hoc, que sea capaz de entregar paquetes de datos de un nodo origen a uno destino, aún y cuando existan nodos adversarios que mientan, ya sea a la hora de informar sobre su distancia hacia el destino o sobre su identidad. Adicionalmente, el protocolo debe ser escalable y hacer uso eficiente de los recursos de la red.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Las aplicaciones móviles distribuidas como las utilizadas en operaciones militares, ayuda en caso de desastres o emergencias, redes que permiten la interacción de personas (como las usadas en reuniones de estudiantes durante conferencias), la seguridad en el proceso de enrutamiento es necesaria para proteger la red en contra de ataques implementados por nodos maliciosos.
Es de vital importancia tener protocolos de enrutamiento seguro para el uso en aplicaciones destinadas a operaciones militares y aplicaciones donde la seguridad en el proceso de enrutamiento sea un requerimiento.
Como se mencionó en la introducción, las MANETs son relativamente fáciles de atacar, por ejemplo un único nodo que publique una distancia 1 (medida en saltos) a todos los destinos tiene el potencial de inutilizar toda la red. Al publicar esta distancia en descubrimientos de nuevas rutas el nodo malicioso puede ocasionar que: se creen ciclos de enrutamiento, él tenga más posibilidades de ser incluido en la nueva ruta, y como consecuencia poder hacer mal uso de la información que fluye a través de él o eliminar tal información.
Debido a que el problema de la seguridad en la transmisión en las redes móviles ad hoc sigue siendo abierto, es indispensable desarrollar mecanismos que doten a las MANETs de tolerancia a dichos ataques, siendo fiables, funcionales y además que sean escalables.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con la creación de tecnologías inalámbricas y aplicaciones distribuidas, el uso de las redes móviles ad hoc (MANETs) han tenido un incremento considerable en su uso, ya sea de manera comercial, educativa, personal o gubernamental. Por lo tanto, primeramente fue necesario ofrecer protocolos de enrutamiento para redes móviles que ofrecieran buen desempeño en tareas de enrutamiento en dichas redes, teniendo como principales retos la movilidad, la reparación de rutas en tiempo real, ser libres de ciclos, proporcionar rutas actuales, mantenimiento de enlaces rotos, etc. Con el fin de cubrir tales necesidades se desarrollaron protocolos de enrutamiento proactivos y reactivos (ambos con ventajas y desventajas), los cuales son empleados en escenarios donde uno se desempeña mejor que el otro. Sin embargo, en los últimos años ha surgido un reto muy importante, como lo es la seguridad en MANETs. Estudios recientes han clasificado a los ataques a redes móviles en dos tipos: ataques contra el enrutamiento y basados en el consumo de recursos. Por lo que ha surgido la necesidad de implementar protocolos que además de ofrecer un buen desempeño ofrezcan mecanismos de seguridad en contra de ataques al enrutamiento como los vistos en la sección 2.2.
En una red ad hoc, desde el punto de vista de un protocolo de enrutamiento, existen dos clases de paquetes: los paquetes de enrutamiento y los paquetes de datos. Ambos tienen diferentes requerimientos de seguridad. Los paquetes de datos son de extremo a extremo y pueden ser protegidos con cualquier sistema de seguridad de extremo a extremo como IPSec[38]. Por otro lado, los paquetes de enrutamiento son enviados a vecinos inmediatos, procesados, modificados, y reenviados.
Otra consecuencia de la naturaleza de la transmisión de paquetes de enrutamiento es que, en muchos casos, existen algunas partes de esos paquetes que cambian durante su propagación. Esto se puede apreciar en el protocolo propuesto, ya que está basado en vector de distancias, en donde los paquetes de control contienen un conteo de salto de la ruta que se está estableciendo y que va cambiando conforme se propaga en la red. Entonces, en un paquete de control se distinguen dos tipos de información: mutable y no mutable. Por lo tanto, el presente trabajo de tesis se enfocó en implementar un protocolo reactivo basado en vector de distancias llamado OSDV, que ofrezca mecanismos de seguridad para asegurar tanto la información mutable como la no mutable, como lo son: cadenas hash (para asegurar información mutable) y firmas digitales(para asegurar información no mutable), ordenación en tiempo.
El objetivo principal que planteó alcanzar esta tesis, fue el desarrollar un protocolo de enrutamiento para una red móvil que sea capaz de entregar paquetes de datos a los destinos aun y cuando existan nodos maliciosos que mientan al informar su distancia o su identidad, con el propósito de hacer mal uso de la información. Con este fin en el capítulo 4 se describe la especificación del protocolo OSDV, en el que se ve a detalle el uso de cadenas hash para asegurar la información mutable y el uso de firmas digitales para asegurar la información inmutable.
Por lo que después de desarrollar dicho protocolo, se procedió a realizar pruebas con diferentes escenarios, tales pruebas fueron llevadas a cabo en el simulador NS-2, con las que podemos concluir lo siguiente:
*	Con el protocolo OSDV se incrementó la posibilidad de entregar los paquetes de datos al destino en todos los escenarios (Deliver Ratio).
*	Detecta a nodos maliciosos y no permite que participen en la ruta.
*	Bajo escenarios sin algún atacante el desempeño del protocolo seguro es igual al del protocolo AODV.
*	Debido a que no siempre se usa la ruta más corta hacia el destino, el tiempo de entrega de los paquetes es mayor y también se envían más paquetes de control.
*	Además, basándose en los resultados experimentales obtenidos en la sección 5-2, se puede afirmar que el protocolo mantiene su operación aún y cuando se variaron parámetros de simulación. Por lo tanto, el protocolo es escalable.
Finalmente podemos afirmar que con la implementación del protocolo OSDV se logró el objetivo de la tesis.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACION EN COMPUTACION 
ENRUTAMIENTO SEGURO Y ESCALABLE EN REDES MOVILES AD HOC
TESIS 
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS DE LA COMPUTACION 
PRESESNTA
ING. MIGUEL RODRIGUEZ MARTINEZ
 DIRECTORES DE TESIS:
DR. ROLANDO MENCHACA MENDEZ
DR. FELIPE R. MENCHACA GARCIA 
MEXICO, D.F., AGOSTO DEL 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>IMPLEMENTACION DE UN MODELO NUMERICO PARA LA ESTIMACION EN UNA REGION GEOGRAFICA DE LA IRRADIACION SOLAR UV Y DEL INDICE UV PARA CONDICIONES DEL CIELO DESPEJADO MEDIANTE CALCULOS DE TRANSFERENCIA RADIATIVA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>La Evaluación de los Productos de Software Asistida por el Sistema MECA.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El control de la calidad del software es una materia que preocupa tanto a los
productores como a los usuarios. Originalmente, la calidad de un programa o
sistema se evaluaba de acuerdo al número de defectos por cada mil líneas de
código. Hoy en día, el concepto moderno de Calidad del Software, requiere de una
congruencia total entre los requerimientos y características del producto, para
lograr una plena satisfacción del usuario. Surgen componentes de la calidad, tales
como: confiabilidad, flexibilidad, facilidad de uso, integridad, consistencia, etc; es
decir, se quieren productos que se puedan transportar, sean fáciles de mantener
y/o ampliar, sencillos de entender, de validación accesible, compatibles con otros
sistemas, rápidos y efectivos, etc.
La evaluación de la Calidad del Software, en la práctica, requiere de métricas que
se aplican a los atributos del software en cada una de sus etapas, para evaluarlas
y determinar su grado de calidad. En la actualidad, no se conoce una herramienta
que realice esta actividad, por tal motivo, en este trabajo se propone la
implementación de un sistema que automatice el Modelo Cualimétrico para la
Evaluación de la Calidad del Software (MECA) y que representa un apoyo
importante en la solución del problema.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar una aplicación que apoye el proceso de control de la calidad,
evaluando los resultados de cada una de las etapas del ciclo de vida del software,
mediante la aplicación de un modelo cualimétrico que siga los lineamientos de la
ISO/IEC 9126 [23].</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El desarrollo de un sistema que se ocupe de evaluar la calidad del software
utilizando métricas de calidad e incluyendo una nueva característica denominada
Calidad en Uso; constituye una herramienta importante que podría incidir
favorablemente en el proceso del control de la calidad de los sistemas
informáticos.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La medición es algo común en el mundo de la ingeniería: se mide el consumo de
energía, el peso, las dimensiones físicas, la temperatura, el voltaje, la relación
señal-ruido, etc. Por desgracia, la medición es menos común en el mundo de la
ingeniería del software, ya que existen problemas al acordar en qué y cómo medir.
La única forma racional de mejorar cualquier proceso es medir sus atributos,
desarrollar un juego de métricas significativas y utilizarlas para proporcionar
indicadores que conduzcan a una estrategia de mejora del producto final.
Por esa razón, se requiere que el diseñador conozca los requerimientos de cada
etapa del ciclo de vida del software. Una vez implementados estos requerimientos,
se pueden valorar de acuerdo a una metodología de evaluación y determinar el
grado de la calidad del producto. Todo software implementado con miras a ser
evaluado, debe contar con un ejecutable o documentación; con el propósito de
que el auditor observe sus atributos (internos y externos) y emita un resultado.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se ha desarrollado una aplicación que apoya al proceso de control de la calidad,
evaluando el nivel de los resultados de las etapas del ciclo de vida del software,
aplicando un modelo cualimétrico que sigue los lineamientos de la ISO/IEC 9126,
por lo que se ha cumplido en su totalidad el objetivo general de este trabajo.
Los objetivos específicos también han sido cubiertos, éstos a continuación se
enumeran:
"Se han añadido a las métricas de MECA, las subcaracterísticas y atributos
de la Calidad en Uso como aportación de este trabajo al modelo
cualimétrico.
"Se ha implementado las características de calidad del MECA, por lo que ha
surgido un control de la calidad de los sistemas informáticos de acuerdo a
los estándares internacionales ISO.
"Al implementarse las características de Calidad, se han probado y
verificado los resultados de las etapas del ciclo de vida del software.
"El Sistema MECA se ha instrumentado de acuerdo al paradigma orientado
a Objetos y se ha programado usando el lenguaje Java obteniéndose las
ventajas de éste.
Aunado a los objetivos generales y específicos cumplidos, se precisan otras
características del Sistema MECA:
Sirve de apoyo al proceso de auditoría informática y de control de calidad ya que
permite la evaluación y determinación del nivel de la calidad de los productos en
todas las etapas del ciclo de vida del software.
Trata al software como producto terminado. Al evaluar la calidad del software a
través de las etapas del ciclo de vida, permite desglosar sus propiedades. El
desglose es para conocer detalladamente la presencia de sus atributos y verificar
su comportamiento para determinar el grado en que las propiedades del producto
cumplen con las especificaciones de calidad internacional.
La instrumentación del Sistema bajo el paradigma Orientado a Objetos y la
programación Java utilizados lo hace más consistente, funcional y portable a los
cambios de la plataforma de hardware y software. Así mismo; el sistema se presta
al enriquecimiento de nuevas características al modelo sin que este
mantenimiento afecte el funcionamiento de sus componentes actuales.
El Sistema MECA sigue un modelo más completo que los presentados en el
capítulo II. La cantidad de usuarios se considera amplia, ya que contempla en su
panorama de uso a los auditores informáticos, técnicos diseñadores de sistemas,
profesionales en computación, analistas y jefes de proyectos, administradores de
sistemas y usuarios con los conocimientos básicos de cómputo que busquen
conocer el nivel de la calidad de un producto de software.
Casi todos los productos elaborados para el uso del cliente habían tenido
indicadores para medir la calidad del mismo, esta medición había sido difícil de
implementar mediante un software por ser intangible. La presencia de la
herramienta proporciona el mejoramiento del proceso de evaluación de los
productos de software. Debido al surgimiento de las normas de calidad ISO se han
mejorado los acuerdos en qué y cómo medir este producto.
Con la medición de la Calidad en Uso mediante la aplicación de la encuesta a los
usuarios, el Sistema MECA permite conocer el grado de satisfacción del cliente
con respecto al producto. Se ha propuesto el análisis de los resultados de las
preguntas a través de la población de usuarios encuestados con la finalidad de
conocer las métricas que satisfacen al cliente y corregir las que fallan en este
aspecto, lo cual redunda en un mejoramiento de la calidad del producto. El informe
de la Calidad en Uso representa los resultados de los procesos utilizados para
prevenir los errores durante el desarrollo del producto y las prácticas, usadas para
detectar los errores que se traducen en software más competitivos, demandados y
confiables; en fin, productos desarrollados con calidad. El costo de la calidad no
consiste en hacer buenos productos sino el precio que se paga por no hacerlo
correctamente.
La herramienta instalada en un servidor de aplicaciones y consultada a través de
Internet, espera muchas visitas, hecho por el cual, la creación de Servlets es un
buen requerimiento para incrementar su funcionalidad como Aplicación Web.
La herramienta de evaluación cuenta con un control de seguridad basado en
claves de acceso. En caso de no haberse finalizado un proceso de evaluación, se
recomienda, guardar el avance. El sistema asigna una calificación considerando
solo los datos presentes en el avance (generalmente, será un valor menor al
esperado como calificación final). La información estará presente en los archivos
por tiempo indefinido; hasta que el evaluador requiera reanudar el proceso. Es
necesario conservar la clave de acceso, el nombre del producto y la etapa de
evaluación para reanudar el proceso. De otra manera, será necesario iniciar
nuevamente la evaluación.
En todos los componentes del sistema ha sido diseñado un método de protección
de las páginas web a través de la programación Java mediante el uso del control
de sesiones, de manera tal que no se permite ejecutar una página sin antes
autentificarse. Esta ventaja cubre la posibilidad de mantener la integridad del
sistema ante intentos de ataques virtuales.
La población de usuarios que se espera es cubierto por la capacidad del
Manejador de Base de Datos, ya que el SQL Server 2000 puede soportar una gran
cantidad de registros por tabla.
El sistema es fácil de mantener y actualizar. Es decir; Se pueden añadir
subcaracterísticas a una característica presente en el sistema o atributos a una
subcaracterística existente. Para estas tareas se requiere el conocimiento de los
niveles de calidad en el modelo, el conocimiento de SQL básico, del procesador
de consultas del SQL Server 2000 y el lenguaje Java básico.
El presente trabajo ha demostrado que, una vez más, la cultura del orden es
primordial al diseñar un producto u ofrecer un servicio de software.
Al hacer uso del Sistema MECA se recomienda tener a disposición del personal
los requerimientos de la documentación, software y sus derivados (según sea la
etapa a evaluar) y verificar detalladamente el contenido de estos durante la
evaluación.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
Centro de Investigación en Computación
Maestría en Ciencias de la Computación
La Evaluación de los Productos de Software
Asistida por el Sistema MECA.
TESIS QUE PARA OBTENER EL GRADO DE MAESTRO EN
CIENCIAS DE LA COMPUTACIÓN PRESENTA:
ALMA DELIA CUEVAS RASGADO.
México, D.F. Julio de 2003</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“SISTEMA PARA CONTROLAR Y GESTIONAR REDES DE AGUA POTABLE EN PEQUEÑAS LOCALIDADES"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El sistema de información geográfica se pretende aplicar a las colonias (unidades -
habitacionales) de Bella Vista y Nueva España que se encuentran ubicadas en la Ciudad
de Tehuacan Puebla, delimitando su alcance solo y exclusivamente a las colonias antes
mencionadas.
Para la gestión de la red, se requiere contar con información exacta, que permita una
vinculación entre los clientes y el sistema de tuberías. El funcionamiento de esta red
debe ser capaz de abastecer a todos sus clientes con agua potable, y entregar un servicio
adecuado y de calidad.
Dentro de la gestión, se encuentra planificar las posibles extensiones de la red, hacer
mantenimiento a la misma, determinar el valor del activo fijo asociado a esta infraestructura, determinar y proyectar las inversiones futuras en extensiones de la
red, generar información para la proyección de la demanda de los clientes y para la
regulación tarifaria, definir sectores de corte y otras tareas que se incorporen en el
tiempo. El uso de tecnologías de información como el SIG (Sistema de Información
Geográfica), o su siglas en inglés GIS (Geographic Information System), permite
representar la realidad de manera visual, ayudar a mantener, generar y usar información
mediante bases de datos para planificar y lograr un análisis temporal de las situaciones
futuras.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El diseño de un sistema de información geográfica de la red de agua potable delimitando sus alcances del mismo a dos unidades
habitacionales.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La Hipótesis de la presente tesis, es que por medio de un sistema de información
geográfica, aplicado a una red de agua potable que suministra el servicio a las unidades
habitacionales de Bellavista y Nueva España, localizadas en la ciudad de Tehuacan
Puebla, se mejora la operación y la gestión del Sistema de agua.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>1.-La gestión de los sistemas de distribución de agua, constituye un motivo de estudio,
actualización e innovación en las diversas dependencias gubernamentales y privadas, en
los sectores involucrados dentro de la sociedad y las acciones orientadas a crear
capacidades de gestión y regulación en el manejo del agua; los cuales son temas
prioritarios para el desarrollo de las comunidades rurales y de la sociedad urbana en
general. Lo anterior implica la necesidad de encontrar y concertar los mecanismos más
idóneos dentro del campo legal, social, político y económico a fin de solucionar los
grandes conflictos existentes entre los usuarios y beneficiarios del agua.
La información existente en los distintos organismos que se utiliza como base para la
toma de decisiones, si bien debe ser pública, resulta de difícil acceso al público en
general. Si bien se han realizado grandes esfuerzos por desarrollar un SIG (Sistema de
información geográfico) completo, debe destacarse que mucha de la información
existente es puntual, dispersa, y todavía no tiene una alto nivel de sistematización.
Como se ha visto, los SIG constituyen una herramienta muy poderosa para la gestión
de información y su relación con algo tan tangible como un sistema de distribución de
agua a presión. Sin embargo, es muy importante conocer los alcances de un sistema
como este para aprovechar sus potencialidades al máximo, utilizándolo en el delicado
proceso de toma de decisiones del organismo operador, el gobierno y las asociaciones
civiles.
En general, a lo largo de esta tesis, resultan evidentes las ventajas que presenta el uso
de Sistemas de Información que pueden ser referenciados a entidades espaciales,
particularmente por la gran utilidad que significa combinar la potencialidad de la parte
gráfica del sistema con un banco de datos interactivo y de actualización automática.
Es claro que la práctica cotidiana en el uso de los SIG en las organizaciones generará
ventajas competitivas, sin importar si el sector de la empresa en cuestión venda
servicios de internet, y se encargue del Sistema de Distribución de agua potable; o bien
se dedique a realizar estudios de mercado para la introducción de nuevos productos.
Sin embargo, es necesario destacar la amplia gama de aplicaciones de índole social
que pueden tener los sistemas de información geográfica y más importante aún resulta
el promover su utilización tanto en el sector gubernamental como en la iniciativa
privada.
2.- En cuanto a los software utilizados en el proyecto, podemos decir que Arc View es
una herramienta de mucha ayuda en los sistemas de información geográfica, ya que
provee herramientas para incorporarlas en la información espacial y atributiva, crear
mapas, realizar consultas, desarrollar análisis espaciales, acceder a base de datos
externas e implementar aplicaciones bajo programación en lenguaje Avenue. De tal
manera que tiene la facilidad de personalizar también su interfaz gráfica de usuario,
consecuentemente nos permite administrar la información geográfica relacionada con la
red de distribución de agua potable, ya que es un software diseñado casi para la
administración y el control de la información espacial.
Respecto a Epanet podemos decir que es muy potente su uso, ya que a través de éste,
se realizan simulaciones en período tanto estático, como extendido del comportamiento
hidráulico y de la calidad del agua en redes de tuberías a presión.
Dicho software, permite seguir la evolución del flujo del agua en las tuberías, de la
presión en los nudos de demanda, del nivel del agua en los depósitos, y de la
concentración de cualquier sustancia a través del sistema de distribución durante un
período prolongado de simulación. Además de las concentraciones, permite también
determinar los tiempos de permanencia del agua en la red y su procedencia desde los
distintos puntos de alimentación.
En cuanto a Excel, se sabe de antemano que es un potente software para manejar
tablas y base de datos electrónicas de forma eficiente, pudiendo así, establecer en las
casillas tanto rangos como fórmulas, de ahí su relación en este sistema.
3.- La implementación de un SIG en este sistema de distribución de agua, ha facilitado
al organismo operador y de mantenimiento, el control y gestión de la red de agua
potable.
Se considera a un SIG, como un sistema de importancia primordial para la sociedad,
toda vez que contribuirá a mejorar la gestión de los inmuebles y servicios dentro de la
localidad en estudio, por lo cual debe ser comprendido y apoyado por los integrantes de
la comunidad. Hasta que esto no ocurra, será muy difícil contar con un sistema
completo a corto plazo, ya que no se dispone de los recursos económicos requeridos
para su desarrollo.
El Sistema Red de Agua Potable, constituye un sistema de información que permite
disponer de información rápida y oportuna sobre las características, ubicación y estado
de los componentes que conforman la red de agua potable de las unidades Bella vista y
Nueva España. La tecnología SIG brinda valiosas posibilidades para la adquisición,
almacenamiento, análisis y producción de información georeferenciada que deben ser
aprovechadas al máximo por los responsables de tomar decisiones de cambio en
relación con el entorno físico, social o económico.
La aplicación de técnicas informáticas, apoyadas en la utilización de medios de
geoposicionamiento (GPS), se muestra más rentable en los trabajos de actualización y
comprobación de datos concretos de la red. Esta metodología permite al operario tener
toda la información de la red en su equipo informático, y comprobar con gran facilidad
la adecuación de los datos almacenados con la realidad que está visualizando.
Independientemente de la técnica empleada en campo para la adquisición de datos, la
finalidad es conseguir almacenar en un sistema de información geográfica los valores
característicos de los elementos de las redes. Se realiza directamente durante el proceso
de adquisición de datos, o bien se implementan posteriormente.
Con el proyecto desarrollado, se ha logrado caracterizar adecuadamente la red de
abastecimiento de agua potable, y se han definido procesos de adquisición de datos más
adecuados a las necesidades de las empresas de gestión. Este es el primer paso que
permitirá la optimización de estos servicios, a través de la utilización de SIG integrados
con aplicaciones para la modelación y el control de las redes, que redundarán en la
calidad de estos servicios fundamentales de nuestra sociedad.
Para concluir, es importante destacar que el uso de los SIG no debe ser manejado como
un problema de tecnología, como ha ocurrido durante ya varios años. Sin embargo, su
uso debe reflejar la necesidad de una herramienta para el manejo de datos espaciales,
con la finalidad de resolver un problema.
4.-El proyecto por ser un sistema de información geográfico, se deberá actualizar y
retroalimentar conforme se tengan necesidades requeridas de la población servida, que a
su vez, son directamente proporcional a las mejoras, gestión, reparaciones,
abastecimiento de la red de agua, y por lo tanto este proceso se vuelve una constante
repetición, lo cual ocasiona que se tengan que tomar medidas de control y actualización
sobre todo, de la red de agua, que solo beneficiará a la población y con este propósito,
tener un mejor servicio de calidad de agua distribuida en el fraccionamiento.
Por lo tanto, respecto a la hipótesis de investigación, se comprueba que a través de un
SIG, los alcances que pueden llegar a tener estos sistemas, solo son limitados por la
base de datos que ellos tengan, así a mayor número de datos, mayor será la información
que de ellos se podrá obtener; y a una constante actualización de sus datos, más precisos
a la actualidad serán los resultados que arroje cuando sean consultados por una persona.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE INGENIERÍA
Y ARQUITECTURA UNIDAD ZACATENCO
SECRETARÍA DE INVESTIGACIÓN Y POSGRADO
DIRECCIÓN DE ESTUDIOS DE POSGRADO
“SISTEMA PARA CONTROLAR Y GESTIONAR REDES
DE AGUA POTABLE EN PEQUEÑAS LOCALIDADES" 
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS EN HIDRÍULICA
P R E S E N T A:
JESÚS CHIRINO GALINDO
DIRECTORES DE TESIS:
M. en C. BRUNO ARTURO JUAREZ LEON
M. en C. LUCIO FRAGOSO SANDOVAL
MÉXICO D. F. NOVIEMBRE DE 2010.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ARQUITECTURA DE SOFTWARE PARA LA BÚSQUEDA Y CONSULTA DE RECURSOS DISTRIBUIDOS SBRE INTERNET</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El Internet se ha convertido en uno de los medios de comunicación más usado en el mundo que permite compartir información. La comunidad virtual ROPVO es una comunidad que realizar a la administración de los recursos oceanográficos mediante el SIBW-Pharos. Actualmente el sistema Pharos permite realizar búsquedas y consultas de datos oceanograficos en una sola computadora. Sin embargo la comunidad ROPVO desea compartir recursos oceanográficos almacenados en diferentes computadoras conectadas a
través de Internet, y Pharos tiene el problema de que no cuenta con un mecanismo que permita realizar búsquedas y consultas distribuidas de recursos oceanográficos sobre Internet. Como resultado de este trabajo se agrega al sistema Pharos la implementación de la ASBUCORDI.
Cada uno de los integrantes de la comunidad ROPVO es una organización independiente, por lo que ay que establecer un mecanismo adecuado de Búsqueda y Consulta entre los organismos de la comunidad ROPVO. Esto es, se debe considerar que las organizaciones de la comunidad virtual se agregan y remueven constantemente, por lo cual el mecanismo de búsqueda y consulta debe poder adaptarse a esta dinámica. Además se deben de tomar en cuenta los problemas de comunicación sobre Internet que se pueden presentar al realizar la búsqueda y consulta de recursos oceanográficos distribuidos. De esta manera, se hace necesario que la comunidad ROPVO cuente con una estructura lógica, que se lleva a cabo mediante el diseño de un esquema de metadatos, para la organización distribuida de sus recursos oceanográficos. Lo anterior plantea el problema de realizar un análisis que nos permita organizar los recursos oceanográficos de la comunidad ROPVO bajo un esquema de metadatos, que pueda ser útil para el mecanismo de búsqueda y consulta distribuida de recursos. Una vez diseñado el esquema de metadatos, se proceder a a introducir estos en una base de datos que llamaremos de ahora en adelante Metabase de Datos.
Una vez organizados los metadatos de los recursos compartidos en una metabase de datos, un mecanismo de búsqueda distribuida requiere resolver el problema de encontrar a través de los metadatos los recursos solicitados a la comunidad virtual ROPVO. Esto es, a través de un mecanismo de consulta de recursos oceanográficos se deberá permitir a los usuarios consultar los recursos oceanográficos localizados por el mecanismo de búsqueda. Para ello, el mecanismo de consulta deber a estar disponible a través de Internet y debería permitir que usuarios inexpertos puedan consultar los recursos oceanográficos a través
del Web. Esto es, se debe permitir a los usuarios consultar recursos oceanográficos, de una manera transparente [7], almacenando los metadatos de los recursos en metabases de datos distribuidas en la comunidad ROPVO. Con respecto a los recursos de bases de datos administradas por el sistema Pharos, sus resultados deberían ser presentados al usuario a través de la Web, en forma de texto, o en gráficas.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar e implementar una arquitectura de software para la búsqueda y consulta de recursos oceanográficos distribuidos que se incorporar ¶a al proyecto ROPVO-GM y que servir ¶a como motor de transferencia de recursos a través de Internet mediante el uso de metadatos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Con base en una arquitectura peer-to-peer es factible diseñar e implementar un mecanismo de software que permita la búsqueda y consulta de recursos oceanográficos distribuidos en una comunidad virtual, utilizando una herramientas de software que sean totalmente libre y metabases de datos como esquemas de conectividad de datos.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los avances en la tecnología de la comunicación, las redes de computo, el almacenamiento y acceso distribuidos de datos están revolucionando internacionalmente a las interacciones científicas[]. Instituciones importantes en Estados Unidos como el Centro Nacional de Datos Oceanográficos (NODS), El Centro Mundial de datos para la Oceanografía, y el Centro para el Análisis de Información sobre Bióxido de Carbono y TCOON actualmente estan compartiendo sus recursos a trav És de Internet. Por otra parte, en
M Éxico se están realizando esfuerzos por recopilar y compartir recursos entre instituciones de la comunidad ROPVO, sin embargo, no se está llevando a cabo estrategias que permitan la compartición de recursos entre instituciones. Este trabajo de investigación propone estandarizar los procesos de almacenamiento de los recursos de las instituciones en bases de datos. Esto permitirá diseñar e implementar una arquitectura
de software que permitirá realizar búsquedas y consultas de recursos distribuidos en la comunidad y que estará disponible para el medio científico y al público en general a través de Internet. En estos momentos se están enfocando los esfuerzos en la recopilación de datos del nivel del mar y datos meteorológicos a lo largo de la costa y de los puertos del Golfo de México. Estos datos serán administrados por diferentes
nodos ROPVO-GM. Posteriormente se incorporará el Centro de Información TCOON, esto ampliará los margenes de información de datos y fomentará el intercambio de ideas y apoyo mutuo entre naciones.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La planeación y desarrollo del proyecto de investigación demostró la importancia del esquema de metadatos estantar FGDC, como método que permite la organización e identificación de los recursos oceanográficos compartidos en los nodos ROPVO mediante un mecanismo de búsqueda automatizado, que al ser implementado en una metabase de datos hace posible la compartición y extracción de recursos oceanográficos distribuidos a través de Internet. Para la adaptación correcta del estándar FGDC a nuestro esquema de
metadatos propuesto en la sección 4.1.1, fue necesario agregar algunos metadatos particulares como idstation, idresource, station name y relation.
La comunidad ROPVO ahora cuenta con un mecanismo que le permite compartir recursos oceanográficos a través de Internet. Estos recursos se encuentran almacenados en diferentes Bases de Datos. Los recursos de las instituciones que participan en la red ROPVO pueden almacenar y compartir recursos que se encuentran almacenados en texto, imagenes, video, sonido, páginas Web o bases de datos. Los cientáíficos
del área de ingeniería oceánica y costera ahora cuentan con una herramienta poderosa para la búsqueda y consulta de recursos oceanográficos distribuidos sobre Internet.
La herramienta de búsqueda y consulta de recursos distribuidos es un medio de consulta de datos oceanográficos útil para la sociedad en general, ya que apoya a las instituciones encargadas de la seguridad en los puertos y protección civil al informarles del clima oceánico del Golfo de México en tiempo casi real. Gracias a esta nueva tecnología, las Instituciones interesadas en compartir recursos oceanográficos tienen la posibilidad de compartir su información a través de Internet.
Los resultados exitosos obtenidos de nuestra herramienta en el proceso de búsqueda y consulta de recursos oceanográficos distribuidos en la comunidad ROPVO, ver sección 5.1. demostró que es factible el desarrollo de aplicaciones distribuidas mediante el uso de software libre.
La implemetación de  Ésta herramienta diseñada bajo una arquitectura peer-to-peer, demostró ser una herramienta factible para el intercambio de recursos oceanográficos distribuidos en la comunidad ROPVO, además esta arquitectura permitió reducir los costos monetarios en equipo de computo, aproximadamente a $1,200.00 por cada nodo conectado a la comunidad. Debido a su naturaleza, nuestra herramienta demostró ser un sistema altamente tolerante a fallos a pesar de la actividad intermitente de los nodos de la comunidad ROPVO, evitando así la caída total del sistema.
El empleo de cookies fue de grán importancia en el desarrollo de este proyecto ya que fu É el medio utilizado para el paso de parámetros entre páginas Web. La utilización de la interfaz de comunicación socket fu É la clave para la comunicación entre aplicaciones de software realizadas en diferentes lenguajes de programación.
El SMBD MySQL demostró ser una herramienta eficaz para el almacenamiento de los metadatos y recursos de la comunidad ROPVO. Sin embargo, Por defecto las bases de datos creada por MySQL no cuenta con el permiso de acceso remoto desde otras computadoras, por lo que fue necesario establecer estos permisos de acceso mediante los comandos setpasswordfor, quien permite establecer la contraseña de acceso a base de datos y grantallprivilegeson, quien permite el acceso al usuario desde una computadora remota.
Finalmente, el sistema Pharos desarrollado por DNR[3], es ahora ampliado con un mecanismo que le permite realizar extracciones y consultas de datos oceanográficos distribuidos sobre Internet.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN CIENCIA APLICADA Y TECNOLOGÍA AVANZADA UNIDAD ALTAMIRA
ARQUITECTURA DE SOFTWARE PARA LA BÚSQUEDA Y CONSULTA DE RECURSOS DISTRIBUIDOS SBRE INTERNET
TESIS
QUE PARA OBTENER EL GRADO DE MAESTRO EN TECNOLOGÍA AVANZADA
PRESENTA
ING. JESÚS MANUEL BORREGO IBARRA
DIRECTOR DE TESIS:
DR. EUSTORGIO MEZA CONDE
ALTAMIRA, TAMPS. 
AGOSTO DE 2008</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>MODELADO DE SOFTWARE CON LOGICA DIFUSA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Para llevar a cabo estas tareas la Dirección de Investigación tales como se encarga de
llevar el registro, dar seguimiento y asignar recursos a los más de 1200 proyectos de
investigación (científicos y tecnológicos, propuestas de estudio y educativos de manera
individual y en programa) que se realizan en las diferentes escuelas de nivel medio
superior, superior y centros de investigación del IPN, se apoya en un sistema de
información que se encuentra en funcionamiento desde el 2003 denominado: Sistema
de Administración de Programas y Proyectos de Investigación (SAPPI): Un sistema de
información que está basado en tecnologías Web y en una base de datos centralizada.
Uno de los criterios que son considerados para asignar recursos a los proyectos de
investigación es la calificación de productividad obtenida en la ficha de productividad
del investigador. La etapa de registro de la ficha de productividad que contempla
SAPPI no aplico a partir del 2005 debido a que la base del conocimiento de SAPPI es
estática y en consecuencia no pudo adecuarse a los cambios y nuevos requerimientos,
lo cual implico:
- La recepción en papel de fichas de productividad.
- Evaluación de alrededor de mil quinientas fichas de productividad
- Captura de los diez criterios de evaluación de las fichas de productividad,
para un polinomio que dicta una evaluación.
Una de los requisitos para que un sistema de información se pueda adaptar a los
cambios es que su base de conocimiento sea dinámica dentro de intervalos difusos.
Con lo anterior se llego a la conclusión de que era necesario realizar el rediseño del
sistema que permitiera el registro de la ficha de productividad en línea y la idea de
aprovechar esta información registrada para poder diseñar un sistema de validación y
calificación de la ficha de productividad. Con lo cual llegamos a la siguiente pregunta
 ¿Cómo realizar un sistema que responda a las variables lingüísticas y a la base del
conocimiento generada por los evaluadores y asignadotes de recursos de la SIP que
han dado resultados en la mayoría de las veces de manera confiable sin perder las
propiedades del SAPPI? Considerando para ello el requerimiento de menos tiempo de
evaluación y asignación, usando a las tecnologías de información, permitiendo:
*	Registrar en línea la ficha de productividad.
*	El personal de la SIP podrá revisar las fichas de productividad de cualquier año
con solo capturar la clave del proyecto o seleccionando la escuela.
*	El analista validará en línea contra documentos aprobatorios lo que registro el
investigador y registrara sus observaciones.
*	Una vez validada la ficha por los analistas de la DI será calificada en
automático, evitando cualquier error y eliminado el tiempo que se requiere para
llevar a cabo la calificación manual y captura de puntajes por rubro de las más
de 1500 fichas q son registradas año con año que se lleva más de un mes.
*	Mantener un histórico de la productividad del investigador.
*	Permitir la generación de los siguientes reportes: tesitas, patentes, libros,
etc.,realizados en el instituto, investigadores con becas COFFA y EDI entre otras.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un modelo de software utilizando lógica difusa de acuerdo a la base de
conocimiento en base a la información de la Secretaría de Investigación y Posgrado
(SIP) para implantar un modelo de evaluación a través de variables lingüísticas para
medir la productividad de los investigadores del instituto y generar una base de
conocimiento dinámica para la asignación presupuestal a los proyectos de investigación.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible aplicar los principios de la lógica difusa para automatizar los procesos de
evaluación de la ficha de productividad y asignación presupuestal, que se realizan en la
Dirección de Investigación del IPN debido a que son procesos con métricas difusas.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los sistemas basados en lógica difusa imitan la forma en que toman decisiones los
humanos, con la ventaja de ser operativamente más rápidos y ajustables dinámicamente
.Estos sistemas son generalmente robustos y tolerantes a imprecisiones y ruidos en los
datos de entrada [1]. Es por ello que aplicaremos la lógica difusa en el diseño un modelo
de software que en su base fue desarrollado a través de las experiencias de los analistas,
autoridades y demás personal de la SIP, requiriendo que el sistema de información
tenga un dinamismo de acuerdo a la evolución de los parámetros que permiten observar
una productividad y asignación presupuestal.
Rediseñar la etapa del registro de la ficha y agregar la validación y evaluación en
automático de la ficha de productividad permite validar al proyecto y determinar el
presupuesto que le será asignado: SAPPI en su estructura, automatiza procesos,
conserva la integridad de la información y evita su duplicidad. Requiriendo integrar a su
diseño una base de datos dinámica que permita al SAPPI adaptarse de acuerdo a la base
de conocimientos evolutiva.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para llevar a cabo el desarrollo de este sistema se realizaron los siguientes pasos:
1. Desarrollo de la base del conocimiento de la ficha de productividad, en base a la
experiencia del personal de la SIP.
2. Determinar las variables lingüísticas y las propiedades del polinomio difuso
considerando las propiedades marcovianas.
3. Construir los módulos de software para evaluar la productividad de acuerdo a la
base del conocimiento y variables lingüísticas previamente establecidas.
4. Desarrollar la base del conocimiento dinámica para la asignación presupuestal,
considerado a las variables lingüísticas.
5. Determinar las variables lingüísticas y las propiedades del polinomio difuso sin
perder sus cualidades de estabilidad.
6. Construir los módulos de software para la asignación presupuestal a proyectos
de investigación para proporcionar una interfaz amigable entre el sistema y los
evaluadores así como a los asignadotes de presupuesto.
7. Enlazar los módulos de software a través del sistema de información SAPPI.
8. Poner en operación el modelo de software y mejorar su operación a través de las
evaluaciones que realizan los usuarios del mismo, en su desempeño,
considerando a funciones de membresía resultantes de las propiedades
estadísticas de las evaluaciones.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Actualmente existen en el mercado numerosos sistemas como SAPPI sin embargo
ninguno se ajusta a las necesidades específicas de la Secretaría de Investigación y
Posgrado. Lo interesante de realizar un proyecto de este tipo implica dar solución a un
problema real, asimilación de tecnología, desarrollo de software a la medida y lo
principal servir de modelo para poder identificar características similares de este tipo de sistemas y desarrollar un generador de sistemas de información que con pocas
adaptaciones se convierta en software a la medida aprovechando los principio de la
lógica difusa para modelar matemáticamente el sistema, identificar su base de
conocimiento, variables lingüísticas y funciones de membresía .
En este capitulo se llevo a cabo el planteamiento del problema en el cual se cuestiona si
existe una relación entre la lógica difusa y el diseño de sistemas de información. Se hizo
una descripción de la metodología para llevar a cabo esta relación, estableciendo que el
principal objetivo de la tesis es desarrollar un modelo de software utilizando lógica
difusa de acuerdo a la base de conocimiento en base a la información de la Secretaría de
Investigación y Posgrado (SIP) para implantar un modelo de evaluación a través de
variables lingüísticas para medir la productividad de los investigadores del instituto y
generar una base de conocimiento dinámica para la asignación presupuestal a los
proyectos de investigación de acuerdo a la hipótesis de que es posible aplicar los
principios de la lógica difusa para automatizar los procesos de evaluación de la ficha de
productividad y asignación presupuestal, que se realizan en la Dirección de
Investigación del IPN debido a que son procesos con métricas difusas
En este capítulo se desarrollo de la base del conocimiento de la ficha de productividad
concentrada en la Tabla 1, se determino que las variables ligüísticas están definidas por
los rubros que se evalúan en la ficha de productividad del investigador (Tabla 1) como
son: número de patentes, libros, tesitas, etc. Así mismo las propiedades del polinomio
difuso. Se diseñaron y construyeron los de los módulos de software para evaluar la
productividad misma que sirvió para el desarrollo de la base del conocimiento dinámica
para la asignación presupuestal. Se determinaron las variables lingüísticas y las propiedades del polinomio difuso para el módulo de asignación presupuestal y se diseño
y construyó el módulo de asignación para llevar a cabo esta asignación.
En este capítulo se discutió y se llevo a cabo el análisis de los resultados con las
preguntas:  ¿por qué no utilizar un software de uso específico?,  ¿qué desventajas tiene
este desarrollo?, ¿cuáles son sus principales avances tecnológicos respecto a su
operación tradicional?,  ¿qué tipo de tecnología de software se uso y por qué esa
tecnología?, ¿qué criterios hicieron que este proyecto fuera viable para la SIP y otros?
,  ¿qué tipo de plataforma es necesaria para usarlo?, ¿qué tipo de experiencia técnica para
usarlo? . Finalmente se platearon cuales fueron los resultados básicos tangibles.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Ciencia Aplicada y Tecnología Avanzada
CICATA
Unidad Legaría
MODELADO DE SOFTWARE CON LOGICA DIFUSA
TESIS
Que para obtener el titulo de:
Maestro en Tecnología Avanzada
Presenta:
Ing. Consuelo Varinia García Mendoza
Director de Tesis:
César Saúl Guzmán Rentería
Codirector de Tesis:
Dr. José de Jesús Medel Juárez
México Distrito Federal, Noviembre de 2006</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“DETECCIÓN Y PREVENCIÓN DE INTRUSIONES USANDO REDES NEURONALES RECURRENTES"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Una computadora es una herramienta auxiliar muy útil y cada vez más necesaria en la vida diariade los individuos, es un apoyo de diferentes actividades humanas y actividades empresariales como realizar compras por Internet, realizar pagos de sus cuentas bancarias, negocios, etc. La computadora es además un medio eficaz para obtener y conseguir información por medio de redes de comunicación interconectadas, conocido como Internet, la cual permite acelerar el desarrollo informático. La desventaja que presenta el amplio uso de “Internet"  es que con el incremento del tráfico de la red, también incrementan conductas dirigidas a causar daños a los sistemas informáticos donde participan diversos intrusos que intentan llevar a cabo objetivos tales como: acceder a la información sin estar autorizados para ello, destruir, ocultar, alterar los datos contenidos en un sistema informático.
El incremento de los delitos informáticos en la red que son mayores a 16,000 incidentes al año[1], se debe a motivos de curiosidad, que son conductas dirigidas a causar daño en el hardwarey/o software de un sistema, beneficios económicos indebidos, destrucción de programas o datos,creación de datos falsos, acceso no autorizado, en fin hay una gran cantidad de circunstancias que llevan a cometer delitos informáticos.
Algunos delitos informáticos se introducen por medio de los que se conoce como puertas
traseras, las cuales el intruso puede acceder o escapar a través de un programa después de su creación. Entre otros se tienen los agujeros de seguridad en los sistemas operativos, agujeros de seguridad en las aplicaciones, errores en las configuraciones de los sistemas y usuarios que carecen de información de forma alterna; así como al hecho de que en muchas ocasiones la información viaja a través de las redes sin ser cifrada. Esto hace que los servicios no sean seguros.
Debido a los problemas que experimentan actualmente las tecnologías informáticas nació la
ciencia “seguridad informática" , la cual tiene por objetivo proteger los sistemas de información contra el acceso no autorizado y la modificación de información que permitan encontrar las evidencias de que la información fue manipulada requeridas por un experto. El experto de seguridad debe analizar la información contenida en un sistema informático o del tráfico de la red, la cual en la mayoría de los casos una cantidad de información extensa, para encontrar evidencias de que se ha cometido un delito informático. Sin embargo, no toda la información capturada es útil para el análisis, lo cual representa una tarea difícil debido a que el tráfico de la red cada día incrementa. Con ello la cantidad de información que debe ser revisada por los investigadores. El trabajo de los expertos de seguridad se basa en analizar las acciones realizadas por los atacantes para seguir las pistas del mismo; ya que no existen suficientes métodos establecidos para la reconstrucción del escenario del sistema informático y de la red. Aunque existen algunos métodos, herramientas, aplicaciones y modelos que facilitan la tarea de la
detección de intrusos de la información contenida en la red; los cuales permiten la obtención de evidencias. Esto no es simple ya que la información alterada tiene la característica de estar oculta y ser imprecisa dentro de los flujos de información que aparentan ser normales. Así para detectarla se han desarrollado diversos algoritmos de grafos, algoritmos de lógica difusa [2], algoritmo Leader-Follower, mapas cognitivos basados en reglas difusas (RBFCM) [3] métodos del espectro de Laplace y basados en el núcleo (kernel), efectos de visualización [4], que pretenden cumplir el objetivo de detector de intrusos.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar e implementar un algoritmo automático y eficiente para la detección y prevención de ataques de red que obtenga un porcentaje de aciertos mayor a 95 % usando RNN en el análisis de la red.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Por medio de un sistema Neuronal, el cual es una técnica que puede identificar la información anormal que presente la evidencia derivada de una acción del intruso. Una adecuada caracterización de la misma junto con un algoritmo de aprendizaje y proceso automático permitirá alcanzar el objetivo de detectar ataques dentro de la red.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El presente investigación se realizará ya que la necesidad de adquirir una detección de ataques en una red informática hoy en día es importante, ya que en la seguridad informática cuentan con métodos para el análisis de información, pero para esto debe tener un porcentaje alto en el reconocimiento de las evidencias.
Por los motivos anteriores el implementar un algoritmo Neuronal para un sistema de seguridad informática es muy útil ya que nos ayudará a hacer una detección y prevención de intrusiones mucho más agiles, dar a conocer la evidencia que en esta desempeña y facilitar el trabajo de un experto de seguridad.
Hoy en día, las detección de intrusiones nos ayuda a facilitar el análisis y reconocer las
evidencias con mejores resultados ayudando a tomar una decisión para actuar sobre estas,
adquiriendo una prevención e incrementando la calidad de seguridad informática.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con el avance de las tecnologías informáticas, el apartado de la seguridad ha ido cobrando cada vez más valor, hasta convertirse en uno de los aspectos más relevantes de esta era. La detección de intrusiones reúne muchos de los elementos necesarios para convertirse en el pilar fundamental del futuro de la seguridad. Su capacidad de análisis y correlación de los datos obtenidos de las fuentes, posiblemente hará de esta tecnología una de las más importantes del sector.
Las principales contribuciones del trabajo se encuentra en: la caracterización de patrones; realizando diferentes escenarios de pruebas con diferentes herramientas, sistemas operativos, y redes IP, el mapeo de datos contenidos en una captura de información de red, elección de patrones para el entrenamiento de RNN; es decir, los parámetros que se relacionan para definir un ataque, el desarrollo e implementación de un algoritmo eficiente y automático aplicada para la detección de intrusiones para agilizar el trabajo del investigador de seguridad, que se dieron a conocer con los resultados de reconocimiento de evidencias y el tiempo en que realiza la detección. El labor del experto de seguridad consiste principalmente en reducir la carga de trabajo de los responsables de seguridad, realizando el análisis de los datos disponibles. La detección de intrusiones complementa y fortalece la seguridad global de cualquier infraestructura.
En este proyecto, se presenta un algoritmo de RNN para la implementación del reconocimiento de evidencias para la seguridad informática. Se realizaron una serie de pruebas para la identificación de evidencias, usando un grupo de capturas con ataques y capturas de flujo normal del tráfico de red para el aprendizaje del algoritmo RNN y las pruebas. Los resultados indicado en el capítulo de pruebas y resultados muestran que una red neuronal entrenada con algoritmos de RNN mejora el rendimiento de la detección de intrusos del sistema de seguridad informática con el uso de la red con recurrencia.
Este proyecto tiene una mejoría respecto al reconocimiento de evidencias ya que al comparar con el sistema propuesto por Jung Sun Kim, Dong Geun Kim y Bong Nam Noh [2] tiene el 93.8% de eficiencia y tiene una alteración en la lectura de los números de paquetes para 3 ataques, la cual el sistema propuesto de este tema de trabajo varían desde un 98.75 % hasta el 100% con una posible alteración en una captura de todo el tráfico de red. En codificar y mapear 3 parámetros de los protocolos, la cual codifica la información para el entrenamiento, una RNN simple con las capas y neuronas necesarias consigue la característica de eficiencia en la etapa de la detección de intrusiones para la extracción de evidencias.
La metodología de investigación fue buena, la cual se consiguió un proceso ordenado, la cual en cada proceso se presentaban nuevas necesidades como el caso de la reducción de información para el entrenamiento de la red neuronal.
Existen diferentes tipos de redes neuronales, las cuales se realizaron algunas pruebas con ella, no era necesario desarrollar un algoritmo RNN complejo ya que una red neuronal simple puede satisfacer la necesidad de una aplicación, en este caso del trabajo se cumplió con el objetivo de agilizar el trabajo de un experto de seguridad.
Se logra la identificación de la información anormal que presenta la evidencia derivada de una acción del atacante, por medio de un sistema Neuronal, la cual es una adecuada caracterización de un algoritmo de aprendizaje y proceso automático que alcanza el objetivo de reducir el tiempo y costo del trabajo empleado en la etapa del análisis forense de la red del investigador forense.
Existe un par de aspectos pendientes de ser mejorados, considerados como trabajo a futuro
próximo son: reconstruir flujos de datos del tráfico de la red a partir de un esquema de monitoreo
e incluir más fuentes de información de otros protocolos de comunicaciones o ataques
informáticos para la detección y prevención de intrusiones y realizar pruebas con otros
escenarios.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE INGENIERÍA MECÍNICA Y ELÉCTRICA
UNIDAD CULHUACÍN
SECCIÓN DE ESTUDIOS DE POSGRADO E INVESTIGACIÓN
“DETECCIÓN Y PREVENCIÓN DE INTRUSIONES USANDO REDES NEURONALES RECURRENTES" 
TESIS
PARA OBTENER EL GRADO DE:
MAESTRO EN CIENCIAS DE INGENIERÍA EN MICROELECTRÓNICA
PRESENTA:
ING. ARACELI BARRADAS ACOSTA
ASESORES:
DR. HÉCTOR PÉREZ MEANA
M. EN C. ELEAZAR AGUIRRE ANAYA
México D.F.
NOVIEMBRE 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>INTEGRACIÓN DE SOFTWARES CAD/CAPP/CAE PARA EL DISEÑO Y PLANEACION DE PROCESOS DE PRODUCCIÓN DE RECIPIENTES A PRESION</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Integrar los softwares CAD/CAPP/CAE para el diseño y planeación de procesos de producción de Recipientes a Presión.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Uno de los principales problemas que han obstaculizado la integración de los sistemas y los departamentos en las empresas, es la dificultad de relacionar de una manera eficiente la información del control de manufactura con los empresariales.
El inconveniente no estriba en la tecnología computacional, sino en la diversidad cultural de los individuos que intervienen en estas áreas. Por una parte, los ingenieros que provienen del medio de la tecnología de información (Ingenieros en Informática) están familiarizados básicamente con los procesos de negocios, poco ó nada entienden de los procesos de manufactura o de transformación. Por otro lado, los ingenieros que provienen de las plantas productivas, no tienen una idea clara de los requerimientos de los sistemas empresariales, ni saben de la importancia que tiene la información en los niveles superiores para una gestión exitosa en la toma de decisiones. También, es usual que estos individuos no compartan los mismos factores críticos de éxito, tienen puntos de vista diferentes sobre lo que es importante y, aún más, muchos términos técnicos empleados en su vocabulario tienen diversos significados entre ellos.
Por eso, la tecnología ha sido uno de los principales motores para generar cambios en los modelos de los negocios a los que las empresas se están adaptando, buscando lograr ventajas competitivas mediante la disminución de costos y el incremento en el nivel de servicio al cliente. Y desde hace más de tres décadas, las empresas han descubierto que la integración, tanto de áreas ó procesos como entre organizaciones, ha sido un medio muy importante para lograr sus metas comunes de éxito y llevarlas a cabo. Existe un elemento común mas allá del giro o del tamaño de las mismas: LA INFORMACIÓN, que sea veraz, precisa y a tiempo para tener una acertada toma de decisiones.
La preocupación de todos los empresarios, es tener participación y competitividad en el mundo de negocios, se están acercando con las firmas de Software para que les solucione sus problemas de integración en sus sistemas de manufactura, y aunque estas firmas le dan algunas soluciones, las inversiones que se requieren son muy grandes, todo queda en sueños y buenas intenciones. Estos empresarios no se dan cuenta que muchas de las soluciones a sus problemas de productividad tanto administrativa como productiva, se encuentran dentro del personal de su organización, solo hace falta un intercambio de comunicación entre ambos, y muchas ganas de hacer cambios en la empresa.
En los últimos años se ha visto un renacimiento de la productividad, como una respuesta a la creciente competencia por el mercado. Esta competencia ha dado lugar a la expansión de los mercados, al adelgazamiento empresarial y a la racionalización de la manufactura y con un buen cimentado concepto de todo lo relacionado a Manufactura, conocer las nuevas tendencias y necesidades tecnológicas modernas para entrarle a la alta competitividad mundial, saber a que se refieren las modalidades de “Justo a Tiempo" , “CIM" , “CEP" , “Manufactura Flexible y Esbelta" , “Sistemas CAPP" , etcétera. Las PyMEs pueden entrarle a este juego de la competitividad mundial, cuidando que la calidad no esté ausente en ningún momento de la Administración de la Producción ó mejor decirlo, estar siempre en la mejora continua en los Procesos de Manufactura.
Debido a que, en el mercado de los softwares y de los sistemas CAPP, la mayoría de estos están enfocados a los sistemas de producción de maquinados y a maquinaria con CNC, además que, las empresas que tienen estos sistemas, son las que tienen la capacidad de invertir grandes cantidades de capital y con la visión hacia las PyMEs que adolecen de ese capital, se enfocó el desarrollo de este proyecto hacia la manufactura de recipientes a presión. Ahora bien, con la integración de estos softwares CAD/CAPP/CAE como el de este proyecto, se puede simplificar muchos trabajos en las empresas, tanto administrativos como operativos (trabajos de taller), reduciendo el tiempo ciclo en la generación de información tanto de diseño, dibujos, cotización y hoja de procesos de producción.
Y con la evaluación de resultados, en primera instancia, aumentamos radicalmente la productividad en los procesos administrativos, desde la creación de información para la cotización al cliente (sin duplicar la introducción de datos en los demás departamentos), hasta que se genera la información hacia el taller. Esta productividad fue principalmente, reducir este tiempo de 35 a 4 días para obtener la información mínima necesaria para la cotización y el área de fabricación y comenzar con la manufactura de los recipientes.
Además, podemos hacer extensivo estos sistemas CAPP hacia las demás áreas administrativas como Costos (Contabilidad), Almacén, Adquisiciones (Compras), etc, podríamos tener un sistema: Potente desde el punto de vista del proceso administrativo; Delgado desde el punto de vista de la mano de obra que intervendrían en el proceso (que serían quizás 3 personas a lo más) y por último; Flexible, por su disponibilidad para los cambios y la versatilidad de tipos de recipientes que se podrían trabajar en este software.
Además de tener la información disponible en el momento, se evitan arrastrar y acumular errores en el proceso administrativo de un departamento a otro, y lo más importante, la vinculación e integración de información entre todos los departamentos de la empresa.
Ahora bien, con el desarrollo de este trabajo bajo un Sistema CAPP, se comprobó que al tener un sistema bien interrelacionado y vinculado entre las áreas de toda la empresa, se puede tener una alta productividad entre departamentos, con una radical disminución de Tiempo Ciclo en la creación de la información para la ejecución de la Manufactura en los Recipientes a Presión.
Esto es, se cumplen con los objetivos trazados al inicio de esta tesis de investigación, en primer lugar, se comprobó que se puede tener una estrecha integración, vinculación e interrelación de datos e información entre áreas, además, se pueden emigrar (ó arrastrar) estos datos entre softwares que antes eran independientes tanto entre ellos mismos como por departamento.
Ahora con este sistema, al reducir drásticamente el Tiempo Ciclo del Proceso Administrativo, convertimos este sistema en una Manufactura Esbelta, evitando el traslado de información en papel entre departamentos, al mismo tiempo con menor personal involucrado, obteniendo mejores resultados con menos recursos y esfuerzos, ósea más productivos.
Además, se recomienda ampliamente que las PyMEs empiecen por acercarse a sus socios de negocios para compartir planes e ideas comunes, y de esta manera llevar a cabo este proceso de infraestructura tecnológica barata y al alcance de ellos pero con orden y así dirigir los esfuerzos de todos hacia un mismo objetivo: “Una Verdadera Ventaja Competitiva" .
Se tiene un gran campo de acción en México para poder implementar un sistema CAPP de este tipo, con la integración de los softwares comerciales (que se manejan independientes) para manejar una sola información electrónica, invirtiendo pocos recursos, dándole una gran fuerza a todo el proceso de Manufactura dentro de la organización. Esto es, porque los productores de las PyMEs no están preocupados por invertir, sino por mantener activas las ordenes de compra de sus clientes y, en consecuencia, cumplir con la nómina, aunado a esto, los dueños de las PyMEs no reconocen el valor de invertir en tecnología ó capacitación y, además no cuentan con instalaciones adecuadas bajo los esquemas de calidad requeridos en el mercado actual. El problema de las PyMEs de nuestro país parece un círculo vicioso, no hay dinero para invertir, y se está perdiendo eficacia con respecto a otras empresas del mundo, lo que representa un alto riesgo en el entorno de feroz competencia global y desventaja para conseguir nuevos clientes e, incluso, conservar a los ya existentes. El panorama no es fácil para el sector de las PyMEs, por lo que la creatividad, la flexibilidad, la pro-actividad y la integración de los departamentos serán fundamentales para superar, en parte el rezago con otras tecnologías y por otro tener las herramientas para entrarle a la competitividad mundial.
Finalmente, en base a la experiencia vivida en el desarrollo de este trabajo de tesis, me permito sugerir a mis colegas que me sucedan (futuras generaciones), ampliar y profundizar estudios de este tipo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
UNIDAD PROFESIONAL INTERDISCIPLIARIA DE
INGENIERIA, CIENCIAS SOCIALES Y ADMINISTRATIVAS
SECCION DE ESTUDIOS DE POSGRADO E INVESTIGACION
INTEGRACIÓN DE SOFTWARES CAD/CAPP/CAE PARA EL DISEÑO Y PLANEACION DE PROCESOS DE PRODUCCIÓN DE RECIPIENTES A PRESION
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN CIENCIAS CON
ESPECIALIDAD EN INGENIERIA INDUSTRIAL
P R E S E N T A:
FERNANDO GARCIA PINEDA
MÉXICO, D.F. MAY 2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DIFRACCIÓN DE HACES POR REDES DE RONCHI FINITAS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En esta tesis hemos tratado la difracción de haces Hermite-Gauss por redes de Ronchi finitas e infinitas
usando la versión bi-dimensional de la ecuación de Rayleigh-Sommerfeld. Hemos concentrado nuestra atención
en la región escalar, donde los efectos de polarización no son importantes. Encontrando que la teoría es
aplicable a cualquier tipo de onda incidente que presente simetría cilíndrica.
Al hacer el tratamiento de redes finitas e infinitas hemos encontrado que el ancho angular aumenta al
pasar de una red infinita a una red finita, y sucede lo mismo también en el espectro de un haz con ancho finito,
en nuestro caso haces Hermite-Gauss.
Por otra parte se ha encontrado que si hacemos incidir un haz Hermite-Gauss de orden se presentan m
mínimos para cada orden . Además la influencia de la posición del haz al incidir en el centro de la red obtiene
su máximo valor.
m
n
Hemos encontrado que la ecuación de redes de difracción en transmisión predice la posición del máximo
o mínimo central para cada orden de los haces Hermite-Gauss de orden , teniendo en cuenta la paridad de
.
n m
m
Cuando se presenta un defecto (tres veces mayor que cualquiera de las rendijas) en una red infinita o
finita el patrón de difracción mantiene la misma forma para ambos casos. Sin embargo cuando se presenta este
mismo caso pero el tamaño del defecto es 0.3 veces menor que cualquiera de las rendijas podemos observar que
a excepción del orden se tiene que dado un haz Hermite-Gauss de orden que incide en este tipo de red,
se tiene mínimos para cada orden en el espectro correspondiente.
n = 0 m
m +1 n     0
Para el experimento de Young se tomó la separación entre las rendijas 10 veces mayor que cualquiera de
las rendijas y observamos que la envolvente de los espectros para los distintos tipos de haces Hermite-Gauss
incidentes es muy parecida a la difracción de dos rendijas. Sin embargo al disminuir la separación de las
rendijas de tal modo que sea del tamaño de cualquiera de las rendijas notamos que los máximos más cercanos al
máximo central aumentan su intensidad y tienen poco menos de la mitad del máximo central.
Por otra parte, notamos que al refinar el experimento de Young y aumentar el número de rendijas, sin
importar el tamaño total de la red, se comienzan a agrupar en lugares bien definidos y después poseen cierta estructura muy parecida al espectro de una onda plana incidiendo sobre una red infinita. Sin embargo, cuando se
aumenta el número de rendijas manteniendo el tamaño original de la red se observa que la mayor parte de la
intensidad comienza a ubicarse alrededor de 0 º a medida que se aumenta el número de rendijas.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE FÍSICA Y MATEMÍTICAS
DIFRACCIÓN DE HACES POR
REDES DE RONCHI FINITAS
Tesis que para obtener el grado de Maestro en
Ciencias con especialidad en Física
Presenta:
Sergio de la Cruz Arreola
Director de Tesis:
Dr. Oscar R. Mata Méndez
Codirector :
Dr. Jaime Ortiz López
México, D.F., Agosto de 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema de construcción de redes semánticas con detección de anáfora</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo principal de esta tesis es implementar el método y la herramienta (el
sistema) para resolución de anáfora pronominal para el español para construcción
de redes semánticas. Asimismo hacer una evaluación del método implementado.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El problema de la resolución de la anáfora ha sido durante años, una
preocupación en la comunidad de Procesamiento de Lenguaje Natural (PLN). Esta
tarea es considerada por muchos, como una de las más importantes y, ha sido
afrontada desde distintos puntos de vista en una variedad de sistemas de
cómputo. Algunos métodos, de conocimiento limitado (knowledge poor), resuelven
la anáfora sin realizar un análisis sintáctico o, realizando análisis parciales o
completos. Los trabajos realizados para el inglés (Hobbs, 1978; Lappin y Leass,
1994; Kennedy y Boguraev, 1994; Baldwin, 1997; Mitkov, 1998) y para el español
(Fernández, 1998; Palomar et al. 2001), coinciden (aunque no hacen uso de ella)
en la necesidad de la semántica como fuente esencial para la adecuada
resolución de la anáfora.
Debido a esta necesidad, diversos autores han planteado métodos de resolución
enriquecidos que combinan la semántica y la sintaxis, lo hacen para el inglés, en
dominios restringidos con definiciones puramente manuales de jerarquías y
rasgos. De igual manera, otros métodos alternativos, incorporan los papeles
sintácticos en patrones de co-ocurrencia mediante estrategias puramente
estadísticas (Dagan e Itai, 1991).
No existe algún método de resolución de anáfora para el español que no use
mucha información semántica (que no está disponible todavía para el español), sin
embargo, se basa en la información sintáctica.
En el Laboratorio de Lenguaje Natural existe un parser sintáctico para el español
que usamos para generar la entrada de nuestro algoritmo de resolución de
anáfora.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Para la construcción de redes semánticas y generalmente para cualquier
procesamiento automático a nivel profundo, es necesario resolver anáfora en
textos.
Se desarrolló la herramienta (el sistema) que permite hacer esta resolución de
anáfora pronominal para el español.
Para eso se implementó el método de R. Mitkov para el español con las
modificaciones pertinentes. También una de las diferencias importantes es que en
nuestro caso usamos el analizador sintáctico a diferencia del método original.
Se hicieron pruebas que dieron 91.66% de precisión sobre un texto pequeño.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Computación
Maestría en Ciencias de la Computación
Laboratorio de Lenguaje Natural y Procesamiento de Texto
Sistema de construcción
de redes semánticas
con detección de anáfora
TESIS QUE PRESENTA
Lic. Omar Alejandro Olivas Zazueta
PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN
DIRECTOR DE TESIS
Dr. Grigori Sidorov
México, D. F., junio de 2006</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>IDENTIFICACIÓN DE FALLAS DE SOFTWARE: CASO DEL SISTEMA DE LLAMADAS DE EMERGENCIA “066"  DEL D.F.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar una metodología para la identificación de defectos (o fallas) de software del
sistema de llamadas de emergencia “066"  del D.F.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se puede argumentar que la aplicación del APP en las etapas iniciales de diseño de
software puede ser muy valiosa para prevenir fallas en las etapas de su implementación y
operación. En uno de los párrafos más citados del artículo por lejos más citado en la
bibliografía de la Ingeniería del Software, Frederick P. Brooks (1987), dice:
“La parte más difícil de construir un sistema es precisamente saber qué construir. Ninguna
otra parte del trabajo conceptual es tan difícil como establecer los requerimientos técnicos detallados, incluyendo todas las interfaces con gente, máquinas, y otros sistemas. Ninguna otra parte del trabajo afecta tanto al sistema si es hecha mal. Ninguna es tan difícil de corregir mas adelante... Entonces, la tarea más importante que el ingeniero de software hace para el cliente es la extracción iterativa y el refinamiento de los requerimientos del producto" .</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL ESCUELA SUPERIOR DE INGENIERÍA MECÍNICA Y ELÉCTRICA UNIDAD PROFESIONAL ZACATENCO “ADOLFO LÓPEZ MATEOS"  SECCIÓN DE ESTUDIOS DE POSGRADO E INVESTIGACIÓN PROGRAMA DE POSGRADO EN INGENIERÍA DE SISTEMAS MAESTRÍA EN CIENCIAS EN INGENIERÍA DE SISTEMAS “IDENTIFICACIÓN DE FALLAS DE SOFTWARE: CASO DEL SISTEMA DE LLAMADAS DE EMERGENCIA “066"  DEL D.F." 
TESIS QUE PARA OBTENER EL GRADO MAESTRO EN CIENCIAS EN INGENIERÍA DE SISTEMAS 
P R E S E N T A ING. YARELI PÉREZ DELGADO 
Director de Tesis 
Dr. JAIME REYNALDO SANTOS REYES 
Octubre 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“REDES NEURONALES RECURRENTES: PRINCIPIOS Y APLICACIONES"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Se han usados varias estructuras de redes neuronales para realizar diferentes tareas, tales como reconocimiento de patrones, identificación de personas y predicción de señales, etc. En la mayoría de los casos, se han usado redes neuronales multicapas hacia adelante simple con algoritmos de retropropagación. Aunque este tipo de redes neuronales han mostrado buen desempeño para realizar varias tareas, éste presenta como limitante que la señal de entrada no tiene una relación temporal entre sí. Sin embargo, este tipo de redes neuronales no han sido la mejor opción, cuando la señal de entrada tiene una relación temporal, ya que no considera esta relación en su funcionamiento.
Las redes neuronales hacia adelante con recurrencia entre algunas capas, se llaman redes neuronales recurrentes. Estas redes pueden manejar los datos en secuencias de tiempo de manera adecuada, debido a que dentro de este tipo de redes neuronales, se manejan simultáneamente señales de tiempos diferentes. Sin embargo, este tipo de redes neuronales no se han explotado suficientemente, debido a su complejidad en la adaptación de los pesos de conexión. Por lo tanto, en esta tesis se investiga y se analiza sobre estructuras existentes y algoritmos de aprendizaje de redes neuronales recurrentes, i.e. la red de Jordan (1986), la red de Elman (1990) y la red de William &amp; Zipser (1989). Finalmente, usando la red neuronal recurrente de William y Zipser(1989) con el algoritmo RTRL se desarrolla un sistema de reconocimiento de hablante independiente de texto, para mostrar la eficiencia de las redes neuronales recurrentes.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Como se había mencionado anteriormente, las redes neuronales multicapas hacia adelante simple sin recurrencia se han usado para resolver varias tareas y se consideran como un modelo de cerebro humano. Sin embargo las conexiones de axones en las neuronas biológicas están realizadas de manera exhaustiva, entre ellas y presentando en muchos casos conexiones que forman bucles. Estos bucles realizan una relación entre señal actual y señal anterior con cierto factor de olvido, por lo tanto se efectúa una predicción con señales anteriores.
Antes de nacer un niño está produciendo 240,000 neuronas por minuto en su cerebro como se observa en la figura 1.1. Si nos referimos al cerebro humano de un niño entre 0 y 5 años, encontramos que los axones se multiplican intensivamente justamente después del nacimiento, generando una maraña de conexiones entre neuronas. El niño entre 6 y 18 meses empieza a balbucear y después de 24 meses (siempre es variable) empieza a hablar. Hasta los cinco años el cerebro del niño puede aprender todos los sonidos de diferentes idiomas. Si solamente se le habla en un idioma sólo las interconexiones que se usan para dicho idioma permanecen. Lo anterior además de la necesidad de recordar nos indica que una red neuronal recurrente se acerca más a una neurona biológica. Aquí cabe mencionar que hay cerca de 50 tipos de neuronas biológicas.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se efectuaron una serie de experimentos de identificación de hablante, usando un conjunto de voces para entrenamiento y para prueba. Los resultados indican que una red entrenada con algoritmo neuronal recurrente (RTRL) mejora el desempeño de los sistemas de identificación en los que se usaron redes de retropropagación (BP). La exactitud de identificación de la red basados en cinco voces de prueba es de 94.4% para las redes RTRL mientras que es de 73.5% para redes BP. Sin embargo, la exactitud de identificación de una red RTRL es ligeramente inferior a las de la red RBF.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE INGENIERIA MECÍNICA Y ELÉCTRICA
UNIDAD CULHUACAN
SECCIÓN DE ESTUDIOS DE POSGRADO E INVESTIGACIÓN
“REDES NEURONALES RECURRENTES: PRINCIPIOS Y APLICACIONES
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN CIENCIAS DE INGENIERIA EN
MICROELECTRÓNICA
PRESENTA:
ING. OSCAR NOGUERA SÍNCHEZ
ASESORES: DRA. MARIKO NAKANO MIYATAKE
DR. HÉCTOR M. PEREZ MEANA
MÉXICO D. F. 31 DE MAYO DE 2006.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Determinación de la tenacidad a la fractura en capas borurizadas empleando redes neuronales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Aplicación de la inteligencia artificial en la evaluación de la fractura por
microidentación en capas de boruros de hierro empleando el método de redes
neuronales para la determinación del valor de tenacidad a la fractura.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Este trabajo de investigación nace de la inquietud de implementar el método de
redes neuronales al proceso de fractura por microidentación en capas
borurizadas. Actualmente, el empleo de la inteligencia artificial en procesos
termoquímicos ha tenido un gran auge. Con esto, es posible automatizar y
optimizar los valores de tenacidad a la fractura para diferentes materiales
expuestos a tratamientos de endurecimiento superficial. Este comentario es
válido por la existencia de más de 19 ecuaciones empleadas para la evaluación
de la fractura por identación en materiales cerámicos. Aún más, no se tiene
conocimiento bibliográfico de la implementación de modelos de redes
neuronales para la determinación de este parámetro mecánico en capas de
boruros de hierro.
La ventaja de implementar esta técnica de inteligencia artificial es la flexibilidad
del método. El entrenamiento y la validación de los valores de tenacidad a la
fractura obtenidos por redes neuronales, puede ser empleado para una amplia
gama de materiales endurecidos por el tratamiento de borurización, y poder
modificar los valores de entrada de la red (longitud de grieta, carga de
aplicación, valores de microdureza y longitudes de las diagonales de
identación) obteniendo resultados confiables con un porcentaje de error
mínimo.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. Se verificó el estudio experimental de borurización en pasta en aceros
endurecidos AISI 1045. Los parámetros experimentales de tiempo, temperatura
y espesor de pasta de boro que influyen en la cinética de crecimiento de las
capas boruradas de la fase Fe2B fueron tomados de [1]. Igualmente, se
retomaron los valores de microdureza (480 mediciones) realizados a diferentes
distancias de microidentación a partir de la superficie del substrato y los valores
de longitud de grieta producidos en los vértices de las identaciones. Los
resultados de la tenacidad a la fractura para las capas de boruros de hierro
fueron obtenidos de la ecuación 

K[c]= 0.028 ((E/H)^(1/2)) * (P/(C^(3/2))).

Las temperaturas
de tratamiento fueron de 920 y 950 °C, con tiempos de 4 y 6 horas y 5 mm de
espesor de pasta de boro. Por otro lado, la carga de aplicación en los ensayos
de microdureza fue de 200 gramos.
2. El método empleado para la determinación de la tenacidad a la fractura en
capas boruradas fue el de redes neuronales artificiales. Con la ayuda del
software MATLAB V7.0 se realizó un programa que simuló la red neuronal. Se
utilizó una topología de red que cuenta con tres capas. La primera capa
corresponde a las entradas, donde se vaciaron los datos de microdureza de la
fase Fe2B para diferentes distancias de microidentación, y la longitud de grieta
correspondiente a cada microidentación. La segunda capa corresponde a la
capa intermedia, la cual consta de 5 neuronas. La tercera capa corresponde a
la salida, donde se introdujeron los datos correspondientes a los valores de
tenacidad a la fractura de la capa. Se construyó una red para cada conjunto de
experimentos a las temperaturas de 920 y 950 °. Los resultados son obtenidos
con la mayor reducción posible, buscando convergencia de la red al realizar el
aprendizaje. Para introducir los datos experimentales de aprendizaje a la red,
fue necesaria la normalización de los valores.
3. Se realizó la evaluación del modelo de redes neuronales con los valores
experimentales de tenacidad a la fractura para el conjunto de experimentos a la temperatura de 1000 °C, con 4 y 6 horas de tratamiento y 5 mm de espesor de
pasta de boro. Esta serie de datos no fue utilizada en el diseño del modelo de
la red neuronal. La evaluación realizada determina la confiabilidad de la
modelación por redes neuronales en los valores de tenacidad de fractura de la
capa borurizada.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La determinación de la tenacidad a la fractura de capas de boruros de hierro
Fe2B, obtenida a través del proceso de borurización en pasta sobre la
superficie de aceros estructurales AISI 1045, ha sido evaluado mediante la
técnica del modelo de redes neuronales empleando el método de retropropagación
del error utilizando el software MATLAB V 7.0. La base de datos
para el entrenamiento de la red neuronal fue soportada por los datos
experimentales del tratamiento termoquímico a las temperaturas de 920 y
950 °C, con un tiempo de 4 y 6 h y un espesor de capa de carburo de boro de
5mm que rodeó a la superficie del substrato antes del tratamiento. Asimismo, la
validación del modelo de redes neuronales para la obtención del valor de KIC se
realizó con datos experimentales externos al entrenamiento de la red, a la
temperatura de 1000 °C con 4 y 6 h de tratamiento y 5 mm de espesor de pasta
de carburo de boro.
Se asumen las siguientes conclusiones:
a) Las microgrietas sobre la superficie de la capa Fe2B se generaron a través
de ensayos de microidentación Vickers con una carga constante de 0.2 kg.,
variando las distancias de aplicación de carga entre 15 - 45 m desde la
superficie hasta la interfase Fe2B/substrato. Sin embargo, la implementación
del modelo de grietas tipo radial-media para la determinación del valor
experimental de tenacidad a la fractura puede estar en duda, debido a que las
grietas tienden a iniciarse en forma superficial en los vértices de la
microindentación sobre la fase borurada. Es factible, utilizar modelos de
agrietamiento alternativos para el cálculo de KIC. Por otro lado, al tratarse el
boruro de hierro Fe2B como una fase intermetálica estequiométrica, los valores
de dureza a lo largo del espesor de capa de la fase borurada no son
constantes, modificando el valor de la tenacidad a la fractura a las diferentes
distancias de la superficie donde se realizaron las microindentaciones.
b) El modelo de redes neuronales fue construido a través del software MATLAB
V7.0, tomando como valores de entrada la microdureza de la fase borurada y la longitud de grieta producida por la identación Vickers, y como valor de salida, el
factor de intensidad de esfuerzos crítico (KIC). El método empleado de
aprendizaje para la red neuronal fue el de retro-propagación del error, donde se
ajustan el valor de los pesos que conectan cada una de las neuronas
reduciendo el error entre los valores producidos por la red y los valores reales.
La función de activación empleada para cada una de las capas de la red
neuronal fue la logarítmica sigmoidal; ésta es una función continua, donde el
entrenamiento utilizado es el de retro-propagación del error, cuyo principio
emplea una regla delta a través de un gradiente descendiente. Por esta razón,
se buscan funciones continuas además de presentar, esta última función,
transiciones suaves. La función logarítmica sigmoidal es una de las más
utilizadas con estas metodologías de entrenamiento.
c) Durante el entrenamiento de la red con la base de datos experimentales, se
buscó que el error existente entre los valores de salida que produce la red, con
los valores de salida reales convergiera a 1x10ÃƒÆ’* ¢Á€¹ Á¢Ã¢â€šÂ¬Ã¢â€žÂ¢6 . Para llegar a esta
convergencia, se determinó como máximo 1200 iteraciones del entrenamiento
de la red. Si la convergencia no se obtenía en menos de las iteraciones
deseadas, era necesario reiniciar los valores de la red y volver a realizar el
entrenamiento. Los datos obtenidos por el entrenamiento de la red en
comparación con los datos experimentales de la tenacidad a la fractura de la
fase Fe2B fueron en promedio menores al 1%.
d) La principal ventaja de esta técnica de inteligencia artificial, es su
aprendizaje conforme se presenta la información. La red realiza las inferencias
y los ajustes pertinentes para reproducir de la manera más fiel posible, los
resultados del proceso original. La capacidad de autoaprendizaje de la red
neuronal es una gran ventaja, ya que dependiendo de los parámetros de
proceso (como el número de iteraciones y el error de convergencia de la red),
esta es capaz de auto ajustarse, modificando los pesos entre neuronas hasta
que los resultados cuenten con el error deseado.
e) La validación del modelo de redes neuronales en la determinación del valor
de tenacidad a la fractura (con datos externos al entrenamiento) fueron en
promedio del 6.77% para el acero borurado AISI 1045 a la temperatura de
1000 °C con 4 h de tratamiento y 5 mm de espesor de pasta de carburo de
boro, y del 9.14% para la muestra con la misma temperatura de tratamiento y
espesor de pasta y 6 h de tiempo de tratamiento. Por otro lado, la desventaja
de este modelo, es que los datos experimentales de KIC no pueden ser
validados fuera del rango de experimentación.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE INGENIERÍA
MECÍNICA Y ELÉCTRICA
SECCIÓN DE ESTUDIOS DE POSGRADO E
INVESTIGACIÓN
T e s i s
Determinación de la tenacidad a la fractura en capas
borurizadas empleando redes neuronales
Que para obtener el grado de:
Maestro en Ciencias en
Ingeniería Mecánica
Presenta:
Ing. Rodolfo Sosa Rojas
Director:
Dr. Iván Enrique Campos Silva
MEXICO, D.F. 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“RECONOCIMIENTO DE ROSTROS UTILIZANDO ANÍLISIS DE COMPONENTES PRINCIPALES: LIMITACIONES DEL ALGORITMO"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La base de fotografías que se utilizará para el análisis, estará limitada a 400 fotografías, correspondientes a 40 sujetos (10 fotografías por cada sujeto). Dicha base de datos, fue realizada por los Laboratorios AT&amp;T de la Universidad de Cambridge, UK.1
De las 400 imágenes, un determinado porcentaje se utilizará como la base de entrenamiento del sistema, y las imágenes adicionales a cada sujeto, se considerarán fotografías que se tomaron en el momento a la persona y se trata de reconocer si es un probable delincuente.
La base AT&amp;T se ha utilizado en gran cantidad de investigaciones a nivel mundial, para probar sistemas de reconocimiento de rostros. Adicionalmente, se utilizarán otras imágenes de objetos y personas que se obtendrán de Internet y del portal del FBI (fugitivos más buscados)2. Se considera que el sistema computacional a desarrollar en la presente tesis, sería una aplicación para el reconocimiento de sospechosos (delincuentes y terroristas).</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un sistema computacional para el reconocimiento de rostros mediante aprendizaje supervisado basado en el análisis de componentes principales, con el propósito de realizar diversos experimentos para obtener las ventajas y limitaciones del procedimiento y proponer de manera conceptual una nueva alternativa de solución.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Se tiene posibilidad de realizar un reconocimiento de rostros utilizando la técnica de análisis de componentes principales?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Las características estadísticas que presentan los pixeles de imágenes digitales de rostros cumplen con los requisitos para realizar un análisis por medio de componentes principales?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Cuántas fotografías por cada sujeto en la “base de entrenamiento"  de rostros son requeridas para llevar a cabo un reconocimiento adecuado utilizando la técnica de componentes principales?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Cuáles serían las principales desventajas que presenta el análisis de componentes principales para llevar a cabo un adecuado reconocimiento de rostros?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Qué otra técnica sería susceptible de emplearse para llevar a cabo un adecuado reconocimiento de rostros?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Utilizando una base de datos conteniendo una sola fotografía digital por persona, frontal y con elementos controlados de luz será posible llevar a cabo un adecuado reconocimiento del rostro de una persona utilizando otra fotografía digital del sujeto.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>A partir de los resultados que se obtuvieron en el análisis estadístico de las imágenes de rostros y en los diversos experimentos realizados, se concluye:
"Debido a la amplia posibilidad de rangos (entre 0.0 y 1.0), que se presentan en imágenes digitales en tonos de grises de rostros humanos, la dispersión que presentan es muy grande. En el análisis realizado a la base AT&amp;T y con fotografías de rostros que se obtuvieron en Internet, el promedio de los tonos de los píxeles que se encontró fue de 0.5208. Y la desviación estándar, de +/- 0.2947. Lo anterior, implica un amplio rango de valores. Por tanto, tratar de obtener algún tipo de análisis por medio de estadística descriptiva únicamente, se considera inadecuado. Se considera que la información del rostro, se encuentra inmersa en las diversas interrelaciones que se presentan entre los píxeles a nivel local en diversas áreas y la estructura latente que se configura por la interrelación de estas áreas.
"Por lo que respecta al supuesto de normalidad univariada y multivariada, se probó, que no se cumple. Tanto a nivel individual de una fotografía, como a nivel global de todo un conjunto de imágenes (se consideró una muestra estadística de fotografías), los rostros no se ajustan a una distribución normal multivariada.
"Por tanto, al violar este importante supuesto estadístico, se considera que la aplicación de métodos estadísticos “tradicionales" : regresión, análisis factorial, análisis de componentes principales y en general, cualquier método relacionado que trabaje bajo métricas en espacios tendrá grandes posibilidades de obtener resultados erróneos. A pesar de la “robustez"  que en general, presentan dichos métodos. 2L
Por lo que respecta al comportamiento del sistema de reconocimiento de rostros en los diversos experimentos realizados, se puede decir lo siguiente:
"Un elemento fundamental, es la asignación adecuada del “umbral"  para clasificar y poder identificar adecuadamente al rostro respectivo. Dicho umbral debe reducir el número de falsos positivos (inocentes considerados como sospechosos) y el de falsos negativos (sospechosos no identificados), al mismo tiempo que trata de maximizar el número de positivos verdaderos (sospechosos identificados). En los diversos experimentos que contemplaron varias fotografías por cada sujeto para entrenamiento de la base (desde 9 imágenes a 1 imagen), se plantea, que un umbral adecuado sería el que se obtiene del promedio de las distancias del rostro al “espacio de rostros" , más dos desviaciones estándar).
"En la experimentación, se encontró que los rostros que se tomaron del portal de sospechosos del FBI, así como las fotografías que se obtuvieron por medio de un scanner (las cuales, no tuvieron un proceso controlado en su adquisición como sería el caso de la base AT&amp;T), presentan distancias muy superiores al promedio de las distancias de fotografías controladas (con respecto al “espacio de rostros" ) de las fotografías controladas.
"La conclusión anterior implica, que el sistema de reconocimiento de rostros, requiere de un “umbral grande"  para poder realizar un adecuado reconocimiento de las fotografías de los sospechosos. Sin embargo, al tener dicho umbral, una gran cantidad de fotografías “controladas" , serían clasificadas como “falsos positivos" . Aparentemente, las fotografías que no presentan un control en el momento de tomarlas, requieren de umbrales superiores.
"El método de componentes principales mostró comportamientos adecuados. El porcentaje de reconocimiento de rostros en condiciones controladas (base AT&amp;T) con respecto a si mismas, obtuvo porcentajes elevados. Sin embargo, se presentan serias deficiencias cuando se mezclan imágenes que no presentan un control en la iluminación, fondo de imagen, resolución, etc.
"En diversos casos, la técnica de componentes principales, mostró gran eficiencia. Por ejemplo, al identificar algunos sospechosos de la base del FBI, cuya edad era muy superior en la fotografía a identificar con respecto a su fotografía de la base de entrenamiento.
"Se encontró, que el método de componentes principales no funciona de forma adecuada con bases de entrenamiento que únicamente cuenten con una fotografía por persona. El número ideal para este tipo de sistemas, sería entre 5 y 7 imágenes por sujeto.
"En relación a la conclusión anterior, se puede mencionar, que la hipótesis del trabajo de la tesis no se cumplió totalmente. Pues si bien, el sistema sí logra realizar reconocimientos de rostros a partir de solo una imagen, la confiabilidad no es adecuada en la mayoría de los experimentos que se realizaron.
"El incumplimiento parcial de la hipótesis, contribuye a reforzar el postulado del autor de la tesis, respecto a las deficiencias que presentan las técnicas basadas en espacios euclideanos y por tanto, investigar métodos en otro tipo de Espacios métricos.
En conclusión, el método de componentes principales; se considera una técnica que permite realizar reconocimientos de rostros sobre todo, cuando se tienen diversas imágenes por persona. Sin embargo, a pesar de lo anterior, la confiabilidad no es completamente adecuada.
Se considera que al tener datos (los rostros), que no cumplen con el supuesto de normalidad multivariada, inciden en las diversas fallas y errores de tales tipos de sistemas.
Por tanto, se plantea la opción de trabajar en un espacio ÃƒÆ’* ¢Á€¹ Á€¦* ¾L, el cual no requiere los supuestos de normalidad, linealidad y homoscedasticidad. Este nuevo método para el reconocimiento de rostros, utilizará un enfoque de aproximación multivariada, utilizando el denominado Algoritmo Genético Ecléctico para obtener bajo un paradigma de optimización combinatoria, la forma y orden del polinomio de aproximación que caracterizará a los rostros.
Asimismo, dicho método requerirá únicamente, de una muestra de los píxeles que configura cada rostro, lo cual redituará en una menor cantidad de almacenamiento y tiempo de procesamiento computacional. Además, al utilizar un enfoque holístico, no requiere de la utilización de características geométricas o “plantillas"  del rostro.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD IBEROAMERICANA
“RECONOCIMIENTO DE ROSTROS UTILIZANDO ANÍLISIS DE COMPONENTES PRINCIPALES: LIMITACIONES DEL ALGORITMO" 
TESIS
Que para obtener el grado de
MAESTRO EN SISTEMAS Y PLANEACION
P r e s e n t a:
CARLOS VILLEGAS QUEZADA
Director
MTRO. JORGE RIVERA ALBARRAN
Asesores:
MTRO. PEDRO FERNANDO SOLARES SOTO
MTRO. FELIPE ANTONIO TRUJILLO FERNANDEZ
MÉXICO,D.F. 2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>CONTRIBUCIÓN AL DISEÑO DE SISTEMAS DOMÓTICOS Y DE ENTRETENIMIENTO UTILIZANDO HARDWARE LIBRE Y
SOFTWARE DE CÓDIGO ABIERTO</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En los últimos años se ha desatado una fuerte competencia para el diseño de plataformas de
software HTPC, de acuerdo al gusto de cada usuario.
Las diversas plataformas de software pueden compararse en prestaciones, potencia,
velocidad, capacidad de grabado PVR (Personal Video Recorder) y el control de sus
opciones de forma sencilla. Un sistema de gestión de contenido basado en una guía de
programación electrónica surge como un componente clave de un sistema de
entretenimiento doméstico [1].
En México, los trabajos de investigación enfocados al estudio y diseño de plataformas de
software HTPC para aplicaciones en sistemas de control, visualización del entretenimiento
y sistemas domóticos son incipientes.
El diseño de un sistema de entretenimiento en los hogares, es muy poco común
debido al elevado costo de adquisición de equipos multimedia para su implementación. La
mayoría de los hogares tiene una computadora de escritorio o computadora personal con
una conexión en internet, en esto se podrá sacar provecho, para armar un equipo con
aplicaciones multimedia, buscando el mejor software donde el usuario pueda interactuar de
manera inteligente utilizando un mando a distancia. En tal virtud, este proyecto de tesis
desarrolla un sistema domótico y de entretenimiento, utilizando software de código abierto
y hardware libre de bajo costo.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>La investigación que se desarrolla en esta tesis consiste en establecer una metodología de
diseño completa de un sistema domótico y de entretenimiento basado en la utilización de
hardware libre y software de código abierto. El sistema desarrollado permitirá la
visualización de contenido digital y el control de aplicaciones y servicios caseros.
Los objetivos particulares que se persiguen son:
1. Comparación operativa de las plataformas de software más conocidas y eficientes
para el diseño de un HTPC, tales como: XBMC, MythTV, MediaPortal y Freevo. Al
estar considerando una computadora personal como Centro de Control Multimedia
(Media Center). Además, se realiza un análisis de los lenguajes de programación
usados en las aplicaciones para el diseño de un ambiente personalizado,
considerando adicionalmente si es un software con código cerrado o abierto.
2. Investigación del hardware soportado por las plataformas HTPC. Consistirá en la
recopilación de información comparativa del hardware que se recomienda y soporta
cada una de las plataformas más sobresalientes.
3. Realizar la elección de una tarjeta de control de comandos adecuada a los objetivos
particulares de diseño del HTPC de esta tesis.
4. Diseñar un sistema domótico y de entretenimiento a modo de una plataforma
HTPC.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Ante la explosión del contenido digital, es evidente la necesidad del diseño de un HTPC
que satisfaga el procesamiento y manejo de formatos video de alta definición y de audio de
alta fidelidad y el ahorro de energías.
En los años venideros, el establecimiento de redes caseras se proyecta para
convertirse en uno de los segmentos de crecimiento más rápidos de la industria de productos de consumo electrónico. Al mismo tiempo, las tecnologías de red avanzan,
elevando las expectativas del consumidor en la capacidad de enlace de estos dispositivos.
La automatización de los hogares permite disminuir el gasto energético para ahorrar
dinero y a su vez cuidar el medio ambiente, brindar comodidad y tranquilidad cuando
estamos dentro o fuera de la casa, aumentar nuestra seguridad, auxiliar y facilitar la
organización de nuestras actividades cotidianas, realizar nuevas tareas desde casa.
Actualmente los hogares disponen de un gran número y sistemas principalmente autónomos
y redes no conectadas entre ellos, como la telefonía, sistemas de acceso, la televisión, redes
de datos (cableadas e inalámbricas), electrodomésticos, equipamiento de audio y video,
calefacción, aire-condicionado, seguridad, riego, iluminación, etc. A base de estos
incrementos se busca el mejoramiento de los sistemas multimedias para automatizar
algunos aplicaciones necesarias, como la grabación y reproducción de videos, sintonización
de audios, distribución de archivos multimedia, etcétera.
La adquisición de conocimientos básicos sólidos, en el ámbito de plataformas de
software útiles para el control de datos digitales, se vuelve necesaria en vista del dramático
aumento de archivos de video alta definición y de audio de alta fidelidad. Tanto el uso cada
vez más extendido de programas electrónicos de diseño por computadora, como la
adquisición y almacenamiento de datos hacen más viable el diseño de sistemas de
despliegue, visualización y la aplicación de estos en sistemas de comunicación de red. No
obstante, se carece de una metodología sistematizada para el diseño plataforma de software
especialmente en aplicaciones en una computadora personal, para lo cual, sin embargo, han
demostrado cualidades especiales muy atractivas. Por lo que es de particular interés hacer
contribuciones de investigación y desarrollo en este aspecto.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>A partir de los resultados obtenidos en el presente trabajo de tesis, se concluye que es
posible realizar la implementación de un sistema domótico y de entretenimiento utilizando
hardware libre y software de código abierto, impactando principalmente en la reducción de
costos de implementación y de diseño.
Un sistema domótico y de entretenimiento no puede ser definido simplemente como
una nueva tecnología, sino más bien como, una nueva forma de pensar, es decir, permite
aprovechar la tecnología ya existente para aprovechar mejor los recursos energéticos,
disminuyendo así los costos de utilización de los distintos tipos de energía y el impacto
medioambiental producido con la generación de los mismos.
Las metas seguidas por un investigador y desarrollador es obtener un software
original para la obtención de un producto más sofisticado consisten en: promover el
intercambio de conocimientos y la colaboración, lograr aplicaciones que tienen el mayor
beneficio para la comunidad en general.
El sistema aquí descrito no pretende ser una solución definitiva, pero sí intenta
incidir en la situación actual. Por ahora, existen hogares o espacios “inteligentes" , y en un futuro no muy lejano aumentará la implantación de este tipo de tecnologías. El sistema
domótico y de entretenimiento desarrollado en esta tesis tiene la ventaja de poder adaptarse a cualquier red subyacente, lo que le asegura una larga vida y poder ser mejorado por cualquier usuario que requiere ser más aplicaciones dentro del software.
La implementación del sistema domótico es funcional, para los servicios que se
seleccionaron como son la iluminación, aire acondicionado, calefacción y alarma. El
sistema tiene la capacidad de visualizar los archivos digitales y grabar simultáneamente
videos, ya sea que se encuentran almacenados de manera local o en la red. Cuenta con la
facilidad de poder agregarle los plugins y scripts para mejor la capacidad gráfica y
funcionalidades, sin perder la robustez del sistema. El sistema implementado en este trabajo de tesis con plataformas tanto de hardware libre como de software de código abierto puede ser realizado y reproducido de forma libre por cualquier persona. Hasta ahora, no se ha reportado en la literatura científica un sistema similar, por lo que representa el primero con estas funcionalidades. Además constituye una alternativa al uso de programas de software libres. Al sistema desarrollado se le pueden incorporar más funcionalidades, siguiendo la metodología de desarrollo planteada en Capítulo IV, y aprovechando la arquitectura ilustrada en el Capítulo V. Los productos requeridos para desarrollar este tipo de sistemas son de precios accesibles, solo se requiere tener un conocimiento detallado de las aplicaciones que se pueden incorporar, así como contar con una idea del impacto y requerimientos necesarios de las aplicaciones sobre los usuarios y servicios potenciales.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>CENTRO DE INVESTIGACIÓN Y DESARROLLO
DE TECNOLOGÍA DIGITAL
MAESTRÍA EN CIENCIAS CON
ESPECIALIDAD EN SISTEMAS DIGITALES
“CONTRIBUCIÓN AL DISEÑO DE SISTEMAS DOMÓTICOS Y DE
ENTRETENIMIENTO UTILIZANDO HARDWARE LIBRE Y
SOFTWARE DE CÓDIGO ABIERTO" 
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS
P R E S E N T A:
ING. VIRGILIO ROSENDO PÉREZ PÉREZ
BAJO LA DIRECCIÓN DE:
DR. JOSÉ CRUZ NÚÑEZ PÉREZ
NOVIEMBRE DE 2010 TIJUANA, B.C., MÉXICO</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Desarrollo de la electrónica y software de control de una instalación de análisis de plasma inducido por
láseres pulsados utilizando la técnica Sonda de Langmuir</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La falta de una masa crítica de técnicas de caracterización de plasmas inducidos por ablación láser, que aumenten la fiabilidad y calidad de los estudios relativos al tema, realizados en el Laboratorio de Tecnología Láser.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar una interfaz electrónica automatizada para emplear la técnica de la sonda de Langmuir. Además, implementar un código de programación (firmware) para controlar y procesar todas las funciones de esta interfaz.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El conocimiento del principio de funcionamiento de la técnica de la sonda de Langmuir permitirá diseñar, desarrollar e implementar tanto una interfaz electrónica, para la adquisición y procesamiento de las señales de corriente suministradas por la sonda, como el firmware encargado de controlar esta interfaz</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La necesidad de contar con nuevas técnicas de caracterización de plasmas producidos por ablación láser que coadyuven a un mejor entendimiento de los fenómenos físico-químicos que están presentes en el proceso, así como expandir las aplicaciones tecnológicas de los trabajos desarrollados en nuestro laboratorio de tecnología láser electrónica. El uso y dominio del protocolo del USB permitirá dotar a esta interfaz con comunicación USB. El diseño de la tarjeta de circuito impreso según los estándares y normas de fabricación establecidos, permitirán a esta interfaz procesar señales de frecuencia de hasta 200 MHz, sin sufrir interferencia por ruido de señales externas o auto inducidas. El uso de un microcontrolador de la familia PIC24F y baterías recargables de iones de Litio añadirán autonomía y portabilidad a la interfaz. Finalmente, la implementación de potenciómetros digitales permitirá realizar un circuito de ajuste de ganancia para garantizar una resolución constante en la adquisición de las señales.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>A partir del estudio del principio de funcionamiento de la técnica de la sonda de Langmuir, fue posible diseñar y desarrollar una interfaz electrónica, para la adquisición y procesamiento de las señales de corriente suministradas por la sonda.
El estudio de la arquitectura del microcontrolador PIC24FJ256GB110, en conjunto con el software OrCAD 15.7 y el uso de los estándares y las normas de fabricación del Instituto de Circuitos Impresos, permitieron realizar un diseño eficiente del PCB con solo doble capa.
La agrupación de los componentes en zonas de blindaje, definidas según el criterio de potencia, frecuencia y el tipo de señales, permitió eliminar ruidos externos así como los autoinducidos.
El desarrollo de un código de programa o firmware utilizando el lenguaje de programación C30 de Microchip permitió la utilización de los diferentes periféricos de comunicación para realizar el control de toda la interfaz.
El uso de una pantalla LCD sensible al tacto y un set de baterías recargables de ión de litio permitió dotar de portabilidad y autonomía a la interfaz electrónica. Estas además proporcionan una fuente de energía libre de ruido. Lo anterior, es también una aportación debido a que no se han reportado sondas de Langmuir portátiles.
Aunque no se comercializa un dispositivo con iguales características [34, 35], la interfaz electrónica desarrollada puede compararse, en un análisis de costo, con distintas tarjetas de adquisición comercializadas. Existen diversas empresas especializadas en el desarrollo de estas tarjetas, ejemplos son National Instrument, Keithley Instruments, DIVA Automation, de las cuales destaca como líder National Instrument por su variedad de formatos. En la tabla 12 se muestran precisamente dos modelos de tarjetas que se escogieron según su similitud en la característica de número de entradas analógicas (2 y 4), para realizar la comparación antes mencionada.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN CIENCIA APLICADA
Y TECNOLOGÍA AVANZADA, UNIDAD ALTAMIRA
Desarrollo de la electrónica y software de control de
una instalación de análisis de plasma inducido por
láseres pulsados utilizando la técnica
Sonda de Langmuir
T E S I S
Que para obtener el grado de:
MAESTRO EN TECNOLOGÍA AVANZADA
P r e s e n t a:
Ing. Daniel Iván Rentería Avalos
Director de tesis:
Dr. Eduardo Marcelo de Posada Piñán
ALTAMIRA, TAMPS. DICIEMBRE DE 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SISTEMA BASADO EN CONOCIMIENTO COMO
HERRAMIENTA DE APOYO PARA LA ADMINISTRACIÓN
DE PROYECTOS DE INGENIERÍA DE SOFTWARE - HIS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo principal que se persigue en este trabajo de tesis es analizar, diseñar y
construir un PROTOTIPO de un Sistema Experto Basado en Conocimiento (Knowledge
Based Expert System) para apoyo a la Administración de Proyectos de Ingeniería de
Software (SE-APIS), que sirva como herramienta inteligente a los Ingenieros de esta
especialidad, fundamentalmente en las fases de estructuración, planeación y programación
de proyectos y asimismo, les brinde consultoría en línea sobre metodologías de Ingeniería
de Software.
Este prototipo de Sistema Experto debe permitir al Ingeniero de Software contar con
información suficiente en línea sobre los conceptos de las diferentes etapas de las
metodologías de sistemas. Lo anterior podrá realizarlo con facilidad por medio de una
interfaz Hombre - Máquina en Lenguaje Natural, que le permitirá consultar y obtener
reportes de cada una de las fases y actividades de la metodología requerida.
El Ingeniero de Software apoyado mediante esta Sistema Experto contará con una
planeación adecuada desde el inicio del ciclo de vida de sus proyectos. A partir de la
creación de proyectos en forma automática que se construirán con una estructura WBS
(Work Breakdown Structure) conteniendo fases de desarrollo, y a su vez cada fase generada
incluirá sus actividades correspondientes.
Después de la creación del proyecto, el Ingeniero de Software por medio de una oración en
Lenguaje Natural podrá programar la red de fases formada con precedencias previamente
almacenadas en la Base de Conocimientos. Esta programación del proyecto incluirá el
cálculo de la ruta crítica de la red mediante un algoritmo inteligente.
El Sistema Experto como apoyo a la administración de proyectos de Ingeniería de Software,
no desarrolla de manera extensa la generación de reglas de conocimiento sobre métricas de
software, sin embargo abre esta posibilidad al incluir el modelo de PUTNAM [PUT78] para el
cálculo de la productividad de proyectos complejos.
Adicionalmente con la elaboración de este trabajo se busca establecer los modelos básicos
para integrar herramientas CASE ´s existentes que sirven como apoyo a la generación de
los diferentes modelos y métodos utilizados en el desarrollo de software, con herramientas
de un contexto más amplio como son las metodologías y la Administración de proyectos que
llevaran a los líderes de Proyectos de Ingeniería de Software a obtener mejores resultados.
Finalmente existe la posibilidad de que este modelo pueda servir como base para trabajos
futuros de tesis en la construcción de un Sistema Experto Integral de Ingeniería de Software.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología de desarrollo aplicada para construir un Sistema Experto que apoye a
los líderes de proyecto en la Administración de Proyectos de Ingeniería de Software se ha
estructurado partiendo de la adaptación de conceptos de la metodología KAMET, y
ordenándolos con la aportación de los valiosos comentarios de los asesores de este trabajo
tesis.
La metodología KAMET(Knowledge Acquisition from Multiple Sources, Cairo, 1998 [CAI98]),
se sugiere como un conjunto de actividades para la adquisición de conocimiento para
realizar una modelación de Sistemas Expertos más que una actividad de extracción o
minería.
La metodología KAMET esta compuesta por cuatro etapas: planeación estratégica del
proyecto, construcción del modelo inicial, construcción del modelo retroalimentado y
construcción del modelo final.
Considerando que esta metodología no es una secuencia inflexible que debe seguirse a lo
largo del desarrollo de los proyectos, se han tomado varios conceptos adaptándolos al
desarrollo de un Sistema Basado en Conocimientos como Herramienta de apoyo para la
Administración de Proyectos de Ingeniería de Software, y que se muestran en el diagrama
de flujo.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El prototipo SE-APIS del Sistema Basado en Conocimiento como Herramienta de apoyo para
la Administración de Proyectos de Ingeniería de Software HIS, es un PROTOTIPO de
herramienta inteligente que puede apoyar a los Ingenieros de esta especialidad en el
conocimiento y estandarización de metodologías de desarrollo de sistemas y la
Administración de Proyectos de Software.
Con la implantación del PROTOTIPO SE-APIS es posible apoyar en la solución de varios de
los problemas detectados en la Administración de Proyectos y manejo de metodologías.
Dentro de las mejoras que se es posible lograr con su incorporación están las siguientes:
*	Mejorar la planeación de los proyectos.
*	Contar con una metodología en línea que guíe en el desarrollo de software.
*	Proporcionar entrenamiento en línea sobre metodologías y Administración de Proyectos
de Software.
*	Obtención de estimados de costo, recursos y duración de las diferentes etapas y
actividades del proyecto.
*	Identificar las actividades críticas del proyecto.
*	Contar con estadísticas mínimas para el desarrollo de software.
*	Incremento gradual del conocimiento del SE-APIS, con la incorporación de un mayor
número de casos de metodologías en su biblioteca.
De acuerdo con él “Estado del Arte"  actual sobre los diversos productos y soluciones que
apoyan a la Administración de Proyectos y desarrollo de software, se observa que aún
cuando existen varias soluciones que apoyan esta actividad, es necesario realizar una
herramienta que apoye a los Ingenieros de Software de manera integral.
Los conceptos de la Ingeniería de Sotware actuales, forman un amplio e interesante dominio
de conocimiento, que es factible de ser incorporado en un Sistema Experto Integral,
manejando diversas técnicas y tecnologías de la Inteligencia Artificial.
El diseño del Modelo del SE-APIS se facilitó gracias a las técnicas de la Inteligencia Artificial establecidas y dentro de las cuales podemos mencionar las siguientes:
*	El uso de estructuras de ranura y relleno débil Marcos u Objetos se adaptó de una forma
natural al tipo de conocimiento de Metodologías de Ingeniería de Software y Proyectos para
sistemas de Información por lo que fue posible aprovechar las propiedades de jerarquía de
clases, herencia y mensajes entre objetos.
*	La interfaz basada en Lenguaje Natural facilitó el diseño de la Interfaz con el usuario, sin embargo aún existe ambigüedad en la interpretación semántica de las frases.
*	La representación del conocimiento relacional simple facilitó el diseño de la generación y almacenamiento de hechos de las estructuras de los Proyectos.
*	El manejo de reglas de inferencia con CD apoyó el diseño del procesamiento de los
diferentes analizadores y controles del motor de inferencia, la creación y programación de
proyectos, el cálculo de parámetros: como el costo, horas y duración con los pesos
estadísticos de las fases y operaciones de las metodologías, así como el cálculo de las
líneas de código con el método de Putnam.
La construcción del PROTOTIPO del SE-APIS en el software CLIPS (C Language Integrated
Production System) se realizó de una manera relativamente sencilla, por lo que es una
motivación a continuar con el desarrollo de este tipo de Sistemas Inteligentes para aplicarlos en el IMP.
Los resultados obtenidos sobre la evaluación cuantitativa y cualitativa del SE-APIS fueron
satisfactorios, permitiendo obtener una constancia de que el objetivo y el alcance planteado se cumplieron ampliamente, bajo los siguientes escenarios de prueba:
*	Interfaz de Lenguaje Natural.
*	Consulta y reportes de Procedimientos y Metodologías.
*	Generación de Proyectos y Métricas de Putnam.
*	Cálculo de la ruta crítica de Proyectos.
El desarrollo de trabajos futuros para construir una Sistema Basado en Conocimiento como
Herramienta Integral de apoyo a la Ingeniería de Software HIS, a partir del PROTOTIPO del
SE-APIS, tiene una gran posibilidad de desarrollarse en un ambiente Internet colaborativo
como JESS y de poder incorporar un servidor de Base de Datos que permita realizar análisis
sobre los proyectos de software y sistemas de información soportado por una Base de
Conocimientos que incluya Métricas de Software, con lo que se obtendrá una Herramienta
que apoye fuertemente a los Líderes de Proyecto de Software en el IMP.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>I N S T I T U T O P O L I T É C N I C O N A C I O N A L
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
SISTEMA BASADO EN CONOCIMIENTO COMO
HERRAMIENTA DE APOYO PARA LA ADMINISTRACIÓN
DE PROYECTOS DE INGENIERÍA DE SOFTWARE - HIS
T E S I S
QUE PARA OBTENER EL GRADO DE MAESTRO EN
CIENCIAS DE LA COMPUTACIÓN
P R E S E N TA:
FELIPE JUÁREZ RODRÍGUEZ
DIRECTOR: DR. BÁRBARO JORGE FERRO CASTRO
CODIRECTOR: DR. ÁLVARO DE ALBORNOZ BUENO
MÉXICO D.F. MARZO 2003</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“LA COMUNICACIÓN DENTRO DEL PROCESO DE GENERACIÓN-ADOPCION DE TECNOLOGÍA CAFETALERA, REGIÓN COATEPEC. CASO INIFAP"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El subsector cafetalero enfrenta problemas de rentabilidad, derivado de su rezago tecnológico, de falta de asistencia técnica, créditos, organización de productores, insuficiente capitalización por falta de infraestructura; y sin embargo Veracruz cuenta con las condiciones para ser un importante productor competitivo en el ámbito internacional.
El estado de Veracruz ocupa el segundo lugar en la producción de café, concentrándose en la zona centro del estado con el 96.1% de los productores dedicados a la actividad cafetalera; para el cultivo del café se siembran más de 151,000 hectáreas y en total son mas de 56,000 familias que dependen de esta actividad productiva.
6
El Campo Agrícola Experimental Xalapa, dependiente del Instituto de Investigaciones Forestales, Agrícolas y Pecuarias (INIFAP), ante el potencial de este cultivo en las zonas de transición y de altas montañas y que abarca las regiones cafetaleras de Coatepec, Misantla y Huatusco, tiene como propósito investigar las condiciones climáticas y ecológicas de estas regiones así como las condiciones económico- sociales de los productores, con el objetivo de generar conocimiento científico e innovaciones tecnológicas para el cultivo del café que contribuyan al desarrollo de una cafeticultura tecnificada, en el trópico húmedo y zonas montañosas del estado de Veracruz.
Por su importancia económica, por el número de hectáreas sembradas, y por el número de familias veracruzanas que dependen del cultivo del café, este trabajo de investigación pretende conocer cual es la situación actual de la tecnología generada por el INIFAP, para el cultivo del café en la región cafetalera de Coatepec y que papel ha jugado la comunicación en la adopción de tecnología cafetalera.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Evaluar el proceso de la comunicación en la generación, validación, difusión y adopción de tecnología cafetalera generada por el INIFAP, en la Región de Coatepec, zona centro del Estado de Veracruz.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El uso inadecuado de los medios de comunicación social para difundir los resultados de la investigación agrícola del INIFAP, tiene como consecuencia la escasa adopción de nuevas tecnologías.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>4.1. Localización de la zona de estudio.
El área de estudio se delimitó tomando en cuenta el área de influencia del Campo Agrícola Experimental Xalapa, dependiente del Instituto Nacional de Investigaciones Forestales y Agropecuarias (INIFAP). Esta área comprende la región cafetalera de Coatepec, donde se encuentran productores cafetaleros que están probando diferentes componentes tecnológicos , para el caso de esta investigación únicamente se evaluara la variedad Oro Azteca.
4. 2. Tamaño de la muestra.
Se determinó tomar al 100% de los productores que están probando algunos componentes del paquete tecnológico sobre el cultivo del café, y que en total sumaron 84 productores distribuidos en diferentes municipios de la región cafetalera de Coatepec, y son los siguientes: Xalapa, Coatepec, Teocelo, Tlacotepec de Mejía y Juchique de Ferrer.
De los 84 productores cafetaleros se entrevistaron 63, ya que los 21 restantes manifestaron que estaban probando las mismas variedades que sus familiares y no tenia caso responder a la pregunta.
4.3. Variables a evaluar
4.3.1. Variables sociales
* Edad: Número de años cumplidos por el productor al momento de la encuesta.
* Escolaridad: Grado escolar cursado por el productor.
* Integrantes por familia: Número de personas que viven en la casa del productor.
* Dependientes económicos: Número de personas que dependen económicamente del productor.
* Exposición a medios de comunicación: Número de veces que un productor se expone a un medio de comunicación (prensa, radio, revistas o folletos, periódicos).
* Preferencia por algún medio de comunicación.
* Organización de productores: Pertenencia a alguna organización de productores (agrícola, ganadera u otra).
* Tenencia de la tierra: Tipos de tenencia de la tierra( pequeña propiedad, ejidal, comunal, rentada).
4.3.2. Variables económicas.
* Ingreso semanal: Cuanto gana el productor en promedio por semana.
* Créditos: Préstamo de alguna institución bancaria al momento de levantar las encuestas.
* Precio por jornal: Salario por un día de trabajo.
* Costo del producto: Costo de los productos agroquímicos.
78
* Precio rural por kg de café cereza: Precio promedio diario que paga el comprador del grano.
* Forma de venta: Es la forma que escoge el productor para vender su producción (en pie, en cereza, pergamino, o por quintal).
* Forma de pago: Es la forma como el comprador paga, (adelantado, de contado, a crédito).
* Rendimiento por hectárea: Kilogramos que se obtienen por cosecha.
* Tipo de comprador: Es la persona que compra la producción y puede ser de la localidad o de la región.
* Asistencia técnica: Necesidad de contar con asistencia técnica y origen de la misma.
4.3.3. Variables técnicas.
* Tipo de suelos: Suelos predominantes en la unidad de producción.
* Actividades productivas: Otra actividad del productor aparte de ser cafetalero.
* Preparación del terreno: Forma de preparar el terreno (mecanizado, tracción animal o manual).
* Tipo de variedades: Variedades de porte bajo o alto.
* Distancia entre plantas: Centímetros utilizados entre planta y planta.
* Trazo utilizado: Forma de trazado para la siembra (hilera, curvas de nivel, curvas en contorno, rectangular, tres bolillos y marco real.).
* Hoyos destapados: Técnica para conservar la humedad antes del transplante.
* Características de la planta: Fuertes, raíces rectas, 2 a 3 cruces.
* Fecha de siembra: Época en que siembra el productor (temporada de secas, de lluvias, de heladas).
79
* Cantidad de plantas por hectárea: Número de plantas sembradas por hectárea en cafetales ya establecidos.
* Sombra del cafetal: Tipo de sombreado que utiliza el productor (temporal o provisional, permanente o definitivo).
* Control de malezas: Tipo y época de control de malezas, que realiza el productor (manual, químico, mixto, biológico o ninguno).
* Labores culturales: Numero de limpias durante el año, que realiza el productor a su cafetal.
* Fertilización: Época de fertilización, tipo de fertilizante, dosis y costo del producto.
* Podas: Práctica para eliminar tejido viejo y para darle configuración a la planta, principalmente para prevenir plagas y enfermedades y para facilitar la operación de la cosecha.
* Encalado: Proporcionar calcio a la planta.
* Tipo de plagas: Identificación que hace el productor de las principales plagas.
* Épocas de incidencia: Fecha de mayor ataque de plagas.
* Forma de control: Tipo de control (manual, químico y cultural) que hace el productor para el control de plagas.
* Nombre del producto: Nombre de los productos químicos.
* Dosis: Cantidad que aplica el productor a la planta.
* Tipo de enfermedades: Identificación de las principales enfermedades que dañan al cultivo.
* Nombre del producto: Nombre del agroquímico que el productor utiliza para el control de las enfermedades.
* Cosecha: Forma de cosechar, número de recolecciones y fecha.
80
4.3.4. Impacto de la Tecnología.
* Opinión del productor sobre la tecnología generada por el INIFAP.
* Riesgos de la nueva tecnología (Implica uso y disponibilidad de maquinaria agrícola).
* Tipo de maquinaria requerida (tractores, mochilas, azadones, machetes) etc...
* Aceptación del paquete tecnológico, es el aumento de la superficie con nuevas variedades.
* Validación del paquete tecnológico del INIFAP por los productores.
* Requerimientos del productor para contar con asistencia técnica del INIFAP.
* Opinión del productor en cuanto al futuro del café.
4.4. Instrumento de recolección de información.
Se utilizo un cuestionario con 37 preguntas, abiertas, cerradas y de opción múltiple.
4.5. Análisis estadístico.
De acuerdo a las encuestas, se procedió a diseñar la base de datos en la hoja electrónica Excel (Microsoft 2000), y una vez concluida la captura de la información se procedió a validar las bases con la finalidad de depurar errores de captura. Después se procedió a formular dos tipos de análisis estadísticos exploratorio simple con gráficos de pastel y barras realizado en el paquete harvard graphics; y análisis de correspondencia realizado en el paquete estadístico statica para determinar si existía alguna relación entre variables, utilizando el estadístico chi-cuadrada para determinar si existen diferencias significativas entre las variables de interés, con un nivel de significancia de 0.05.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En base a los resultados obtenidos esta investigación concluye lo siguiente:
1.- Se propuso inicialmente que el uso inadecuado de los medios de comunicación social para difundir los resultados de la investigación agrícola, tiene como consecuencia la escasa adopción de nuevas tecnologías, lo cual se constato, ya que no se hace un uso adecuado y racional de los programas dirigidos a los productores cafetaleros de la Región Coatepec, aún cuando registra altos niveles de audiencia el programa televisivo “Veracruz Agropecuario"  con un 27% de exposición diaria por parte de los productores.
Este programa se transmite por “Radiotelevisión de Veracruz"  TV MAS, propiedad del Gobierno del Estado de Veracruz y con una amplia cobertura hacia los estados de Puebla, Tlaxcala, Hidalgo, Pachuca, Tabasco y parte de chiapas; con transmisiones diarias de 19:30 a 20:30 p.m. y en donde el INIFAP a través de sus campos experimentales en la zona centro del estado de Veracruz, no ha logrado consolidar un espacio exclusivo, para dar a conocer las recomendaciones de la tecnología que genera, únicamente utiliza está estación para promocionar eventos como el “Día del agricultor"  por decir alguno.
El programa agrícola radiofónico “Amanecer Veracruzano" , transmitido por Radio Universidad, con programación cultural registro un 14.29% de exposición diaria por parte de los productores, quizá este bajo porcentaje es producto de la propia cultura del productor donde este esta habituado a escuchar otro tipo de música (ranchera, tropical o grupera) y en esta estación la programación normal, es música clásica o instrumental.
Es importante mencionar que esta estación radiofónica es propiedad de la Universidad Veracruzana, y se le ha dado poca importancia a la transmisión de información agropecuaria. Actualmente no cuenta con ningún tipo de evaluación para conocer la penetración de está radiodifusora entre los productores cafetaleros de la región Coatepec.
2.- Se considero importante identificar la situación actual del binomio comunicación-uso de la tecnología cafetalera, lo cual se constató en las comunidades de Tlacotepec de Mejía, Xihuitlán y Chiltoyac en donde los técnicos del INIFAP fueron los canales de comunicación preferidos por los productores, como difusores de las nuevas tecnologías generadas por el Campo Agrícola Experimental Xalapa, con el 31.6%, seguido de televisión y radio en forma correspondiente.
3.- Se constató que el impacto socioeconómico de la tecnología generada en café por el INIFAP, a través del Campo Agrícola Experimental Xalapa, se traduce en que un 28.6% sí piensa aumentar la superficie de café con nuevas variedades siempre y cuando sean generadas por el INIFAP, ya que, con un 82.54% indicaron los productores que las variedades son resistentes a plagas, enfermedades, sequías y son fuertes y vigorosas, por lo que se obtienen buenos rendimientos y por lo que un 11.11% han adoptado al 100% la variedad Oro Azteca.
4.- Tomando en cuenta los antecedentes en el Marco de Referencia, en donde se mencionan cuales han sido los procesos de comunicación para dar a conocer los resultados de la investigación agrícola, que van desde: “Parcelas Demostrativas" , “Días del Agricultor" , y un uso esporádico de los medios de comunicación social (Prensa, radio, televisión), se registro con un 77.7% que los técnicos del INIFAP han demostrado ser personas confiables y responsables, y con un fácil manejo de la comunicación, por lo que indicaron sean ellos los que sigan proporcionando asistencia técnica.
5.- Es importante destacar que el proceso de adopción de tecnología por parte de los productores cafetaleros en la zona de estudio ha sido escasa por varias razones:
a) No existe financiamiento para el cultivo de café, esto se registro con un 57.14%, aunado a la inestabilidad en el precio del producto, ya que durante el ciclo 2001-2002 el precio de compra por kilogramo de café cereza se cotizó de $1.00 a $1.30, al igual que en el ciclo 2003-2004 .
b) Solo un 11.11% decidió adoptar la variedad Oro Azteca, identificándose como productores con un nivel de escolaridad alto, con tenencia de la tierra en pequeña propiedad y con más de 7 hectáreas; mientras que el resto de los productores tienen una tenencia de tipo ejidal con un 61.1%, y sus ingresos promedio a la semana es de menos de $300.00.
c) Finalmente aún cuando las variedades generadas por el INIFAP, son prometedoras, lo cierto es que existe abandono en los cafetales, un futuro incierto en el producto y con el 55.6%, los entrevistados opinaron que el café ya no es rentable.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD IBEROAMERICANA
“LA COMUNICACIÓN DENTRO DEL PROCESO DE GENERACIÓN-ADOPCION DE TECNOLOGÍA CAFETALERA, REGIÓN COATEPEC. CASO INIFAP" 
TESIS
PARA OBTENER EL GRADO DE :
MAESTRA EN COMUNICACIÓN.
PRESENTA:
YOLANDA SOSA MARTINEZ.
DIRECTOR DE TESIS:
MTRA: SYLVIA HORTENCIA GUTIERREZ YBERIA
ASESOR:
DR.JUAN FRANCISCO ESCOBEDO DELGADO
ASESOR:
MTRO:MOISES ALEJANDRO PEREDO SALINAS
2004. MÉXICO, D.F.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“LA NAVEGACIÓN EN LA INTERNET Y SUS EFECTOS EN EL APRENDIZAJE DE LOS ALUMNOS DE NUEVO INGRESO EN LA U DE O CICLO ESCOLAR 2001-2002"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El uso y aprovechamiento de la Internet solo ha redituado beneficios sustanciales en los países desarrollados, en donde está siendo utilizada ya de manera cotidiana e intensiva por los alumnos de diferentes niveles educativos y por la sociedad en general. En los países emergentes, en nuestros países, la realidad es otra. Nos encontramos con un panorama desolador: tecnología obsoleta, aplicaciones fuera de contexto, uso no académico, reticencia a su uso por algunos sectores tradicionalistas de la sociedad y muchos más.
En los centros de educación superior, es sin duda amplio el esfuerzo que se realiza para dotar de infraestructura informática a las instituciones, sin embargo, esto no es suficiente para que los alumnos aprovechen de manera total la potencialidad de esta tecnología. Se da por hecho que la cibernética y la Internet por si solas tienen la capacidad de hacer que el alumno aprenda. El problema es que el alumno no conoce de manera cabal esta herramienta del conocimiento. En nuestro país y en muchos otros, la Internet no es utilizada para generar nuevos conocimientos y solo se utiliza como medio de información. No es utilizada por nuestra sociedad como un medio más para incrementar el capital cultural a través de la educación.
En estos momentos, las universidades aún no han sido capaces de replantear sus currículos, actualmente basados en la cultura del libro, no se ha dado el uso creativo de la Red y sus recursos y además no se ha terminado por capacitar a la planta docente en lo que podríamos llamar la cultura del uso de la información por la Internet.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de esta tesis es conocer las repercusiones que tiene la navegación en la Internet, en el aprendizaje de los alumnos de nuevo ingreso de la Universidad de Occidente</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Existe una gran variedad de estudios sobre la navegación en la Internet, su uso en educación y el hipertexto. En este sentido se antoja interesante una investigación sobre los efectos en el aprendizaje que genera dicha navegación en alumnos de una universidad pública relativamente nueva, misma que podría generar nuevas interrogantes y que permitiría además la posibilidad de seguir construyendo esta área del conocimiento.
La idea central de esta investigación es lograr identificar el verdadero significado de la metáfora “navegación en la Internet" , ya que se considera que de sí, su significado denota una confusión para los alumnos que tratan de acercarse a la cultura y al conocimiento en general.
Es necesario desvelar la imagen de todo fácil que muestra la Internet. En educación como en todos los campos del conocimiento, no siempre lo fácil es lo mejor. Generalmente lo fácil suele ser lo más difícil.
No se trata ya de usar la Internet, así como así, sino de dominar sus contenidos, aprovechar su potencial educativo, de producir y transmitir conocimiento en lugar de solo buscarlo, terminar con esa ilusión que nos hace creer que ya todo estuviese creado, que todo es tan fácil como dar un “click"  al ratón.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En esta investigación se hace uso de metodologías de corte cuantitativo pero preferentemente se utilizan instrumentos metodológicos de tipo cualitativo, en virtud de estar enfocada a la observación de las conductas y de los efectos que la navegación en la Internet genera en los alumnos de nivel superior.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este apartado haremos un ejercicio de reflexión, con aproximaciones que se han obtenido después del análisis de la información proporcionada por la encuesta y entrevistas aplicadas a alumnos de nuevo ingreso de la U. de O., Unidad Guasave. Así mismo haremos algunas aproximaciones que pretenden ser la conclusión a la que hemos llegado en cuanto al tema tratado.
V.1 La Institución y la información que proporciona la Internet.
Es evidente que el cambio que plantea la adopción de las nuevas tecnologías en la educación deberá ser paradigmático. Los actuales sistemas educativos en nuestros países emergentes deberán ser repensados, deberán ser transformados a raíz. Ya no es posible pensar en un sistema educativo que centraliza la información, que presenta planes y programas rígidos, que genera productos estandarizados, homogéneos, con planes y programas iguales para todo el territorio nacional, con alumnos pasivos y consumidores de información y con maestros que solo representan un punto intermedio entre el alumno y la información, sujetos a un currículum inflexible y que potencian la memorización de contenidos por sobre la construcción del conocimiento por parte del alumno. Por el contrario, requerimos de un sistema educativo que nos lleve de la centralización a la dispersión en red para la diseminación y control de los contenidos educativos, con programas mediáticos flexibles y optativos, con un modelo interactivo y constructivista, que diversifique y personalice a los egresados, inserto en la globalización e internacionalización y con alumnos que interactúen y participen a través de los medios de información y comunicación.
Vivimos en una sociedad de la información que se basa en una revolución tecnológica, misma que plantea la existencia de un boom en la cantidad de información disponible y
120
que otorga una importancia relevante a la selección de la información. No se trata ya de tener acceso a la información, se trata ahora de saber que información seleccionar y los que es más, poder reflexionar sobre ella, analizarla de manera crítica, generar nuevos contenidos a partir de ella. En la encuesta realizada a los alumnos de nuevo ingreso de la U. de O. se manifiesta que los alumnos no seleccionan la información con criterios adecuados. El 2% de los encuestados no sabe que hacer con la información encontrada en la Internet, el 13% solo la lee, el 10% la imprime y la guarda, el 15% la copia y la guarda, el 45% la utiliza para trabajos académicos y el resto la lee y la comparte con sus condiscípulos y amigos. Urge que los docentes nos dispongamos a enseñar a los alumnos técnicas de aprendizaje que les permitan poder analizar y utilizar de manera adecuada la gran cantidad de información que les proporciona la Red.
V.2 Los docentes, los alumnos y la Internet.
Algunos alumnos opinan lo siguiente cuando se les afirma que los maestros de la Universidad de Occidente consultan la Red para preparar sus clases:
“Yo he visto que los maestros siempre usan los mismos libros, a la antigüita. Hay maestros que realizan las mismas prácticas, año con año a todos los grupos. Inclusive, esa práctica no la regresan al alumno, para que no la copien el próximo año" .
“Creo que algunos maestros, no todos, utilizan la Internet. Tal vez por falta de tiempo o por que no les interesa, tal vez por que no conocen. Por eso muchos maestros no utilizan la Internet" .
“No. no. Muy poco. Mínimo" . (CONSULTAR ANEXO 3)
La institución a través de sus maestros, debe seleccionar las fuentes de información que permitan obtener los mejores resultados al más bajo costo, que impliquen la mínima perdida de tiempo y puedan ser utilizados en distintas oportunidades.
121
El docente debe prever, seleccionar y organizar las fuentes de información que apoyarán cada situación de aprendizaje, con el objeto de lograr las mejores condiciones que permitan alcanzar los objetivos propuestos.
Mi práctica docente me ha demostrado que los alumnos no discriminan entre una y otra información bajada de la Internet. Lo que es aún más reprobable, algunos profesores universitarios frecuentemente obtienen información de la Red totalmente fuera de contexto y de tiempo y la proporcionan a los alumnos para el desarrollo de su aprendizaje.
La información que le proporcionamos al alumno debe ser pertinente, es decir, debe estar constituida por elementos que motiven la curiosidad del mismo y que le permitan desarrollar sus habilidades y destrezas, así como los valores que la propia universidad intenta inculcarle. La información debe ser suficiente para formar profesionales integrales, comprometidos con su tiempo y circunstancia.
La información debe estar disponible en el momento en que se necesite. La Red nos permite en todo momento, tener a la mano la información necesaria para desarrollar el aprendizaje de los alumnos y la capacitación del docente.
La información debe ser adecuada a las características de los alumnos. Cada alumno tiene diferentes formas de aprender, la Internet, nos proporciona elementos para poder satisfacer las necesidades de aprendizaje de cada uno de ellos. Corresponde al docente la función de identificar esa manera que el alumno tiene de aprender. Es esa una de las funciones más importantes del docente.
En realidad no todos los docentes son intermediarios entre los contenidos de la Red y el alumno. Por el contrario, algunos solo son un polo de esa relación. En la práctica, muchos profesores ordenan al alumno que busque información en la Red, cuando su
122
obligación es referirse a lugares, páginas en la Red, en donde el alumno pudiera encontrar la información que requiere y que después entre ambos analizarán, leerán de manera crítica para poder acceder al conocimiento.
“(La Internet)... Te abre más el horizonte de la información. Si por ejemplo un maestro te da un tema, lo desarrollas, ya buscas en Internet en donde hay especialistas que te están generando esa misma información pero con mayor profundidad, más actualizada, resumida" .
“La verdad, sería padre que los maestros tuvieran disponible una página, con el material necesario. Creo que facilitaría el estudio, sería más accesible. Yo sí la leería" ¦" 
“Es un punto fundamental porque, ahora, hablando del nuevo modelo educativo de la universidad, se está hablando de un sistema de tutorías, hay que trabajar como debe ser. Uno como alumno, a realizar investigación, al desarrollo y planteamiento de los temas y los maestros nos podrían asesorar y porque no hacerlo a través de la Web. Llegar al sitio donde el maestro tenga toda su información, todas sus bases de datos almacenadas y uno de ahí poder recoger información, hacer el análisis del trabajo y entregarlo el día que fuera requerido" . (CONSULTAR ANEXO 3)
Los alumnos intuyen que la Internet puede ser la base para un cambio de paradigmas en lo educativo, los maestros tenemos la obligación de guiarlos por el uso adecuado de la Internet y hacernos copartícipes del cambio educativo. Aquí cabe hacer hincapié en que el uso de la Internet en la escuela debe ser para hacer eficiente el proceso de aprendizaje y no para fomentar la idea de que todo es fácil en la educación y más con el uso de la Red.
La Internet como apoyo didáctico - pedagógico va adquiriendo un papel más relevante a medida que la moderna tecnología se va incorporando a la tarea educativa. En sus inicios la Red solo tenía objetivos militares y científicos, hoy día, es en la educación en donde la Internet esta llamada a cumplir con una de sus más grandes funciones. La prueba esta en la creación de la Internet 2. (VER ANEXO 1)
123
El uso de estos recursos depende del grado de conocimiento que sobre la Red tenga el profesor. Es necesario que la universidad capacite a sus docentes y alumnos sobre el análisis crítico de los contenidos bajados de la Internet. Un análisis crítico puede considerarse como la combinación de dos componentes:
* ¦ Un conjunto de habilidades para procesar y generar información y opiniones.
* ¦ El hábito, basado en un compromiso intelectual, de usar esas habilidades para guiar la conducta.
El profesor en su mayoría tiene miedo, o presenta resistencia al uso de la Internet. Una gran proporción de docentes tienen miedo al pensamiento creativo, están en contra de todo lo que amenace su rutina, su tradición pedagógica. Enseñan como sus maestros lea enseñaron a ellos. Es difícil que cambien, sin embargo es necesario que lo hagan. Para ello está la Internet y sus contenidos. Es función de la Institución, el hacer que los docentes manejen de manera correcta los recursos que brinda la Internet. Se plantea así un cambio de paradigma en la educación. La educación basada en el texto, debe fluir hacia el hipertexto. La interactividad, los significados, la interpretación deberán ser las competencias a enseñar en el futuro inmediato a los alumnos de educación superior y de manera general a todos los niveles. Es necesario que los alumnos comprendan, identifiquen la relación entre significante y significado, partes importantes de un signo. La Internet proporciona signos que es necesario desentrañar. Debemos enseñar al alumno para que pueda interactuar con la computadora y con la Red.
V.3 Los alumnos y la Internet
De los alumnos encuestados el 7% aseguró que ninguno de sus maestros tienen su propia página en la Internet, el 80% aseguró que solo algunos y el 13% afirmó que todos sus maestros tienen una página en la Web.
124
A la pregunta  ¿Estudia usted para el examen de alguna materia, con información localizada en la Internet? El 55% de los alumnos encuestados contestó que nunca lo hace, el 58% dijo que algunas veces y solo el 2% aseguró que siempre lo hace. El alumno no valora aún la importancia que puede tener la información bajada de la Internet, porque está inmerso aún en el carácter tradicional de la educación en donde es el profesor el que aporta la información, el sabelotodo que a petición y por delegación de la institución educativa pretende ser.
A la pregunta  ¿Cuál es el motivo más importante que te hace entrar a la Internet? La mayoría contestó que para buscar tareas. Esto es importante por que la Internet es mucho más que un lugar para encontrar tareas, solo que el alumno no tiene idea de todo lo que puede encontrar en ella. Es importante que el alumno sea enterado de todo lo que puede hacer en la Red.
La Red por su complejidad plantea la existencia de información sin estructura ni uniformidad, en algunos casos, información superficial y estandarizada que ocasiona una gran dispersión y disminución de la atención. Los alumnos encuestados manifestaron poner poca importancia a las ventajas que brinda la Internet en el proceso de aprendizaje. El 61% de los alumnos que entran a la Internet, lo hace solo para entretenerse, el 40% lo hace con el propósito de obtener información específica.
A la pregunta: En la Internet existen muchas formas de divertirse  ¿Es válido para ti que se haga ese uso de la Internet? El alumno contesta:
“Es válido. Por que como dije todos tienen derecho al acceso. Y en base a los principios de Internet, que era cultural, no es válido. Internet era para aprender, era para ayudar, era para el área laboral y cultural y ya se le ha encaminado a otros usos como es la pornografía y los juegos. Inclusive, ahora te aparecen juegos de azar, que son los casinos. Apuestas por Internet directamente. Y entonces, pues eso no se vale" .
125
“Creo que debe de haber de todo. En ocasiones no es muy recomendable darle un uso para otras cosas que no sea adquirir conocimientos. A la gente le interesa lo más fácil y lo más fácil, es la diversión" .
“Claro que sí. En la Red existen cosas muy interesantes para salir de la rutina" . (CONSULTAR ANEXO 3)
Y esa es otra de las grandes ventajas de la Internet. Por su naturaleza Hipermedia puede educar en la diversión. Puede generar, de esta manera, un aprendizaje lúdico. El aprendizaje lúdico es placentero, espontáneo y voluntario, tiene un fin en sí mismo, exige la participación activa del alumno y guarda ciertas relaciones con atrás actividades como la creatividad y la solución de problemas. Esta podría ser una nueva línea de investigación, se debe considerar, con mayor interés, el aprendizaje desde la perspectiva de los juegos.
El 28% de los entrevistados están de acuerdo en que la Internet solo sirve para divertirse, el 19% está en desacuerdo. El 53% no supo que contestar. Lo lúdico también puede ser aprovechado en el aprendizaje. Lo lúdico en el ciberespacio, su aspecto novedoso, debe ser aprovechado para aprender por parte de los alumnos. Debe ser aprovechado asimismo por el maestro para motivar al alumno.  ¿Quién pudiese negar que los juegos olímpicos pueden ser aprovechados como fuente inagotable de elementos para motivar el aprendizaje de las diferentes disciplinas?
A pesar de esto los alumnos de la U. de O. solo entran a la Internet cuando los maestros les encargan trabajos con información de la Red, es decir, por obligación. El 20% de los alumnos encuestados solo entra una vez al mes a la Internet, el 7% consulta la Red cada quince días, el 38% una vez a la semana, el 20% cada tercer día y solo el 15 % ingresa todos los días.
A la pregunta:  ¿Qué harías tú si te pidieran que organizaras un grupo de trabajo a través de la Internet? El alumno entrevistado contesta:
126
“Que sería bueno, que me parecería una buena propuesta, difícil, difícil por mi situación, y de mi situación me refiero al poco aprendizaje que me ha brindado la universidad. No me ha ayudado la universidad al decir les vamos a dar un curso de Internet, como hacer una Red" . (CONSULTAR ANEXO 3)
Considero necesario que el alumno reciba capacitación sobre el manejo de la Red. La Internet suele ser tan complicada, que a veces el alumno termina por alejarse de ella. El uso de la Internet se torna complejo y frustrante si no se tiene una adecuada capacitación.
Como vemos, la Internet nos proporciona recursos para hacer que el alumno se interese por el aprendizaje. Al buscar información para trabajos específicos de alguna asignatura, prefiere páginas con mucha información, pues, generalmente lo que desea es cumplir con la cantidad de páginas impresas que el profesor pone como límite para dar validez al trabajo, sin importar mucho que dicha información sea irrelevante o inútil. El alumno al encontrar información en la Red, no es seducido por los contenidos, no es motivado para seguir investigando, solo trata de cumplir con la cuota de hojas impresas que el solicita el docente. El 68% de los alumnos encuestados solo entra a la Internet para buscar información para elaborar trabajos encargados por los maestros.
Nos ha tocado vivir en una sociedad que posibilita la existencia de una industria cultural en expansión, que difunde y promociona determinados valores sociales en base a los parámetros marcados por la sociedad de consumo. La televisión y los demás medios de comunicación nos están llevando a una cultura de la imagen. De repente y por efecto de los medios, la imagen resulta ser lo más importante para todos.
A la pregunta:  ¿Cómo te gusta que sean las páginas que visitas en la Red.  ¿Con muchas imágenes o con muchos textos? El alumno contesta:
“Que sea una combinación. Que tenga imágenes y textos, para que se a atractiva la página" .
127
“Que tengan un buen diseño, que no sean cansadas. Ni mucho texto, ni mucha imagen.
 ¿Qué significa para ti que una página sea “cansada" ?
“Mucho texto, con un mal formato, con mal diseño. Una página reseca. Y ya, una página con buen diseño, atractiva, con colores, imágenes, movimiento, pues" ¦ como que entretiene un poquito más" .
“Interactivas, son interesantes. Pero me gustan con mucho texto y con pocas imágenes. Con una imagen sería suficiente. Para los trabajos de la universidad se necesitan páginas con mucho texto" . (CONSULTAR ANEXO 3)
El alumno considera aquí, que las páginas con muchas imágenes son las más interesantes. Si la página no cuenta con las mismas o con animación el alumno se aburre, no aprovecha el contenido. Solo en el caso en que el profesor le ponga una tarea y le asigne una determinada cantidad de páginas a cubrir con el trabajo, entonces sí, el alumno busca este tipo de información, pero, no la lee. Solo se limita a copiarla y en el mejor de los casos, a almacenarla en el disco duro, pero hasta ahí. La imagen predomina. El alumno la busca, por que vivimos en una cultura en la que predomina lo visual. Por que los demás medios así lo han determinado.
Por otra parte, las imágenes que se transmiten por la Internet, dado su carácter multimedia pueden ser capaces de engañar a los sentidos. El profesor en la actualidad debe estar capacitado para poder guiar a sus alumnos a través de las metáforas que regularmente son las imágenes que se publican en los hipermedia. Como lo sostiene Roland Barthes, las imágenes sin un texto que las expliquen no son nada, no nos dicen nada. En el aula el maestro tiene la obligación de explicar el mensaje que las imágenes y los textos pretenden darnos. El alumno tiene la falsa percepción de que la Internet soluciona todo problema de información, el alumno tiene la idea de que en la Red todo es fácil.
A la pregunta: Se dice que en la Internet se puede encontrar todo. Toda la información.  ¿Qué opinas al respecto? El alumno contesta lo siguiente:
128
“Se dice, se dice y está. Lo difícil es saber encontrarla. Saber como llegar, como tener la ruta de acceso a esa información. Incluso, tener el tiempo para enfrentarte a la computadora, para buscar la información. Si tiene suerte te va a llevar un minuto encontrar la información que buscas. Si tienes mala suerte te va a llevar 2 o 3 horas. Sobre todo esos buscadores que te dan 3500 o más páginas. Para cuando encuentras lo que buscas, ya se te fueron 2 o 3 horas" .
 ¿Qué actitud asumes cuando al buscar información en la Internet, no encuentras lo que necesitas?
“La verdad, me pongo histérica cuando no encuentro algún tipo de información. Cuando me tardo mucho, visito muchas páginas y no encuentro realmente la información que quiero. En Internet existe muchísima información, pero lo que uno quiere no se encuentra, entonces me pongo muy molesta" . (CONSULTAR ANEXO 3)
Es decir, existe la falsa percepción de que en la Internet todo es fácil. Que con un golpe de suerte el alumno va a encontrar lo que necesita. Lo cierto es que los docentes debemos preparar al alumno para que interactúe con la Red. De no hacerse lo estamos poniendo ante un reto que podría tener graves consecuencias.
Debe el profesor poder desarrollar en el alumno competencias basadas en un modelo participativo y constructivista de la enseñanza-aprendizaje. Que el alumno sea capaz de decodificar mensajes y contenidos, analizando, jerarquizando y ordenando la información difundida por los medios y las nuevas tecnologías de la información. En el constructivismo se promociona una estrategia pedagógica de motivación y autoexpresión. El supuesto fundamental del constructivismo es que los seres humanos construyen, a través de la experiencia, su propio conocimiento y no simplemente reciben la información procesada para comprenderla y usarla de inmediato; es necesario crear modelos mentales que puedan ser cambiados, amplificados, reconstruidos y acomodarlos a nuevas situaciones. Vivimos en un entorno cultural cambiante a velocidades supersónicas, lo que hoy es tecnología de punta, mañana mismo, ya es obsolescencia. El constructivismo es una propuesta de aprendizaje que se basa en el
129
supuesto de que los seres humanos construyen su propia concepción de la realidad y del mundo en que viven.
El profesional del futuro deberá ser capaz de utilizar los medios y las nuevas tecnologías para adquirir, generar y desarrollar el conocimiento que le permita adaptarse a una realidad en rápido proceso de cambio y transformación.
Por otro lado, al hacer un análisis más profundo de los resultados de la encuesta aplicada a alumnos de nuevo ingreso de la Universidad de Occidente, se obtuvieron los siguientes resultados:
De los 20 alumnos con calificaciones promedio de 9.0 a 10.0 el 60% navega menos de una hora en la Internet a la semana, el 25% navega dos horas, el 7.5% navega de dos a tres horas y otro 7.5% navega más de tres horas cada vez que ingresa a la Red. Con relación a los 20 alumnos con promedios más bajos (de 7.0% a 8.0) los resultados obtenidos son los siguientes: el 40% navega menos de una hora, el 35% navega de una a dos horas, el 20% navega de dos a tres horas, y solo el 5% navega más de tres horas.
Un análisis más acucioso de otros aspectos de la encuesta, nos muestra los siguientes resultados. De los 17 alumnos encuestados, que cuentan con computadora en su casa un 58% tiene calificaciones entre 9 .0 y 10.0, el resto tiene calificaciones entre 7.0 y 8.0. Las calificaciones de 9.0 a 10.0 pertenecen a alumnas que cuentan con computadora en su casa. Las calificaciones de 7.0 a 8.0 pertenecen a alumnos que también cuentan con computadora en su casa. Esto sugiere otra línea de investigación, que analice los efectos del uso de la Internet en los alumnos de educación superior desde una perspectiva de género.
Asimismo, de los 9 alumnos encuestados que cuentan con servicio de Internet en su casa (58% de los que tienen computadora en su casa) el 50% son alumnas de las cuales
130
el 50% tiene un promedio de calificaciones entre 9.0 y 10.0. Todos los alumnos que tienen servicio de Internet en su casa, tiene un promedio de calificaciones entre 7.0 y 8.0. Con estos resultados podemos inferir que no existe evidencia de que el tiempo de navegación en la Internet pueda afectar el promedio de calificaciones de los alumnos de la U. de O. Estos resultados son lógicos en cierta medida ya que la currícula de educación superior, a pesar de los grandes esfuerzos que han realizado, aún está basada en una cultura libresca, es decir, los alumnos todavía no han desarrollado las nuevas competencias que la sociedad de la información requiere de ellos y los maestros aún dependen del texto para preparar sus clases. En nuestra sociedad, aún se utiliza a la Internet solo como algo que complementa al aprendizaje a través de los libros, pero es evidente que debemos considerar a la utilización de este instrumento didáctico como algo inevitable, dado el nivel de avance del conocimiento en la actualidad.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD IBEROAMERICANA
ESTUDIOS CON RECONOCIMIENTO DE VALIDEZ OFICIAL POR
DECRETO PRESIDENCIAL DEL 3 DE ABRIL DE 1981
“LA NAVEGACIÓN EN LA INTERNET Y SUS EFECTOS EN
EL APRENDIZAJE DE LOS ALUMNOS DE NUEVO INGRESO
EN LA U DE O CICLO ESCOLAR 2001-2002" 
TESIS
Que para obtener el grado de
MAESTRO EN COMUNICACIÓN
Presenta:
JUAN DE DIOS RODRIGUEZ BALDENEBRO.
Director:
Dr. Jesús Octavio Elizondo Martínez
Revisores:
Dr. Juan Francisco Escobedo Delgado
Mtra. Liliana Borquez Borbón
México, D. F. 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>UNA ESTRATEGIA DE INTERVENCIÓN PARA EMPRESAS PROVEEDORAS DE SERVICIOS DE CONSULTORÍA EN SISTEMAS COMPUTACIONALES</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En años recientes la Industria del Desarrollo de Software en México opera bajo un esquema
perverso de precios bajos, en el que las empresas con menor poder de negociación
normalmente son las más castigadas hasta el grado de desaparecer. En este medio los retos a enfrentar definen las siguientes áreas críticas, sobre las cuales debe ponerse principal atención para el desarrollo de la industria</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar una estrategia de intervención enfocada a empresas consultoras en sistemas
computacionales, basada en gestión por procesos para la alta dirección y la gerencia, así como la operación de proyectos de desarrollo y mantenimiento de software.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La oportunidad en el entorno global de la Industria de Software mexicana continua presente, sin embargo la limitada capacidad de las pequeñas y medianas empresas para asegurar la calidad de sus procesos, certificarse y definir estrategias, que son las que demanda el mercado nacional y global, la falta de recursos humanos certificados en alguna tecnología en particular y la carencia de infraestructura adecuada impide la evolución del sector.
Por otro lado no solo las áreas críticas que señala la industria forman parte de las deficiencias que se presentan; La falta de un enfoque interno que se encuentre dirigido a los problemas derivados de la administración general, el diseño de procesos de la organización, que en algunos casos son demasiado complejos y que a su vez no cuentan con indicadores de medición, son otro de los problemas que presentan las empresas del sector.
En este contexto del análisis realizado a la industria se identifican dos enfoques diferentes. Un enfoque externo relacionado con las áreas críticas que presenta la industria; y un enfoque interno relacionado con la gestión propia de las empresa del sector, que en su mayoría son de tamaño micro y pequeño.
El proceso de intervención, que consta de la ubicación y el diseño del estado actual del sistema de la organización y la descripción de la función y principales actividades de la misma.
Posteriormente, del diagnóstico del cual se identificaran los principales problemas por resolver y se formulan las estrategias que permitieran enfrentarlos, mismas que sirven de pauta para diseñar el estado deseado de la organización, frece una solución a ambos enfoques, en la parte externa al utilizar la SSM y el análisis FODA, se tienen estrategias que permiten tomar decisiones y acciones especificas en cuanto a las áreas criticas de la empresa, las oportunidades de mercado que se pueden atacar y las fortalezas y debilidades para sostener esas estrategias, en el enfoque interno, el modelo de proceso ofrece una pauta que permite a una micro y pequeña empresa un esquema de trabajo diario planeado, medido y controlado para poder ofreces un servicio que cumpla con los requerimientos de los clientes.
Finalmente se diseño un sistema de indicadores para cada uno de los procesos propuestos.
Como resultado del proceso de intervención, se identifican los siguientes elementos clave para la integración y desarrollo de la estrategia propuesta:
* La estrategia de intervención desarrollada esta estructurada en los tres niveles que
conforman una empresa, el nivel estratégico, el táctico y el operativo. La carencia de las
adecuadas acciones en cualquiera de estos niveles inutilizara las acciones realizadas en los otros dos.
* Las propuestas a nivel estratégico y táctico, son de aplicación general y no están limitadas por las particularidades del nivel operativo, esto abre la oportunidad de tomar esta estrategia como base para desarrollar estrategias similares pero aplicadas a otras esferas de negocios.
* La poca información existente para la gestión de procesos de desarrollo de software no es fácilmente aplicable, dadas las características del mercado interno de la Industria del
Software en México, es decir, no siempre los lineamientos son prácticos a la realidad de las empresas. Pero con la alineación a la estrategia de intervención propuesta, se espera salvar estos obstáculos y proporcionar una herramienta practica para las MPyMES que conforman la Industria del Software en México.
Como siguiente nivel queda sumarse a la tarea de diseñar una metodología que permita llevar a cabo la implantación de la estrategia propuesta. Las líneas de investigación propuestas son las siguientes.
1. En lo relacionado con la Gestión del Negocio se propone la elaboración de un Plan
Estratégico que formalice los objetivos de la organización, la forma de medir el logro de los objetivos, los procesos requeridos, la cartera de proyectos, la estructura de la organización, así como un Plan de Comunicación para dar a conocer los elementos del Plan Estratégico.
2. Como área de oportunidad del negocio se encuentra la elaboración de un Plan de Gestión de Proyectos que integre los objetivos, alcance, recursos, acciones y programa de trabajo para generar y cerrar oportunidades de proyectos, así como la descripción de las actividades para gestionar los proyectos externos e internos.
3. En lo relacionado a la Operación de Proyectos de Desarrollo y Mantenimiento de
Software se propone la adopción de Metodologías Ágiles como apoyo para estructurar,
planear y controlar el proceso de desarrollo de software. En las estrategias ágiles se intenta ser lo más flexible posible, que el cliente pueda cambiar los requisitos cuando quiera y que el código funcione bien. Para ello se valoran las siguientes ideas:
* Individuos e interacción frente a procesos y herramientas.
* Software que funciona frente a documentación exhaustiva.
* Colaboración del cliente frente a contratos.
* Responder al cambio frente a seguir el plan.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD NACIONAL AUTÓNOMA DE MÉXICO
PROGRAMA DE MAESTRÍA Y DOCTORADO EN INGENIERÍA.
FACULTAD DE INGENIERÍA.
UNA ESTRATEGIA DE INTERVENCIÓN PARA EMPRESAS
PROVEEDORAS DE SERVICIOS DE CONSULTORÍA EN
SISTEMAS COMPUTACIONALES
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN INGENIERÍA.
INGENIERÍA DE SISTEMAS EN PLANEACIÓN.
PRESENTA:
SALMA LUCERO SÁNCHEZ OCAMPO
TUTOR:
DR. JAVIER SUÁREZ ROCHA.
2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>PROPUESTA METODOLÓGICA PARA EL ANÁLISIS TÉCNICO Y ECONÓMICO DEL USO DE CALENTADORES SOLARES DE AGUA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo de este trabajo es proponer una metodología de evaluación técnica y económica,
que permita determinar cuantitativamente los beneficios económicos y ambientales que una
familia que habite en la Ciudad de México o su zona metropolitana obtendría si decidiera
acoplar un calentador solar de agua a uno convencional en su vivienda, para el
calentamiento de agua exclusivamente para la ducha.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En este trabajo se toma el Objetivo 10 del Plan Nacional de Desarrollo, “Reducir las
emisiones de GEI, enfocándolo principalmente en la promoción del uso eficiente de energía
en el ámbito doméstico, en particular para el calentamiento de agua para la ducha en el
sector residencial.
Se considera que las emisiones generadas principalmente en las grandes ciudades mediante
la utilización de tecnologías convencionales que trabajan con combustibles fósiles, las cuales en la mayor parte de los casos presentan un atraso considerable por lo que tienen baja eficiencia, contribuyen en forma muy clara en las emisiones de GEI del país.
Dado que el Objetivo 11 aborda el tema de la “difusión de información, se cree necesario
continuar en forma más amplia la campaña de información y concientización a nivel
nacional en el uso y aprovechamiento de las energías renovables en el sector residencial, ya que es en éste donde se tiene la mayoría de los usuarios de energía, con lo cual se espera contribuir en la disminución de las emisiones de GEI y al ahorro energético en el país.
Por ello, para el presente trabajo se desarrollará una herramienta que permita informar y
dar a conocer al ciudadano común, los beneficios económicos y ambientales que puede
obtener en su hogar, derivados de la utilización en forma híbrida de calentadores solares de agua para el uso diario en la ducha.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>De acuerdo al objetivo principal de este trabajo, el cual fue desarrollar una metodología de
evaluación técnica y económica que permitiera determinar cuantitativamente los beneficios
económicos y ambientales que, podría obtener una familia que habite en la Ciudad de
México o su zona metropolitana, si decidiera acoplar un calentador solar de agua a uno
convencional en su vivienda, exclusivamente para el calentamiento de agua para la ducha,
y de acuerdo al análisis de los resultados generados mediante la utilización de la herramienta
HIFUCSA, se obtuvieron las siguientes conclusiones:
* La metodología desarrollada permite obtener: a) un análisis técnico, el cual
determina el volumen de combustible consumido para satisfacer la demanda
adicional de agua caliente y las emisiones generadas de CO2 al ambiente, y b) un
análisis económico, el cual determina la rentabilidad y viabilidad económica en el
uso de la tecnología solar para el calentamiento de agua para la ducha. Por ello
consideramos que en este sentido se cumplen los objetivos particulares de la
presente tesis.
* Mediante la aplicación de dicha metodología y empleando la herramienta HIFUCSA,
se pueden observar y visualizar en forma práctica y sencilla los parámetros
importantes en este tipo de tecnologías, así como las ventajas y beneficios que se
obtienen al instalar un calentador solar acoplado a uno convencional de gas. En este
sentido se cumple con el objetivo de estimar los beneficios energéticos, económicos y
ambientales que nos planteamos.
Conclusiones del análisis técnico
* Los casos analizados muestran que los ahorros son mayores para las familias que
cuentan con un calentador solar de depósito; sin embargo, dichas familias gastan
más en combustible que aquellas que cuentan con un calentador convencional
instantáneo.
* Las familias que utilizan un calentador instantáneo con Gas LP, consumen menos
combustibles fósiles.
* El mayor ahorro energético, al acoplar un CSA, lo obtienen las familias que utilizan
Gas LP.
* La reducción de emisiones generadas al medio ambiente es mayor cuando se utiliza
Gas Natural en lugar de Gas LP.
* A nivel técnico, es conveniente que familias con más de cinco integrantes utilicen
utilizar 2 CSA, ya que obtienen un considerable ahorro energético y una reducción
en las emisiones de CO2 al ambiente.
Conclusiones del análisis económico
* Las técnicas de evaluación de proyectos, utilizadas en todos los casos, arrojaron
valores favorables, por lo que se concluye que es económicamente factible el
proyecto.
* La recuperación de la inversión es menor cuando la familia cuenta con calentador de
depósito y utiliza Gas Natural.
* La recuperación de la inversión es mayor cuando la familia cuenta con un calentador
Instantáneo y utiliza Gas LP.
* El mejor rendimiento de la inversión se obtiene con un calentador de depósito y
empleando Gas Natural.
* El VPN es menor para el caso que emplea un calentador instantáneo con Gas LP;
mientras que es mayor para el caso que usa un calentador de depósito y Gas
Natural.
* En el caso de la TIR, muestra un valor mayor para el calentador de depósito con Gas
Natural; mientras que para el caso calentador instantáneo con Gas LP se reduce
significativamente, sin embargo en ningún caso la TIR es menor que la TMAR, por lo
que el proyecto sigue siendo factible.
* Los valores arrojados por las técnicas utilizadas muestran que el caso en el cual
sería más conveniente tomar la decisión de acoplar un CSA a un calentador
convencional, es aquella en que la familia cuenta con la configuración Calentador de
DeÃ¡Â¹â"osito y Gas Natural, seguida la confiuración Calentador de Depósito y Gas LP.
* Para el caso de familias mayores de 5 personas, al acoplar un segundo CSA, la
inversión inicial se duplica, pero también es el caso de los ahorros, por lo que los
valores de la TR, ROI y TIR permanecen constantes.
En resumen, podemos concluir que utilizar un CSA para uso residencial puede considerarse
como una buena opción para obtener beneficios tanto en el ahorro energético como a nivel
ambiental, así como una disminución en el gasto por combustible para las familias que
decidan acoplarlo a su calentador convencional para calentar agua para la ducha,
independientemente del sistema convencional con que se cuente.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD NACIONAL AUTÓNOMA DE MÉXICO
PROGRAMA DE MAESTRÍA Y DOCTORADO EN
INGENIERÍA
FACULTAD DE INGENIERÍA
PROPUESTA METODOLÓGICA PARA EL ANÁLISIS
TÉCNICO Y ECONÓMICO DEL USO DE
CALENTADORES SOLARES DE AGUA
T E S I S
QUE PARA OPTAR POR EL GRADO DE:
MAESTRA EN INGENIERÍA
SISTEMAS - INVESTIGACIÓN DE OPERACIONES
P R E S E N T A :
L. I. LOURDES YOLANDA FLORES SALGADO
TUTOR:
M. en I. HIRAM RUIZ ESPARZA GONZÁLEZ
2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Minimización del ataque DDoS sobre servidores de comercio electrónico en Internet, utilizando las técnicas de predicción linea l2</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En este trabajo se aborda una descripción general de las variantes del ataques DoS. Sin
embargo, la variante de interés, es la del ataque DDoS, la cual será el tema central de este trabajo. Se utilizará el concepto de Predicción Lineal, para detectar dicha variante y posteriormente se describirá un método para minimizar el mencionado ataque; también a
manera de ejemplo se llevará a cabo un diseño del método empleado para observar sus
componentes más importantes.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo de este trabajo es diseñar un mecanismo que permita minimizar los efectos del
ataque DDoS sobre servidores de comercio electrónico en Internet. Sin embargo, este
mecanismo no se debe de tener funcionando siempre, debido a un gasto de recursos
innecesario, es por ello, que se utilizará la Predicción Lineal para detectar el comienzo de un ataque de esta naturaleza.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En definitiva el ataque DoS con todas sus diferentes variantes, incluida la variante DDoS, es sumamente peligroso para los servidores que se dedican al comercio electrónico, debido a las perdidas que se registran por efectos de las distintas variantes de este ataque. Por otro lado, las soluciones convencionales o las recomendaciones mencionadas por la literatura, no hace frente de manera solida a la amenaza y debido a esto, este ataque es mucho más peligroso que un malware común que se pudiera esconder un equipo de oficina.
Entendida su naturaleza del ataque DDoS no se puede esperar resolver, con algún software
antivirus o algo semejante, pues no es un modulo de software que se encuentre corriendo en
el servidor con alguna firma fácil de detectar, sino que es una enorme oleada de tráfico, la cual hay que contener de alguna manera.
La solución propuesta en este trabajo, no versa en la sencillez, de un simple algoritmo, sin embargo, no es lo bastante complicada o compleja, para no implantarse, y minimizar los
efectos del mencionado ataque.
Como se explico en el capítulo 5, los requerimientos tecnológicos son muy pocos, los módulos de software a diseñar son muy sencillos, las búsquedas en las tablas del Horizonte de Sucesos,fueron optimizadas y siendo un poco diestro en el lenguaje de programación que se deseará emplear, todavía se podría mejorar aún más.
Hablando del esquema visto en los ejemplos de:
Internet  Horizonte de Sucesos  Router  Switch  Servidor</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD AUTONOMA NACIONAL DE MEXICO 
PROGRAMA DE MAESTRIA Y DOCTORADO EN INGENIERIA 
FACULTAD DE INGENIERÍA
NOMBRE DE LA TESIS
T E S I S
QUE PARA OPTAR POR EL GRADO DE:
MAESTRO EN INGENIERIA
INGENIERÍA ELECTRICA EN TELECOMUNICACIONES
P R E S E N T A:
ING. CUAUHTEMOC CARLON CARLON
TUTOR:
Dr. ENRIQUE DALTABUIT GODAS
2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>IMPLEMENTACIÓN DE MECANISMOS DE SEGURIDAD EN EL SISTEMA DE GESTIÓN ACADÉMICO/ADMINISTRATIVA DEL POSGRADO DE LA FACULTAD DE ECONOMÍA, UNAM</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Cualquier sistema de comunicaciones conlleva diversos riesgos operacionales que pueden tener su origen en diversas fuentes: errores en la entrada de datos, amenazas internas y/o externas, la propia infraestructura de red, la ingeniería social e incluso desastres naturales.
La seguridad establecida en la División de Estudios de Posgrado de la Facultad de Economía
comienza a ser insuficiente para la infraestructura actual de comunicaciones y operaciones que se realizan, para los sistemas de información con que se cuenta así como para el crecimiento que se está dando en cuanto a proyectos de investigación e interrelación con investigadores de otras entidades académicas nacionales e internacionales.
La información sensible y/o crítica de la DEP-FE al ser procesada, almacenada, impresa,
eliminada y transmitida a través de diversos medios dentro y fuera de la organización debe ser
protegida, por lo que cuanto antes se adopten las medidas preventivas y correctivas para evitar
y/o disminuir las vulnerabilidades de seguridad, éstas representarán un menor costo y serán más
efectivas con la consecuente minimización del riesgo o pérdida.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El presente trabajo de tesis tiene como objetivo la implementación de mecanismos de seguridad en los sistemas de información académico-administrativos de la División de Estudios de Posgrado de la Facultad de Economía que protejan la información con la que se trabaja de manera cotidiana.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para realizar el presente trabajo se utilizó como metodología de desarrollo la norma ISO/IEC 17799, la cual integra aspectos relacionados con las tecnologías de la información y aspectos administrativos tales como dirección, supervisión y control de recursos de las organizaciones a niveles tácticos, estratégicos y operativos, constituyendo así un marco de seguridad a todos los niveles dentro de la organización.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La seguridad de la información ha cobrado gran importancia en los últimos años, en especial porque casi todas las organizaciones han automatizado el proceso y almacenamiento de su información.
Esta seguridad en los sistemas de información debe ser vista a tres grandes niveles: seguridad interna, seguridad externa y seguridad de interfase. La seguridad externa se ocupa de la protección contra intrusos y desastres; la seguridad de la interfase con el usuario se encarga de establecer la identidad del usuario antes de permitir el acceso a los sistemas, y la seguridad interna se encarga de asegurar una operación confiable y sin problemas que garantice la integridad de los datos.
Estos niveles de seguridad son distintos para cada organización, ya que cada una requerirá un nivel de seguridad específico debido a que sus necesidades definen lo que para esa entidad significa seguridad.
Para implementar un nivel específico de seguridad es importante apoyarse en estándares, ya que el uso de éstos brinda mayor confianza debido a que cualquier especialista puede entenderlo, y no se está limitado al uso del producto de un fabricante particular; es por ello que se eligió el estándar internacional ISO/IEC 17799 cuyo objetivo es mejorar la eficiencia de la seguridad de la información mediante la implementación de una serie de controles asociados a las tecnologías de la información y a la infraestructura de comunicaciones, la cual proporciona las siguientes ventajas:
* Aumento de la seguridad efectiva de los sistemas de información
* Correcta planificación y gestión de la seguridad
* Garantías de continuidad del negocio
* Mejora continua a través del proceso de auditoría interna
* Incremento en los niveles de confianza
* Aumento del valor comercial y mejora de imagen de la organización
ste estándar sirvió de apoyo para desarrollar un Sistema de Gestión de Seguridad de la
no de los graves problemas que subsiste en la implementación de cualquier control de seguridad
* Auditorías de seguridad más precisas y fiables E Información, cuya implementación es bastante compleja y requiere de un alto grado de compromiso, ya que al involucrar a todos los empleados de la organización implica el manejo de todas sus necesidades, así como del análisis de comportamientos y malas prácticas por parte de ellos o de externos.
U es la renuencia de los usuarios finales a acatar las políticas de seguridad, sobre todo en lo referente a la navegación por Internet y al manejo de correos electrónicos. Es por esto que se hace evidente la concientización de los usuarios respecto a las implicaciones de los problemas de seguridad, lo cual puede prevenir y disminuir el impacto de los incidentes cuando éstos ocurran.
Cabe aclarar que para mantener un nivel de seguridad específico se debe mantener actualizado el SGSI, ya que cubre aspectos relacionados con la política, la estructura organizacional, los procedimientos, los procesos y los recursos necesarios para la gestión de la seguridad de la información que a su vez contempla todas las áreas organizacionales de cualquier entidad, pues se cubren aspectos organizacionales, lógicos, y físicos, los cuales pueden estar en constante cambio.
Es importante destacar que no existe un control de seguridad único, sino que se deben combinar diversas tecnologías y herramientas utilizadas tanto en la protección como en ataques a los sistemas de información, para que con ello las organizaciones cuenten con diversas capas de seguridad y así poder detectar el problema en algunos de estos puntos antes de que llegue a la información crucial. Estas actualizaciones deben ser dadas a conocer a todo el personal involucrado, para que así realmente sean atendidas y respetadas.
Ahora bien, considerando que las causas de incidentes de seguridad informática pueden tener diversas fuentes (incluyendo la inexperiencia, el mal uso de las aplicaciones, los empleados disgustados y accidentes de mantenimiento), cualquier infraestructura de información debe contar, al menos, con las siguientes herramientas de seguridad:
* Software antivirus. Debe tenerse un mantenimiento adecuado para garantizar la protección
de los quipos.
* Infraestructura de red segura. Deben considerarse conmutadores, switches y ruteadores que permitan una seguridad mínima a través de filtrado de paquetes y servicios de
identificación y autenticación, así como una conectividad segura implementando a su vez
la seguridad perimetral.
* Hardware y software dedicados a la seguridad de la red. En este caso se puede hablar de firewalls, tanto institucionales como personales.
* Servicios de identidad. Servicios que permiten la identificación de usuarios y las operaciones que realizan sobre la red. Estos servicios deben incluir contraseñas, certificados digitales y claves de autenticación digital.
* Encriptamiento. Mecanismos que garantizan que la información no pueda ser interceptada o
leída por elementos no autorizados.
* Administración de la seguridad. Mecanis
entre los elementos mencionados a fin de mantener un nivel mínimo de seguridad para el
sistema en cuestión. Tenemos mediante los cuales se conserva la interrelación
Con los mecanismos de seguridad y la Política de Seguridad de la Información implementados
podemos decir que se ha avanzado en la protección de la información generada, almacenada,
transmitida y procesada de manera cotidiana en la División de Estudios de Posgrado de la
Facultad de Economía.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD NACIONAL AUTONOMA DE MEXICO
PROGRAMA DE MAESTRIA Y DOCTORADO
EN INGENIERIA
FACULTAD DE INGENIERIA
IMPLEMENTACIÓN DE MECANISMOS DE SEGURIDAD EN EL
SISTEMA DE GESTIÓN ACADÉMICO/ADMINISTRATIVA DEL
POSGRADO DE LA FACULTAD DE ECONOMÍA, UNAM
TESIS
QUE PARA OPTAR POR EL GRADO DE
MAESTRA EN INGENIERÍA
INGENIERÍA ELÉCTRICA EN TELECOMUNICACIONES
P R E S E N T A :
LAURA LÁZARO RODRÍGUEZ
DIRECTOR DE TESIS
DR. FRANCISCO J. GARCÍA UGALDE
CODIRECTOR
M. EN I. JORGE VALERIANO ASSEM
MEXICO, D.F. 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Análisis del Comportamiento Dinámico de Redes Inalámbricas de Banda Ancha Fijas Basadas en el Estándar IEEE 802.16</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La principal área de oportunidad que nos brindan las redes IEEE 802.16 es, que el alcance del estándar no
define como forzosos muchos detalles técnicos, por ejemplo las siguientes cuatro funciones para que una
red inalámbrica de banda ancha soporte el concepto de calidad de servicio (QoS- Quality of Service):
*	El control de admisión de usuarios, así como la reservación y asignación de ancho de banda (tema
de nuestro análisis).
*	Orden de transmisión de los paquetes en función al tipo de clase de servicio (QoS). Se utilizan
algoritmos conocidos como “Traffic Scheduling" .
*	Clasificación de tráfico.
*	Técnicas para monitorear y organizar el tráfico entrante en la red. Los algoritmos utilizados se
denominan de “shaping"  y “policing" .
Estas faltas de definición de una regla general hacen la diferencia al momento de desarrollar un algoritmo e
implementarlo en un equipo que cumpla con la certificación WiMAX.
Esto nos da la pauta de poder aportar ideas y procedimientos con el fin de hacer eficaz aquellas partes que
aún no están completamente definidas.
En el caso particular de controlar el acceso al medio (manejo de colisiones de los usuarios que tienen que
competir por ancho de banda), como hemos mencionado, el estándar IEEE 802.16 no especifica cómo, ni
cuál algoritmo de control de admisión sobre nuevas peticiones de ancho de banda debería implementarse.
Los sistemas basados en este estándar se caracterizan por tener una configuración punto-multipunto (PMP),
en donde una sola EB (Estación Base) provee de una conexión (con sus respectivos servicios) a un conjunto
de ESs. Por lo tanto varias ESs podrían estar compitiendo por obtener los servicios de dicha conexión al
mismo tiempo, por eso la relevancia de tener un buen algoritmo para resolver rápidamente las colisiones
que pudieran generarse.
La resolución de colisiones se considera para el caso de aquellos usuarios con una clase de servicio (QoS)
no periódica, es decir, en aquellas clases de servicio que no tienen previamente reservada una región para
transmitir, por ejemplo BE.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El principal objetivo de la elaboración de este trabajo es la de utilizar un modelo matemático, más sencillo
que lo hasta ahora desarrollado con probabilidades binomiales (sobre GPRS, detalladamente descrito en las
referencias [ 1 ] y [ 2 ]).La idea es, generar distintos escenarios teóricos en una red basada en el estándar
IEEE 802.16-2004, mismos que podrán ser evaluados dinámicamente (vía simulación).</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Procederemos a ejecutar el método científico con el fin de obtener conocimiento y plasmarlo en este trabajo
de Tesis.
Elegir y definir el problema, en nuestro caso es proponer estrategias de mejoramiento en la resolución de
colisiones durante el periodo de contención para solicitar ancho de banda para aquellas clases de servicio
que no sean periódicas.
Estructurar un marco teórico. Inicialmente nuestro desarrollo se basará en el paradigma de resolución de
colisiones en las ranuras de contención utilizadas para solicitar ancho de banda (para clases de servicio no
periódicas), analizado a detalle en el rango de referencias marcadas entre [6 y 11]. Por lo tanto tomaremos
en cuenta publicaciones que analicen el comportamiento de dichas ranuras para redes inalámbricas, como
la descrita en la referencia [ 7 ].
Nuestro punto de partida serán algunos estudios previos realizados sobre manejo de tráfico IP sobre una red
GSM utilizando las capacidades de GPRS, analizados a detalle en las referencias [ 1 ] y [ 2 ]. Está
demostrado, con la implementación de esta técnica en redes celulares, que se mejora el uso de los recursos
de la red, proporcionando a los usuarios una forma de transmitir datos en una red inalámbrica diseñada para
transmitir originalmente voz.
Se procederá a realizar un análisis exhaustivo sobre esta tecnología con el fin de obtener los conocimientos
y hacer las adaptaciones necesarias para aplicar ciertos conceptos en una red IEEE 802.16. Y así, reusar
algunos de los mejores criterios para implementar un sistema eficiente para administrar las colisiones en las
ranuras de contención.
Es muy importante considerar los avances relacionados con algoritmos aplicados en la capa MAC para
redes IEEE 802.16 con mejoras a los algoritmos ya existentes, como el mostrado en la referencia [ 4 ], con
la idea de no duplicar o imitar desarrollos anteriores.
Establecer hipótesis acorde a la solución esperada. Nuestra hipótesis inicial se basa en que una vez
que se desarrolló y probó un modelo teórico que proponga un manejo eficiente de las ranuras de contención
y después de analizar el comportamiento dinámico de una red basada en el estándar IEEE 802.16 podamos
obtener mejores tiempos de respuesta a los que actualmente existen, un ejemplo de ello se muestra en la
referencia [ 8 ].
Proponer una forma eficiente de administrar las colisiones en el área de contención, lo cual se traducirá en
una utilización eficiente del espectro radioeléctrico (dado que se disminuirá y en el mejor de los casos se
eliminarán las colisiones), no importando del tipo de clase de servicio que tengan los usuarios ni el número
de usuarios simultáneos en la red.
Probando nuestras hipótesis. Para la parte del análisis de los algoritmos implementados en el modelo de
GPRS se utilizará el modelo de GSM (GPRS) desarrollado para MATLAB Versión 2007b, el cual fue ya
previamente demostrado como un caso de éxito en la referencia [ 12 ].
Una vez teniendo los resultados usando dicho modelo (iterativo utilizando probabilidades binomiales)
aplicado a una red basada en IEEE 802.16, se probará un nuevo modelo basado en ecuaciones en
diferencias con el fin de simplificar el desarrollo matemático. Así, reducimos la necesidad de procesamiento
para obtener los resultados teóricos similares al modelo binomial original pero con menos iteraciones.
En el caso de probar las adaptaciones y mejoras de los modelos desarrollados para GPRS en una red IEEE
802.16 se utilizarán los ejemplos de simulación en OPNET Modeler ® versión 14, OPNET Wireless Suite y el
Modelo WiMAX más actualizado para este software (versión Julio 2007) implementadas en el trabajo
descrito en la referencia [ 5 ].</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Dando un puntual seguimiento a las hipótesis propuestas y después de haber analizado a fondo, como
antecedente, los trabajos anteriores sobre la eficiencia de una red GSM (usando GPRS). El primer objetivo
fue el utilizar un modelo matemático más sencillo que nos llevara a obtener resultados teóricos satisfactorios y
a su vez, nos permitiera requerir menos recursos informáticos al aplicarlo a una red inalámbrica de banda
ancha basada en el estándar IEEE 802.16-2004.
Contundentemente podemos afirmar que el tiempo que toma el nuevo modelo (con ecuaciones en
diferencias) en converger en relación con el de la red GSM (considerando la probabilidad de captura y manejo
de probabilidades binomiales), el cual es casi 8 veces menor.
En la mayoría de los casos, inclusive para el caso de cuando hay 100 usuarios simultáneos en la red, el
nuevo modelo itera solo 4 veces antes de encontrar el componente del retardo total del ciclo de transmisión
de un paquetes que corresponde al tiempo que el paquete pasa en contención (“C" ). El encontrar el valor de
este dato depende de parámetros probabilísticos, motivo por el cual, determinar este componente del retardo
(contención) en el proceso de transmisión de un paquete implica la parte más complicada del algoritmo.
Una vez cumplido el primer objetivo, procedimos a adaptar el nuevo modelo matemático con las
características de la capa de acceso al medio de una red inalámbrica de banda ancha basada en el estándar
IEEE 802.16-2004 a realizar un sin número de pruebas teóricas variando todos los parámetros posibles.
Los parámetros que fueron variando y con los que obtuvimos los resultados teóricos fueron:
*	Slots de contención.
*	Tamaño de los paquetes.
*	Número de usuarios activos en la red.
*	Tamaño del UL-MAP.
La idea era contar con suficiente información para poder dar una opinión técnica sobre cuales son los mejores
parámetros de configuración del sistema en un momento dado para construir los escenarios y así la red
funcione de manera eficiente para aquellos usuarios que tengan una calidad de servicio (QoS) del tipo BE.
Para poder tener los escenarios teóricos, y como se menciona en el Capítulo 5, se desarrollaron 2 programas
con la finalidad de:
*	Implementar el nuevo modelo matemático. Este programa fue elaborado en Visual C++) y se incluye la
versión con comentarios detallados en el Apéndice B.
*	Obtener el número de bloques (codewords usando codificación Reed-Solomon) requeridos para
transmitir paquetes de distinto tamaño. Este programa fue elaborado con Visual C++ y se incluye la
versión con comentarios en el Apéndice C.
Con el fin de mostrar claramente los resultados teóricos se utilizaron gráficas (se incluyeron tridimensionales)
en donde se muestra en una lámina todos los escenarios posibles.
De dichas gráficas podemos concluir que para que un sistema inalámbrico basado en el estándar IEEE
802.16-2004 tenga la eficiencia suficiente para poderlo hacer comercial se deben tener un escenario en el que
la EB pueda manejar todos los parámetros analizados en forma dinámica.
Al diseñar el modelo del sistema inalámbrico de banda ancha se debe tener en cuenta la capacidad de
adaptación dinámica a los requerimientos (ancho de banda, QoS, MAC, etc) de todos los usuarios en un
momento dado.
Una de las hipótesis iniciales se basaba en la eficiencia de la red inalámbrica podría depender de la cantidad
de slots de contención, pero en realidad la eficiencia depende también de otros parámetros. Por ejemplo, el
tamaño de los paquetes codificados que se están transmitiendo.
Lo que sí es muy importante hacer notar es que la cantidad adecuada de slots de contención es directamente
proporcional al tamaño de los paquetes que los usuarios transmiten en forma simultánea, recordemos que los
paquetes son codificados antes de ser transmitidos. Entonces el número de slots de contención debe ser
múltiplo (idealmente) del número de bloques codificados que se requieren para transmitir un paquete de cierto
tamaño.
Es crítico para la eficiencia del sistema que se ajuste el número de slots de contención en función del tamaño
de los paquetes en forma adecuada, ya que de no hacerlo entonces tendremos un impacto negativo, adicional
al aumento del número de usuarios simultáneos.
Como hemos ya concluido, el manejo de los slots de contención es delicado para la eficiencia de la red,
porque tan malo es tener en exceso como tener pocos.
Cuando tenemos pocos, debido a que los paquetes que transmiten los usuarios son grandes (más de 512
bytes) tenemos una eficiencia muy buena pero tenemos como consecuencia negativa un incremento en la
demora en el procesamiento y transmisión de los paquetes. La EB sólo puede atender a pocos usuarios
contendientes a la vez, generando más colisiones en la zona de contención.
En nuestro trabajo se presentó una extensión a lo anteriormente propuesto para el manejo de la subcapa de
acceso al medio de una red inalámbrica basada en el estándar IEEE 802.16-2004. Los resultados obtenidos
de citado modelo analítico han sido verificados vía simulación.
Encontramos además que el modelo matemático nuevo es lo suficiente preciso como para analizar las
tendencias en la eficiencia del sistema, dado que en todos los casos se obtienen resultados numéricos dentro
de los intervalos de confianza.
Con este modelo, se deja un escenario abierto a realizar investigaciones adicionales que tengan que ver con
la inclusión de un algoritmo de backoff con el fin de optimizar las colisiones de los usuarios que contienden por
ancho de banda, y así disminuir el retardo de espera para transmitir en periodos de alta utilización.
También queda pendiente la experimentación con ciertos tipos de tráfico específicos, adicional al tamaño de
los paquetes, ciertas aplicaciones como HTTP o VoIP presentan comportamiento en el manejo de la
transmisión de sus datos totalmente diferentes. Hay que generar escenarios en los que el sistema maneje
simultáneamente distintas aplicaciones y generar las mismas gráficas de este análisis para acercarnos cada
vez más a un verdadero punto de equilibrio al momento de diseñar una red con las características del
estándar IEEE 802.16-2004.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD NACIONAL AUTÓNOMA DE MÉXICO
POSGRADO EN CIENCIA E INGENIERÍA DE LA COMPUTACIÓN
Análisis del Comportamiento Dinámico de Redes
Inalámbricas de Banda Ancha Fijas Basadas en el
Estándar IEEE 802.16
Tesis para obtener el grado de
Maestro en Ingeniería
(Computación)
Presenta
Ing. Javier Chapa López
Director de Tesis
Dr. Víctor Rangel Licea
Ciudad Universitaria, México D.F. Mayo 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"ANALISIS DE PROTOCOLOS DE RUTEADORES"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Realizar un análisis completo y claro sobre la operación de los ruteadores y que sirva como apoyo didáctico y de reafirmación de conocimientos en el estudio de las redes de computadoras.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1) Definición del título de el tema de tesis.
2) Descripción de lo que la tesis plantea.
3) Análisis de viabilidad.
4) Elaboración de la introducción, objetivos, motivación, alcances,
metas, calendario de actividades.
5) Investigación bibliográfica (libros, escuelas, internet, etc..).
6) Análisis de los diferentes temas recabados.
7) Análisis de- la operación de los ruteadores.
8) Documentación .
9) Publicar artículo.
10) Conclusiones y bibliografías.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Lamentablemente no existe información técnica a la mano
referente a la operación, manipulación y configuración de enrutadores. Sin
embargo, se logró recabar información importante en el presente trabajo.
Esto hace necesario el continuar investigando sobre el tema, y
tratar de recabar información lo más concreta y entendible posible para
que los alumnos que estudian licenciaturas afines puedan tener
información lo más completa posible
El campo de estudio de los dispositivos de conectividad de redes y
telecomunicaciones es muy amplio, y así como se ha buscado reunir
información referente a los ruteadores, existen otro tipo de dispositivos
que pueden ser tema de estudio y poder reunir un conglomerado de
dispositivos del mismo tipo (por ejemplo, switches, puentes, gateways,
etc.).
Un estudiante o persona interesada en este tipo de dispositivos
deseará encontrar un sitio en internet 0 un manual que integre
información de todos ellos.
Dentro de que ha dejado en lo personal el presente trabajo, se
pueden ennumerar de la siguiente forma:
1) Entender lo importante que es la función de un ruteador dentro
de la interconexión de redes.
2) La inteligencia que poseen estos tipos de dispositivos.
3) Su estatus dentro del modelo de referencia OSI.
4) Conocer los distintos modos de direccionamiento IP por los
cuales se basa un ruteador para enrutar la información.
5) Relacionado con el direccionamiento IP, se entendió el uso de
las clases que enmarcan las direcciones IP que manejamos
comúnmente a través de internet.
6) Conocer cómo un enrutador posee tablas internas y por medio
de las llamadas métricas de enrutamiento selecciona el camino a seguir para enviar un paquete por medio de alguna de las
técnicas de enrutamiento existentes.
7) Conocer qué son las técnicas o protocolos de enrutamiento y
conocer en que consisten.
8) Una muy importante para mí, es saber que un ruteador, como
cualquier otro dispositivo, que normalmente no lo manejamos
tan a menudo, como una impresora, tarjeta de red, plotters,
concentradores, etc., tienen una importancia primordial y muy
significativa, porque sin ellos no podríamos navegar por ejemplo
en el internet, o extender una red pública o privada cuya
demanda requiera un crecimiento ineludible.
9) En lo futuro y en lo personal se seguirá integrando información
referente no sólo a ruteadores, sino a otros dispositivos de
conectividad como un gateway o un puente.
Sugiero que en el plan de estudios de las carreras, como
Telemática, se creara una asignatura en la cual se estudiaran este tipo de
dispositivos y en lo posible poder equipar un laboratorio de prácticas
exclusivamente enfocadas a estos dispositivos.
Analizando las métricas que se mencionaron , considero que la de
Vector de Distancias es muy buena porque permite a ruteador tener los
trayectos de los diferentes destinos, a diferencia de la de Estado de
Enlace que guarda una imagen de la red completa. Considero que el
Estado de Enlace requiere más memoria para guardar todo el entono de
la red, que algunas veces no es necesario tenerlo todo.
De hecho, hablando de los protocolos de enrutamiento, el protocolo
RIP es un protocolo de Vector de Distancias, porque emplea solamente
cuenta de proyectos.
Aunque, si se desea confiabilidad en la red, es mejor emplear el
protocolo IGRP, porque además de que es un desarrollo de Cisco, es un
protocolo de Vector de Distancias utiliza otras variables para determinar la trayectoria de enrutamiento, como son: ancho de banda, soportar otros
procesos IGRP, etc..
Hablando del protocolo OSPF , considero que es uno de los más
sencillos, debido a que se basa en la selección de la ruta más corta
primero.
El protocolo HELLO es una variante del IRP, que utiliza un retraso
para determinar cuál es la mejor ruta.
Finalmente , considero que si se tiene una red grande cuyas
necesidades de enlaces a distancia entre muchos usuarios y con
necesidades de confiabilidad, se debe optar por IGRP.
Si se tiene una red no muy grande y con ciertas tolerancias de
conectividad se puede optar por IRP.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
"ANALISIS DE PROTOCOLOS DE RUTEADORES"
TESIS
PARA LA OBTENCION DEL GRADO DE:
MAESTRO EN CIENCIAS, AREA TELEMATICA
PRESENTA
ING. JUAN GARCIA VIRGEN
ASESOR
M.C. ARMANDO ROMAN GALLARDO
COLIMA, COL. JUNIO DEL 2000.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Desarrollo del Metalenguaje DBML implementado en XML</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El presente trabajo de investigación se origina de la necesidad de contar
con una herramienta para el desarrollo en la Web que tenga una mejor estructura
que las hoy existentes como es el caso de HTML. Cuando se trabaja alguna
aplicación sencilla, el HTML es más que suficiente, sin embargo, cuando se
requiere realizar trabajos que involucren el manejo de bases de datos y de
información no estática sino dinámica, el HTML se vuelve insuficiente.
Existen varios leguajes para la programación de paginas WEB en entre los
que se encuentran ASP, PHP, CGI, JSP, con los cuales se pueden manipular
bases de datos de manera dinámica. Para poder diseñar paginas utilizando estos
lenguajes, es necesario que los programadores tengan un buen conocimiento de
ellos.
Para una persona que no esta involucrada con la programación en WEB y
le interesa consultar información de una base de datos de manera dinámica desde
WEB, le resulta difícil hacerlo ya que como se ha dicho tendrá que conocer uno o
varios lenguajes para poder hacerlo o en su caso contratar a un experto. La
presente investigación tiene como principal finalidad, desarrollar una herramienta
que permita realizar consultas a una base de datos de manera sencilla sin
necesidad de ser un experto en la programación de WEB.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un metalenguaje implementado en XML que permita monitorear cualquier base de datos utilizando instrucciones personalizadas</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En la actualidad, la programación en web utilizando HTML está muy
limitada, ya que este lenguaje fue originalmente diseñado como forma de
presentar información estática, sólo para fines de despliegue. Actualmente se necesita conectar páginas web a bases de datos de servidores, las cuales están
cambiando constantemente y esto exige nuevas metodologías para la
manipulación de información. HTML se ha ido adaptando a las necesidades que
está demandando la programación en Web, lo anterior ha provocado que HTML
haga tareas para las cuales no está destinado.
XML más que crear páginas web, permite diseñar otros lenguajes, en
donde somos libres de crear nuestras propias etiquetas, todas las que se
necesiten para presentar información que se desea ofrecer en Internet. El diseñar
un metalenguaje permite a las empresas desarrollar aplicaciones personalizadas
para mostrar y recibir información.
Actualmente en el Gobierno del Estado de Colima, cuenta una dependencia
(Desarrollo Institucional) que esta encargada de los proyectos de reingenieria que se
llevan acabo en cada una de las dependencias del Gobierno, los proyectos de
reigenieria están desarrollados en diferentes lenguajes de programación y también
la información esta almacenada en diferentes bases de datos, como son ORACLE,
SQLSERVER, VISUAL FOXPRO y MYSQL, entre otras. Desarrollo Institucional
se enfrenta a dos grandes problemas: el primero consiste en que tiene que estar
monitoreando cada uno de los proyectos constantemente y el segundo que no
cuenta con los recursos necesarios para comprar todas las licencias de los
diferentes manejadores de bases de datos. Una posible solución consiste en
desarrollar un metalenguaje que nos permita acceder a cualquier base de datos y
a cualquier tabla de las bases de datos, en donde podamos manipular y visualizar
todos o algunos de los campos.
El desarrollo del metalenguaje no solo solucionará la problemática con la
que se encuentra la institución antes mencionada, sino que también servirá para
todas aquellas personas que no son expertas en la programación en WEB, pero
que sin embargo les interese estar monitoreando información en la WEB de
manera sencilla.
En resumen el desarrollo del metalenguaje DBML ofrecerá una herramienta
inteligente para todas aquellas personas que deseen monitorear bases de datos
sin necesidad de contar con conocimientos en los lenguajes de WEB el acceso a al
información.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En esta investigación se han analizado las bondades del XML, donde se
rescatan tres cosas muy importantes de este metalenguaje: mejor estructura que la
que se tiene con HTML, los documentos XML nos permite crear nuestras propias
etiquetas, y por último los documentos XML cuentan con reglas sencillas par poder
diseñarse y sean válidos para los analizadores de estos documentos.
Existe una gran diferencia entre XML y HTML, ya que XML fue creado para
desarrollar otros lenguajes, mientras que HTML es un lenguaje, en otras palabras se
puede decir que con XML se puede llegar a crear un lenguaje como HTML. Sin
embargo, para algunas aplicaciones que se realicen con XML ocuparán también
desarrollar algunas partes en HTML, en la presente investigación se utilizo XML para
facilitar la comunicación entre el usuario y las bases de datos, pero el resultado se
presenta en HTML.
El crear un documento XML no es nada complicado, ni se requiere de software
especial para realizarlo, con sólo tener un editor de texto, como lo es el Bloc de
Notas se pueden crear dichos documentos.
Siguiendo las reglas básicas que se presentaron en primer capítulo se puede
diseñar con facilidad un documento XML. Sin embargo, un documento XML por sí
solo no puede resolver todo, es decir que si tenemos un documento XML sólo nos
servirá para ver la información de forma estructurada, XML requiere de otros software
como pueden ser ASP, PHP, XSL, para poder obtener buenos resultados. Para este
inve stigación se utilizó ASP con el fin de leer los documentos XML y realizar las
instrucciones necesarias para conectarse a la base de datos y presentar los
resultados en HTML.
El utilizar instrucciones SQL para realizar consultas a las bases de datos,
ayudó a que el metalenguaje sea sencillo de implementar, ya que la mayoría de los manejadores de bases de datos utilizan SQL para realizar sus consultas a sus
bases de datos.
El concepto del metalenguaje DBML es muy interesante ya que el usuario del
metalenguaje no necesitará saber cuestiones técnicas especializadas como los
conceptos de PARSERS, SAX, DOMParser, CreateObject, ADODB, EXECUTE,
RECORDSED, entre otros. El usuario del DBML, sólo se concentrará en diseñar
bien sus documentos XML y nada más. Para el usuario es transparente la
comunicación entre su documento XML y su base de datos.
El permitir conectarse a diferentes bases de datos con sólo crear varios y
diferentes documentos XML hace del metalenguaje DBML una herramienta muy
interesante, de tal manera que se pueden realizar otras investigaciones las cuales
ayuden a desarrollar un metalenguaje más completo, donde por ejemplo se tenga
una opción que cargue un formulario en el que se puedan agregar, modificar y
eliminar registros de una base de datos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
Facultad de Telemática
Desarrollo del Metalenguaje DBML implementado en XML
Tesis
Que para obtener el grado de
Maestro en Ciencias, Írea Telemática
Presenta
Lic. Alejandro García Nava
Asesor
M. C. Armando Román Gallardo
Colima, Col., Mayo de 2003</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“LOCALIZACIÓN DE UN ROBOT MÓVIL A TRAVÉS DE VISTAS CONOCIDAS "</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El problema consiste en lograr que un robot utilice informacion visual para identifcar su posicion dentro de un ambiente conocido. Para lograr esto se cuenta con
una base de datos de informacion que reune la experiencia visual del robot. La base
de datos de imagenes (o BDimag) sirve como la memoria de referencia del robot,
ya que asi ³ consta con una serie de vistas con ubicaciones conocidas (coordenadas de
posicion y orientacion defnidas). Se define la pose del robot como la coordenada con-
junta de posicion y orientacion. El robot debe de poder comparar lo que esta viendo
en el momento de navegar (la vista actual) contra su memoria, y decir a cuales de las imagenes que tiene en la BDimag se le parecen mas. Es importante hacer notar que la similitud entre dos imagenes puede deberse a que los puntos de vista que
las originaron fueron tomados desde posiciones cercanas. Esto se traduce a que las
coordenadas relacionadas con cada imagen candidato son posibles poses cercanas al
estado real del robot.
En este punto se extrae la pose real a partir de poses relativas usando dos metodos,
para ser combinados posteriormente: El primer metodo, basado en agrupacion de
puntos en cumulos, se basa en el hecho de que se cuenta con una serie de poses
posibles en las que pueda estar el robot. Estos puntos estaran ordenados en cumulos,
de los cuales se puede obtener un centroide. En caso que se tenga un solo cumulo,
su centroide se toma como la pose real del robot. De haber mas de uno, se altera
ligeramente la posicion (por ejemplo un giro de 30o), se captura una imagen nueva
y se repite el ciclo (comparando ahora contra los candidatos de la etapa anterior).
Esta logica aplica tanto a la posicion como a la orientacion del robot y se detalla en
el capitulo 5. En el segundo metodo, se aprovechan las restricciones de la geometria
epipolar aplicadas al par de imagenes parecidas (la vista actual y cada candidato).
Esto se explica a detalle en el capitulo 6.
Al final, las estimaciones de cada metodo se combinan para generar una estimacion
visual final.  Ésta, a su ves podra combinarse con el modelo tradicional de seguimiento
por odometría (en caso de conocerse la posición inicial) y así precisar la posición del
robot durante una serie de movimientos.
El proceso completo puede verse en la  figura 3.1. Los elementos mostrados en la
figura se explicar en los siguientes capítulos.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>La meta final consiste en construir un sistema modular de localizacion visual para un robot movil con camara monocular que logre entregar un buen estimado comparativo de la posicion y orientacion.  Éste objetivo se debe de alcanzar tanto para el problema de localizacion global como para el de rastreo de posicion.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo se realizo la integracion de varios modulos con el objetivo de
encontrar la pose de un robot en un escenario del cual se conocen algunas vistas. Los
modulos probados fueron:
1. Deteccion de correspondencias bajo el descriptor SIFT
2. Localizacion de pose por recuperacion de la geometria epipolar de la escena.
3. Localizacion de pose por aplicacion del algoritmo QTc.
4. Prediccion de posicion usando odometria.
5. Localizacion por la incorporacion de la prediccion de posicion y la medicion de
esta bajo el esquema del filtrado de Kalman.
En lo que respecta al metodo elegido para comparar imagenes, SIFT se conformo como un robusto sistema de deteccion de caracteristicas. Sin embargo, el numero
y tipo de caracteristicas detectadas, en conjuncion con la comparacion exhaustiva entre imagenes resulta en un lento proceso de comparacion. La inclusion del sistema de
umbralizacion de comparacion resulto en una aceleracion de alrededor del 400% de los tiempos iniciales. Aun asi, no se probo el sistema con el metodo de primer cubo primero
(best bin first), que reporta fuertes incrementos en la velocidad de comparacion [32].
La inclusion del algoritmo de GTM a la comparacion de imagenes ayudo a eliminar
la gran mayoria de ruido del sistema de medicion (visual). Esto hace posible el uso
de tecnicas como la de geometria epipolar.
El metodo de acumulacion denominado QTC resulto ser una buena aproximacion
inicial a la posicion del robot. Este metodo se relaciona de manera natural con el
concepto de nodos de referencia bajo el cual se propuso la resolucion de este trabajo.
Las ideas de centroides de posicion a partir de las vistas candidatas permitio generar
aproximaciones tanto de posicion como de orientacion con resultados similares a los
reportados por otros trabajos en el area. El metodo tiene el inconveniente de depender absolutamente de la base de datos de imagenes del ambiente de localizacion. La
adquisicion de las imagenes puede tomar mucho tiempo y requiere precision al generarse. La velocidad de localizacion bajo este metodo es directamente proporcional a
la cantidad de imagenes de referencia con las que se cuente. Para el escenario en el
que se realizaron las pruebas, la cantidad de imagenes a ser comparadas para el problema de localizacion global fue de 328. El tiempo promedio de comparacion para la
localizacion global fue de 24 segundos (incluyendo el conjunto de pruebas que se tuvo
que verificar). Para el caso de rastreo de posicion, contando con la posicion inicial, el
tiempo promedio fue de 4 segundos. Este tiempo de procesamiento es alto si se desea
alcanzar la localizacion en tiempo real. No obstante, el sistema tiene multiples puntos donde puede lograrse una aceleracion como lo son la calidad de comparacion, la
discriminacion de candidatos cercanos (por la definicion de la distancia de vecindad)
y los parametros mismos de la obtencion de caracteristicas SIFT.
La metodologa conocida como localizacion por geometria epipolar resulto, dentro
del escenario de pruebas, demasiado inexacta y susceptible a ruido como para generar
una posicion (x; y) estable. Sin embargo, aprovechando el concepto de candidatos
parecidos de QTC, se logro generar una estimacion de orientacion muy cercanas a las
de QTC y que ayudaron a la verificacion de las estimaciones del metodo acumulativo.
Debido a la forma en que se implemento la geometria epipolar, este depende de una
primera aproximacion por parte de QTC y por lo tanto añade su tiempo de operacion
al tiempo de la estimacion final. No obstante, este metodo se basa en operaciones con
matrices que pueden realizarse en tiempo real y por lo tanto su añadido de tiempo es
minimo (de 1 a 3 segundos).
La metodologia presentada resuelve los dos problemas de localizacion: localizacion
global y rastreo de posicion. Los modulos se pueden modificar para aumentar la
precision de localizacion. Esto es practico si se cuenta con una buena capacidad de
procesamiento ya que al aumentar la precision se incurre en un aumento de carga de
procesamiento y por lo tanto en mayor tiempo de operacion. Es importante mencionar
que no se ha probado el sistema en un ambiente dinamico con objetos o personas
moviendose en el campo de vision del robot.
En conclusion, se presenta un metodo modular que resuelve los problemas de localizacion global y seguimiento de posicion sujeto a la existencia de una base de datos de
imagenes. La metodologia se basa en un sistema imperfecto y limitado (un robot con
alta incertidumbre en el movimiento y un sistema monocular de vision) y logra generar estimaciones de buena calidad aprovechando la incorporacion de varias tecnicas de
procesamiento de señales y analisis de patrones, como lo son GTM y QTC. En total,
se cuenta con un sistema que incorpora varias tecnicas que se apoyan entre s ¶ ³ para generar una buena aproximacion de la posicion del robot, y que pueden ajustarse
(alterando los parametros intrinsecos a cada modulo) para adaptarse a escenarios de
distinta indole.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD NACIONAL AUTÓNOMA DE MÉXICO
POSGRADO EN CIENCIA E INGENIERÍA DE LA COMPUTACIÓN
“LOCALIZACIÓN DE UN ROBOT MÓVIL A TRAVÉS DE
VISTAS CONOCIDAS " 
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN INGENIERÍA
(COMPUTACIÓN)
P R E S E N T A:
PABLO FRANK BOLTON
DIRECTOR DE TESIS: DR. YANN FRAUEL
México, D.F. 2009.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“HERRAMIENTA DE ADMINISTRACIÓN DE PROYECTOS PARA MOPROSOFT"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Desde la aprobación de MoProSoft como norma mexicana, surgió el interés en las empresas de
desarrollo de software de adoptarlo como modelo de procesos. La adopción incluye el aspecto de la administración de proyectos que afecta a las PyMES y áreas internas dedicadas al desarrollo y/o mantenimiento de Software, ya que MoProsoft está dirigido a este sector de la industria.
Una ayuda para la adopción de MoProSoft es proponer herramientas necesarias para la
administración de los proyectos, brindando los mecanismos necesarios para realizar una correcta gestión de los documentos generados en los procesos de Gestión de Proyectos y Administración del Proyecto Específico de las categorías de Gerencia y Operación respectivamente, según lo que marca MoProSoft en estos procesos.
La herramienta que se desarrollará tendrá por nombre Herramienta de Administración de Proyectos para MoProSoft (HeAP MoProSoft). La idea de desarrollar esta herramienta surgió como un proyecto, para el curso de Ingeniería de Software Orientada a Objetos, llamado “Tlamatini HeAP MoProSoft" , este proyecto sirvió como antecedente de este trabajo de tesis en el que retomamos las ideas y el esfuerzo que ya se había realizado.
A diferencia de otras herramientas que no han sido diseñadas especialmente para la implementación de MoProSoft y de realizar las actividades de administración de proyectos manualmente. HeAP MoProSoft será una herramienta diseñada especialmente para la Administración de proyectos para empresas dedicadas al desarrollo y/o mantenimiento de software, que quieren adoptar MoProSoft como su modelo de procesos favoreciendo su implementación, reduciendo el tiempo y proporcionando las plantillas con la información mínima requerida para generar los documentos que estos procesos requieren, apoyando a los roles de MoProSoft: que son Responsable de Gestión de
Proyectos (RGPY) y Responsable de la Administración del Proyecto Específico (RAPE).
Este sistema en su primera versión considera a esos dos usuarios y cubrirá algunas de las
actividades, de nivel 1 y 2 de capacidad de procesos, correspondientes a los procesos de Gestión de Proyectos y Administración del Proyecto Específico de las categorías de Gerencia y Operación respectivamente. Las actividades que este sistema cubrirá fueron elegidas por ser las mínimas requeridas para comenzar con las prácticas de administración de proyectos según lo que marca MoProSoft.
Para el proceso de Gestión de Proyectos de la Categoría de Gerencia se realizarán las siguientes actividades, el usuario responsable de esto será el RGPY:
"Generar el Plan de Proyectos realizando las siguientes actividades:
o Generar el Registro del Proyecto
o Generar la Descripción del Proyecto
o Asignar Responsable de Administración del Proyecto Específico
"Administración del personal
"Consultar el Plan del Proyecto Específico
Para el proceso de Administración del Proyecto Específico de la Categoría de Operación se
realizarán las siguientes actividades, siendo responsable el usuario RAPE:
"Generar el Plan del Proyecto Específico que está conformado de la siguiente manera:
o Definir Ciclos y Actividades
o Registrar Tiempo Estimado
o Registrar Costo Estimado
o Generar el Plan de Adquisiciones y Capacitación
o Conformar el Equipo de Trabajo
o Generar el Plan de Manejo de Riesgos
o Generar el Protocolo de Entrega</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El desarrollo de este trabajo de tesis cumplió con el objetivo de crear una herramienta para la
administración de proyectos basada en MoProSoft, y que formara parte del software libre.
La experiencia de seguir el Proceso Unificado para el desarrollo de software fue muy satisfactoria
porque permitió retomar el trabajo en todo momento ya que siempre se contó con un proceso
documentado y ordenado.
La utilización del marco de trabajo Struts, fue una buena elección, ya que permite desarrollar
aplicaciones Web con una arquitectura muy bien definida y estructurada, el desarrollo del sistema
fue muy ágil una vez que se superó la curva de aprendizaje. La implementación del patrón MVC, a
través de Struts, permite un excelente mantenimiento de la aplicación ya que el modelo, la vista y el
controlador se encuentran claramente separados, lo que permite que la realización de cambios,
agregar nuevas funcionalidades en cualquiera de los tres niveles se realice casi insospechadamente
para el resto de la aplicación. Otra de las ventajas de haber trabajado con Struts es facilidad de
integración, es decir, en cada iteración que se implementaba un nuevo caso de uso, se realizó muy
fácilmente.
Los primeros pasos implementando Struts se convirtieron en un reto debido a la curva de
aprendizaje adicional que surge cuando no se tiene experiencia implementando marcos de trabajo.
La principal ventaja de HeAP MoProSoft es contar con una herramienta diseñada especialmente
para apoyar en las actividades de administración de proyectos siguiendo MoProSoft. Otras ventajas
son las que nos ofrece el que sea una aplicación Web, como facilidad de acceso pues únicamente
es necesario contar con una conexión a Internet y con cualquier navegador Web. HeAP MoProSoft
es una herramienta que forma parte del software de distribución libre.
Entre las desventajas de esta herramienta, por ser una aplicación Web, se encuentran la velocidad
de acceso y de respuesta ya que esta depende de la velocidad de la conexión a Internet, es decir del
ancho de banda con el que se cuente.
Entre los trabajos futuros se encuentra agregar la Categoría de Alta Dirección y el Proceso de
Gestión de Negocio, la Categoría de Gerencia con el Proceso de Gestión de Procesos y Gestión de
Recursos y finalmente la Categoría de Operación el Proceso de Desarrollo y Mantenimiento de
Software. Y robustecer los procesos implementados en esta tesis.
La integración con dos trabajos de tesis titulados: “Sistema de tableros de control para gestión de
proyectos para MoProSoft"  [13] y “Sistema Multi-Agente de apoyo para el proceso de Administración
de Proyectos Específicos de MoProSoft"  [14]. Al incluir el sistema Multi-Agente se tendrá una guía y
apoyos para realizar las actividades del proceso. Los tableros de control permitirán visualizar el
desempeño de los proyectos de la empresa.
Al término de estas tres tesis, incluyendo ésta, se contará con una herramienta diseñada
especialmente para facilitar la adopción de MoProSoft, reduciendo el tiempo de aprendizaje de las
empresas para la implementación de los procesos de Gestión de Proyectos y Administración del
Proyecto Específico, proporcionando plantillas y herramientas necesarias, para generar y administrar
la documentación correspondiente a cada proceso.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD NACIONAL AUTÓNOMA DE MÉXICO
POSGRADO EN CIENCIA E INGENIERÍA DE LA COMPUTACIÓN
MÉXICO, D. F. 2008
“HERRAMIENTA DE ADMINISTRACIÓN DE PROYECTOS
PARA MOPROSOFT" .
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRA EN INGENIERÍA
(COMPUTACIÓN)
PRESENTA:
BRENDA DANIELA TORRES CASTILLO
DIRECTORA DE TESIS:
M. EN C. MA. GUADALUPE ELENA
IBARGÜENGOITIA GONZÁLE</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ABCSIS: ARQUITECTURA BASADA EN COMPONENTES DE SOFTWARE PARA LA INTEGRACIÓN DE SERVICIOS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Crear una metodología de desarrollo de software basado en SOA para el software
SIABUC, con la finalidad de extender los servicios que actualmente se ofrecen.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La arquitectura ABCSIS permitirá, a las instituciones que hacen uso de SIABUC y
que cuenten con personal de perfil informático o áreas afines, poder implementar
mecanismos interoperables que permitan la comunicación con otras aplicaciones.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para la realización de este proyecto se siguieron una serie de pasos, los cuales se
describen a continuación:
"Investigación documental
Consiste en buscar información acerca de las tecnologías relacionadas con el
desarrollo de la arquitectura propuesta, principalmente artículos, así como
libros de actualidad, en el caso de los artículos la mayor fuente de consulta fue
la biblioteca digital ACM, así como artículos creados por empresas de
renombre como IBM, Microsoft y organismos independientes como Apache
Group, OASIS, entre otros.
"Diseño de la arquitectura
Elaboración del modelo conceptual de la arquitectura propuesta, básicamente
se genero un esquema de la arquitectura ABCSIS con el funcionamiento
propuesto.
"Desarrollo del prototipo funcional
Consistió en la elaboración de una aplicación que consume el servicio de
reservación de libros para demostrar su funcionalidad e interoperabilidad.
"Evaluación del prototipo funcional
Esta etapa consistió en realizar pruebas de operación y pruebas de
rendimiento.
"Documentación de la investigación
Consiste en redactar el documento de la tesis.
"Análisis de los resultados obtenidos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La información obtenida en la prueba de operación muestra que a pesar de que un
poco más de la mitad (62%) de los participantes tiene experiencia en el uso de las
nuevas tecnologías, y con ciertas carencias de actualización profesional, cumplen
con sus funciones encomendadas de apoyo técnico a las bibliotecas.
Esta realidad del perfil informático identificada en la prueba, puede ser incluso
más deficiente en otras instituciones. Para sostener esta aseveración se cuenta con
la experiencia que se ha obtenido con más de 5 años brindando soporte técnico a
infinidad de usuarios bibliotecarios e informáticos de instituciones a nivel
Latinoamérica. Estas condiciones prevalecen en las instituciones que usan SIABUC,
donde son pocas las que tienen el apoyo continuo del personal de informática, y por
eso es de suma importancia ofrecer un mecanismo tecnológico que facilite las tareas
de instalación, configuración y puesta en marcha, donde la plataforma ABCSIS viene
a cubrir esta carencia.
Con los resultados obtenidos en las pruebas, se puede concluir que se ha
cumplido el objetivo de este trabajo, encapsular una gran funcionalidad y ofrecerla
para que los usuarios finales puedan incorporarla a sus desarrollos, incluso de
manera más fácil y sencilla como se hacia con la plataforma anterior basada en CGI.
También, en base al cuestionario realizado, la hipótesis ha quedado comprobada, ya
que el 88% de los encuestados consideran factible la interconexión de ABCSIS con
otros sistemas.
Actualmente ya se encuentra disponible una aplicación funcional denominada
iSIABUC. El encargado de crear dicha aplicación también participó en la prueba y le
resultó muy fácil realizar el acoplamiento de ABCSIS con la aplicación front-end. La
aplicación consiste en una interfaz Web para SIABUC que consume todos los
servicios que ABCSIS provee, desarrollando así un mecanismo que ofrece distintos
servicios vía Web a la comunidad universitaria (Guedea, 2009). La dirección
electrónica para acceder a ésta aplicación es la siguiente:
http://siabuc.ucol.mx/s9mashup.
Las expectativas de este proyecto se han cumplido satisfactoriamente,
creando una metodología de desarrollo de software basado en SOA para el software SIABUC, con la finalidad de extender los servicios que actualmente se ofrecen en la
biblioteca a partir de la funcionalidad básica existente en SIABUC.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
Facultad de Telemática
ABCSIS: ARQUITECTURA BASADA EN COMPONENTES
DE SOFTWARE PARA LA INTEGRACIÓN DE SERVICIOS
TESIS
Que para obtener el grado de
MAESTRO EN COMPUTACIÓN
PRESENTA:
ING. HUGO CÉSAR PONCE SUÍREZ
ASESORES:
M. en C. JOSÉ ROMÍN HERRERA MORALES
D. en C. PEDRO DAMIÍN REYES
COLIMA, COLIMA. NOVIEMBRE DE 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SIMULACIÓN DE LA RED INALÁMBRICA DE BANDA ANCHA CON TECNOLOGÍA WIMAX PARA EL ESTADO DE COLIMA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la actualidad, en México no existen tendencias institucionales que favorezcan el abatimiento de la brecha digital y aceleraren el proceso de adopción tecnológica en los distintos ámbitos de la realidad nacional, este factor constituye el más importante impedimento para avanzar como país a la sociedad de la información, razón por la cual, reducir rápidamente la exclusión digital debe ser el punto central de cualquier estrategia que se proponga a nivel nacional (Talavera Hernández, 2010). La exclusión digital desde el punto de vista social propicia la desigualdad en el desarrollo de la población. Las personas que no tienen acceso a tecnologías de información carecen del medio que les permite mejorar su educación, acceder a la salud y otros servicios sociales, y en general, el acceso a un medio importante para mejorar sus vidas y sus conocimientos.
Desde un enfoque económico, la exclusión digital significa menos usuarios para los operadores de redes públicas y para el desarrollo en México de una economía digital robusta, afectando el desarrollo de los principales sectores económicos claves para la competitividad nacional.
En términos de política, el acceso a las redes sociales en línea significa crecientemente un medio para el empoderamiento de las personas, y para el ejercicio de los derechos, motivo por el cual, la exclusión digital genera dos grupos de ciudadanía: de primera y de segunda (Talavera Hernández, 2010). Además de analizar la exclusión digital desde tres elementales puntos de vista, es importante describir la problemática del proyecto con un enfoque técnico. La implementación de infraestructura para una red inalámbrica con tecnología WiMAX no resulta económica, por tal razón la etapa de planeación de la red adquiere gran importancia. La simulación de procesos constituye una valiosa herramienta que se utiliza en la planificación de redes con el propósito de observar la evolución, comportamiento y funcionamiento del sistema sin necesidad de contar con la infraestructura real del escenario propuesto. Implementar la red inalámbrica de banda ancha con tecnología WiMAX para el Estado de Colima sin realizar previamente la etapa de planificación, diseño y análisis es una inversión arriesgada. Factores como: diferentes topologías del terreno, áreas de cobertura, tráfico de red y pérdida en la propagación de la señal se analizan con el uso del simulador, de esta manera determinar acertadamente la viabilidad técnica del proyecto.
A través de la correcta simulación de la red Estatal inalámbrica de banda ancha se evita el gasto innecesario de recursos económicos, técnicos y humanos, colaborando eficazmente con el cumplimiento de objetivos establecidos en la Agenda Digital del país y contribuir a reducir la exclusión tecnológica específicamente en el Estado de Colima.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Simular la red inalámbrica de banda ancha con tecnología WiMAX para el Estado de Colima.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La simulación de la red inalámbrica de banda ancha para el Estado de Colima permitirá analizar y evaluar de forma virtual el comportamiento de los enlaces WiMAX.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En la actualidad contar con un eficiente sistema de comunicación para el intercambio y acceso a la información en el momento oportuno permite constantemente el desarrollo y crecimiento de empresas, organizaciones e instituciones gubernamentales a nivel local, nacional y mundial. Tomando en cuenta este contexto surge la necesidad de implementar en el Estado de Colima una red global de datos que permita agilizar e integrar los procesos de comunicación digital entre las entidades gubernamentales que lo conforman. En este capítulo se explican detalladamente todas las tareas de estudio, análisis y procedimientos realizados en las etapas de diseño y simulación del comportamiento de la red inalámbrica de banda ancha con tecnología WiMAX para el Estado de Colima.
4.1 Diseño de red
En proyectos de telecomunicaciones, para implementar y desplegar infraestructura es fundamental realizar el diseño lógico y simulación de la red antes de su instalación física. En ambientes inalámbricos, la correcta elaboración de estas etapas permitirá
establecer adecuadas políticas de seguridad informática y evitar problemas como: pérdida de datos, interferencias que afecten el desempeño en las conexiones y/o caída reincidente de enlaces de comunicación.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Implementar infraestructura para crear la red inalámbrica de banda ancha con tecnología WiMAX para el Estado de Colima no resulta económico, por tal razón es necesario realizar previamente la etapa de planificación, análisis y simulación de la red con el propósito de evaluar de forma virtual el comportamiento de los enlaces. Los modelos de propagación WiMAX experimentalmente se probaron en escenarios específicos, contienen factores de corrección que permiten ampliar su uso a ambientes genéricos, sin embargo, dependen de las condiciones físicas del terreno para pronosticar con certeza y exactitud la difusión de la señal. El resultado en los cálculos de predicción varia de un entorno a otro, por tal razón para conocer la forma en que las ondas electromagnéticas viajan a través del espectro no es suficiente utilizar solamente un modelo de propagación, es necesario contrastar la predicción de varios modelos para determinar la viabilidad técnica de los enlaces planteados. Con el análisis de resultados que se obtuvo al calcular la expansión de la señal WiMAX se concluye que los modelos de propagación inalámbrica SUI y ECC-33 no son aplicables a la realidad orográfica del Estado de Colima debido a que se crearon y se probaron en metrópolis donde los grandes edificios permiten ganar altura en la ubicación de antenas.
Los modelos de propagación Okumura, Espacio Libre y COST 231 Hata se ajustan a las condiciones del terreno montañoso que existen en el Estado de Colima. La ubicación estratégica de las estaciones base que se propone en este trabajo garantiza la operación de la red en la frecuencia de 5.8 GHz tanto para enlaces con línea de vista y enlaces en condiciones sin línea de vista. Una correcta ubicación de antenas sectoriales permite irradiar energía a los lugares indicados y evita ubicar innecesariamente antenas omnidireccionales. La Jurisdicción 1 es la que tiene mayor densidad de usuarios en la red Estatal, la Jurisdicción 2 y Jurisdicción 3 presentan similar número de clientes y la Jurisdicción 4 es la que concentra menor cantidad de estaciones suscriptoras. El simulador OPNET Modeler ® permite modificar y configurar parámetros técnicos de las estaciones base y estaciones suscriptoras con el propósito de adaptar sus características a equipos reales que en la actualidad se comercializan. Para garantizar la disponibilidad de datos en la red Estatal es necesario considerar escenarios de contingencia que simulen la pérdida de conexiones ante cualquier eventualidad. Los enlaces alternos que se proponen soportan eficientemente el tráfico de red generado. La dorsal principal de la red Estatal permite su interconexión con tecnología inalámbrica WiMAX considerando que la calidad de los enlaces depende de la ganancia de las antenas.
Para crear la dorsal principal de comunicación se requiere antenas con 22 dBi de ganancia, sin embargo, proporcionar 24 dBi a las conexiones mejora considerablemente la calidad en la transmisión de datos. La simulación de la red con tráfico de video conferencia de alta calidad es un proceso que demanda grandes cantidades de memoria RAM y tiempo de procesamiento. El tráfico pesado para aplicaciones de video conferencia genera un mayor retardo en la entrega de paquetes. La implementación de WiMAX en este escenario es una solución eficaz para solucionar el problema planteado. Finalmente se concluye con la aceptación de la hipótesis planteada y el cumplimiento total de los objetivos iniciales del proyecto.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima 
Facultad de Telemática 
SIMULACIÓN DE LA RED INALÍMBRICA DE BANDA ANCHA CON TECNOLOGÍA WIMAX PARA EL ESTADO DE COLIMA 
TESIS 
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN COMPUTACIÓN PRESENTA: 
Ing. Stalin Xavier Caraguay Ramírez 
ASESORES: 
D. en C. Raúl Aquino Santos 
M. en C. Omar Ílvarez Cárdenas 
Colima, Col. Diciembre de 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DISEÑO Y CREACION DE PORTAL WEB DONDE SE OFRECERAN SERVICIOS DE INGENIERIA CAD, FEM Y ANALISIS DE VIBRACIONES</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Debido a la experiencia adquirida en industrias metal mecánicas, he decidido aplicar y obtener ventajas de los conocimientos adquiridos en dichas empresas, para así ofrecer servicios de ingeniería utilizando el recurso de la red de redes, debido a la gran penetración e influencia que tiene en la actualidad, el presentar bienes y servicios por Internet.
7
Se buscará captar la atención de los clientes por medio de una exposición masiva y un tiempo de respuesta casi inmediato en la recepción, entrega de planos y documentos de diseño, esto se logra aprovechando las herramientas que Internet ofrece, por otro lado utilizar mano de obra estudiantil, ya que como se ha citado anteriormente las universidades cuentan con mucha mano de obra y recursos a los cuales se les puede obtener mayor aprovechamiento económicamente y técnicamente. Y una vez creado el vinculo entre mentor (maestro), alumno (aprendizaje) e industria (cliente), inducimos un circulo virtuoso donde todas las partes ganan.
Básicamente podemos dividir el concepto del portal en Internet, ofreciendo tres servicios básicos:
Diseño Mecánico: Este se llevará acabo utilizando básicamente Autocad y Mechanical Desktop como principales paquetes de CAD; La idea que se tiene es poder captar la necesidad del cliente de la forma más amigable posible, ofreciendo la posibilidad de intercambiar archivos usando extensiones IGES, DXF ó DWG dependiendo las plataformas con las que trabaje el cliente. De tal forma se podrán tener de forma rápida y oportuna los croquis o borradores con los que el cliente desee trabajar.
De esta forma el cliente que cuente con tan solo Autocad podrá acceder a este servicio y hacer sus peticiones de “maquila" de dibujos y planos en 2 y 3 D , de la misma forma podrá recibir su información por la misma vía rápida.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se explicó a detalle el concepto de Internet y las diferencias que existen entre los diferentes servicios que se ofrecen dentro de esta Red mundial, dando a conocer sus orígenes, limitantes y tendencias. Se planteó el panorama que se enfrenta hacia el siguiente siglo, y en especial los retos a vencer para México como país y como conjunto de empresarios de diferentes tamaños, sectores e industrias.
El aspecto más llamativo de Internet durante los últimos años es el crecimiento del cual hemos sido testigos. Es evidente que la rapidez con la que esta tecnología avanza es de magnitudes nunca antes vistas en ningún otro medio. Este crecimiento hace que Internet se convierta en un tema de interés para cualquier sociedad que busque nuevas fronteras y oportunidades.
Después de analizar y estudiar algunas de las herramientas más utilizadas en la actualidad que existen en el mercado de la informática para desarrollar en general sitios Web y en particular un Punto de Venta Virtual, es fácil darse una idea de la rapidez con la que Internet cambia. Las herramientas que causan conmoción en el ámbito tecnológico el día de hoy, pueden resultar obsoletas el día de mañana. Haciendo de este tema, uno de gran complejidad e interminable en cuanto a su estudio respecta.
Se explicó el concepto de Comercio Electrónico, un tema del cual pocas personas conocen o dominan y en el cual muchas corporaciones están interesadas. Para lograr una mejor comprensión del Comercio Electrónico, se describió cada componente y etapa del desarrollo, las diferentes herramientas y tecnologías utilizadas en la actualidad para su aplicación, y los alcances y limitaciones que lo caracterizan hoy en día.
Al entender la importancia de la estrategia de Mercadotecnia, y la aplicación de diferentes principios no-técnicos en la creación de un Punto de Venta Virtual, se hizo claro que para lograr el éxito de un proyecto de esta índole hace falta más que el dominio de las diferentes tecnologías disponibles; es necesario que el negocio completo se involucre en el desarrollo, que el personal de las diferentes áreas existentes aporte sus conocimientos, su experiencia y su creatividad, para así generar sinergia y dar lugar al nacimiento de un sitio Web que no solo venda, sino también sirvan de tarjeta de presentación de la empresa ante los usuarios de la Red y clientes potenciales en el ámbito mundial; que informe, fomente el interés en los visitantes y beneficie tanto a la imagen pública de fa organización como a su desarrollo y crecimiento.
Es claro que las ventas en Internet atraen a un gran número de compañías que esperan expandir sus fronteras e incrementar su ganancia", sin embargo, se explicó que el lanzamiento de un producto en Internet no es necesariamente un éxito garantizado ya que las características únicas de Internet como medio de comunicación y las del Punto de Venta como tal, lo hacen ideal para ciertos proyectos pero al mismo tiempo menos provechoso para otros.
Durante la investigación y recopilación de información acerca del tema, resultó evidente que existe una gran variedad de publicaciones que pretenden definir al Comercio Electrónico y la creación de un Punto de Venta Virtual, algunas de ellas se enfocan en el aspecto técnico (protocolos de transacción, seguridad, lenguajes de programación e instalación de redes y servidores entre otros), otras publicaciones se enfocan en el aspecto administrativo y de Mercadotecnia (publicidad, relaciones públicas, atención al cliente, entre otros) Después de una investigación intensa, resultó claro que hacía falta bibliografía que definiera el Comercio Electrónico desde sus principios fundamentales y ofreciera una clara explicación de la creación de un Punto de Venta Virtual en Internet abarcando todos sus aspectos.
Para establecer los requisitos de la creación de un Punto de Venta Virtual en Internet, así como definir sus elementos más importantes, la manera en la que una organización debe conducirse para iniciar las ventas en Internet y mantener un punto de Venta exitoso, se definió un Modelo simple, que puede ser utilizado por cualquier negocio para planear un Punto de Venta Virtual, evaluar sus posibilidades de éxito, diseñarlo, implantarlo, darlo a conocer y mantenerlo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD IBEROAMERICANA
 VIBRACIONES"
TESIS
QUE PARA OBTENER EL GRADO DE
MESTRO EN INGENIERIA INDUSTRIAL EN SISTEMAS DE MANUFACTURA
PRESESNTA 
FRANCISCO JAVIER VILLANUEVA HERNANDEZ
MEXICO D.F. 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>MPMPES: Modelo de Procesos para Micro y PequeñasEmpresas Desarrolladoras de Software</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La industria reconoce que las Micro y Pequeñas Empresas Desarrolladoras de
Software (MPEs) tienen una importante contribución en la economía mundial,
desarrollando partes importantes de software que son fácilmente integrados en
empresas de mayor tamaño.
Estándares internacionales y modelos como ISO/IEC12207 ó CMMI, fueron
desarrollados para agrupar lo mejor y más apropiado de las buenas prácticas y
usos del desarrollo de software, sin embargo estos estándares no han sido
diseñados para MPEs (integradas desde 1 a 25 empleados), y por consecuencia
es difícil aplicarlos en ellas (Laporte 2008).
Investigaciones muestran que las MPEs, tienen percepciones negativas de los
estándares y modelos de procesos, primariamente por el alto costo,
documentación excesiva y burocracia. Como adición las MPEs muestran
dificultad al relacionar estándares como ISO/IEC 12207 a las necesidades de su
negocio y justificar la aplicación de un estándar internacional de este tipo en sus
operaciones (Laporte 2008).
La mayor parte de las MPEs, no tienen los recursos necesarios para la
aplicación de un estándar de gran tamaño, y por consiguiente no ven redituable
su implantación (Laporte, Alexandre et al. 2008).
México cuenta con una posición favorable para convertirse en un competidor de
talla mundial en el ramo de la industria de software, gracias a su ubicación
geográfica, perfil demográfico y estado de desarrollo tecnológico. No obstante el
potencial de desarrollo es evidente, la industria de software es apenas incipiente
en nuestro país: participa con tan sólo el 0.10% del PIB (cifras de 2000). Aunque
no existe un padrón exhaustivo de esta industria que proporcione información
exacta, una muestra de 206 empresas desarrolladoras de software muestra el
perfil actual de la industria que es mayoritariamente micro y pequeña, con un tamaño muy inferior al del promedio internacional, que es de 250 empleados
(Secretaria de Economia 2009).</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Crear un Modelo de Procesos para Micro y Pequeñas Empresas (MPEs)
Desarrolladoras de Software que asegure la calidad de sus productos y servicios
conforme a los Estándares Internacionales de Calidad.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Desarrollar un Modelo de Procesos específico para MPEs apegado a
Estándares Internacionales, Modelos de Procesos y Metodologías Agiles para
Desarrollo de Software, crea condiciones para asegurar la calidad de los
productos y servicios de las MPEs.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Existen puntos clave que son de beneficio para los clientes de las
organizaciones dedicadas al desarrollo y mantenimiento de software como son:
*	Certidumbre, ya que las empresas verificadas deben llevar a cabo sus
actividades con prácticas validadas por las normas mexicanas.
*	Calidad, porque al llevar a cabo buenas prácticas de desarrollo y
mantenimiento de software los resultados son medibles
*	Capacidad, ya que los procesos con los que se desarrolla ó mantiene el
software son repetibles (Alvarez 2009).
Las buenas prácticas de desarrollo y mantenimiento de software son el pilar
fundamental en el que las organizaciones dedicadas a esta actividad centran el
logro de sus objetivos de negocio, de esta forma los clientes se aseguran que las organizaciones son capaces y cumplen correctamente con su objetivo que es
el desarrollo de software de calidad.
Se estima que en 2006 existían en México 4'290,108 empresas, de las cuales el
99.8 por ciento son Micro, Pequeñas y Medianas Empresas.
Una microempresa se considera a la que tiene entre 0 y 10 trabajadores. Esto
es así, independientemente de que el negocio se dedique a la industria, al
comercio o los servicios (Secretaria de Economia 2009).</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El proyecto se desarrollará siguiendo los procesos considerados en MoProSoft y
las metodologías ágiles, integrando las fases del ciclo de vida tradicionales con
los procesos de administración de proyectos y los procesos de soporte
necesarios para un desarrollo satisfactorio del trabajo de investigación
propuesto.
A continuación se describen las etapas y procesos considerados para la
consecución del proyecto.
*	Investigación bibliográfica y definición del problema a investigar.
*	Análisis de micro y pequeñas empresas: Definición de los requerimientos,
especificación y estructura de la organización.
*	Identificación de uso de estándares internacionales en las micro y
pequeñas empresas nacionales e internacionales.
*	Modelo apegado a MoProSoft y metodologías ágiles para desarrollo de
software.
*	Reuniones semanales para revisar los avances del proyecto.
*	Seguimiento de tesis y productos generados. Revisión de tesis,
elaboración de artículos de investigación
*	Liberación del proyecto.
*	Documentación del proyecto.
*	Elaboración de informes semestrales: Informe semestral e Informe final.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Las empresas, en su estructura, sus procesos, y en todas las disciplinas que la
integran, buscan lograr la calidad en productos y servicios, la certificación en
algún estándar internacional o usando metodologías de procesos, lo anterior va
de la mano con obtener lo que la organización necesita:
*	Productos de calidad;
*	Fiables;
*	Estables;
*	Funcionales;
*	y que reflejen los requerimientos.
Si bien los estándares internacionales como ISO/SC7, son usados en empresas
de mediano y gran tamaño, aun no son fácilmente implementadas en empresas
pequeñas y muy pequeñas, muchos son los factores: pocos recursos, personal
reducido, además de la dificultad que encuentran organizaciones de este tipo
para adaptar estos estándares, sin embargo este tipo de empresas requieren y
reclaman el uso de estándares y obtener una certificación con los mismos.
Diversas investigaciones, abundan en como acotar estos estándares, tomando
el camino de la fusión entre los mismos, integrando metodologías de procesos,
ciclos de vida, metodologías ágiles, etc. El campo de acción es muy amplio,
sobre todo en Latinoamérica donde la mayoría de las organizaciones son de tipo
pequeño y muy pequeño, Grupos internacionales buscan integrar estos
estándares en la empresa, y obtener resultados tangibles en las mismas.
En esta tesis se desarrolló un modelo capaz de solucionar esta problemática
obteniendo de MoProSoft (Oktaba 2007) su estructura y acotando sus procesos
a los requeridos por una empresa de tipo MPE, en el nivel de Desarrollo y 
Mantenimiento de Software se utilizaron las fases de la metodología ágil XP como estructura principal, a cada fase se fusionaron las prácticas, herramientas
y actividades de las siguientes metodologías:
*	XP;
*	SCRUM;
*	Crystal;
*	DSDM.
Con lo anterior obtenemos lo mejor de cada metodología, ya que con el análisis
concluimos que cada metodología es aplicable en casos específicos, con
diferentes técnicas, procesos, actividades y herramientas formales y no
formales. Así el equipo de desarrollo de software tiene variedad tanto formal, no
formal, esquemática y grafica para desarrollar cada fase del proceso de
desarrollo y mantenimiento se software.
Si bien existen cantidad de implementaciones de modelos y paradigmas para el
desarrollo y mantenimiento de software, sin embargo el contar con un modelo de
este tipo para una micro y pequeña empresa nos sirve como una guía fácil,
adaptable y entendible para asegurar la calidad en los productos y servicios.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
MPMPES: Modelo de Procesos para Micro y Pequeñas
Empresas Desarrolladoras de Software
TESIS
Que para obtener el grado de
Maestría en Computación
PRESENTA
Emilio Damián Bueno Vélez
ASESOR
Dr. Nicandro Farías Mendoza
COASESOR
Dr. Víctor Hugo Castillo Topete
Colima, Colima. Enero de 2012</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>PROCESAMIENTO Y ANALISIS DE IMAGENES SATELITALES UTILIZANDO LOGICA DIFUSA PARA APOYAR EN EL ESTUDIO DE CASOS DE DENGUE</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Facilitar la ubicación de zonas probables fuentes de reproducción del mosquito
Aedes aegypti causante del dengue hemorrágico por medio de un sistema similar a
un SIG (Sistema de Información Geográfica).</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es posible desarrollar de manera eficaz un programa en lenguaje Python que
permita analizar con Lógica Difusa una imagen de satélite?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Es factible aplicar la Lógica Difusa para obtener un eficiente análisis de imágenes
satelitales?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Es recomendable segmentar por medio de patrones de falso color, la vegetación y
cuerpos de agua, encontrados en imágenes satelitales para considerar la
probabilidad de que ahí se desarrolle el mosquito causante del dengue?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La aplicación de un algoritmo con Lógica Difusa y falso color para el
procesamiento de imágenes satelitales, apoyará favorablemente la detección de
zonas donde el mosquito Aedes aegypti causante del dengue hemorrágico se
desarrolla.
La variable dependiente:
Obtención de información adecuada, oportuna y pertinente que conduzca a la
identificación de zonas del estado de Colima consideradas probables zonas de
riesgo para la reproducción del mosquito Aedes aegypti causante del dengue
hemorrágico.
La variable independiente:
Programa de procesamiento digital de imágenes satelitales con análisis de
Lógica Difusa para la detección de vectores del Dengue.
Variables intervinientes:
Mala calidad de las imágenes de satélite, ruido aleatorio en las imágenes, equipo
de cómputo deficiente o lento para el procesamiento de imágenes.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Es importante poder contribuir a la erradicación de problemas como la enfermedad
del dengue hemorrágico, el cual se ha incrementado desde el año 2000 a 2006
cuando se reportaron en México 15, 866 casos de dengue correspondientes al
Estado de Colima 4,495 (5%). Para el año 2006 se confirmaron 27,287 de los cuales
4,477 pertenecían nuevamente al Estado de Colima, siendo considerado dentro de
los estados con mayor tasa promedio de casos dengue hemorrágico (CONAVE y
SSA, 2007). Según indicadores proporcionados por instituciones de salud en el
estado, la atención de casos incluye defunciones a causa de esta enfermedad,
recordando que se presenta en todo el mundo.
Sumado a todos los esfuerzos del sector salud y de las indicaciones a la
población por parte del mismo, este trabajo presenta sólo una propuesta con un
nuevo enfoque que ayude a detectar de forma previa el lugar de reproducción del
mosquito cuando las variables del ambiente sean factibles para llevarse a cabo dicha
reproducción. Es imprescindible hacer algo, ya que el causante es un enemigo casi
silencioso, por poco invisible y no distingue edad, raza o género.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En esta sección se utilizó una metodología exploratoria, basada en la
elaboración incremental de prototipos de software (Mcconnel, 1998) y se llevaron a
cabo pruebas de testeo de ingeniería de software (Ammann y Offutt, 2008) y de
usabilidad (Nielsen, 1993).
"Consultar bibliografía acerca de métodos difusos y Sistemas de Información
Geográfica.
"Investigar y aprender la programación en el lenguaje Python.
"Desarrollar un método similar a un Sistema de Información Geográfica.
"Diseñar las pantallas y botones que permitirán un uso más fácil y amigable del
sistema.
"Utilizar y adaptar un algoritmo para segmentar imágenes, basado en técnicas
de Lógica Difusa, en lenguaje Python.
"Llevar a cabo la prueba del sistema para corroborar que reconoce las
variables establecidas y realiza el correcto reconocimiento de patrones de
falso color.
"Documentar el funcionamiento del sistema así como los resultados
proporcionados.
"Presentar una propuesta del sistema que ayudará a analizar imágenes
satelitales, plasmándola en una tesis.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los resultados de la segmentación de una imagen y su análisis muestran que es
posible determinar el porcentaje de riesgo que tiene alguna zona de interés en el
estado, en cuanto a la reproducción del mosquito causante del dengue. Este
porcentaje de riesgo puede mejorar la toma de decisiones al permitir comparar su
variabilidad a través del tiempo consultando la base de datos que se utilizará de
resguardo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
PROCESAMIENTO Y ANÍLISIS DE IMÍGENES
SATELITALES UTILIZANDO LÓGICA DIFUSA PARA
APOYAR EN EL ESTUDIO DE CASOS DE DENGUE
TESIS
que para obtener el grado de:
MAESTRA EN COMPUTACIÓN
Presenta:
LI. JARIDE DEL ROCÍO TORRES ALONSO
Asesor:
D. en C. MIGUEL ÁNGEL GARCÍA RUIZ
COLIMA, COLIMA. AGOSTO DEL 2011.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ANÁLISIS Y PROPUESTA DE UN ESQUEMA DE CALIDAD DE SERVICIO (QoS) PARA LA RED DE LA UNIVERSIDAD DE COLIMA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Proponer un esquema de Calidad de Servicio para la red de la Universidad de Colima,
mediante el análisis de las arquitecturas de QoS factibles para aplicarse a este entorno y
el estudio de las prestaciones de los equipos de red de la dorsal.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Con la aparición de nuevas aplicaciones de tiempo real como telefonía o voz sobre IP
(VoIP), telecontrol, videoconferencias, entre otros; la actual dorsal de la red de datos
Universitaria necesita proveer los niveles de Servicio, adecuados con sus equipos de red que
tienen este potencial, pero que aún no se explotan al 100%. De allí que es necesario proponer
y desarrollar nuevos esquemas para que red de la Universidad de Colima asegure una buena
Calidad de Servicio (QoS) acorde a la importancia de los datos que se envían y reciben.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Como parte del análisis, se concluye que los enrutadores de la dorsal Universitaria
tienen soporte total para Servicios diferenciados (DiffServ), lo que hace factible la
aplicación de Calidad de Servicio dentro de la dorsal de la Universidad de Colima; lo que
no garantiza tener Calidad de Servicio es fuera de su dorsal Universitaria, debido a que
Telmex no se compromete a proveer algún esquema de Calidad de Servicio en particular.
Los equipos de PacketShaper por su capacidad de capa 7, lo hace más
accesible para manejar capacidades de ancho de banda de hasta 100Mbps, actualmente
empleado para el enlace de 34 Mbps hacia Internet, y cuya configuración proporciona
calidad de servicios a aplicaciones críticas.
La arquitectura de servicios integrados, a pesar de ser soportados en los
equipos SSR 8000 y SSR 8600, no se encuentra disponible al 100% en la versión de
firmware de los equipos. Además, es necesario que la totalidad de equipos de la
dorsal cumplan con esta arquitectura para tomarla en cuenta como opción.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA 
ANÍLISIS Y PROPUESTA DE UN ESQUEMA DE CALIDAD
DE SERVICIO (QoS) PARA LA RED DE LA
UNIVERSIDAD DE COLIMA
TESIS
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN COMPUTACIÓN
PRESENTA:
ARTURO TORRES GUTIÉRREZ
ASESOR:
M.C. LEONEL SORIANO EQUIGUA
COQUIMATLÍN, COLIMA. SEPTIEMBRE DEL 2003.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DESARROLLO DE SOFTWARE, APLICADO A LA GEOMETRÍA ANALÍTICA, A NIVEL MEDIO SUPERIOR</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Elaborar un software adecuado a los cursos de geometría analítica de la
Universidad de Guadalajara y de la Universidad de Colima.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La Elaboración de este Software se baso en la siguiente hipótesis:
Es posible lograr software donde, su contenido sea acorde a los programas
escolares, y al nivel que el grado de educación lo requiera.
Donde el Alumno Interactúe de forma dinámica con la computadora,
construyendo su propio conocimiento de una manera significativa y grupa ¡,
reforzando sus conocimientos básicos, y logrando aplicar los conocimientos en su
entorno.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La firma de las acuerdos comerciales internacionales como el Tratado de
Libre Comercio de América del Norte (TLCAN) y la corriente globalizadora que
impulsa el desarrollo económico en nuestra sociedad actual, ha traído como una de
múltiples consecuencias el que hoy en día sea muy común ver computadoras en
todas partes.
Debido a que la dinámica actual de esta sociedad de consumo es mucho más
rápida que la comparada con la que se tenia hace 20 años, se vuelve imprescindible
el incluir a la computadora como herramienta de trabajo en las aulas, ya que las
actividades nuevas exigen tiempo libre para dedicárselas a la deducción y la
inducción.
Los tiempos aquellos, en que se le dedicaba el mayor tiempo a los despejes
algebraicos, parece que están pasando de moda. Los alumnos, la mayoría de las
veces se muestran distraídos, cuando se abordan los temas de una manera
tradicional. Es muy probable que el tiempo de cambiar de estrategias en la
enseñanza de las matemáticas, ya este aquí.
Las herramientas actuales para la comprensión de la geometría analítica no
se ajustan del todo a los contenidos manejados.
Por lo que es muy importante, que se incluya a la computadora como una
herramienta elemental para su aprendizaje, y no como una simple y potencial
maquina de escribir, como desafortunadamente ha sido empleada hasta hoy.
Además, si se continúa utilizando los métodos tradicionales, con el cúmulo de
nuevos conocimientos que el alumno tiene que aprender, para estar al día, no se
alcanzaría a cumplir con todo el programa de estudio de un semestre.
De aquí la importancia de estar al día, con lo nuevo.
La utilización de este software plantea como un objetivo particular para el
estudiante, el investigar y hacer un análisis real de las ecuaciones y las familias de
curvas a las que pertenecen.
Se pretende que el uso de este Software; a diferencia de que cuando no se
contaba con esta herramienta, solo nos alcanzaba para deducir formulas,
algebraicamente y de una manera raquítica; nos haga disponer de tiempo, libre
disponible para analizar de manera formal los resultados obtenidos. Situación que
antes no se presentaba, por falta de tiempo.
Con este software prácticamente se han eliminado los problemas técnicos que
se tenían, con las aplicaciones de software existentes, para poderlos adecuar a la
currícula, que demandan la Universidad de Guadalajara y la Universidad de Colima.
Además de que se han implementado rutinas que no presentan las fallas de
funcionamiento, que se tenían en las otras aplicaciones.
La práctica que actualmente desarrollan los profesores en el aula y que cada
día toma una fuerza mayor es la de proporcionarle al alumno los medios para que
por sí mismo construya su propio conocimiento, a la vez que desarrolla este tipo de
construcción se le debe dirigir su avance de acuerdo al interés que muestre por la
materia en cuestión. Esto le va permitiendo que vaya formando sus juicios para
fundamentar su criterio, lo cual le permite a su vez reconstruir otros conceptos que
satisfacen sus necesidades. Quintero y Ursini (1988) comentan "Las confirmaciones
y suposiciones dan lugar a reformulaciones que permitan la elaboración de nuevos
marcos.".
Entendiendo como nuevos marcos, niveles de conocimiento más complejos
que son fruto de las formulaciones que nos facilitan abordar problemas y
conceptualizaciones de un nivel más complejo. Estas son actividades que en la
enseñanza tradicional no es posible lograr por la esquematización tan rígida a la que
el alumno se enfrenta; cuando lo conveniente debería ser que él, con base en su
avance cognitivo, valore la importancia que representa cada uno de los conceptos
que construye, a través de semejanzas que apropia de su realidad y a las cuales da
forma en su interpretación personal.
Se debe a esas interpretaciones, a que se le este dando un fuerte apoyo con
la ayuda de microcomputadoras, pero no con el objeto de tener esta herramienta
como algo tajante e inflexible, mucho menos para usarla en el desarrollo de una
labor mecánica; por el contrario, la microcomputadora, actualmente se está usando
como un medio para lograr que al alumno se le facilite la forma en cómo adquiere el
conocimiento que busca a su alrededor, facilitándole también que sus ideas
adquieran una objetividad más clara, mismas que le ayuden a plantearse
creativamente otras suposiciones con las cuales pueda confrontar sus resultados y
de esta forma objetivarlos en algo que corresponda a su entorno. Sin embargo, se
debe encaminar, por parte del maestro, el trabajo que el alumno quiere aprender, sin
que las microcomputadoras puedan sustituir su labor, lo cual nunca se podrá lograr.
La microcomputadora, como ya antes lo habíamos comentado sólo es una
herramienta que facilita el logro de los objetivos que se plantearon para lograr el
conocimiento de la materia.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología empleada en la elaboración de esta investigación fue:
La investigación se inicio en el calendario 1999 B, con el uso de los Software
Funpol y Cónicas, como prototipos.
En el calendario 2002 B, se elaboro el nuevo Software, corrigiendo las fallas
observadas en los prototipos utilizados.
Se uso el Software en ese Semestre.
Se empleo la metodología de la INVESTIGACIÓN ACCIÓN[7].
Se recuperaron las clases por medio de videos, como herramienta para
levantar el REGISTRO DE LA PRÍCTICA DOCENTE[8], en caso de que fuera
necesaria una revisión a fondo.
En cada clase se tomaron notas de la interacción del software con los
alumnos, para detectar errores, o interfaces poco manejables.
También se tomaron notas de las innovaciones necesarias al programa.
Después de la aplicación del software, y con base a las notas obtenidas se
hicieron los cambios y correcciones necesarias al programa.
La nueva versión del Software, se utilizo en un semestre mas, de la misma
manera anterior.
Se corrigieron los errores, y se implementaron las mejoras, para obtener esta
última versión</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Como se puede observar en las encuestas [26], La esperanza de los alumnos -
hayan tenido o no, la oportunidad de utilizar las computadoras en su aprendizaje- de
visualizar las matemáticas de una forma mas clara y concreta y lo menos abstracta
posible, esta depositada en la utilización de las computadoras en su aprendizaje.
Aunque también es cierto, porque así lo dicen, es necesario que se esfuercen
más para que realmente aprendan.
Otro hecho que se pudo observar, es que los alumnos piden formas de
enseñanza más atractivas, y acorde a su medio ambiente actual. Las utilizadas en la
forma tradicional, pudieran parecer poco motivadoras, y provocar el ausentismo en el
aula, o lo que es peor, la falta de compromiso con la materia.
Se observó en la 3a encuesta que, usando la computadora, el alumno
aprendió a analizar y se le facilita el reflexionar, importando poco que no posea los
conocimientos de álgebra necesarios.
El temor natural que el Alumno enfrenta ante la materia de Geometría
Analítica que aparentemente necesita de todos los niveles anteriores de
matemáticas, desaparece en parte, al sentir apoyo en la computadora, pero también
aquí el profesor debe poner mucho de su parte para simplificarle a los alumnos el
análisis, y no meter los en dependencias del álgebra que nada tienen que ver con
analizar, sino es confirmar de forma practica lo anteriormente aprendido.
De acuerdo a las encuestas, se puede decir que, es posible lograr software donde,
su contenido sea más acorde a los programas escolares, y al nivel que el grado lo
requiera. Donde el Alumno Interactúe de forma dinámica con la Computadora, construyendo su propio conocimiento de una manera
significativa y grupal, reforzando sus conocimientos básicos, y logrando aplicar los
conocimientos en su entorno.
Pero para lograrlo, el programador debe estar inmerso en el salón de clases,
ó mejor aun, que el profesor que imparte la materia, sea el mismo que elabore el
Software.
Solo que para que esto sea posible, los ingenieros en Software, deben
dedicarse a crear estos ambientes gráficos de fácil programación, donde sea
posible, "centrarse en el que y no el como".
Elaborar prototipos basados en software ya existentes, agilizan, el logro de
objetivos.
Poner a prueba los prototipos, en sus ambientes reales, nos acerca más a
lograr los objetivos esperados.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE INGENIERÍA ELECTROMECÍNICA
DESARROLLO DE SOFTWARE, APLICADO A LA GEOMETRÍA
ANALÍTICA, A NIVEL MEDIO SUPERIOR.
TESIS
PARA OBTENER EL GRADO DE:
MAESTRO EN COMPUTACIÓN.
Que presenta:
Ing. José de Jesús Sánchez Herrera
Asesor:
M.C. Marco Antonio Pérez González.
Coasesor:
M.C. Pedro Ramón Gómez López.
15 de Mayo de 2003 El Naranjo Col. México</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SISTEMA INTELIGENTE CONVERSACIONAL PARA LA ORIENTACION VOCACIONAL</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA 
FACULTAD DE TELEMATICA
SISTEMA INTELIGENTE CONVERSACIONAL PARA LA ORIENTACION VOCACIONAL
TESIS QUE PARA OBTENER EL GRADO DE 
MAESTRA EN COMPUTACION
PRESENTA: 
ANA CLAUDIA RUIZ TADEO 
ASESORES:
DR. MIGUEL GARCIA RUIZ 
DR. NICANDRO FARIAS MENDOZA
JULIO 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>PARALELIZACIÓN DE FILTROS DE CORRELACIÓN PARA DETECCIÓN DE OBJETOS CON MATLAB</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Implementar un ambiente de procesamiento en paralelo y los algoritmos para realizar el reconocimiento de objetos con filtros de correlación en imágenes a color en forma confiable y rápida.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Debido al incremento de la resolución en las imágenes actuales, el reconocimiento de objetos requiere mayor procesamiento, en cuyo caso, el poder de múltiples procesadores o de múltiples núcleos puede disminuir en gran medida el tiempo de ejecución, que en el caso de la programación secuencial tradicional no puede lograrse. Con el poder del procesamiento paralelo muchos procesos pueden alimentarse en distintas unidades de procesamiento, obteniéndose una mejora sustancial en los tiempos de ejecución, por ello, se vislumbra como una solución para obtener el mejor desempeño de los algoritmos que requieren un gran poder computacional.
Actualmente los equipos personales cuentan con procesadores potentes que integran más de un núcleo de procesamiento, pero que no se utilizan de forma óptima. A pesar de que el sistema operativo realiza las tareas administrativas sobre los programas a ejecutar, los programas que en ese momento se ejecutan podrían estar construidos con lenguajes tradicionales que los limitan a ejecutarse en una sola unidad de procesamiento, ignorando las demás. Este escenario se presenta en la mayoría de los casos y en diferentes sistemas operativos.
El trabajo que se presenta en este documento tiene como objetivo presentar una solución de programación paralela para llevar a cabo el reconocimiento de objetos con filtros correlacionados en imágenes a color, y mostrar las ventajas y desventajas del ambiente paralelo en cuestión.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo se muestra que la programación paralela no es una tarea sencilla, incluso personas capacitadas pueden tener problemas. Esto debido a la dificultad de identificar el código secuencial que puede programarse en paralelo, y a las variables dependientes que pueden limitar la ejecución en múltiples procesadores. La gran diversidad de problemas requiere algoritmos paralelos específicos para el problema y arquitectura donde se ejecuta. La asignación total de los recursos que se disponen en donde se ejecuta el algoritmo paralelo, no garantiza que el algoritmo se desempeñe de forma eficiente, en ocasiones repercute de forma negativa en su desempeño, incluso, puede generar costos monetarios por usar unidades de procesamiento que no necesita.
El algoritmo de filtros de correlación, permite detectar un objeto en particular dentro de una imagen, sin embargo, es necesario que ofrezca confiabilidad en la detección y localización. Al aplicar la programación en paralelo al algoritmo se lograron reducir los tiempos necesarios para completar estas tareas hasta en un 63.87% asignado recursos de forma óptima, de esta forma los tiempos de espera de resultados se reducen considerablemente. Se observó que el asignar más de 7 procesadores al algoritmo paralelo decreció el ahorro en tiempo y con claras tendencias a ser menos eficiente, además de usar más energía y unidades de procesamiento. El cálculo de la eficiencia mediante la ecuación (9) confirma claramente que el uso de los recursos disminuye conforme se incrementan los procesadores asignados.
Los resultados de las simulaciones por computadora que se realizaron y que se presentan en este trabajo, muestran el desempeño alcanzado por los filtros ejecutados en un ambiente paralelo con respecto a la programación secuencial tradicional. Sobre los resultados obtenidos, es importante señalar que dada la cantidad de experimentos realizados en cada simulación, la información que se obtiene es lo suficientemente confiable para hacer las conclusiones presentes.
Cabe señalar que los resultados obtenidos con la programación secuencial fueron los mismos a los obtenidos en ambiente paralelo, con la diferencia de ahorro en tiempo. Esto beneficia en gran medida a quienes requieren llevar a cabo tareas de reconocimiento de objetos. De tal manera la programación paralela muestra ser una solución viable sin la necesidad de mucha inversión en infraestructura, pudiendo integrar equipos de económicos de bajo poder computacional al ambiente paralelo y distribuir pequeños procesos para su solución en cada equipo integrado, obteniendo ahorros en tiempo y un incremento en el poder de procesamiento de manera significativa.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
PARALELIZACIÓN DE FILTROS DE CORRELACIÓN PARA DETECCIÓN DE OBJETOS CON MATLAB
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN COMPUTACIÓN
Presenta
ING. MARTIN SARABIA AQUINO
Asesores:
Dra. Erika M. Ramos Michel
Dr. Juan Manuel Ramírez Alcaraz.
Colima, Col., Enero de 2012</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>MODELO DE CALIDAD PARA LA MICROEMPRESA BASADO EN MOPROSOFT</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En esta sección se mencionan situaciones problemáticas de las microempresas en cuanto a la adopción de modelos de calidad así como porcentajes de las que cumplen con esas características.
“La administración del proyecto de Software permite asegurar que éste se lleve a cabo a tiempo y de acuerdo a la planeación y a los requerimientos"  (Mejía, 2004).
México ya dio un gran paso al contar con la norma mexicana NMX-I-059-02 (MoproSoft) dirigida a las PyMES Hasta el año 2005 sólo se podían encontrar documentadas las experiencias de los casos de estudio del programa de Pruebas controladas, en el que se implantó MoproSoft en cuatro empresas y se evaluaron sus procesos utilizando el Método de Evaluación de Procesos de Software (EvalProSoft) (Alquicira y Su, 2005).
Una de las características más generales de las Micro, Pequeñas y, en algunos casos, de las Medianas empresas, es el uso intensivo de mano de obra no calificada. Esto origina que las remuneraciones percibidas por los trabajadores sean bajas, lo que provoca entre otras cosas una rotación alta de personal. Hay que sumar que en muchas ocasiones los trabajadores no perciben todas las prestaciones que señala la ley (Espinosa y Pérez, 2007). Una microempresa es aquella que está compuesta por menos de 10 personas (Dopacio, 2004).
El proceso de Software se está convirtiendo en una preocupación mayor para las empresas desarrolladoras de Software como una de las maneras de asegurar la calidad mientras se desarrolla un sistema con el proceso de Software.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Establecer un modelo de calidad para microempresas desarrolladoras de Software que agilice los procesos y reduzca el tiempo de entrega del producto final.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>MoproSoft puede ser reducido y adecuado a las microempresas, creando un modelo de calidad para ellas basándose en las prácticas más comunes y obtener un modelo sobre el cual se pueda desarrollar un sistema.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología es híbrida debido a los diversos métodos empleados y se basa en el Proceso Racional Unificado llamada RUP por siglas en ingles.
RUP es una metodología de desarrollo para proyectos de Software que define claramente quién, cómo, cuándo y qué debe hacerse en el proyecto. Son características esenciales (Gómez, 2007):
*	Se dirige por los casos de uso: que orientan el proyecto, a la importancia para el usuario y lo que éste quiere
*	Está centrado en la arquitectura: que relaciona la toma de decisiones que indican cómo tiene que ser construido el sistema y en qué orden, es iterativo e incremental: divide el proyecto en miniproyectos para cumplir objetivos de manera más depurada.
A su vez divide en 4 fases el desarrollo del Software (Mendoza, 2004):
*	Inicio: El objetivo en esta etapa es determinar la visión del proyecto.
*	Elaboración: En esta etapa el objetivo es determinar la arquitectura óptima.
*	Construcción: Etapa en la que el objetivo es obtener la capacidad operacional inicial.
*	Transmisión: Su objetivo es obtener el entregable del proyecto
Para cumplir con la metodología primeramente se realizó una búsqueda de libros, artículos, revistas, ponencias, etc., en gran parte con el apoyo de los asesores en cuanto a la recopilación de artículos científicos, lo que sirvió para definir un estado del arte e identificar lo que se ha hecho en el área en lo referente a MoproSoft y la adopción de estándares de calidad por parte de las microempresas.
Dentro de la definición del estado del arte se realizó una investigación de Modelos de Calidad y las causas que motivaron el origen de MoproSoft. Posteriormente, se revisaron los trabajos relacionados o similares en adopción de Modelos de Calidad, después se buscaron los detalles de una microempresa tales como el número de personal y el comportamiento laboral que tienen dentro de ésta y el modo utilizado para implementar otros modelos en las empresas que desarrollan Software. De igual modo se buscaron las situaciones que dificultan la adopción de Modelos de Calidad, es por eso que se propuso reducir en lo posible MoproSoft para hacer más fácil su adopción. Para identificar los procesos y áreas e inclusive el modo más viable para adaptar el nuevo modelo se tuvo el apoyo de personas que ya trabajaron y cuentan con certificación de MoproSoft.
En lo que respecta al modelo de calidad para la microempresa basado en MoproSoft se buscaron los tipos de Metodologías, resultando las de tipo ágil como las más apegadas a la forma de trabajo de las microempresas de acuerdo a sus características. Posteriormente se buscaron artículos donde se presentaran resultados de las metodologías más utilizadas en las microempresas del país, dicha información concuerda con lo descrito en este trabajo en cuanto a que las microempresas prefieren adoptar Metodologías ágiles.
Dentro de los artículos sobre microempresas se encontró que MoproSoft es utilizado en gran parte de ellas y también cuentan con menos de 10 personas laborando.
Con base en MoproSoft se realizó una comparativa, por lo que se descartaron los procesos a nivel Gerencia que son muy pesados o difíciles de cubrir para las microempresas de acuerdo a sus capacidades y se eliminó el nivel de Alta dirección, asumiendo que en una microempresa este nivel es cubierto por el gerente o dueño.
Respecto al nivel de Operación se realizó una propuesta nueva basándose en los resultados que indican que las Metodologías ágiles más utilizadas son la Extremme Programming (Xp) y Scrum, lo que originó una unión de estas metodologías, dicha combinación ya ha sido propuesta y probada por diversos autores.
Posteriormente, se describieron los procesos del nuevo modelo respetando tal y cual como están descritos por MoproSoft los requisitos de procesos, se ejemplificó esta propuesta con un trabajo realizado en la Facultad de Ingeniería Mecánica y Eléctrica de la Universidad de Colima. Aunque este trabajo no fue realizado por una microempresa se describió como tal con base a que fue un proyecto con el mismo número de personal involucrado y de acuerdo a artículos científicos se realizó en el mismo tiempo que le tomaría a una microempresa.
Por último se hizo una discusión de los elementos del modelo y se demostró el cumplimiento de la hipótesis de trabajo, así mismo se hacen recomendaciones de trabajo a futuro tomando en cuenta lo descrito en este trabajo y el de los autores interesados en adoptar modelos de calidad en las microempresas.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>6.1 Discusión
El desarrollo de este modelo de calidad para la microempresa basado en MoproSoft tiene importancia en cuanto a que toma las partes más significativas de algunas de las metodologías más utilizadas por parte de las microempresas en el desarrollo de software y sirve de base para certificarse en MoproSoft o ISO, lo que les haría más competitivas en caso de que exista alguna licitación cuyos requisitos sean la certificación en alguna de las normas anteriores. Algunos de los problemas enfrentados en el desarrollo fue la poca disponibilidad de material respecto a los requisitos necesarios para certificarse en MoproSoft, la falta de encuestas sobre el número de microempresas certificadas y la norma que adoptaron. El modelo desarrollado sólo aplica a microempresas y se sugiere que esté compuesta por al menos 5 empleados asignando roles específicos en funciones para el cumplimiento del proyecto.
6.2 Cumplimiento de las hipótesis de trabajo
Considerando los resultados obtenidos en las pruebas, se logró reducir MoproSoft y adecuar a las microempresas a través de un nuevo modelo que agiliza los procesos y reduce el tiempo de trabajo comparado con una forma de trabajo sin organización, el modelo baso en las metodologías más utilizadas como Xp y Scrum. Se logro identificar las características de MoproSoft que son más viables en el ambiente de desarrollo de una microempresa debido al número de personal con el que cuentan y las dificultades que presentan, se pudo desarrollar con éxito un Sistema de Registro de Actividades bajo el nuevo modelo, por lo que se cumplió con la hipótesis de trabajo planteada en un inicio.
6.3 Recomendaciones
Con el propósito de continuar con la investigación en el futuro se hacen las siguientes recomendaciones:
Realizar una unión entre diferentes metodologías a las mostradas con el fin de abarcar un mayor número de microempresas y que el cambio que se realice afecte el mínimo posible a su forma de trabajo.
Realizar una implementación del modelo en microempresas con un número de empleados superior a siete y analizar el funcionamiento del modelo y las mejoras posibles.
Probar el sistema propuesto al desarrollar un sistema de mayor complejidad y número de módulos que el sistema con el que fue probado.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>FACULTAD DE TELEMÁTICA
MODELO DE CALIDAD PARA LA MICROEMPRESA BASADO EN MOPROSOFT
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN COMPUTACIÓN
Presenta:
SERGIO ALAN FLORES ROSALES
Asesores:
Dr. C. Nicandro Farías Mendoza
M. en C. Armando Román Gallardo
COLIMA, COL., FEBRERO DE 2012</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>TOPICOS DE ALGEBRA CON OBJETOS DE APRENDIZAJE IEEE-LOM</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El ambiente tradicional de enseñanza es usualmente a través del salón de clases y totalmente presencial, donde el maestro da unas lecturas a un grupo de estudiantes, quienes esperan usar sus notas y textos para preparar un examen que normalmente aplican y de esta forma se demuestra que han aprendido. En la mayoría de estos casos se requieren de elementos que ayuden al fortalecimiento de los conocimientos vistos, o una explicación diferente a la presentada por el profesor. Es entonces cuando el desarrollo y aplicación de un objeto de aprendizaje así como la de un sistema de administración tienen como objetivo el ayudar al estudiante en el incremento de la adquisición de conocimiento, potencializando la comprensión y entendimiento de los contenidos, logrando de esta manera un nivel de aprendizaje más alto.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Generar una herramienta de soporte para el estudio del álgebra en el nivel medio superior a través de un sistema computacional que administre los objetos de aprendizaje con IEEE-LOM asociados con diversos tópicos de esta disciplina.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La educación es una de las actividades más importantes del ser humano. Hoy en día, son múltiples los recursos y herramientas que se han desarrollado para apoyar esta actividad en sus diferentes niveles, sin embargo, debido a la alta demanda de servicios educativos, el desarrollo de nuevas herramientas que den soporte al proceso de enseñanza aprendizaje son cruciales y de suma relevancia. Una de las principales dificultades con las que se encuentra un educador es la falta de comprensión de los temas debido al tiempo de duración de las clases; es por esto que los objetos de aprendizaje (OA) intervienen de una manera decisiva ya que pueden ser consultados fuera de clases y cualquier número de veces, fomentando de esta manera el entendimiento y comprensión de los temas vistos en clases.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Se propone el desarrollo de un sistema de administración en objetos de aprendizaje desarrollados bajo el estándar IEEE-LOM, mediante la aplicación de tecnologías Web, para determinar el nivel de soporte efectivo en el proceso de enseñanza aprendizaje de las asignaturas de álgebra en el nivel medio superior.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>4.1 Discusión de resultados obtenidos
La educación hoy en día ha cambiado, han sido múltiples factores que han provocado estos cambios, las nuevas tecnologías de información y comunicación, la globalización, entre otras. Hay factores que generan una perspectiva en la cual no se concibe al estudiante sentado en un aula frente al profesor recibiendo una clase, por el contrario, el estudiante está fuera del aula sin la intervención directa de un profesor; estudiando material de una asignatura mediante alguna herramienta de cómputo. Los objetos de aprendizaje ofrecen una oportunidad para estudiantes con diferentes estilos de aprendizaje, estudiantes que tal vez no aprenden a la primera y es necesario repasar la lección o el material más de una vez, el estudiante que por diferentes razones no puede asistir o formar parte de un sistema escolarizado. Para estos tipos de estudiantes o para estudiantes regulares en sistemas escolarizados los objetos de aprendizaje ofrecen un apoyo en el estudio del álgebra.
Un sistema fácil de utilizar es importante para promover el uso de los objetos de aprendizaje, un sistema el cual no requiera de la ayuda personal de algún técnico, y que las distintas funciones que el sistema ofrezca estén bien integradas. Todo lo anterior permitirá que la mayoría de los usuarios no encuentren problemas en aprender a utilizarlo y que les permita sentirse de cómodos y confiados.
4.2 Cumplimiento de la hipótesis
Se determina en el presente trabajo que el uso de objetos de aprendizaje son instrumentos de soporte efectivo en el proceso de enseñanza aprendizaje de las asignaturas de álgebra en el nivel medio superior, lo anterior con fundamento en la posibilidad de localizar y acceder a los objetos de aprendizaje desde una localización distante y mostrarlos en diferentes localizaciones; es posible utilizar objetos desarrollados mediante una serie de herramientas y usarlos en otros entornos gracias al estándar IEEE-LOM. El objeto de aprendizaje soporta cambios en la tecnología sin rediseño o modificación pero sin duda la reusabilidad es una característica muy importante que permite la flexibilidad de incorporar objetos de aprendizaje en múltiples aplicaciones y contextos.
4.3 Cumplimiento de los objetivos
En el desarrollo del presente proyecto se lograron cumplir los siguientes objetivos:
1. Utilizar el estándar IEEE-LOM.
2. Realizar búsquedas simples.
3. Desarrollar funciones de desplegado de contenido amplio de un Objeto de Aprendizaje (impresiones en pantalla y papel).
4. Almacenar los perfiles de usuario.
5. Manejar Objetos de Aprendizaje de nuestra propiedad (Fase inicial)
6. Implementar un repositorio para almacenar metadatos.
7. El sistema está pensado para soportar dos tipos de usuarios, administrador y estudiante.
8. El administrador puede dar de alta nuevos objetos, así como bajas, modificaciones y consultas.
9. En un repositorio los objetos de aprendizaje se almacena.
10. Productos notables y factorización son los temas en los que se especializa el sistema.
Con el cumplimiento en buena medida de los objetivos antes mencionados se cumple con la construcción de una herramienta de soporte para el estudio del álgebra en el nivel medio superior. Esta construcción se realizó mediante un sistema computacional que administra objetos de aprendizaje con IEEE-LOM asociados con diversos tópicos de esta disciplina.
4.4 Importancia de los resultados obtenidos
En la actualidad la educación requiere de recursos que permitan cubrir las diferentes demandas de un estudiante, hoy en día no se puede afirmar que existe un solo tipo de estudiante, existen diferentes perfiles de estudiantes. Los objetos de aprendizaje ofrecen cualidades que apoyan al estudiante en el estudio de alguna materia. En el caso del álgebra mediante una herramienta de administración de objetos de aprendizaje, los estudiantes hacen uso de estos recursos de manera sencilla sin la necesidad de haber aprendido demasiados conceptos informáticos y sin la necesidad de ayuda personal. Algo muy importante es que el objeto de aprendizaje siempre está disponible.
4.5 Posibles aplicaciones
La principal aplicación se orienta al soporte en el estudio del álgebra en el nivel medio superior, sin embargo, no hay que descartar niveles de estudios inferiores o superiores como son el nivel básico (secundaria) y nivel superior (licenciatura). Aunque los requerimientos de enseñanza aprendizaje son diferentes en estos niveles, los objetos de aprendizaje representan una opción adecuada gracias a sus diferentes cualidades.
Existen en nuestro país varios programas de estudio a distancia para diferentes niveles de estudio, la compleja situación que en ocasiones conlleva desplazarse hasta comunidades alejadas por parte de un instructor o maestro abre una buena oportunidad de aplicación para esta herramienta.
4.6 Limitaciones de la investigación
Del área de las matemáticas, sólo algunos tópicos de álgebra orientados al nivel medio superior han sido el motivo de investigación, por cuestiones de tiempo el resto de temas referentes a matemáticas así como a otras disciplinas se han contemplado como investigaciones en lo futuro.
La utilización del formato XML para metadatos en sintaxis SQL, así como la creación de un portafolio personal de objetos de aprendizaje, fueron características no desarrolladas por cuestiones de tiempo, pero se han contemplado como investigaciones en lo futuro.
Algunas de las dificultades con la que me encontré al momento de desarrollar el sistema fue el determinar la mejor manera para estructurar el formato de captura de un objeto de aprendizaje de acuerdo al IEEE-LOM, por lo extenso del estándar la captura tuve que hacerla directamente en la base de datos. Al momento de probar el sistema, por el tipo de usuarios, adolecentes todos, la mayoría de ellos al principio no entendían el cuestionario, por lo que tuve que dedicarle una sesión de 50 minutos para explicarles pregunta por pregunta del cuestionario SUS, con ejemplos a lo que se refería cada una de ellas o cual era el propósito de cada pregunta.
En el caso de la enseñanza de las matemáticas, los estudiantes que encuesté me mencionaban que les parecía bien tener disponible recursos de apoyo; pero que creen que no podrían llevar un curso completo de álgebra en línea, mencionaban que es muy importante para ellos la conducción de un docente, de alguna forma interpreto que por su edad y por la necesidad de una figura que los guie, el paso de emigrar de clases presenciales a no presenciales, les costaría mucho trabajo.
4.7 Recomendaciones para investigaciones en lo futuro
A futuro los objetos de aprendizaje bajo el estándar del IEEE-LOM pueden ser usados en distintas asignaturas, además del álgebra en aplicaciones específicas haciendo uso nuevamente de la reusabilidad, propiedad estratégica de los objetos de aprendizaje.
Se recomienda utilizar el formato XML para metadatos en sintaxis SQL, así como la creación de portafolio personales de objetos de aprendizaje.
Incluir la lista completa de tópicos de álgebra que contiene el programa de estudios de la Universidad de Colima (nivel medio superior).
Los objetos de aprendizaje son elementos fundamentales en un nuevo concepto de enseñanza-aprendizaje, su propósito es transformar o solo cambiar la forma en que se percibe el aprendizaje, usando diferentes tipos de contenidos, desarrollo y diseño, puede ser posible que este tipo de recursos sea orientado a estudiantes no solo de educación media superior, el futuro de los objetos de aprendizaje tienen un margen más amplio que el de los estudiantes preparatorianos, puede incluirse desde la educación básica hasta la capacitación laboral.
En la actualidad se plantea que muchas de las habilidades y de los conocimientos que una persona desarrolla y domina se inician a edades tempranas, aprovechándonos de la presencia de la tecnología en los artículos dirigidos a los niños y niñas, los objetos de aprendizaje podrían tener un campo estratégico, para incluirse en algún dispositivo cuyo propósito sea enseñar, estimular o capacitar.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA 
FACULTAD DE TELEMATICA
TOPICOS DE ALGEBRA CON OBJETOS DE APRENDIZAJE IEEE-LOM
TESIS 
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN COMPUTACION
PRESENTA:
ISC.SAUL GUTIERREZ DIAZ
ASESORES:
D. EN C. NICANDRO FARIAS MENDOZA
D. EN C. MIGUEL ANGEL GARCIA RUIZ
COLIMA, COL ENERO DE 2012</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>CREACION DE UN SITE VIRTUAL PARA LA FIME</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Ser el primer sitio WEB de este tipo en el País.
l Dar continuidad a la línea de investigación de Realidad Virtual recientemente
inaugurada en la FIME.
l Continuar con el permanente afán de superación tecnológica que ha
caracterizado a la Facultad de Ingeniería Mecánica y Eléctrica, y que le ha
permitido situarse en un nivel preponderante entre las diversas facultades de
la Universidad de Colima y del país.
l Incrementar la proyección de esta Facultad, y su área de Posgrado, en el
ámbito nacional y mundial, por la gran cantidad de visitas que se esperan en
este sitio; lo cual permitirá que sus instalaciones, proyectos de investigación,
carreras a nivel licenciatura, área de Posgrado, planes de estudio etc. sean
aún más conocídos por los usuarios de Internet.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>l.- Lo primero que se realizó fue una amplía investigación en el campo de la
Realidad Virtual, para conocer el estado del arte de esta disciplina y, partiendo de
ahí, se inicio el desarrollo del proyecto teniendo ya una sólida base de
conocimientos que permitió elegir las mejores opciones de diseño y desarrollo de
mundos virtuales.
2.- Paralelamente a la investigación de esta disciplina se digitalizaron imágenes
de toda la Facultad para tenerlas presentes durante el diseño, y fueron utilizarlas
en el desarrollo del proyecto.
3.- Una vez concluida la investigación de la Realidad Virtual, y ya con las
imágenes digitalizadas de la FIME, se procedió al análisis de toda esta
información para decidir cuáles serán las mejores opciones de desarrollo del
sistema.
4.- Después de analizar la información recopilada, y teniendo como premisa que
la recreación debería apegarse un cien por ciento a la realidad, y de que el SITE
seria visitado vía Internet, se concluyó que lo más adecuado era elaborar un
archivo por cada uno de los edificios de la FIME y desplegar, además, una
pequefia leyenda alusiva.
5.- Una vez concluido el diseño se procedió a la implementación, la cual fue
llevada a cabo en VRML 2.0. Inicialmente se desarrollaron prototipos en diversos
programas de diseño gráfico en tercera dimensión, como Corel, trueSpace4 y
Virtus Walk Through VRML, que después permiten la conversión de estos archivos
al formato VRML, pero esto no resultó muy práctico debido a que los archivos
generados eran del orden de los 10 MB o aún mucho mayores, lo que hacía que
su carga por Internet fuera sumamente lenta y no era en modo alguno la mejor
solución, así que se optó por programar directamente en VRML y no utilizar ninguno de estos paquetes de diseño gráfico en la generación de los mundos
virtuales.
6.- Por último se montó la página en un sitio de Internet y se estuvieron
haciendo pruebas de carga con diferentes equipos de comunicación hasta lograr
un compromiso entre la definición y la rapidez de carga.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Las aplicaciones de la realidad virtual pueden darse, como hemos visto en el
desarrollo del presente documento, prácticamente en cualquier ámbito de la
ciencia y tecnología pasando por aplicaciones de entretenimiento (las cuales
fueron la primera aplicación que tuvo la realidad virtual y las gráficas en 3D) y
cultura lo cual hace que, a pesar de no ser un campo nuevo, continúe siendo un
área fértil de oportunidades.
Otra conclusión importante que se desprende del análisis de la historia y
aplicaciones de la realidad virtual es que, en la gran mayoría de los casos y,
debido al amplísimo campo de acción de la RV, que va desde aplicaciones a nivel
atómico hasta la recreación de todo un medio ambiente particular de un planeta
pasando por el desarrollo de aplicaciones médicas y diseño de naves espaciales,
el desarrollo de una aplicación de realidad virtual debe involucrar un equipo
multidisciplinario, donde confluyan especialistas en desarrollo de software de 3D y
realidad virtual, especialistas en desarrollo de hardware e investigadores en el
área particular de la ciencia y la tecnología para la cual se quiera desarrollar una
aplicación.
En los inicios de la realidad virtual, al igual que en la mayoría de los avances
científicos, las primeras investigaciones y dispositivos desarrollados se debieron a
investigadores solitarios que con sus propios medios y escasos recursos y que,
muy probablemente, sin visualizar totalmente el gigantesco campo de acción que
estaban abriendo con sus investigaciones se dieron a la tarea de desarrollar
dispositivos y aplicaciones que inicialmente fueron relativamente sencillos y
probablemente sin una aplicación practica inmediata.
No fue sino hasta pasados algunos años, cuando las gigantescas corporaciones
wrciales e instituciones militares se dieron cuenta de la “mina de oro"  que podría resultar del desarrollar la realidad virtual, que se empezó a apoyar
fuertemente el desarrollo de software y dispositivos virtuales a raíz de lo cual la
realidad virtual ha tenido un auge impresionante que continúa creciendo día con
día al descubrirse continuamente nuevos campos de acción en los cuales puede
aplicarse.
Las aplicaciones reales, sin embargo, por muy sofisticadas que sean, nunca
estarán a la altura de nuestra imaginación y un mercado impaciente demandando
continuamente nuevas, mejores y más reales aplicaciones hace que sea
doblemente difícil para los investigadores mantener la paciencia y la validez
científica de sus trabajos.
Podemos pensar de pronto que la tecnología de realidad virtual tiene un toque
mágico ya que sus límites son las fronteras de nuestra imaginación a partir de la
cual podemos desarrollar infinidad de aplicaciones prácticas.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA 
FACULTAD DE INGENIERIA MECANICA Y ELECTRICA
corpus_proyectos EN CIENCIAS, AREA COMPUTACION
"CREACION DE UN SITE VIRTUAL PARA EL FIME"
TESIS
QUE PARA OBTENER EL GRADO DE 
MAESTRO EN CIENCIAS, AREA EN COMPUTACION 
PRESENTA 
ING. RUBEN ZEPEDA GARCIA
ASESOR
M.C. RODOLFO GALLARDO GONZALES
ABRIL 2000</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DESARROLLO DE UN MOTOR DE BÚSQUEDA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Una gran cantidad de información disponible en cualquier entidad contenedora (real o virtual)
puede llegar a ser en sí misma un problema, tal es el caso de la Web. Sin embargo si a este le
agregamos la terrible desorganización existente en parte provocada por la falta de reglas y
estándares y por la imposibilidad de regular la publicación de documentos o de implementar
medidas para detener el avance del problema, entonces la situación empeora.
Con la llegada de la computadora podríamos imaginarnos (en un mundo ideal) que el
problema se simplifica, no obstante, la estructura tan abierta de Internet (combinada con la
falta de estándares y regulación ya mencionadas) que facilita la publicación masiva y
descontrolada, pone de manifiesto la complejidad que existe en la clasificación,
almacenamiento y recuperación de la información, y me permite asegurar que la falta de
control y estandarización facilita el agravamiento del problema.
La finalidad de publicar documentos como es obvio, es que la información o para ser más
genéricos, el contenido de tales documentos se haga del dominio público, es decir que esté
disponible y fácilmente accesible.
No obstante la realidad es otra. En una encuesta (la cual se detalla más adelante en el
desarrollo de la propuesta principal del este documento) aplicada a 276 estudiantes, 25
profesores, profesionistas y personas en general que usan o han usado algún sistema de
búsqueda en Internet y que fue realizada exclusivamente para esta investigación se determinó
que uno de los principales problemas con el uso de estas herramientas es la enorme cantidad
de referencias que resultan de una consulta pero lo que más complica el asunto es que un alto
porcentaje de los resultados no tiene ninguna relación con la información buscada. Desde
luego son varias las razones para un porcentaje tan bajo de eficiencia pero lo importante es que
la gran cantidad de información de la Web no está resultando tan disponible y fácilmente
accesible como se pretende realmente que sea. Charles Morris, editor de WDVL (Web
Developer's Virtual Library, www.wdvl.internet.com) en un artículo de junio del año 2000 titulado “ ¿Están muertos los motores de búsqueda?" , comenta que hay profesionistas
reconocidos que plantean a los motores de búsqueda como una tecnología en fase terminal, no
obstante también señala que con todo y los problemas inherentes al uso de motores de
búsqueda y directorios (véase la definición de los sistemas de búsqueda en el capítulo I)
continuarán usándose porque hasta el momento es lo único que tenemos para encontrar
información en la Web.
En lo personal creo y mi propósito en este documento es proporcionar bases para apoyar mi
teoría, que la creación y uso de motores de búsqueda especializada (en este caso en la
investigación) y con algunas innovaciones pueden mejorar en gran medida todo el proceso
desde la publicación, recopilación de datos, clasificación y obtención de información.
En este punto creo necesario plantear cual es la diferencia entre una colección de documentos
bien controlada y otra que no lo es, como en el caso de la Web y proporcionar con esto un
panorama más amplio de la magnitud del problema.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Hacer uso de los conocimientos adquiridos durante el curso de la maestría para el diseño,
desarrollo e implementación de un sistema de búsqueda para la Universidad de Colima,
específicamente para la Facultad de Ingeniería Mecánica y Eléctrica.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La búsqueda de instrumentos que fortalezcan el proceso de enseñanza aprendizaje es y ha sido
durante mucho tiempo uno de los aspectos que mayormente han preocupado a la Universidad
de Colima, en su afán por lograr la formación de profesionistas de calidad. Es precisamente en
este punto donde el presente proyecto de investigación y documento de tesis encuentra su
justificación.
Dado que las instituciones de enseñanza y específicamente en este caso la Universidad de
Colima en la actualidad se preocupan y ocupan de proveer de equipo y herramientas de
software, así como enlaces de considerablemente buen ancho de banda a Internet tanto a
estudiantes, como a profesores e investigadores, es conveniente también dotar de elementos
que faciliten el mejor aprovechamiento de los recursos otorgados. El presente proyecto de
investigación es un esfuerzo más por facilitar el proceso de enseñanza aprendizaje y de
investigación como parte esencial de la función formadora de profesionistas de calidad de la
Universidad de Colima para la sociedad.
Por otra parte es importante señalar que el sólo hecho de fomentar la investigación en
estudiantes y profesores es en sí una justificación suficientes por la generación de
conocimiento que esto implica. Más aún si se considera que el producto de esta investigación
podrá ser la base que sustente y justifique a su vez futuros trabajos de investigación que se
relacionen con temas muy variados, tales como la programación en Internet, la programación
Web, la obtención o recuperación de información de colecciones heterogéneas (como la Web),
la publicación Web y los sistemas de búsqueda por mencionar algunos.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para llegar a la presente propuesta, se utilizaron principalmente cuatro instrumentos de
investigación:
a) La investigación documental, es decir la recopilación de información por medio de la
consulta de libros impresos y electrónicos, revistas, artículos Web y bibliotecas
electrónicas.
b) Entrevistas no estructuradas, principalmente aplicada a 25 profesionistas de distintas áreas
(informática, administración, contabilidad, derecho, matemáticas, entre otras), que se
dedican principalmente a la docencia y/o son estudiantes de posgrado.
c) Cuestionario, que se aplicó a 276 estudiantes en su mayoría de la licenciatura en áreas de
la Informática, Administración, Contabilidad y Comercio Exterior de diversos grados.
d) Evaluación de productos de software, específicamente sistemas de búsqueda, tanto
directorio, motores de búsqueda, híbridos y metabuscadores.
La finalidad fue definir los sistemas de búsqueda, los tipos, sus componentes, su
funcionamiento, sus etapas y sus características; identificar la problemática en el uso de este
tipo de sistemas; y las ventajas y desventajas comparativas entre los distintos productos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
Facultad de Ingeniería Mecánica y Eléctrica
DESARROLLO DE UN MOTOR DE BÚSQUEDA
Tesis que para obtener el grado de
Maestría en Ciencia Írea Computación
Presenta
Rutilio Rodolfo López Barbosa
Asesor
M.C. Rodolfo Gallardo Rosales
Coquimatlán, Colima, Octubre 2002</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“DESCRIPCIÓN DEL EMPLEO DE LAS COMPUTADORAS POR PARTE DE LOS DOCENTES Y ALUMNOS DEL NIVEL SUPERIOR DE LOS CAMPUS COLIMA Y COQUIMATLÍN DE LA UNIVERSIDAD DE COLIMA, COMO FUNDAMENTO DE UN MODELO PEDAGÓGICO PARA EL USO DE LAS MISMAS" .</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>A manera de estrategia general para la estructuración y desarrollo de esta
investigación es importante enunciar lo que es el propósito principal y secundario de
la misma:
Elaborar el diagnóstico de los usos y aplicaciones que los profesores y
estudiantes de los Campus Colima y Coquimatlán de la Universidad de Colima, le
dan a los diferentes equipos de cómputo, con la finalidad de generalizar los
resultados al nivel superior de esta institución educativa.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La escuela como institución educadora no ha cambiado mucho en los últimos
200 años, las prácticas de los procesos de enseñanza-aprendizaje
fundamentalmente han estado basadas en el uso del pizarrón y el discurso verbalista
del profesor, asimismo los procesos para tener el conocimiento por parte de los
estudiantes tienen como eje central el discurso del profesor, los textos básicos del
programa de la materia y algunas fuentes adicionales de información como lo son
revistas o periódicos afines a la asignatura.
Este panorama educativo poco a poco ha sido desplazado por uno que incluye
diferentes capacidades y actitudes para el alumnado y para el profesorado de la
institución educativa, esto como consecuencia natural de la introducción en la
educación de las nuevas tecnologías de la información y la comunicación, teniendo
como punto de referencia el uso de la computadora con fines didácticos.
La computadora, junto con las tecnologías de la información y la comunicación
ha venido a dar un giro al modelo educativo tradicional, propiciando la creación de
nuevos ambientes de aprendizaje, en donde el profesor ya no es quien tiene toda la
información, sino que ésta se encuentra en una gran variedad de fuentes y formatos,
siendo las de características digitales las más actualizadas y con mayores recursos.
Internet es una fuente inagotable de recursos digitales, que abarca tanto áreas
de recreación como de educación, ha llegado a ser un vehículo de uso común en las
instituciones de educación superior tanto de países desarrollados como en vías de
desarrollo. La computadora es una de tantas máquinas que hoy en día puede
manejar la información que se tiene a través de la red mundial, el uso que se le da en
la escuela ha permitido solventar algunos problemas relacionados con la obtención
de fuentes informativas de las diferentes áreas de conocimiento.
Los centros de cómputo en la Universidad de Colima, concentran la mayor
parte de los equipos informáticos de la institución, mismos que están conectados en
su mayoría a la red mundial, dando como resultado que los estudiantes puedan
acceder de manera fácil a un gran caudal de información y además los profesores
puedan tener contacto con bases de datos especializadas y colegas de diferentes
partes del mundo. Esto da la oportunidad de enriquecer el proceso de enseñanza, al
cambiar radicalmente tanto el papel del profesor como del estudiante.
El uso de software con funciones tutorales o de desarrollo de modelos que
simulan fórmulas matemáticas, componentes arquitectónicos, estructuras químicas,
organismos y una gran cantidad de objetos del mundo real, han hecho de la práctica
educativa un entorno basado principalmente en imágenes en movimiento, audio,
texto con características no lineales y un entorno electrónico en donde se puede
tomar la clase de manera presencial o a distancia, es decir el salón de clases como
tradicionalmente lo conocemos ha cambiado, paulatinamente está llegando a
convertirse en un entorno en donde no necesariamente el profesor y el alumno están
en contacto cara a cara, sino que esto puede ser mediado a través de componentes
audiovisuales, proporcionados por una computadora.
Esto necesariamente implica cambios en la estructura del modelo pedagógico
y de los propios contenidos de aprendizaje, de aquí que, particularmente es un punto
de interés en el estudio en cuestión, si consideramos que la educación es un proceso
social y un factor de cambio.
Además, el uso de las computadoras tanto por parte de los estudiantes como
de los profesores en nuestra máxima casa de estudios ha cambiado algunas de las
estrategias didácticas empleadas por los docentes, generando una serie de nuevos
comportamientos en los estudiantes (como por ejemplo la búsqueda de información
en la red mundial), además ha servido para desarrollar nuevas capacidades en los
mismos, tales como el manejo de software aplicado a un área de conocimiento, la
discriminación de grandes volúmenes de información con la finalidad de comprender
un compendio que permita englobar aspectos sobresalientes sobre un tema en
particular, mismos que posteriormente servirán para elaborar desde una tarea
escolar hasta fundamentar una investigación.
Estas nuevas capacidades del alumnado y el profesorado deben de ser
estudiadas para poder dar un enfoque de uso adecuado, estructurar un nuevo plan
de estudios y retomar los aspectos importantes y necesarios del modelo educativo
tradicional para incorporarlos a un proceso de enseñanza basado en el uso de la
tecnología, considerando de manera significativa las aplicaciones que se le pueden
dar a una computadora en el nivel superior.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Dada la naturaleza propia de este trabajo, se consideró emplear como parte
de la metodología de la investigación, al estudio descriptivo, mismo que se encuentra
dentro del llamado paradigma empírico analítico, en la categoría de estudios ex-postfacto
o de aquellos que se llevan a cabo después de haberse dado los hechos.
El estudio descriptivo, que se ha llevado a cabo básicamente sirve para la
fundamentación de lo que bien podría ser un estudio de mayor alcance, ya que en
este caso sólo se pretende identificar algunas de las situaciones relevantes
relacionadas con el uso que hacen los alumnos y maestros del equipo de cómputo
en los Campus Colima y Coquimatlán de la Universidad de Colima.
La elección del estudio descriptivo fue hecha con base en la necesidad de
plantear un marco contextual relacionado con los usos de los equipos de cómputo en
nuestra máxima casa de estudios, esto en virtud de que no existen los datos
necesarios ni las interrelaciones entre las variables básicas para fundamentar una
investigación con otras características.
Con el objeto de proveer de un marco contextualizador sobre los usos de los
equipos de cómputo en la Universidad de Colima este proyecto se apoya en dos
cuestionarios, uno dirigido a los profesores de las facultades y escuelas involucradas
en el estudio y otro destinado a los estudiantes de dichas instituciones.
En total el estudio está dirigido a 15 escuelas y facultades de nuestra máxima
casa de estudios, mismas que están distribuidas como a continuación se señala:
Campus Colima
a).- Facultad de Ciencias
b).- Escuela de Filosofía
c).- Facultad de Ciencias Políticas y Sociales
d).- Facultad de Contabilidad y Administración
e).- Facultad de Derecho
f).- Facultad de Enfermería
g).- Facultad de Letras y Comunicación
h).- Facultad de Medicina
i).- Facultad de Psicología
j).- Facultad de Telemática
k).- Facultad de Trabajo Social
Campus Coquimatlán
a).- Facultad de Arquitectura
b).- Facultad de Ciencias Químicas
c).- Facultad de Ingeniería Civil
d).- Facultad de Ingeniería Mecánica y Eléctrica
Estas escuelas y facultades representan el 52.2% de las existentes en el ciclo
escolar 1999-2000 y comprenden el 74.1 % de la matrícula estudiantil para el mismo
período lectivo, asimismo los profesores que laboran en estas instituciones,
conforman el 54% del total de trabajadores del nivel superior.
Dada la magnitud de la población objeto de estudio, los resultados pueden ser
generalizados para toda la universidad, con la particularidad, de que los resultados
con sus consabidas precauciones, pueden ser extensivos al nivel medio superior de
la institución.
Para poder obtener los datos significativos a partir de los cuestionarios
aplicados, se hizo necesario aplicar algunos procedimientos estadísticos tales como
el análisis correlacional, la elección de un método de muestreo y el análisis
cualitativo de la información sistematizada a partir de los instrumentos de
investigación.
La encuesta destinada a los estudiantes consta de 34 preguntas, divididas en
4 apartados, los cuales son:
a).-Aspectos generales
b).-Aspectos relacionados con la docencia
c).-Usos de las computadoras
e).-Internet
Respecto al instrumento empleado para recabar información proveniente de
los docentes, se consideran un total de 44 preguntas, estructuradas en 4 apartados:
a).-Datos generales
b).-Capacitación en el uso de la computadora
c).- Empleo de la computadora como auxiliar didáctico e
d).- Internet
En algunos casos las preguntas contenidas en ambas encuestas se pueden
correlacionar, tal es el caso del apartado relacionado con el empleo de la
computadora como auxiliar didáctico; la didáctica de la enseñanza e internet.
Con la finalidad de tener un nivel de confianza elevado (95%), se considera la
elección de la muestra distribuyéndola en una serie de estratos, en donde a partir de
cada una de las Escuelas y Facultades, se hace un respectivo desglosamiento de los
mismos, esto conforme al esquema No. 1.
Partiendo de que la población estudiantil de las Escuelas y Facultades
involucradas en el estudio, es amplia, se requerirá de la toma de una muestra con la
cual se llevará a cabo el desarrollo del proyecto de investigación, implementándose
el mismo proceso para la elección de los profesores participantes en la encuesta (ver
esquema No. 2).</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>A partir de los resultados obtenidos en la encuesta aplicada tanto a profesores
como estudiantes del nivel superior de la Universidad de Colima, se distinguen por lo
menos seis puntos bien definidos, hacia los cuales desde nuestro punto de vista, se
debe de considerar la estructuración de un modelo de enseñanza-aprendizaje
basado en el uso de la computadora:
El cambio de énfasis de la enseñanza hacia el aprendizaje. Este cambio de
énfasis no es accidental, es fruto principalmente de tener los medios para salirse del
enredo que traemos desde la revolución industrial respecto a que la educación es
para todos, por lo tanto es masiva, estándar y la única manera racional en aquel
momento es la institución de la escuela y el salón de clases alrededor de la
enseñanza dictada por un maestro. De ninguna manera era posible ofrecer
educación masiva a todos los estudiantes cuidando de lo que aprenden, ellos tienen
que seguir al maestro.
El segundo punto de cambio importante es el del rol del maestro: de expositor
a guía y en última instancia como administrador de medios. Deja el maestro de ser
considerado como almacén del saber y por lo tanto dispensador omnipotente del
mismo. La enorme cantidad de información que existe sobre cualquier tema es tal,
que es imposible pensar que puedan existir enciclopedistas o personas que
pretenden saber todo de un tema. El conocimiento generado es según se calcula
cuatro veces mayor que hace 10 años (Ajzen y Fishbein, M.). Afortunadamente están
los medios electrónicos para ayudar con esta gran cantidad de información y
conocimiento.
De aquí se desprende el tercer punto para el cambio: de los datos al
conocimiento,  ¿Qué es lo importante en aprender: datos aislados, sin significado o tal
vez información que articula datos y los estructura. De la psicología del aprendizaje,
se desprende que el aprendizaje llega a ser conocimiento no cuando se memoriza o
se relaciona lo nuevo con lo conocido, sino cuando esa nueva información es
interiorizada y sirve para aprender nuevas cosas, para generar nueva información y
por ende conocimiento. Parecería entonces que un cambio importante en las
estrategias educativas es el pasar de la memorización a la navegación de
información y al uso de la misma (conocimiento). En una educación basada en la
utilización de la información para generar más información que es lo que llamamos
conocimiento.
Un cuarto aspecto de cambio es también radical, ya que nuestra cultura ha
estado basada en el libro y en el texto, para ser cambiada a una cultura multimedio,
en la que ya no pensamos tenemos que leer de algo para conocer sobre algo,
podremos verlo, oírlo, tocarlo y más importante aún interactuar con esto. De aquí se
desprende el quinto eje de cambio que es la interacción, que cambia la forma de
trabajo en la educación de ser ente pasivos que escuchan al maestro y tal vez hacen
ejercicios de asignación en sus casas, a una forma de aprendizaje como actores
partícipes de nuestro propio aprendizaje. Un aprendizaje activo, con opciones,
permitiendo equivocarse y aprender de los errores además en forma inmediata. La
interacción permite además el seleccionar en buena medida lo que se quiere y
cuando se quiere. El cambio es radical, imagínense (y no será muy lejano el día) en
el que se pida a los estudiantes que entreguen una investigación en multimedio, en
vez de la clásica monografía. Hay que imaginarse la cantidad de creatividad envuelta
en hacer el multimedio versus a la de una simple monografía. Es un salto cualitativo
en la cultura. Estos cambios volvemos a insistir no solo son del medio, sino de la
expresión, de la forma de almacenar, de la forma de pensar y transmitir, de la cultura
misma.
Finalmente y no menos importante es la desincronización de la educación (en
el tiempo y en el espacio): todos y en particular los jóvenes podrán aprender en
distintos momentos y en lugares diferentes. El poder de las redes de
telecomunicaciones en la educación es formidable, ya que permite vehicular una
gran cantidad de información en ambos sentidos y además esta información no sólo
es de tipo texto, sino multimedio. El poder ser almacenada permite ser consultada en
forma diferida y a nuestro propio ritmo, no son solo palabras que el viento se las
lleva, es información almacenada.
Se puede decir que cuando se habla de computadoras y educación quedan
implícitas una serie de relaciones que se establecen entre el proceso educativo y el
gran campo de la computación. Esta relación es posible manejarla desde tres
enfoques, educación en computación, en este caso se habla de la computación como
objeto de estudio, educación por computación que es el más se relaciona con
nuestro trabajo como profesores, (aplicaciones de la computación como medio de
apoyo) y la educación para la computación que más que aprender computación se
refiere a la necesidad de tomar en cuenta a la computación como un elemento de la
cultura de esta época, y para el mundo futuro que ya estamos visualizando, esto es
en cuanto a la forma de conseguir información, tomar decisiones, resolver problemas
etc.
Es un hecho innegable el uso extendido que se hace de los equipos de
cómputo en la Universidad de Colima, mismo que se puede clasificar en tres grandes
modos de empleo:
1.- Procesamiento de información (herramienta)
2.- Interacción
3.- Comunicación
El procesamiento de la información incluye:
Procesadores numéricos, de palabras, auxiliares de sistemas gráficos,
sistemas manejadores de bases de datos, sistemas de autoedición, hojas de cálculo
ampliadas y sistemas integrados.
Función pedagógica: Diseminación de información desarrollo de habilidades
verbales, aprendizaje de idiomas, desarrollo de habilidades de procedimiento,
aprendizaje de solución de problemas, aprendizaje de habilidades analíticas,
aprendizaje de habilidades de presentación, aprendizaje de habilidades de expresión
artística, etc.
La Interacción se refiere a: evaluación automatizada, instrucción asistida por
computadora, video interactivo, hipermedios, realidad virtual y multimedios
interactivos inteligentes.
Función pedagógica: Ejercitación y práctica, presentación de información,
solución de problemas, enseñanza tutorial, juegos, simulaciones, aprendizaje
heurístico, aprendizaje procedimental.
La Comunicación abarca: bancos de información interactivos, correo
electrónico, sistemas de conferencias por computadora y sistema de comunicación
multimedios por computadora.
Función pedagógica: Aprendizaje de información verbal, desarrollo de la
expresión, desarrollo de las habilidades para el análisis y síntesis de texto, desarrollo
de un juicio crítico, solución participativa de problemas etc.
Un aspecto que no se ha comentado es el de los docentes;  ¿qué opinan?,
 ¿cuál es su actitud hacia las computadoras?  ¿Ha cambiado su papel como profesor?
Se ha podido observar que generalmente los profesores tienen una tendencia
a rechazar el uso de las computadoras y los audiovisuales. Mucho es debido a la
falta de conocimiento de cómo se usan estos medios, otro puede ser al temor de
sentirse desplazado y uno más puede ser la accesibilidad de este tipo de tecnología.
Por otro lado el papel del profesor con la influencia de las nuevas tecnologías
necesita redefinirse. Ya no es el que posee toda la información sino más bien debe
saber como orientar al educando para conseguir dicha información haciendo uso de
la computadora y algunos de los recursos con los que cuenta, tal es el caso del uso
de los servicios de la red mundial: world wide web, correo electrónico, bibliotecas
digitales, bases de datos e información en línea por medio del chat.
Por lo tanto a manera de sugerencia, se considera necesaria la capacitación
de profesores en el uso de las computadoras, además de una cultura computacional,
esto implica desarrollar programas computacionales como apoyo para la enseñanza
de alguna asignatura o utilizar programas existentes. Esto desde luego acompañado
con una adecuada selección en la adquisición de equipo, y distribución en su uso.
Dado que la computadora es un instrumento lo suficientemente dúctil como
para permitir desarrollar material capaz de potenciar la creatividad y con una
reproductibilidad instantánea, debería ser parte integrante de una nueva actividad
pedagógica con un nuevo enfoque que permita aprovechar su capacidad de
interactividad y su adaptabilidad a diferentes grados de interés y particularidades de
una asignatura, aspecto que no se debe dejar de lado en la capacitación de los
docentes.
Finalmente, con relación a la instrucción, se considera que la actividad en el
aprendizaje es importante -se aprende más cuando el alumno se involucra en el
proceso- el uso de recursos informáticos ofrece la particularidad de obligar a un
100% de participación activa. Esto genera una integración entre las habilidades de
los propios programas y las habilidades de los estudiantes, que quedan explícitas. Se
genera así un proceso de retroalimentación entre el usuario y la máquina donde la
interfaz es el programa. Dado que los procesos informáticos integran las ayudas en
línea en los sistemas y en los propios programas, favorecen la experimentación y la
prueba y el error. En este caso el error forma parte de una dinámica que lo utiliza
para construir un nuevo conocimiento, permitiendo una autocrítica que es
constructiva. La computadora permite el aprendizaje individualizado, siempre hay
una respuesta instantánea ya sea del sistema o del software.
Por el motivo citado se debe tener cuidado al emplear software educativo,
pues debe tener la suficiente flexibilidad como para tener en cuenta las
características propias del usuario; debe posibilitar llenar vacíos conceptuales o
detectar su existencia y orientar al alumno hacia su solución; el error deber permitir la
autocrítica constructiva y promover el desarrollo de nuevas habilidades,
conocimientos o destrezas, debe ofrecer al profesor y al alumno la posibilidad de vivir
experiencias que difícilmente se pueden llevar a cabo con otros medios. Con los
avances en el área será posible la creación de nuevos sistemas educativos que
potencien la tarea de educadores y alumnos, cambiando la forma actual de
relacionarse y los métodos aplicados.
El aumento del flujo de información debería desplazar la tradicional insistencia
de los docentes en los contenidos a la importancia en la estimulación de un
pensamiento crítico, la toma de decisiones, las técnicas de comunicación, el proceso
cognitivo y la resolución de problemas. Hoy es tan importante la capacidad para
buscar información y evaluarla, como conocerla e incorporarla. Así mismo debemos
crear conciencia de que las destrezas adquiridas en clase en el uso de los recursos
informáticos serán obsoletas en poco tiempo dada la velocidad de desarrollo de la
tecnología. La formación debería apuntar a independizarse de los contenidos
formales e inmediatos para generar en el aprendiz la capacidad de desarrollar ideas
y aplicar sus conocimientos para adaptarse rápidamente a los cambios en el entorno
y a los nuevos estímulos. El docente debería ser un facilitador del aprendizaje y no
un "transmisor de contenidos". Los recursos informáticos se deberían integrar en una
estrategia didáctica de enseñanza - aprendizaje dentro de la cual desempeñen una
función y produzcan un compromiso entre las partes con la satisfacción de sentirse
todos involucrados en una misma tarea. De esta forma sería posible una
retroalimentación positiva permanentemente, generando cambios conceptuales que
enriquecerían tanto a educadores como a aprendices.
Tomando en cuenta las condiciones socioeconómicas, políticas y culturales, y
los diferentes elementos (alumnos, docentes, computadoras) que integran el
mecanismo para el aprendizaje, se hace evidente que a mejores condiciones que
permitan ensanchar el espectro cognitivo mejores resultados va a obtener el
organismo a aprender.
La computadora puede ser un medio ideal para emitir los estímulos que
provoquen la integración de una nueva metodología de aprendizaje formal.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
Maestría en Ciencias, área: tecnología y educación
“DESCRIPCIÓN DEL EMPLEO DE LAS COMPUTADORAS POR
PARTE DE LOS DOCENTES Y ALUMNOS DEL NIVEL SUPERIOR
DE LOS CAMPUS COLIMA Y COQUIMATLÍN DE LA
UNIVERSIDAD DE COLIMA, COMO FUNDAMENTO DE UN
MODELO PEDAGÓGICO PARA EL USO DE LAS MISMAS" .
Tesis que para obtener el grado de maestro en ciencias, área: Tecnología y
Educación, presenta:
Profr. y Lic. Rodolfo Rangel Alcántar
Asesora: M.C. Sara Sandoval Carrillo.
Colima, Col., Enero de 2002.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DISEÑO Y ANÍLISIS DE UN CASO DE ESTUDIO APLICADO AL LENGUAJE DE MODELADO UNIFICADO</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Analizar y diseñar un caso de estudio, aplicando los principios del Lenguaje
de Modelado Unificado.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Cuando se tiene la experiencia de estar en contacto con el sector laboral y el
académico referente a una profesión, resulta evidente que la caducidad del
conocimiento puede llegar en el breve momento en el que una persona se aleje de
las fuentes de nuevas ideas, esto debido al rápido desarrollo y difusión que tienen
las nuevas tecnologías. Refiriéndonos al área de la informática y los sistemas
computacionales, estas tecnologías van enfocadas tanto al hardware (que
corresponde a la parte física de una computadora) como al software (que se
refiere a las aplicaciones que una computadora puede ejecutar), y es en éste
último, en el que se enfocará la presente investigación.
El software, es sin duda, la forma más palpable para medir el desempeño de una
computadora, debido a que entre mejor diseñado esté el software aplicado a una
computadora, mayor será el aprovechamiento que se tenga del hardware, y es
aquí donde radica la importancia de un diseño eficiente del software. Es necesario
recordar que el software nace con la invención de la primera computadora, y el por
qué de su nacimiento se refiere a la necesidad de especificar una forma de
"decirle" a la computadora las tareas que debe ejecutar. Desde entonces,
numerosos y variados paradigmas han surgido en torno al diseño y elaboración de
aplicaciones de software, que con el paso del tiempo aumentan su grado de
complejidad. En este terreno de los paradigmas es que surge el lenguaje de
modelado unificado, como una alternativa para el diseño de sistemas con gran
cantidad de software. Este lenguaje de modelado unificado surge en Enero de
1997, como una respuesta a las necesidades del mercado para contar con un
lenguaje estándar de modelado; y es debido a su relativamente "corta" edad que
no se encuentra muy difundido en las escuelas, ya que este proceso requiere de
tiempo para que el lenguaje sea conocido, utilizado y comprobado por usuarios y
después llevado al ámbito académico. Refiriéndonos a éste dentro de la Facultad
de Telemática de la Universidad de Colima, el lenguaje de modelado unificado no
ha llegado a este nivel de difusión, esto con base en que los planes de estudio de dicha Facultad, correspondientes al año de 1999, no contemplan el lenguaje de
modelado unificado en su estructura. Es por esta razón, que se presenta un caso
de estudio, en el cual se plantean de una forma clara y explícita, las etapas y
artefactos involucrados en el desarrollo de sistemas de información, utilizando el
lenguaje de modelado unificado para su especificación. La mayor importancia
radica no en el caso de estudio en particular, sino en la forma en se aplica el
lenguaje de modelado unificado para identificar y alcanzar los objetivos del diseño,
ya que éste obliga al analista, a adoptar un punto de vista orientado a objetos, el
cual es muy diferente al diseño estructurado.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Esta investigación se realizó, bajo al perspectiva de la investigación aplicada,
en la cual se manejan tres niveles de información, ya que en las
investigaciones concernientes al ámbito académico es necesario hacer
referencia, directa o indirectamente, a los todos los niveles que tienen que ver
con la información relacionada con el caso de estudio; lo que no podría ser
de otra forma, pues quedarse solamente en el plano teórico sin establecer
conexiones con la realidad empírica para poder trabajar con ésta, nos llevaría
a una formalización de la teoría poco creativa para el desarrollo del trabajo
científico.
El primer nivel implica el manejo de las teorías generales del diseño de
software, el cual tiene dos corrientes principales: el diseño estructurado y el diseño
orientado a objetos. Del diseño orientado a objetos, que es el que forma parte del
caso de estudio que se presenta en este trabajo, se analizaron conceptos
particulares como los lenguajes de diseño orientados a objetos, en particular, el
lenguaje de modelado unificado. El segundo nivel consiste en analizar
información proveniente de distintas fuentes, como: investigaciones
documentadas en bibliotecas e informes publicados en Internet relacionados con
el lenguaje de modelado unificado, elaborando las fichas bibliográficas
correspondientes . Entre éstas diversas fuentes se contó además, con la
entrevista cualitativa, que es la recopilación de información en forma directa, cara
a cara, es decir, el entrevistador obtiene datos del entrevistado siguiendo una serie
de preguntas preconcebidas (Muñoz, 1998). En el proceso de análisis de la
información, debió distinguirse entre la aquella que resulta significativa para
estudiar el problema y la que por estar dirigida a otras situaciones, no tiene
puntos en común con la problemática a tratar, o simplemente resulta inoperante.
El propósito fue, básicamente, contar con información confiable y congruente con
la realidad. El tercer nivel implica el manejo de información obtenida mediante
un acercamiento con la realidad, esto a través de la entrevista realizada a Carlos Maldonado Villaverde, y las pláticas sostenidas con el asesor de este trabajo, y de
mi experiencia sobre el tema. En éste nivel, se recopila información sobre los
aspectos más sobresalientes de este caso de estudio, como son conceptos
generales del diseño orientado a objetos, la plataforma de aplicación, y el lenguaje
de diseño. Los tres niveles del marco teórico se integran en una visión de la
totalidad que permite contextualizar concretamente el caso de estudio en
cuestión. Evidentemente, la integración de todos estos elementos se hizo de
una manera que se observe coherencia en la presentación de materiales
teóricos, así como de todas las ideas que se manejan. Esto permite tener una
idea más clara y exacta de los principios y reglas que intervienen en el caso
de estudio de este trabajo.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>CUANDO COMENCÉ A ELABORAR ÉSTE CASO DE ESTUDIO APLICADO AL
LENGUAJE UNIFICADO DE MODELADO,ME PROPUSE QUE ÉSTE TRABAJO
FUERA UN MEDIO A TRAVÉS DE CUAL EXPRESAR EL ANÍLISIS Y EL DISEÑO
BASADO EN EL LENGUAJE UNIFICADO DE MODELADO.UNA VEZ CUBIERTAS
LAS EXPECTATIVAS PROPUESTAS AL INICIO DEL CASO DE ESTUDIO, SE PUEDE
AFIRMAR QUE EL LENGUAJE DE MODELADO UNIFICADO POSIBILITA AL
DESARROLLADOR PARA QUE MODELE UN SISTEMA DE SOFTWARE ANTES DE
ESCRIBIR UNA SOLA LÍNEA DE CÓDIGO,LO CUAL REPRESENTA UNA GRAN
VENTAJA AL MOMENTO DE IMPLEMENTAR DE IMPLEMENTAR UN SISTEMA,YA
QUE ES BIEN SABIDO QUE ENTRE MENOR TIEMPO ABARQUE LA FASE DE
ANÍLISIS Y DISEÑO,MAYOR SERÍ LA CANTIDAD DE PROBLEMAS Y
SITUACIONES RIESGOSAS QUE SURJAN DURANTE LA FASE DE
IMPLEMENTACIÓN DE UN SISTEMA. ES INUDABLE QUE EL APLICAR ESTE
TIPO DE DISEÑO,COMBINADO CON UN LENGUAJE ESPECIALIZADO PERMITEN
AL ANALISTA,AL DISEÑADOR,AL PROGRAMADOR,AL USUARIO FINAL Y A
TODAS LAS PERSONAS INVOLUCRADAS EN UN PROYECTO DE SOFTWARE;
TENER UN NIVEL DE COMUNICACIÓN UNIFICADO. ES PALPABLE QUE EL
LENGUAJE UNIFICADO DE MODELADO FACILITÓ MUCHO LAS TAREAS DE
ANÍLISIS Y DISEÑO DE ESTE CASO DE ESTUDIO, Y QUE SU APLICACIÓN A
SISTEMAS CON GRANDES CANTIDADES DE SOFTWARE Y GRUPOS DE TRABAJO
ES FACTIBLE Y RECOMENDABLE. EL LENGUAJE UNIFICADO DE MODELADO
PERMITE QUE LOS DESARROLLADORES INCURSIONEN EN EL PARADIGMA DEL
DISEÑO ORIENTADO A OBJETOS PERO A NIVEL DE ANÍLISIS Y DISEÑO,Y NO
NIVEL DE IMPLEMENTACIÓN. ESTO ES MUY IMPORTANTE DEBIDO A QUE POR
LO REGULAR,PARA EL NIVEL DE ANÍLISIS Y DISEÑO SE APLICA EL ENFOQUE
ESTRUCTURADO Y PARA LA IMPLEMENTACIÓN EL ENFOQUE ORIENTADO A
OBJETOS,LO CUAL RESULTA EN UNA TAREA NO DEL TODO GRATA CUANDO SE HABLA DE SISTEMAS CON GRAN CANTIDAD DE SOFTWARE. ES DEFINITIVO
QUE ADOPTAR LA POSTURA ORIENTADA A OBJETOS DESDE LOS PRIMEROS
NIVELES CONLLEVA AL DESARROLLO DE UN SOFTWARE CON MEJOR CALIDAD
Y QUE FACILITA LA EVOLUCIÓN DEL SISTEMA A TRAVÉS DEL TIEMPO LO CUAL
LE PERMITIRÍ TRASCENDER,COSA QUE NO SUCEDE MUY REGULARMENTE
CON EL ENFOQUE ESTRUCTURADO.
EL TEMA PRINCIPAL DE ESTA DE TESIS ES EL LENGUAJE UNIFICADO DE
MODELADO Y PUEDE AFIRMARSE QUE SU APLICACIÓN AL CASO DE ESTUDIO
PARTICULAR FUE SATISFACTORIA, YA QUE SE PROPORCIONARON LOS
ARTEFACTOS NECESARIOS PARA LA FASE DE ANÍLISIS Y DISEÑO. LOS
OBJETIVOS DE LA TESIS SE CUMPLIERON,YA QUE SE LOGRÓ ESPECIFICAR EL
DISEÑO DE UN CASO DE ESTUDIO,APEGÍNDOSE ÉSTE, A LA NOTACIÓN DEL
LENGUAJE UNIFICADO DE MODELADO. LOS TEMAS CONTEMPLADOS DENTRO
DEL MARCO TEÓRICO FUERON ANALIZADOS Y APLICADOS, Y PERMITIERON
LLEGAR A UNA ESPECIFICACIÓN DE DISEÑO DEFINITIVA, LA CUAL CONSTA DE
DIAGRAMAS DE SECUENCIA,CONTRATOS,CASOS DE USO,DIAGRAMAS DE
CLASES Y MODELOS CONCEPTUALES,LOS CUALES EN SU CONJUNTO
REPRESENTAN LOS PLANOS QUE ESCENIFICAN LA CONSTRUCCIÓN DEL
SOFTWARE.
Como experiencia particular, el incursionar en este tipo de trabajos ha
proporcionado al autor, una grata y rica experiencia de aprendizaje, que le
permite analizar y darse cuenta de las diferentes formas que hay para resolver un
problema, en cuanto a diseño de software se refiere, y que el aplicar una nueva
estrategia y lograr los objetivos planteados puede resultar muy reconfortante.
Esto, tomando en cuenta que siempre debe apostarse por las nuevas tecnologías
que proporcionan herramientas más eficaces y especializadas para el desarrollo
de una tarea en particular.
Es definitivo que el modelado orientado a objetos es una realidad y que las
tendencias se inclinan cada vez más por lenguajes de diseño orientados a objetos
como el U.M.L., dejando de lado el enfoque algorítmico que por mucho tiempo ha
prevalecido y prevalece aún en nuestros días, haciendo del diseño de software
una tarea cada vez más complicada, debido a que el software, se vuelve cada
más extenso y complejo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMATICA
TESIS QUE PARA OBTENER EL GRADO DE MAESTRO EN
CIENCIAS, ÍREA DE TELEMÁTICA.
DISEÑO Y ANÍLISIS DE UN CASO DE ESTUDIO APLICADO AL
LENGUAJE DE MODELADO UNIFICADO.
PRESENTA:
ING. CARLOS ULIBARRI IRETA
ASESOR
M.C. ARMANDO ROMAN GALLARDO
COLIMA, COL. JUNIO 2002</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo> DESARROLLO E IMPLEMENTACION DE UN SISTEMA DOMOTICO EN UN HOGAR DEL ESTADO DE COLIMA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En varios países las tecnologías orientadas a la automatización de los hogares
han sido implementadas con éxito, pero el desarrollo así como la implementación de
las mismas se ha rezagado en México y aún más en Colima, ya que es un estado
pequeño y poco inmerso en este tipo de tecnologías. Esto abre una brecha
importante entre la calidad de vida de los hogares del primer mundo respecto a los
nuestros. México es un país en vías de desarrollo y se requiere que las personas se
integren a la tecnología en general, y que mejor forma que sea directamente en su
hogar.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo principal de este estudio es implementar un sistema domótico en
una hogar en el Estado de Colima.
Se pretende generar un documento que permita dar a conocer las tecnologías
orientadas a la automatización de los hogares, mejorando el entorno doméstico en
cuanto a la forma de interactuar con sus aparatos electrónicos e iluminación, además
de provocar el interés hacia los sistemas domóticos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El desarrollo del sistema, por su misma naturaleza se dividió en dos etapas:
*	La etapa del hardware, en la cual se selecciona el espacio habitacional, el
estándar y los dispositivos de control adecuados, así como configuraciones
realizadas.
*	La etapa del software, donde se diseña y desarrolla una interfaz de control por
computadora.
Es necesario comenzar por la etapa del hardware, ya que el software se
desarrolló en torno a los dispositivos que se han definido para controlar.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El desarrollo e implementación de un sistema domótico en un hogar en el
Estado de Colima ha concluido, dando a conocer tecnologías de automatización de
hogares a algunas personas y además, gracias a la adecuada selección y
disposición de dispositivos de control, así como el diseño de una interfaz de control
por computadora, se lograron una serie de objetivos planteados desde un principio.
Aunque la inversión realizada para ello fue considerada en la mayoría de los
casos adecuada y en algunos alta, se puede considerar que se cumplió el objetivo de
mantenerla baja, ya que comparado con otras tecnologías para la automatización de
hogares, X10 es la que mayor penetración y menores costos tiene en el mercado,
mientras que los proveedores seleccionados son los más competitivos en cuanto a
precios y servicio, por lo que se eligió el estándar y los dispositivos de control
adecuados para las necesidades de este proyecto.
El impacto estructural en la vivienda, causado por la instalación de los
dispositivos de control, fue considerado “muy bajo"  (Anexo 9) por la mayoría de los
encuestados, esto debido a que los módulos X10 adquiridos son fáciles de instalar y
configurar, evitando la necesidad de perforar paredes, reemplazar el cableado
eléctrico existente o añadir nuevo.
La instalación de los módulos X10 externos es una tarea bastante sencilla, ya
que solo se requiere configurar el código de la casa y unidad para conectarse a la
corriente eléctrica, la desventaja es que su tamaño no es tan pequeño como para
pasar desapercibido en donde importe la decoración del hogar, dificultando en
algunas ocasiones el acceso a otros contactos. Estos inconvenientes se pueden
solucionar reemplazándolos por módulos internos, los cuales se empotran en la pared sin que las personas noten que se trata de un dispositivo X10 pero con las
desventajas de un mayor costo y necesidad de contar con personal calificado en
instalaciones eléctricas para su colocación.
Aún cuando los encuestados presentan muy diferentes niveles de ingresos, la
totalidad considera factible la introducción de tecnologías orientadas a la Domótica
en otros hogares en el Estado de Colima, y con el rápido crecimiento de la capital,
gracias a las nuevas obras de urbanización, este tipo de propuestas parecen tener
buenas posibilidades de implementarse.
El aumento en la comodidad y en la seguridad fueron los principales motivos
por lo que a los encuestados les gustaría ampliar este sistema a la totalidad del
hogar, cumpliéndose algunos de los objetivos principales de la Domótica:
incrementar la seguridad y el confort de las personas.
Se recibieron propuestas por parte de los encuestados sobre ampliaciones,
alternativas y mejoras del sistema, así como preguntas acerca de dispositivos
existentes y sus precios, revelando interés hacia este tipo de propuestas y
cumpliéndose con otro de los objetivos propuestos.
Se desarrolló una interfaz de control por computadora amigable y sencilla de
operar por los usuarios, aún sin que tuvieran una amplia experiencia en computación,
logrando puntajes de usabilidad que van desde el 92.5% y en varios casos del 100%,
con un promedio final de 96.9%, lo cual está por encima del 70% esperado. Esto
refleja que las decisiones tomadas respecto a utilizar los planos de la casa, la
combinación de colores utilizada y la disposición de los menús en el diseño de la
interfaz fueron un acierto importante para que fuera claramente aceptada por los
usuarios.
Al final, aunque solo se controló un pequeño número de aparatos electrónicos
y luces en el hogar, debido a la limitante del presupuesto, fue posible desarrollar e
implementar exitosamente un sistema en el cual los usuarios apreciaran de manera
natural las ventajas de utilizar la domótica en su vida diaria.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
DESARROLLO E IMPLEMENTACIÓN DE UN SISTEMA
DOMÓTICO EN UN HOGAR DEL ESTADO DE COLIMA
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS, ÍREA TELEMÁTICA
PRESENTA:
ING. DANTE ISRAEL TAPIA MARTÍNEZ
ASESOR:
M. C. JUAN MANUEL RAMÍREZ ALCARAZ
COLIMA, COLIMA. SEPTIEMBRE DE 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“SIMULACIÓN ESTOCÍSTICA PARA PREDICCIÓN EN LOS DEPORTES"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un sistema que utilice la simulación estocástica para predecir
los resultados del torneo mexicano de fútbol de primera división, demostrando que
la simulación es una herramienta útil para conocer el comportamiento de los
sistemas, antes de que los eventos involucrados ocurran.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Después de haber hecho un análisis sobre distintos enfoques posibles para
este trabajo, se propone la siguiente hipótesis:
“Un modelo de simulación estocástica por computadora es capaz de
predecir con un alto grado de confianza, los resultados del torneo de fútbol
mexicano de primera división"</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La simulación ha sido uno de los principales objetivos del uso de las
computadoras desde sus inicios, son muchos los recursos que se han destinado al
desarrollo de hardware poderoso que permita simular sistemas complejos, sin
embargo es posible atacar muchos problemas mediante el uso de una PC y
lenguajes de programación o utilizando software de simulación.
Es muy importante cuando se va a implementar algún sistema, predecir su
comportamiento y saber cuales serian las condiciones que favorecerían el
desempeño del mismo, esto es posible lograrlo utilizando la simulación.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Antes de describir la forma en que opera el sistema, recordaremos un punto
mencionado anteriormente con respecto a la simulación, “los fenómenos que se
analizan no son inherentemente estocásticos o deterministas, sino que es elección del observador" ; así también, lo es el hecho de determinar que tipo de distribución
será utilizada para describir el comportamiento del fenómeno.
Cuando se describió el proceso Poisson, se enfatizó en que éste permite
determinar la probabilidad de que ocurra un número designado de eventos en un
espacio de tiempo continuo. Se asume que los goles que ocurren en un partido no
están relacionados entre sí, es decir un gol no depende del gol anterior y la
ocurrencia es totalmente aleatoria. Para este experimento se utiliza la distribución de
Poisson para reproducir este proceso.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Considerando los resultados obtenidos de las múltiples pruebas realizadas, se
puede concluir que se acepta la hipótesis planteada. Un modelo de simulación
estocástica por computadora es capaz de predecir con un alto grado de confianza,
los resultados del torneo de fútbol mexicano de primera división.
Uno de los problemas principales al momento de crear el modelo del torneo de
fútbol mexicano, fue implementar la llamada fase de recalificación; primeramente por
el hecho de no conocer totalmente el sistema empleado por la FMF, para determinar
que equipos son los que entran en esta fase, y en segundo término, por lo complejo
que resulta determinar cuales equipos deben jugar un encuentro más antes de pasar
a la fase final del torneo.
Este trabajo puede servir como base para desarrollar sistemas de simulación
aplicados a otros deportes o disciplinas cuyo comportamiento sea similar al del
torneo de fútbol mexicano de primera división, tal es el caso de los torneos de fútbol
de España, Italia, y otros países.
La utilidad principal de este tipo de simulación se puede enfocar hacia los
medios de comunicación como comentaristas deportivos o los encargados de las
columnas deportivas de los periódicos y revistas especializadas, pues tienen en este
tipo de sistemas una herramienta que les permite hacer predicciones basadas no
solamente en observaciones.
Se puede mejorar este sistema, ampliando su capacidad para soportar los
cambios que se pueden dar en el torneo, como sucedió después del torneo de
verano 2001; estas modificaciones incluyen el numero de equipos participantes, la
cantidad de juegos por jornada y la cantidad de jornadas por torneo.
A continuación se mencionan otros puntos de importancia que se pueden
concluir de este trabajo.
Es necesario trabajar en la elaboración de algoritmos que permitan a una
computadora realizar el trabajo para el que se requiere mucho tiempo de estudio o la
solución de ecuaciones complejas.
Existen herramientas de software que son capaces de simular muchos tipos
de procesos, pero es mas practico realizar software especializado para atacar un
problema especifico.
El modelar un sistema tiene grandes beneficios entre ellos podemos
mencionar:
*	Reducción de costos de implantación
*	Una mejor planeación
*	Un mejor entendimiento del sistema
Existe un punto muy importante que debe de tener un modelo que simule un
sistema y es el hecho de poder responder preguntas del tipo  ¿qué pasa si?, ya que
estas preguntas nos permiten comprender mejor el sistema, nos permiten ubicar
cuellos de botella y también ayudan a visualizar el comportamiento del sistema en
cualquier tipo de condición.
La confiabilidad que se puede tener en un modelo de predicción o de
simulación de sistemas, depende directamente del buen estudio que se haga del
mismo y del conocimiento que se tenga de él, pues esto permite conocer los factores
que intervienen en el comportamiento del sistema y ayuda a determinar que variables
se requieren y como se comportan.
En lo que respecta a la herramienta adecuada para desarrollar e implementar
el modelo de simulación, es recomendable buscar alguna herramienta para realizar simulaciones sobre el tema que se esta tratando, en caso de no existir o de que la
herramienta que existe no satisfaga todos los requerimientos que se tienen, se puede
utilizar una herramienta general que permita desarrollar un modelo que trabaje en
todos los puntos que del modelo se requieren, recordemos que los lenguajes de
programación pueden ser tan potentes como el programador requiera.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
“SIMULACIÓN ESTOCÍSTICA PARA PREDICCIÓN EN LOS
DEPORTES" 
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS, ÍREA TELEMÁTICA
PRESENTA
ING. HUMBERTO RAMÍREZ GONZÍLEZ
ASESORES
PH.D. CARLOS MOISÉS HERNÍNDEZ SUÍREZ
M.C. MARTÍN SANTOS VIRGEN
COLIMA, COL., JUNIO DE 2001.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Evaluación del Sistema Automatizado para la Evaluación y Seguimiento de la Tutoría en la Universidad de Colima</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Todo sistema se diseña para satisfacer necesidades y se evalúa para obtener
sistemas que cumplan con las especificaciones con las que fue diseñado.
Aunque en la Universidad de Colima existe el sistema en-línea SAESTUC para el
apoyo de las tutorías, éste no ha sido evaluado, siendo ésta una actividad esencial
en todas la ramas de las TI. Por consiguiente, no se conocen las fortalezas ni áreas
de oportunidad del sistema, las cuales son indispensables conocer para lograr la
satisfacción de los usuarios, en particular para identificar cuáles son los motivos por
los que algunos profesores tutores no usan el SAESTUC como herramienta de
apoyo, y el porqué dejaron de usarlo.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Evaluar el Sistema Automatizado para la Evaluación y Seguimiento de la Tutoría en
la Universidad Colima (SAESTUC).</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cumple el SAESTUC con los estándares de usabilidad establecidos por la
ISO 9241-11?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Cuál es el porcentaje de eficacia, eficiencia y satisfacción con el que cuenta
SAESTUC según los tutores usuarios del SAESTUC?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Se le tendrían que hacer modificaciones al SAESTUC para lograr una
plataforma interactiva con las características necesarias para lograr la
satisfacción de los usuarios del sistema?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿El 100% de los profesores tutores utilizan el SAESTUC como herramienta
de apoyo en la organización de sus tutorías?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Cuál es el motivo por el cual algunos tutores no utilizan el SAESTUC como
herramienta de apoyo a las tutorías?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Se establecieron hipótesis estadísticas de estimación con base a los datos
obtenidos de la aplicación de la investigación no experimental bajo el enfoque
cualitativo a través de una entrevista a profesores tutores de las facultades de
Telemática en el Campus Colima, la Facultad de Contabilidad y Administración en el
Campus de Tecomán y la facultad de Pedagogía en el Campus Villa de Ílvarez, de
la Universidad de Colima.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Las tutorías, hoy en día son un factor fundamental en la disminución de los altos
índices de deserción y rezago, atribuidos a la falta de apoyo a los alumnos por parte
de las instituciones educativas. Los docentes universitarios participan en las tutorías
para mejorar el rendimiento académico, solucionar problemas escolares, desarrollar
hábitos de estudio, trabajo, reflexión y convivencia social, impartidas de forma
individualizada o grupal (ANUIES,2000).
Las tecnologías de información son herramientas que se pueden utilizar como apoyo
a la organización, planeación y seguimiento de las tutorías. En la Universidad de
Colima por medio de la Dirección de Orientación Vocacional y Educativa se creó un
sistema que sirve como herramienta de apoyo en la planeación, organización y
seguimiento de las tutorías llamado SAESTUC. Sin embargo, dicho sistema no es
utilizado por todas las facultades de la Universidad de Colima, y en aquellas que lo usan algunos tutores han dejado de utilizarlo, o simplemente nunca lo han usado. En
la presente investigación se evalúa el SAESTUC usando a los estándares de
usabilidad establecidos por la ISO 9241-11 en los aspectos de eficiencia, eficacia y
satisfacción. Además, se evalua cuál es la opinión que tienen los tutores y
tutorados usuarios del sistema respecto a las herramientas que tienen disponibles en
dicho sistema, así mismo se determina cuáles son los factores que han impedido el
uso de esta herramienta por algunos tutores.
La evaluación de usabilidad se aplicó a dicho sistema existente, con profesores
tutores y alumnos de la Universidad de Colima. Los conceptos y programas de
tutorías para el cual se requiere cubrir las necesidades que se avalúan en esta
investigación serán para el llamado “Programa Regional de Tutorías"  aprobada por
la Asamblea de la Región Centro Occidente de la ANUIES en el año 20002.
Los resultados de la presente investigación serán entregados a la Dirección General
de Orientación Vocacional y Educativa de la Universidad de Colima, para que sean
tomados en cuenta en la mejora de la siguiente versión del sistema SAESTUC, la
cual se planea realizar en dicha dirección. De esta manera se pretende que el 100%
de los tutores y tutorados usen el sistema SAESTUC como herramienta de apoyo
para impartir y recibir tutorías.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En el presente capítulo se describe la metodología utilizada para el desarrollo de esta
investigación, la cual se clasifica como sistemática y empírica12.
Además de clasificarse como no experimental, se clasifica dentro del enfoque
mixto (cuantitativo / cualitativo), ya que se utilizan mediciones cuantitativas con datos
cualitativos, con un diseño transeccional descriptivo, el cual se caracteriza por
recolectar datos en un solo momento (en un tiempo único) algunos autores lo
explican como una fotografía de algo que está sucediendo, indagando la incidencia y
valores en que se manifiesta una o más variables o grupos de personas.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La presente tesis estuvo orientada a evaluar el SAESTUC, determinando el
porcentaje de usabilidad, con el que éste cuenta, en las variables de: eficiencia,
eficacia y satisfacción establecidos por la ISO 9241 - 11. Encontramos que el grado
de usabilidad es menor al 50% (primera hipótesis). Además analizamos cuáles son
las herramientas que se le tienen que agregar o modificar al sistema para lograr que
los profesores tutores lleven a cabo sus tareas de planeación, organización y
seguimiento de las tutorías que imparten, y de esta manera contribuir al aumento del
porcentaje de agrado del los tutores hacia al sistema.
Comprobamos (segunda hipótesis) que menos del 50% de los profesores
tutores de la Universidad de Colima siguieron usando por segundo semestre el
sistema SAESTUC como herramienta de apoyo en la planeación, organización y
seguimiento de las tutorías, después de haberlo utilizado por primera vez durante el
semestre Agosto 2006 - Enero 2007. Obtuvimos el porcentaje de tutores que usan
el SAESTUC, al momento de la investigación, además del porcentaje de tutores que
alguna vez usaron el sistema y que al momento de la investigación no lo usaban, por
razones que ellos mismos expresaron (ver sec. 4.2.4). Del mismo modo se obtuvo el
porcentaje de tutores que desconocen la existencia del SAESTUC.
Los porcentajes y opiniones antes mencionadas de los usuarios del sistema,
servirán a la Dirección de Orientación Vocacional, para tomar las medidas
necesarias, adaptando y modificando el sistema para que los tutores que no usan el
sistema se incorporen a usarlo, y para que los profesores que actualmente los usan
no dejen de hacerlo.
Además de las variables y sub-variables analizadas, se obtuvieron datos
adicionales como son: el uso que dan los alumnos al SAESTUC, observándose que
sus actividades se centran en proporcionar sus datos, dejando de lado las
actividades que ayudan a planear sus tutorías. Durante el levantamiento de las encuestas, también se observó que: si las recomendaciones de los profesores se
toman en cuenta, la interacción tanto de tutores como de tutorados con el sistema,
sería mayor.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
Facultad de Telemática
Evaluación del Sistema Automatizado para la
Evaluación y Seguimiento de la Tutoría en la
Universidad de Colima
TESIS QUE PARA OBTENER EL GRADO DE
MAESTRA EN TECNOLOGÍAS DE INFORMACIÓN
Presenta:
Lic. Imelda Carelia Peña González
Asesores:
D. en C. Jorge Rafael Gutiérrez Pulido
M. en C. Ricardo Acosta Díaz
Colima, Col; Abril de 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>PROPUESTA DE ARQUITECTURA DE GESTIÓN DE IDENTIDADES DIGITALES PARA EL CONSORCIO MINERO BENITO JUÍREZ PEÑA COLORADA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>A lo largo de los años, la complejidad y diversidad de los sistemas de cómputo que posee la empresa Consorcio Minero Benito Juárez Peña Colorada, S.A. de C.V. (C.M.B.J. Peña Colorada) ha crecido en forma considerable, lo cual ha incrementado la labor de gestión de la seguridad de los sistemas de información. La empresa ha valorado siempre la importancia que tiene la información en su desempeño, la cual le otorga una ventaja competitiva en su desarrollo y operación.
Para el manejo adecuado de la operación de todos los procesos de la empresa, se cuenta con los siguientes sistemas de información:
*	Financiero.
*	Recursos Humanos.
*	Control de Mantenimiento de equipos.
*	Planeación de Operación de la Mina.
*	Control Automatizado de Procesos.
*	Calidad del Producto en todas sus fases (desde la extracción hasta el producto terminado).
*	Planeación de Reservas de Mineral.
En la actualidad cada sistema de información tiene su propia autenticación, control de acceso y administración de la seguridad lo que ocasiona que el usuario tenga que realizar el ingreso en cada uno de los sistemas y los administradores de los sistemas de información realicen labores administrativas (altas, bajas, cambios de usuarios y autorizaciones de usuarios) por cada sistema de información.
El contar con diferentes sistemas de información ocasiona que las actividades de la Coordinación de sistemas se incrementen y por lo tanto se dificulta la gestión de la seguridad de la información.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar una arquitectura integral de gestión de identidades digitales para la simplificación del proceso de administración de la autenticación y autorización de la seguridad en los sistemas informáticos existentes en Consorcio Minero Benito Juárez Peña Colorada, S.A. de C.V.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Todos los sistemas son susceptibles para conectarse a un control único de acceso a los datos?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El principal beneficio obtenido de la implementación, es la creación de un solo punto de acceso para controlar la seguridad de todos los sistemas informáticos que sean susceptibles de conectarse a la solución. Los beneficios se dan en dos vertientes:
*	Coordinación de Sistemas. Se tendrá un sólo punto de control para la gestión de los usuarios de cada sistema, así como las autorizaciones correspondientes a cada sistema informático.
*	Usuario Funcional. Tendrá la ventaja de contar con un sólo punto de acceso por medio de Inicio de sesión único (por sus siglas en inglés SSO Single-Sing On) que le permitirá trabajar con las autorizaciones correspondientes en los sistemas informáticos que tenga asignados.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La seguridad de los sistemas informáticos es un elemento de amplia cobertura en todas las organizaciones en la actualidad, por lo tanto, lograr un balance entre la seguridad de los sistemas informáticos y la facilidad de acceso a los mismos, resulta una tarea complicada y difícil. Las organizaciones sufren en la actualidad constantes cambios en sus procesos para lograr su supervivencia en los negocios, por consiguiente, la seguridad de los sistemas informáticos tiene que evolucionar de la misma forma sin dificultar la forma de acceder a ellos.
Con el diseño de arquitectura propuesto se cubre la simplificación de acceso y administración de la seguridad de los sistemas informáticos bajo el siguiente esquema orientado a usuarios:
*	Usuarios generales. En la arquitectura actual los usuarios generales cuenta con ocho usuarios diferentes, así como la contraseña respectiva, para poder utilizar los diferentes sistemas informáticos existentes. Se debe recalcar que cada cuenta de usuario es diferente en cinco de ellos. Con el diseño propuesto, se tiene una sola cuenta de usuario con su respectiva contraseña para lograr el acceso adecuado hacia todos los sistemas informáticos.
*	Administradores de equipo de cómputo. Al ser los responsables de la gestión de la seguridad de todos los sistemas informáticos, por medio del diseño propuesto, se logra centralizar la gestión de la seguridad de todos los sistemas informáticos existentes en la empresa, logrando una gran disminución en las tareas del personal de la Coordinación de Sistemas.
La arquitectura propuesta define que todos los sistemas informáticos son susceptibles de conectarse a la solución de manejo de identidades digitales: en primera instancia por medio de un adaptador, y en segunda instancia, los sistemas informáticos que se requieran agregar al diseño y no cuenten con un soporte para la conexión, lo puedan realizar a través del directorio activo de Microsoft Windows Server, señalando que dicho enlace no cambia la mejora lograda en cuanto al acceso hacia los sistemas informáticos.
Finalmente, se destaca que, al contar con una arquitectura centralizada, se logra la simplificación de las actividades diarias de los administradores de equipo cómputo principalmente, logrando con ello que el personal involucrado dedique un mayor tiempo a labores de análisis y monitoreo logrando con ello cultura de prevención riesgos y amenazas.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>FACULTAD DE TELEMÁTICA
PROPUESTA DE ARQUITECTURA DE GESTIÓN DE IDENTIDADES DIGITALES PARA EL CONSORCIO MINERO BENITO JUÍREZ PEÑA COLORADA
TESIS
Que para obtener el grado de:
MAESTRO EN TECNOLOGÍAS DE INFORMACIÓN
Presenta:
Refugio Javier Villalobos Becerra
Asesores: D. en C. Juan Antonio Guerrero Ibáñez
D. en C. Carlos Alberto Flores Cortés
Colima, Col., Enero de 2012</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DESARROLLO DE SISTEMAS ADAPTIVOS Y REDES NEURONALES USANDO EL METODO DE PERTURBACION SIMULTANEA.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El desarrollo de sistemas no - lineales usando el método de perturbación simulánea, así
mismo la imlementacion de este método en el desarrollo de filtros adaptivos lattice tipo
IIR y FIR.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Aunque en la actualidad existen muchos tipos de filtros y sistemas no lineales, es
indispensable continuar la investigación hacia la obtencion de algoritmos capaces de
poder hacer más sencilla su implementacion, tales como el del método de perturbación
simulánea capaz de poder simplificar el desarrollo de este tipo de filtros y de sistemas no lineales y lineales.
Además es necesario preservar los detalles finos y al mismo tiempo tener una buena
ejecución, es importante en muchas aplicaciones como: reconocimiento de imágenes,
desarrollo de filtros, control de sistemas y otros más.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se ha desarrollado en este capitulo el diseño y aplicaciones del diseño de filtros con
criterio de error cuadrático medio mínimo (MSE), también usando el método de
perturbaciónsimulánea. Aunque en sus comienzos se describiera en términos analógicos,
el hecho de que el denominador sea una función definida positiva invalidaba su diseño
óptimo con las técnicas tradicionales de síntesis de redes analógicas. En el fondo, al estar
el diseño analógico hipotecado en su mayor parte al diseño en frecuencia el filtro usando
el SPSA no es realizable directamente y requiere de aproximaciónes de difícil control de
cara a su impacto en la calidad final conseguida.
En tecnología digital, al facilitar el diseño temporal directo mediante filtros FIR, el
filtrado MSE pasó a implantarse en prácticamente todas las aplicaciones de procesado de
señal. Desde receptores de comunicaciones, codificadores de fuente, prospección
acústica, etc.
El diseño del filtro se ha realizado vía el denominado principio de ortogonalidad de datos
y referencia, que equivale a la derivación directa del objetivo e igualarle a cero. Haciendo
uso de la coherencia espectral se ha vuelto al dominio de la frecuencia. Esto ha permitido
descubrir un método de conocer las prestaciones del filtro antes de proceder a su diseño.
A continuación, se ha pasado a implementar el filtro directamente desde los datos y a
validar lo que con valores esperados se había realizado mediante sumas directas de
energía de error y estimadores de covarianza y correlación. La importancia de la
consideración de los transitorios de un FIR se ha hecho evidente, dando lugar a varias
alternativas. El diseño de mayor complejidad y calidad, denominado método de
covarianza, se sitúa por encima del denominado de correlación de menor complejidad.
El predictor lineal con SPSA específico, donde la referencia es la propia señal ha
merecido más de un apartado. Su conexión con modelos AR y, como se vera, su
importancia en análisis espectral justifican per-se su inclusión. Al margen, claro esá, del
interés sistema como predictor o interpolador de datos incompletos.
La estructura de matriz de Toeplitz en los datos, cuando se emplea el método de
correlación provoca el interés del algoritmo de Levinson. Se ha hecho evidente que su
interpretación va mas allá que un método de invertir matrices de Toeplitz. La estructura
en celosía y el concepto de predicción forward y backward cierran el tema
proporcionando una estructura que por múltiples razones es más ventajosa que la
tradicional de retardos: la dinámica es decreciente, no se calcula de nuevo todo el filtro
cuando se incrementa el orden, etc.
Finalmente, a modo de ejemplo, y excluyendo dos grandes áreas de aplicación como son
vocoders y ecualizadores en comunicaciones, se han presentado algunas aplicaciones del
filtrado MSE, al margen de que las excluidas ya justificarían por ellas solas su interés.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE INGENIERIA MECANICA Y
ELECTRICA
SECCION DE ESTUDIOS DE POSGRADOS E INVESTIGACION
UNIDAD CULHUACAN
DESARROLLO DE SISTEMAS ADAPTIVOS Y REDES
NEURONALES USANDO EL METODO DE PERTURBACION
SIMULTANEA.
PRESENTA:
ING. JORGE IVAN MEDINA MARTINEZ
DIRECTOR:
DR. HECTOR PEREZ MEANA
Mexico D.F. Junio 2003</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Diseño y Desarrollo de un Data WareHouse para la Corett Colima</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseño y Desarrollo de un DataWareHouse para el Corett Colima</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Desarrollar un almacén de datos (data warehouse) y un sistemas de administración de datos y procesos para la Corett, permite el mejor control y actualización de datos para mejorar la atención de usuarios en todas las áreas involucradas en el proceso de regularización de asentamiento humanos irregulares que realiza la institución.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La buena ejecución de estos procesos tiene un impacto positivo en la mayoría de las áreas de trabajo y en las actividades del personal, especialmente para las áreas de oficina.
Cabe mencionar que llevar a la práctica estas acciones no generará un costo adicional a la institución ya que sólo se busca se haga un mejor uso y un mejor aprovechamiento de los recursos con los que ya cuenta la delegación corett.
El nivel de capacitación requerida para el personal en la aplicación de las acciones es totalmente alcanzable, es necesario un entrenamiento simple y una corta demanda de tiempo.
En trabajo conjunto de las áreas y los sistemas hacen posible esta alternativa para aumentar la productividad y eficiencia de la Delegación Estatal Colima y como fin último brindar un mejor servicio a los avecindados en el proceso de regularización de sus predios.
El atraso que tiene la delegación en el uso de nuevas tecnologías es considerable, por esta razón resulta de gran importancia el desarrollo y aplicación de este proyecto.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El proyecto se desarrolla siguiendo los procesos considerados en el método de prototipos, integrando las fases del ciclo de vida actuales y las propuestas por éste para el desarrollo del proyecto y los procesos de soporte necesarios para un desarrollo satisfactorio del trabajo de investigación propuesto.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Son evidentes los cambios que están teniendo lugar en la sociedad actual, donde el desarrollo tecnológico de nuestro entorno es tan amplio y acelerado que en ocasiones puede superar nuestra capacidad de adaptación al nuevo medio.
Hemos podido ver como la incorporación de nuevas tecnologías es un compendio, por un lado de necesidad y por otro, de conocimientos. Su intermediación se logra a través de los servicios que proporcionan las modernas infraestructuras y dispositivos. La Data warehouse, cuyos servicios se basan en el desarrollo y convergencia de medios como redes locales, Internet, procesos y las computadoras (Rainardi, 2008), no tiene como barrea el conocimiento tecnológico: los usuarios no tienen que ser ingenieros o informáticos, sino gente normal y por qué no, los avecindados, que necesitan estar mejor informados y más oportunamente de sus trámites y esto permitan una relación satisfactoria entre el trabajo, la calidad en el servicio y cumplir así con sus necesidades.
Todos tenemos que aprender y educarnos en la comprensión de la complejidad que pueden ser incorporar las nuevas tecnologías, teniendo en cuenta que, en ocasiones, la primera barrera que hay que vencer en este proceso está en nuestro interior y corresponde a esa desconfianza, actitud o creencia errónea de que alcanzar nuevos conocimientos corresponde sólo a una etapa de nuestra vida y que, llegado un momento, ya no podemos aspirar a más; en esta experiencia la resistencia a las nuevas formas de trabajar a se ha ido superando satisfactoriamente.
Es cierto que con el paso de los años los sistemas se van haciendo obsoletos en relación a nuevas necesidades. En muchos casos las nuevas tecnologías pueden disminuir tales deficiencias si conseguimos ir adaptándonos y dedicarles el tiempo necesario, teniendo en cuenta que llegado el momento, se tendrá un gran cúmulo de experiencias.
El cambio se está produciendo. Ahora es el momento de obtener respuesta por parte de la Administración que debe tener en cuenta que la realidad de las competencias tiene que estar contemplada en todas las disciplinas como forma de mantener una institución productiva. Es obvio que hay que partir de las necesidades reales de las personas y de los servicios más demandados para por último, adaptar los procesos y la tecnología.
La actual política social, en líneas generales, intenta congraciarse con este sector de la población ofreciendo programas encaminados a mejorar la calidad de vida y el nivel de salud. Sin embargo, aún son escasas las actividades que logran equilibrar la balanza, a pesar de que es sabido que los programas de combate a la pobreza han tenido resultados a cuenta gotas.
Son muchos los ejemplos de voluntad para poder tener instituciones que ofrezcan calidad en el servicio; y se pueden ver en distintos programas lanzados a nivel nacional, pero falta mucho por hacer, sobre todo porque en México no se ha podido mejorar e igualar el nivel de vida de la mayoría de los ciudadanos.
Sea pues este trabajo una pequeña contribución en busca de mejorar el servicio de una institución como es la Corett que permite a las familias más necesitadas seguridad jurídica en su patrimonio habitacional.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
Diseño y Desarrollo de un Data WareHouse para la Corett Colima
TESIS
Que para obtener el grado de
Maestría en Computación
PRESENTA
Germán Alcántar Sandoval
ASESOR
Mtro. Armando Román Gallardo
COASESOR
Dr. Nicandro Farías Mendoza
Colima, Col. Mayo de 2012</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“RECUPERACIÓN DE EVIDENCIA MÉDICA CONCIENTE DEL CONTEXTO UTILIZANDO CÓMPUTO UBICUO"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los médicos por lo general acceden, evalúan e interpretan la literatura médica basada en casos previos como apoyo para la toma de decisiones clínicas; sin embargo, deben ser selectivos ante la creciente sobrecarga de información a la que tienen acceso; artículos, guías de práctica médica, notas de pacientes y reportes, por mencionar algunos medios y elementos informativos. Además, el ritmo acelerado de sus actividades, no deja tiempo suficiente para realizar búsquedas en Internet, en repositorios médicos especializados o en bibliotecas digitales.
Para reducir esta problemática, un grupo de médicos de la Universidad de McMaster en Canadá, propuso la continuación de un movimiento dentro de la enseñanza y práctica de la medicina que fue iniciado por Sackett-a, Richardson, Rosenberg, y Haynes-a (1996), al cual denominaron “Medicina Basada en Evidencias (MBE)"  (Ottolenghi-b, 2002), que no es más que tomar decisiones médicas basadas en una información previamente validada.
La medicina basada en la evidencia, ha sido utilizada desde hace muchos años entre los especialistas de la salud. Hasta ahora la información podía obtenerse a través de tres principales actividades como:
1. Reuniones y contactos con otros colegas para compartir experiencias.
2. Asistencias a congresos.
3. El estudio de libros y revistas.
Éstas fuentes de conocimiento son útiles y de fácil acceso para aquellos profesionales de la salud que desarrollan su trabajo en hospitales ubicados en las grandes ciudades urbanizadas; sin embargo, para aquellos que se encuentran ubicados en provincias, o bien, desempeñan su labor desde equipos de trabajo o de investigación médica, el mantenerse al día les requiere gran esfuerzo y dedicación. Inclusive aún disponiendo de los medios o herramientas necesarias, muchas veces la falta de tiempo limita la asistencia a cursos de actualización y el estudio.
Las nuevas tecnologías, específicamente la Internet y las aplicaciones moviles, brindan la oportunidad de prestar servicios a domicilio, sin limitaciones de horario y localización geográfica, pero no todo está solucionado, el gran problema de la Internet es localizar información directa, es decir, lo importante es encontrar lo que se quiere en el menor tiempo posible.
Uno de los problemas detectados dentro de las tecnologías es que no es posible utilizar tecnología diseñada para oficinas dentro de los hospitales ya que el trabajo medico exige mucha movilidad, interrupciones, mucha comunicación que son consideraciones importantes en relación al trabajo normal de oficina, basándonos en estas observaciones afirmamos que las tecnologías móviles como una PDA son realmente necesarios para realizar investigación (Bardram, 2002).
Estudios recientes presentan una fuerte evidencia de aceptación de la Internet como mecanismo para la entrega de información médica (Jeannot, Scherer, Pittet, Burnand, &amp; Vader, 2003) y de que los profesionales de la salud se benefician de las nuevas herramientas de búsqueda de información basadas en el Web entre las cuales destacan Altavista, Yahoo y Google que sirven para la búsqueda y recuperación de la información más directa y también para apoyar a tomar decisiones clínicas para el manejo y análisis de pruebas de los pacientes hospitalizados (Schwitzer, 2002).
Es común que los médicos se enfrenten a una multitud de casos para los que deben encontrar con prontitud respuestas de las que depende la vida de sus pacientes. Con frecuencia la solución se obtiene con base en base a la experiencia de casos anteriores, pero si no se considera suficiente entonces se recurre a la consulta de un colega experto. Esta aproximación no es del todo adecuada, ya que generalizar a partir de la experiencia propia o ajena no sistematizada y obtenida con un número limitado de casos, puede no ser adecuada e inducir errores (Ottolenghi-a, 2002; Ottolenghi-c, 2000).
Los médicos realizan lecturas de fragmentos de libros o documentos de temas específicos en base al momento profesional por el que estén pasando. También asisten a cursos y congresos pero siempre se quedan con la sensación de nada es suficiente. Su ojo clínico no ve todo lo que quisieran y su experiencia se ve a veces disminuida por la evolución inesperada de alguno de sus pacientes.
Una solución rápida a este problema sería utilizar un sistema de recuperación de información. Pero el problema es que los sistemas de recuperación de información tradicionales están divorciados del contexto dentro del cual ocurre la solicitud de información (consulta de usuario). La solicitud de información ocurre por una razón (tarea actual del usuario) y esa razón coloca la solicitud dentro del contexto necesario para interpretarla y procesarla.
Sin tomar en cuenta este contexto, la solicitud se torna altamente ambigua, generando resultados incoherentes y usuarios insatisfechos (Budzik &amp; Hammond, 2000).
Esto es evidente en el sector salud, en donde grandes cantidades de información médica se encuentran disponibles en el Web y fácilmente pueden proveer sobrecarga de información. Una alternativa de reducir este problema es utilizar una aplicación CAR, la cuál es un sistema de recuperación de información que tiene como finalidad recuperar y presentar información basada en el contexto de los médicos (Brown &amp; Jones, 2000).</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Optimizar tareas médicas para reducir el tiempo de análisis de evidencia médica basada en el contexto haciendo uso del cómputo ubicuo.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Qué información contextual utilizan los médicos para recuperar información utilizando un sistema CAR?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Cuáles son las ventajas y desventajas del uso de aplicaciones CAR?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Es posible hacer uso de aplicaciones CAR para clasificar información contextual?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Cuáles son los problemas que pueden presentarse al utilizar aplicaciones CAR para clasificar información contextual?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>TODO:validar si se alcanzo el objetivo y comprobar que optimizo tareas medicas
(Carroll &amp; Rosson, 1992) Expresan que el uso de escenarios es una técnica que
permite identificar especificaciones de diseño de sistemas de software. Por otra
parte, Rodríguez et. al., 2003, indican que estudios empíricos realizados ayudaron a
desarrollar sus diseños y a comprender las condiciones para que la tecnología puede
ser implantada. Debido a que el objetivo de esta investigación es desarrollar y
evaluar un sistema que permita recuperar evidencia médica utilizando ontologías y
conciencia del contexto, la metodología a seguir toma como base experiencia previa
incorporando características de los estudios anteriores. Esta metodología es
ilustrada en la figura 7 tomada de Rodriguez, Favela, Gonzalez and Muñoz, (2003).
El caso de estudio ayuda para encontrar los escenarios que satisfagan el
planteamiento del problema, los cuales son enriquecidos con la literatura. En esta
parte del estudio se realizan iteraciones hasta identificar un conjunto de
especificaciones adecuadas para la resolución del objetivo. Posteriormente, en base
a dichas especificaciones se realiza el análisis, diseño y la implementación de una
aplicación que satisfaga el objetivo de la investigación. Finalmente, se evalúa el uso
de la aplicación realizada, para validar los objetivos de la investigación.
2.1 Comprensión inicial y Revisión de literatura
Las herramientas desarrolladas para hospitales han sido temas de estudio para los investigadores en los ultimos años. Esos estudios han sido principalmente acerca del intercambio de información. Consecuentemente el permitir que la gente pueda interactuar con el paciente a durante el día, los médicos deben confiar en artefactos que sirven de contenedores de información relevante del paciente, los cuales sirven como un medio de comunicación con otros colegas Bossen identifico algunos artefactos que sirven para visualizar la información de los pacientes.[3] estos artefactos son como PDA's que sirven para visualizar la información del expediente médico del paciente y revisión de literatura que soporte algún tratamiento a seguir.
2.2 Caso de Estudio y Escenarios
Este estudio se llevo a cabo en el Hospital Civil en la ciudad de Colima, México. Esta es una institución pública que provee servicios médicos para la población del estado de colima según datos del INEGI del 2005 cuenta con 567, 996 habitantes. Parte de la investigación fue revisar los procesos que realizaban los médicos del hospital.
Una vez que se ha clarificado el entendimiento de los procesos, se identifico el uso de escenarios que son situaciones que ejemplifican diferentes usos típicos del sistema. Los escenarios más adelante muestran el uso de la MBE y la Recuperación de información contextual.
Escenario 1:
Juan es un doctor del hospital y realiza su ronda final y el notifica que el paciente Pedro no está respondiendo bien a su medicamento, Juan desea poder contribuir a resolver el problema y revisa el expediente médico del paciente. Si Juan no conoce ese padecimiento entonces puede dejar una nota al médico que realizara la revisión mas tarde. Pero Juan tiene la posibilidad de visualizar los tratamientos que existen para solucionar el problema haciendo una consulta al repositorio de información.
2.3 Análisis y diseño
En este punto se realizo una investigación para ver cuál era la información que necesita ver el médico, de qué manera accedan a la información de los expedientes médicos teniendo en cuenta que este último es un documento de vital importancia para que se pueda emitir un tratamiento y diagnostico.
Otros documentos importantes son las notas médicas y notas de evolución médicas, así como información relevante de casos anteriores que puedan contribuir a la solución del problema. La parte de diseño se muestra más a detalle en el capítulo 3. Pero como un requerimiento importante en el diseño de interfaz fue el manejo de la PDA ya que el espacio de ese dispositivo móvil es muy reducido es necesario clasificar la información que va a ser mostrada al usuario porque puede llegar a perder mucho tiempo navegando entre tanta información.
Las partes de implementación y evaluación de resultados se muestran a detalle en el capítulo 3. Es importante tener en cuenta que el propósito de este estudio es mostrar el uso de las aplicaciones CAR hoy en día y dar respuesta a nuestras preguntas de investigación anteriormente planteadas y así conocer algunos detalles como la información necesaria por parte de los médicos para obtener información y cuáles son las ventajas, desventajas y problemas de usar aplicaciones CAR.
Lo anteriormente expuesto se realizó conjuntando un número de estudiantes en un ambiente de trabajo normal que uso de nuestro sistema CAR, donde se les uso un caso de estudio y ellos necesitaran llegar a la solución haciendo uso del sistema. Los resultados de esta investigación se obtuvieron en base a esa encuesta realizada a continuación se describe a detalle a cuantas personas se realizaron encuestas y así como el procedimiento que se siguió para la obtención de los resultados que se mostraran en el apartado de resultados y discusión.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se presentó una descripción del trabajo realizado que consistió en diseñar una herramienta que recupere información la cual constituye evidencia haciendo uso de un repositorio de información; además de diseñar un ambiente para almacenar información que constituye evidencia, la cuál puede ser consultada a través del Internet.
Para finalizar la descripción de este trabajo de investigación, a continuación se comentan en forma breve los logros obtenidos, las aportaciones que se hicieron y las recomendaciones para trabajo futuro.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
MAESTRÍA EN TECNOLOGÍAS DE INFORMACIÓN
“RECUPERACIÓN DE EVIDENCIA MÉDICA CONCIENTE DEL CONTEXTO UTILIZANDO CÓMPUTO UBICUO" 
TESIS QUE PARA OBTENER EL GRADO DE
MAESTRO EN TECNOLOGÍAS DE INFORMACIÓN
PRESENTA:
LUIS GUADALUPE MACÍAS TREJO
ASESORES:
D. EN C. JUAN JOSÉ CONTRERAS CASTILLO
M. EN C. RICARDO ACOSTA DÍAZ
COLIMA, COL; 25 DE MARZO DE 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DESARROLLO E IMPLEMENTACIÓN DE UN MUSEO VIRTUAL INTERACTIVO DE MIGRACIÓN</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Actualmente no existen documentos que registren cómo se ha conformado la población colimense a través de su historia, de dónde han venido los pobladores, a dónde se han ido los colimenses. Las implicaciones de ese tránsito nos hacen ser como somos, lo cual resulta fundamental.
Lo que se busca es que no se siga perdiendo esa información y recuperar la historia de las trayectorias migratorias de los colimenses a través del registro que se haga en línea.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Elaborar un modelo de museo virtual interactivo de la migración con base en el análisis de esquemas similares, que involucre en su desarrollo herramientas de código abierto (Open Source Software).</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los museos son memoria y por lo tanto, patrimonio de los pueblos. Sin embargo, en los últimos tiempos la tecnología también se ha hecho presente en ellos de diversas maneras, haciéndolos interactivos y proporcionando las herramientas para disfrutar y aprovechar al máximo las visitas, pero también dando la pauta para el diseño de museos virtuales como el museo virtual de la ciencia, (museovirtual.csic.es), el de Segóbriga (www.segobrigavirtual.es), que nos muestra las piezas arqueológicas de la zona; el museo virtual de las telecomunicaciones (www.eurocommuseum.com), e incluso muchos de los museos tradicionales han tenido que incorporar estas herramientas a su oferta como son el Museo del Vaticano (http://mv.vatican.va/4_ES/pages/MV_Musei.html), el Louvre (www.louvre.fr) y el Del Prado (http://cvc.cervantes.es/ACTCULT/museoprado/), entre otros.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Partiendo del importancia del estudio del fenómeno migratorio en el estado de Colima, junto con el hecho de que las tecnologías de información sin duda permiten que la relación museos/tecnología sea cada vez más estrecha; se puede determinar que la necesidad de una herramienta tecnológica que facilite tanto la exhibición como la recolección de información relevante en el estudio de la migración es resuelta con el desarrollo especializado del Museo Virtual Interactivo de Migración.
Como sabemos, actualmente los espacios museístico ya no se limitan al espacio físico, en algunos casos, el espacio arquitectónico y el virtual, se complementan, en otros, como en este proyecto, únicamente existe un espacio virtual. Es necesario entender que el espacio virtual no está condicionado a la desaparición de los espacios arquitectónico, la brecha digital nos ha dado suficientes evidencias para asegurar que uno no sustituye al otro. No se trata de renunciar a los espacios arquitectónicos tradicionales para conformarnos sólo con lo virtual. Se trata de proponer y fomentar estos nuevos entornos que nos permiten romper las barreras del tiempo y del espacio.
Cada día la tecnología avanza a pasos agigantados, si bien es cierto que además de ofrecer una herramienta de recolección de información, este proyecto nos permite un espacio museístico sin tantas restricciones (lugar y tiempo) como hace algunos años, y genera interactividad con los usuarios con el objetivo de retroalimentar sus contenidos, actualmente, cada vez que navegamos por un museo virtual, los contenidos parecen estar inmóviles, ninguna sala está llena, ocupada o algún objeto fuera de su lugar, todo se ve como si nadie más haya pasado por ahí anteriormente. Sería interesante un entorno digital donde nos “encontremos"  con más personas, probablemente los museos virtuales se podrán convertir en nuevos espacios de convivencia, sitios de encuentros para compartir las experiencias y cultura, todo en un entorno de realidad virtual.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>FACULTAD DE TELEMÁTICA
DESARROLLO E IMPLEMENTACIÓN DE UN MUSEO VIRTUAL INTERACTIVO DE MIGRACIÓN
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN TECNOLOGÍAS DE INFORMACIÓN
P R E S E N T A
CRISTÓBAL VILLASEÑOR GALVÍN
ASESORES:
M. EN C. RICARDO ACOSTA DÍAZ
D. EN F. MA. ALEJANDRA ROCHA SILVA
COLIMA, COL., JULIO DE 2012.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“Modelo Socio - Tecnológico de la Red Inalámbrica de la Universidad de Colima" </Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Con la popularidad y efectividad que las redes inalámbricas están obteniendo y el decremento de costos en equipos portátiles, cada vez es mayor el número de personas que solicitan el servicio de conectividad sin cables, por lo mismo instituciones educativas deben estar en la constante búsqueda de las mejores técnicas de creación y administración de redes inalámbricas las cuales ofrecerán este servicio a su población universitaria.
Por la cercanía que el personal encargado de la administración de la WUCOL ha tenido con otras entidades educativas se considera que en México existen universidades y tecnológicos que usan métodos poco eficaces para proveer red inalámbrica en sus instalaciones, por la poca administración, falta de métodos de seguridad avanzados y mala colocación de los puntos de acceso.
Para conocer el estado actual de esta problemática se realizó una recopilación de información sobre características de redes inalámbricas de 16 entidades de educación superior con las cuales se ha tenido contacto y obteniendo a través de entrevistas realizadas al personal del área de comunicaciones de cada una de ellas se obtuvieron los resultados de la siguiente tabla.
Esta tabla se clasifico por cuatro importantes pilares para el funcionamiento de una red inalámbrica la cual debe de contar con una administración que gestione todos los puntos de acceso que conformen la red, y si está es centralizada sería mejor controlada. En el sentido de autenticación se sabe que el método WEP y WAP son los más comunes en la actualidad mientras que los que utilizan servidores Ldap pueden llegar a ser más seguros y mejor administrados. El mapeo es importante para el manejo de una red inalámbrica ya que este sirve de guía y documentación de la red lo que permite mejor organización para el administrador. Y por último la seguridad es un tema que nunca debe de faltar en la creación y trabajo de cualquier red en este caso se mide la seguridad en control de accesos,autenticación y uso de técnicas para proteger la red inalámbrica de cada institución.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Sistematizar la experiencia de la implementación y evolución de la red inalámbrica WUCOL para generar un modelo socio - tecnológico que sea factible de replicar y sirva como instrumento para facilitar la detección de los elementos a mejorar de acuerdo al Plan Institucional de Desarrollo 2010-2013 de la Universidad de Colima.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Con la elaboración del modelo Socio-Tecnológico de la WUCOL se organizará la información primordial del funcionamiento en el servicio de conectividad inalámbrica de la Universidad de Colima, permitiéndole a otras entidades educativas utilizarla como ejemplo para la implementación o mejoramiento de sus propias redes inalámbricas.
Por otra parte del modelo de la WUCOL obtendremos el panorama actual de la red inalámbrica, lo que apoyará a la nueva visión del Plan Institucional de Desarrollo 2010-2013 en las políticas orientadas hacia las tecnologías.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La creación de un modelo que cuente con información detallada de un caso de éxito en implementación de una red inalámbrica así como su manejabilidad de crecimiento y mejoramiento en sus técnicas y métodos de administración, seguridad, documentación y análisis, como lo es la WUCOL de la Universidad de Colima, puede ser tomada como ejemplo para otras instituciones educativas y adquirir de este lo que más les convenga para mejorar su propia red inalámbrica.
Este modelo no solo permitirá apoyar a otras instituciones educativas en la creación o mejoramiento de sus redes inalámbrica, si no también brindará un panorama actual de la red inalámbrica WUCOL y así observar en qué punto se encuentra, y si lo que la conforma hoy día podrá cubrir los parámetros tecnológicos que se pretenden incluir en el plan institucional de desarrollo que la Universidad de Colima comenzó a implementar.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para crear el modelo sobre la red inalámbrica de la Universidad de Colima se consideraron tres elementos clave: principios, estrategias y componentes. Estos deben reflejar la misión de la WUCOL, que es ofrecer un servicio de calidad a los universitarios, y así poder compartir dicho modelo con otras instituciones educativas y usarlo como referente del estado actual de la red inalámbrica, para ello se realizó una investigación basándose en la sistematización de la experiencia con una metodología inductiva.
Un método inductivo se presenta por medio de casos particulares, sugiriéndose que se descubra el principio general que los rige. Se basa en la experiencia, en la participación, en los hechos y posibilita en gran medida la generalización y un razonamiento globalizado (Martínez &amp; Sánchez, 2010).
Se utilizó este método de investigación porque se efectúo en este trabajo una inducción hacia la experiencia, obtenida de la recopilación y creación de información en la red inalámbrica de la Universidad de Colima, posteriormente esta experiencia se sistematizó para crear el modelo de la WUCOL y así poder ofrecerlo para su réplica en alguna realidad similar a la Universidad de Colima.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Al analizar detalladamente la red inalámbrica de la Universidad de Colima con el objetivo de crear el modelo Socio - Tecnológico de la WUCOL se observaron las fortalezas y debilidades con las que esta cuenta, lo que permitió mejorar algunos procesos y eliminar falencias encontradas específicamente durante el trascurso de la creación de las estrategias del modelo, y así lograr fortalecer el contenido del mismo.
Se crearon técnicas para optimizar la administración de la WUCOL, como el mejoramiento de las políticas de seguridad existentes desde los inicios de la red inalámbrica, el desarrollo del análisis de coberturas inalámbricas a un nivel más detallado, creación de mapas lógicos específicamente de la WUCOL y sus enlaces, elaboración del compendio de resolución de problemas técnicos, entre otros, lo que fortaleció la forma de gestionar la red inalámbrica de la Universidad.
Finalmente al concluir el modelo quedó más entendible la funcionalidad que este tendrá dentro y fuera de la Universidad pues claramente refleja el estado actual de la red inalámbrica, lo que apoyará internamente para proponer mejoras y externamente para servir de guía a instituciones educativas que deseen aplicar la modalidad utilizada en la WUCOL.
Por otra parte este trabajo se convirtió en una herramienta valiosa para el departamento de la WUCOL, plasmando en resumen la documentación más importante con la que contaba el administrador, generando una guía que se podrá utilizar durante los próximos años con fines de capacitación y de seguimiento.
Por último conocer el estado actual de otras Universidades con la que se mantuvo contacto durante la creación de este modelo, el panorama general que se tenía de la WUCOL fue satisfactorio, al notar el buen nivel con el que cuenta la Universidad de Colima comparado con otras instituciones educativas en cuestiones de conectividad, lo que motiva a generar nuevos proyectos a futuro para el mejoramiento de la administración de la red inalámbrica universitaria.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Facultad de Telemática
“Modelo Socio - Tecnológico de la Red Inalámbrica de la Universidad de Colima" 
TESIS
Para Obtener El Grado De
Maestra En Tecnologías De Información
PRESENTA
L.I. Krishna Neith Guzmán Benavides
ASESORES
M. en C. Omar Ílvarez Cárdenas
M en C. Fermin P. Estrada González
Colima, Col. Mayo 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DISEÑO E IMPLEMENTACIÓN DE UN SITE DE INTERNET QUE CONTENGA LA INFORMACIÓN INTEGRAL A LOS PROGRAMAS DE POSGRADO DE LAS UNIVERSIDADES
PÚBLICAS DE LA REGIÓN CENTRO OCCIDENTE DEL PAíS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Difundir los posgrados en educación de nuestro país a nivel nacional e
internacional a través de una página de dinámica en internet creada mediante la
tecnología de Comman Gateway Interfase (CGI).</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Anteriormente la información que se tenía de programas de posgrado,
solamente existía en la propia institución, por lo cual cuando se interesaba por algún
programa se tenía que comunicar directamente a la institución, lo que originaba
tardanza en obtener la información, por el horario de oficina esto sin mencionar de
que si alguien se comunicaba del extranjero pudiendo ser de que cuando en aquel
país era de día en nuestro fuera de noche por lo cual sería un poco complicado
obtener dicha información.
Esto no sucede gracias al Internet porque se puede tener acceso las 24 horas
del día de todo el año.
Por tal motivo se diseña la página de posgrados en Educación de la Región
Jentro Occidente de México que dará los siguientes beneficio:
> Promover los estudios de posgrado entre los estudiantes, docentes, autoridades
y público en general a nivel nacional e internacional.
> Facilitar la consulta de la información a través de la página de internet con
accesos a base de datos.
> El actualizar una página dinámica en Internet con accesos a base de datos suele
ser una tarea fácil, ya que en ocasiones sólo se requerirá modificar un el registro
o adicionar un registro de una nueva institución y el mismo programa creará la
página.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
MAESTRíA EN CIENCIAS, ÍREA COMPUTACIÓN
DISEÑO E IMPLEMENTACIÓN DE UN SITE DE INTERNET
QUE CONTENGA LA INFORMACIÓN INTEGRAL A LOS
PROGRAMAS DE POSGRADO DE LAS UNIVERSIDADES
PÚBLICAS DE LA REGIÓN CENTRO OCCIDENTE
DEL PAíS
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRA EN CIENCIAS, ÍREA cowumc~óN
PRESENTA
Patricia Yanira Olmos Díaz
ASESOR
M.C. Rodolfo Gallardo Rosales
Coquimatlán, Colima, febrero de 2000</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ANÍLISIS COMPARATIVO ENTRE LA FACULTAD DE TELEMÍTICA Y FACULTAD DE PEDAGOGÍA SOBRE LA UTILIZACIÓN DE LA COMPUTADORA POR PARTE DE LOS DOCENTES EN EL PROCESO ENSEÑANZA - APRENDIZAJE.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>De acuerdo a los avances de la tecnología y la incorporación de ésta
en la Universidad, es importante observar de cerca su incorporación dentro
del aula pero revisando mas detalladamente las nuevas maneras de enseñar.
Para que las formas de educar alcancen su verdadera dimensión, se requiere
dar un estilo distinto de actuar al docente, solamente con profesores
preparados para la utilización de los nuevos recursos pero sobre todo,
realmente convencidos de su utilidad, se puede lograr en la población
estudiantil un alto grado de conocimientos en informática y demás áreas.
Redefinir el papel docente no implica que deban convertirse en
expertos en informática aquellos que no lo son, solo es necesario que
adquieran un nivel aceptable de alfabetización en el uso y aplicación
adecuada de las computadoras, que les permita incorporar con mayor
facilidad las herramientas educativas en la impartición de sus clases
perdiendo el miedo a verse superados por la tecnología, de tal manera ques
que el profesor pueda desarrollar mejor su papel de guía en vez de
transmisor de información.
Partiendo de las diferentes actitudes que se puede llegar a tener
frente al uso de las computadoras se vuelve pertinente indagar el uso y
aplicación que los docentes tienen hacia el empleo de la tecnología existente
en sus planteles ya que si se cuenta con la infraestructura y no se emplea,
se está realizando un desperdicio económico perdiendo la oportunidad de
desarrollar habilidades, destrezas y conocimientos útiles para los alumnos y
para los docentes mismos.
Por otra parte, si se cuenta con las herramientas tecnológicas y
espacios físicos para ésta y no se utiliza adecuadamente se corre el riesgo
de subemplearla y no cubriría con los verdaderos objetivos de la tecnología
que es entre otros aspectos facilitar la adquisición del aprendizaje.
Para poder utilizar a plenitud las ventajas de usar la computadora,
se debe impartir entre los docentes una formación que los ponga en
condiciones de incorporar la tecnología a la enseñanza y los capacite como
multiplicadores de su uso. En la actualidad, está bien visto que las escuelas
se encuentren bien dotadas de equipo tecnológico sin detenerse demasiado
en analizar el uso que se le da.
Se considera de poca actualidad las facultades que no cuentan con
sala de informática; y los profesores que no usan estos recursos no están
al día, éstos y tantos otros son pensamientos comunes pero  ¿Cómo, de qué modo y para qué se utiliza el centro de cómputo, las computadoras en
los módulos de los docentes y en las aulas?, o  ¿Cuándo y en qué función,
de qué objetivos se debe utilizar una computadora en el aula? .
Analizar si los docentes del área tecnológica, concretamente de la
Facultad de Telemática, tienen mayor conocimiento del empleo de la
tecnología existente en su plantel, (más que los profesionistas de una
carrera de corte social o humanista como lo es la Facultad de Pedagogía)
es el tópico de la presente investigación. Para ello se deberá realizar un
diagnóstico previo que nos indique la situación en la que se encuentran
ambos planteles respecto al equipamiento de tecnología y posteriormente
realizar un análisis comparativo entre ambos, en cuanto a actitudes para
utilizar la computadora, empleo de ésta en el proceso enseñanza -
aprendizaje y actualización en el campo informático.
Aunque muchas tecnologías más recientes que la computadora, como
el uso de telefax, videocasetera, televisión por cable y por satélite, se
pueden considerar como parte de las tecnologías útiles en la educación,
para el presente estudio transversal, se pretende conocer solamente la
utilización de las computadoras solas o conectadas en red por parte de los
docentes de las facultades de Pedagogía y Telemática de la Universidad
de Colima durante el período febrero - julio de 2001, brindando con ello la
posibilidad de realizar acciones concretas para trabajar con las
computadoras.
Se eligieron estos planteles por estar estrechamente relacionados con
la maestría en ciencias área Tecnología y Educación, se considera que el
plantel de Telemática se encuentre como un elemento representativo del
área de Ingeniería y Tecnología y la Facultad de Pedagogía del área de
Educación y Humanidades. Ambas áreas contemplan el contexto de la
maestría.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Conocer y analizar el uso de la computadora en la planeación de clases e
impartición de éstas, y establecer parámetros para evaluaciones futuras del
uso y cambios de actitud en la población docente de los planteles objeto de
estudio.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Estudios ponen de manifiesto que en las aulas es donde se desarrollan
procesos de enseñanza-aprendizaje, con apoyo de las computadoras se
observa:
"Una fuerte interacción entre tradición e innovación.
"La estructura de un aula computadorizada tiende a facilitar tipos de
interacciones que no se dan en un aula tradicional.
"El trabajo con la computadora sirve como estímulo a la interacción
constructiva entre estudiantes"  (Maldonado, 1991).
Se puede decir que la existencia de tensiones diferentes a las existentes
en las aulas tradicionales puede facilitar la redefinición del papel social del
educador y que si se impulsan prácticas que animen a compartir
experiencias y a discutir nuevos marcos de referencia, se pueden generar
cambios valiosos para la educación.
Aunque se debe mencionar que la adopción de la tecnología informática
por parte de los docentes en la educación está relacionada con diversos
factores de carácter social y económico que se concretizan en el acceso a
esta tecnología, la cual parece ir en aumento en nuestro país.
Algunos autores hablan de una masa crítica o número de individuos que
son necesario que se involucren con una determinada tecnología para
asegurar una adopción general de un grupo social determinado
(Markus,1987).
Si tal presunción es correcta, los programas de informática educativa no
podrán impactar en los escenarios escolares mientras no se asegure la
adhesión entusiasta de los docentes, convencidos de las ventajas que ofrece
esta tecnología para la enseñanza y el aprendizaje. En este proceso, se alude
más a los procesos motivacionales internos que a incentivos externos para
asegurar la adopción.
Los docentes pueden ir cambiando su esquema de ver la educación,
participando más activamente en el proceso enseñanza - aprendizaje de una
manera más actualizada, no dejándose llevar por la anterior forma de
enseñar; aunque la tecnología no es la solución de todos los problemas
educativos, existen tendencias que muestran que se convertirá en un agente
activador del proceso de cambio en la educación.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Tomando el conjunto de los profesores y considerando valores
promedio, permite apreciar las tendencias generales dominantes sobre
distintos aspectos relacionados con el tema que nos interesa. Se observa
una disposición positiva, mayor uso y conocimiento de la tecnología, por
parte de los profesores de la Facultad de Telemática asociada
mayoritariamente al progreso y considerada como factor estratégico. Se
enfatiza la necesidad de adquirir cultura tecnológica por parte de los
docentes de la Facultad de Pedagogía incluyendo de conocimientos
generales de computación. Son escasas las posturas críticas, o las
consideraciones políticas o ideológicas orientadas al rechazo de la
tecnología. Puede notarse alto interés por enseñar tecnología, y confianza
en la capacidad para hacerlo por parte de la Universidad de Colima ya que
al crear el CIAM (Centro Interactivo de Aprendizaje Multimedia) se
pretende incentivar la educación para que los docentes utilicen de la mejor
manera posible medios tecnológicos existentes en los planteles. Es posible
detectar que la problemática a la que se enfrentan los profesores en cuanto
a su acercamiento a la tecnología, tiene que ver con programas de
capacitación acorde a sus necesidades y saberlos utilizar dentro del salón
de clase así como requerimientos y recursos con que cuentan en el aula.
Si bien estas consideraciones nos pueden brindar una visión acerca del
grado de involucramiento o lejanía que tiene un profesor con respecto a la
tecnología, también nos dan pauta para reflexionar sobre aspectos que
pueden ser de suma importancia para la toma de decisiones en torno a
esta temática, por ejemplo: el tipo de capacitación que los maestros reciben
y los que necesitan, el uso que le dan a la tecnología dentro de su labor
docente, los medios e influencias a los que recurren los maestros para
apropiarse de dicha tecnología, etc.
Los aspectos anteriores orientan en cuanto a necesidades concretas de
capacitación y/o programas de intervención coadyuven al enriquecimiento
profesional.
Los resultados conducen a un mismo punto, la necesidad de formar
permanentemente al profesorado de ambos planteles objeto de estudio,
formar no solo en el manejo del equipo de cómputo, sino en el buen uso
de la tecnología en el proceso enseñanza - aprendizaje, también es de
vital importancia apostar por la integración de la tecnología en todos los
planteles de nivel superior pero incorporando un nuevo modo de
enseñanza en la que los profesores se desempeñaran participando
convencidos de las bondades que ofrece la tecnología vinculada con la
enseñanza.
Por otra parte, al estar tanto profesores como alumnos y administrativos
inmersos en una nueva cultura que supone nuevas formas de ver y
entender el mundo que nos rodea y, que ofrece nuevos sistemas de
comunicación interpersonal hace pensar en una repercusión en el ámbito
educativo. En esta nueva cultura que se desarrolla en el mundo cambiante
de la sociedad de la información, el rol de los docentes más que "enseñar"
(explicar-examinar) unos conocimientos que tendrán una vigencia limitada,
deben ayudar a los alumnos a "aprender a aprender" esta cultura del
cambio y promover su desarrollo cognitivo y personal mediante actividades
que tengan en cuenta sus características (centradas en el alumno) y que
les exijan un procesamiento activo de información (no una recepción
pasiva-memorización). Los estudiantes por su parte tienen una diversidad
en situaciones educativas, por lo que es aconsejable que los docentes
trabajen en colaboración con otros colegas pero, manteniendo una actitud
investigadora en clase, observando y reflexionando sobre la propia acción
docente y buscando progresivamente mejoras en las actuaciones acordes
con las circunstancias, metas institucionales y en la utilización de la
tecnología en el proceso enseñanza - aprendizaje.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÍTICA
Maestría en Ciencias, Írea Tecnología y Educación
ANÍLISIS COMPARATIVO ENTRE LA FACULTAD DE TELEMÍTICA Y
FACULTAD DE PEDAGOGÍA SOBRE LA UTILIZACIÓN DE LA COMPUTADORA
POR PARTE DE LOS DOCENTES EN EL PROCESO ENSEÑANZA -
APRENDIZAJE.
Presenta
Amalia Isabel Jiménez Vázquez
Asesora
M.C. María Andrade Aréchiga
Colima, Colima, Enero 2002
TESIS</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>PROPUESTA DE POLITICAS DE SEGURIDAD DE LA INFORMACION PARA EL SISTEMA SIABUC</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Las organizaciones públicas y privadas día a día generan conocimiento, datos,
reportes, actas y material de diferente índole importante para ellos, lo cual representa
toda la información que requieren para su funcionalidad, ésta en la mayoría de los
casos es almacenada en diferentes formatos tanto físicos como electrónicos, que
además es almacenada y puesta a disposición para el personal que requiere hacer
entre otras cosas toma de decisiones, planes, reportes, inventarios, etc.,
El acceso a la información se ha vuelto más fácil debido al avance en las tecnologías
de la información, esto ha permitido que se pueda tener acceso a información en
formato digital que se encuentra almacenada en nuestros dispositivos, localmente o
de manera remota en diferentes formatos, como archivos, documentos, imágenes o
base de datos.
Si la información es usada de forma adecuada puede resultar benéfica para las
organizaciones dueñas de ella, pero ésta puede ser interceptada, robada, modificada
o accedida por personas ajenas, resulta peligroso para los propietarios de la
información, debido a que se puede hacer mal uso de ella, pues se puede usar en
contra o para perjudicar a la organización.
Por lo anteriormente citado es necesaria la implementación de herramientas,
controles y políticas que aseguren la confidencialidad, disponibilidad e integridad de la información, con ellos garantizar que la información sea accedida solo por quienes
deban, esté disponible cuando se requiera para quienes estén autorizados y
permanezca tal y como fue creada por sus propietarios y la actualización de ella, esté
dada dentro de los controles que implemente la institución.
En el escenario de la Universidad de Colima, caso del sistema SIABUC no está
exento de una problemática de este tipo, por lo que se propone implementar este tipo
de mecanismos para asegurar la información contenida en este sistema, controles y
políticas de seguridad que estén alineadas a estándares internacionales tales como
ISO 27001:2005, así mismo establecer un precedente para que el resto de las áreas
de la institución que manejan información sensible adopten medidas similares.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Proponer un esquema de políticas de seguridad de la información del acervo
bibliográfico electrónico del sistema SIABUC de la Universidad de Colima alineado a
estándares internacionales.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿El sistema SIABUC cuenta con políticas y procedimientos para proteger la
información?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Se tienen identificados los activos del sistema SIABUC?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Existen mecanismos para la clasificación de la información del sistema
SIABUC?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿El sistema SIABUC cumple con algún estándar de seguridad de la
información?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Cuando se habla de la implantación de controles, es para disminuir el riesgo al qué
están expuestos los activos, lo cual dependerá del tipo de activo a proteger,
amenazas, vulnerabilidades, a que esté expuesto y con ello el tipo de control a
implementar; no necesariamente todos los mecanismos significan un desembolso
para la organizaciones, habrá algunos que sea una política interna como por
ejemplo, elaborar una política de contraseñas, o una política del uso de los recursos
informáticos.
En otras ocasiones será necesario venderles a las organizaciones, la idea de
proteger a sus activos, esto puede ser a través de un análisis de riesgo lo cual va a
permitir conocer el entorno donde están y con ello los riesgos a los que están
expuestos; conociendo a esto último probablemente, vean la necesidad de invertir o
generar controles de seguridad, equiparable a nuestra vida diaria no cambiamos
nuestro régimen alimenticio hasta que nos dicen que estamos enfermos de
colesterol, diabetes o alguna enfermedad que ponga en riesgo nuestra vida, igual
pasa en algunas organizaciones no implementan controles de seguridad hasta que
ven que están comprometidos sus activos.
Una de las grandes amenazas de las organizaciones son los usuarios (Fernández de
Lara, 2007) quienes pareciera que se las ingenian para comprometer sus
contraseñas y la información de las organizaciones donde laboran.
Las amenazas humanas no se limitan a los usuarios que utilizan los recursos, existe
un problema aun mas peligroso, los desarrolladores de software, los responsables de
los servidores, los responsables de las redes de voz y datos; que ponen en peligro la
seguridad de la información, en los eventos de seguridad, los conferencistas
subrayan la importancia del desarrollo de software y bases de datos que hagan de la
integridad de la información que manejan, su columna vertebral, a los responsables
de servidores que implementen herramientas, que minimicen las vulnerabilidades
de los sistemas que en ellos instalen, asi como los servicios mínimo necesarios.
Pero parece que pasan por alto estas recomendaciones y su lema es “ el chiste es
que funcione" . Por algo es la gran variedad de vulnerabilidades, en los sistemas
operativos, aplicaciones, base de datos. Todo por la poca importancia que se da al
tema de la seguridad.
Por lo anteriormente citado es necesario estudiar las medidas que se toman para
proteger la información en el sistema SIABUC que utiliza la Universidad de Colima.
Esto llevará a la concientización y la implantación de políticas, controles y
procedimientos que eviten la divulgación de la información de forma inadecuada.
Esto también permitirá que otras áreas de la Universidad tomen medidas similares
para proteger su información.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMATICA
PROPUESTA DE POLITICA DE SEGURIDAD DE LA INFORMACION PARA EL SISTEMA SIABUC
TESIS QUE PARA OBTENER EL GRADO DE 
MAESTRA EN TECNOLOGIAS DE INFORMACION
PRESENTA: 
AMALIA FLORES MUÑOZ 
ASESORES:
M.EN C. RAYMUNDO BUENROSTRO MARISCAL 
M. EN C. JOSE ROMAN HERRERA MORALES 
COLIMA,COL;JUNIO DE 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SISTEMA DE MONITOREO Y ALARMAS PROTOTIPO, PARA PLANTAS DE FUERZA AUXILIAR Y BANCOS DE BATERÍAS, UTILIZANDO UN ENLACE VÍA RADIO MÓDEM</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>“Desarrollar un sistema prototipo que monitoree las señales producidas por las plantas de fuerza y bancos de baterías y envíe a través de radio módems la información obtenida hacia una oficina o central remota" .</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Debido a lo crítico que es el sistema de alimentación de energía eléctrica, para que las empresas de telecomunicaciones puedan proporcionar un servicio de comunicación ininterrumpido para los usuarios, éstas compañías, como ya se comentó, se ven obligadas a instalar equipos de alimentación auxiliar (bancos de baterías y plantas de fuerza), también conocidos como “sistemas de alimentación ininterrumpida"  independientes de la proporcionada por C.F.E.
Él sistema que se desarrollará en esta tesis a diferencia del utilizado por TELMEX proporcionará información más detallada de las señales generadas, debido a que mostrará gráficamente en el monitor de la PC el nivel de voltaje, corriente o frecuencia que el sistema de alimentación ininterrumpida genere. De tal forma que se pueda conocer el comportamiento a través del tiempo del equipo.
Éste proyecto permite aplicar de manera integral los conocimientos adquiridos durante los estudios de la maestría en ciencias área telemática, debido a que, es necesario contar con conocimientos en las áreas de electrónica, comunicaciones digitales, telecomunicaciones e informática.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Es importante señalar que en esta tesis se hará referencia a los equipos electrógenos, y/o baterías con que cuenta TELMEX en las centrales telefónicas o repetidoras de microondas,por lo que las características de los equipos mencionados en este trabajo serán válidas sólo para TELMEX.
Para el desarrollo de ésta tesis se considerará que las señales producidas por la planta de fuerza, banco de baterías, C.F.E., y sistema de transferencia de energía de la planta, ya están acondicionadas para su adquisición.
En el proyecto se utilizarán señales generadas en el laboratorio para simular las proporcionadas por el equipo de fuerza debido a que no se cuenta con una planta de energía auxiliar a disposición para la elaboración de este trabajo.
Partiendo de lo anteriormente comentado el proyecto se dividirá en las siguientes partes (ver figura 1.1):
-Adquisición de las señales a monitorear utilizando un microcontrolador.
-Transmisión y recepción de las señales mediante radio módems.
-Graficación de las señales recibidas en la pantalla de la PC.
-Pruebas finales del comportamiento del sistema.
Está tesis se encuentra organizada, en nueve capítulos. En el primer capítulo titulado “microcontroladores y microprocesadores" , se describen cada uno de ellos, y se justifica la elección del microcontrolador.
En el segundo capítulo, “conversión analógica-digital" , se repasan los conceptos básicos sobre la conversión A/D y los sistemas de adquisición necesarios para comprender el funcionamiento del módulo de conversión A/D del microcontrolador.
En el tercer capítulo, “transmisión" , se repasan los conceptos sobre la transmisión de datos y se describe el funcionamiento de la USART del microcontrolador 16F874.
El cuarto capítulo, “implementación del monitoreo de las señales digitales y programa del microcontrolador" , se comenta la importancia de la implementación del reloj de tiempo real, se explica como se realizó el monitoreo de frecuencia, C.F.E., y transferencia del sistema, se presentan las características principales del programa del microcontrolador y la explicación de su funcionamiento.
En el quinto capítulo, “control del puerto serie de la PC" , se repasan los conceptos básicos para el control del puerto serie de una computadora, para finalmente elaborar el programa de recepción de la PC.
El sexto capítulo, “filtros" , trata los conceptos necesarios sobre los filtros que se requieren para la elaboración de este trabajo, y se diseñan dichos filtros.
El séptimo capítulo, “graficación en la PC" , se describe la lógica de funcionamiento del programa de graficación, y se presentan los resultados de las pruebas del sistema incluyendo los filtros digitales.
En el octavo capítulo, “radio módems" , se explica su funcionamiento y los aspectos a considerar para su elección. Finalmente en el noveno capítulo, se presentan las conclusiones de este trabajo.
En el Anexo I, se presenta información amplia sobre los sistemas de alimentación ininterrumpida usados por TELMEX. En los Anexos II, III, IV, se listan los programas.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En esta tesis se desarrolló un sistema prototipo el cual es capaz de monitorear las señales producidas por una planta de fuerza y/o bancos de baterías.
La información adquirida por el sistema es enviada vía radio módems hacia una oficina o central remota, y es presentada al usuario a través de gráficas de tendencia representativa de cada una de las señales. Además cuenta con un visor de gráficas anteriores, un visor de alarmas, y un módulo de configuración de alarmas.
El módulo principal del sistema de monitoreo genera las graficas actuales de las señales (de las últimas 12 horas de trabajo), dispara las alarmas cuando existen, y genera los archivos correspondientes por día. El visor de gráficas anteriores es capaz de regenerar la gráfica de una señal de un determinado día, y además permite imprimir su reporte respectivo. El visor de alarmas por semana permite generar e imprimir un reporte de las alarmas ocurridas de la semana correspondiente al día elegido. Y finalmente el módulo de configuración de alarmas en el cual el usuario puede configurar los niveles y tiempos de alarmado para cada señal.
De tal forma que el sistema desarrollado en esta tesis es superior al sistema S.M.A.R.T. reporter ya comentado en la introducción de esta tesis, ya que este sólo indica si un equipo determinado se encuentra encendido o apagado.
El proyecto desarrollado permite que las personas encargadas de las plantas de fuerza y/o bancos de baterías, realicen un mejor monitoreo ya que cuentan con una mayor cantidad de información, y esta a su vez describe con mayor detalle el comportamiento de los equipos, a través de lo cual el personal de mantenimiento puede decidir en que momentos es necesario tomar acciones correctivas.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA 
Maestría en Ciencias área Telemática
SISTEMA DE MONITOREO Y ALARMAS PROTOTIPO, PARA PLANTAS DE FUERZA AUXILIAR Y BANCOS DE BATERÍAS, UTILIZANDO UN ENLACE VÍA RADIO MÓDEM
TESIS
Que para obtener el grado de
Maestro en Ciencias área Telemática
Presenta
Ing. Castro Palazuelos David Enrique
Asesor
M. en C. Ornelas Arciniega Gilberto
Colima, Col., febrero de 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>APLICACIÓN DE UNA HERRAMIENTA DE REALIDAD VIRTUAL COLABORATIVA Y ADAPTACIÓN DE MODELOS GRÍFICOS EN 3D PARA EL APOYO DEL DIAGNÓSTICO MÉDICO DE LESIONES ÓSEAS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Cuando una persona de cualquier edad se encuentra realizando una actividad y por diversas
circunstancias sufre una lesión ósea, lo primero que se hace es llevarla a un hospital, donde se valora la situación del paciente y se da un diagnóstico.
En ciertos casos las decisiones que toman los médicos al momento de diagnosticar a un
persona con una fractura pueden ser mejores si se consulta a otro médico y se tienen dos o más opiniones.
El problema es que los médicos a quien se acudiría para que dieran su opinión se encuentran en diferentes lugares debido a que son especialistas y por lo general en una institución médica sólo hay uno.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Poner a disposición de los médicos una interfaz colaborativa de realidad virtual en donde se
tenga la posibilidad que un grupo de médicos puedan analizar las lesiones óseas que presenta
un paciente en particular y puedan intercambiar información de los diagnósticos, mediante el
uso de voz (audioconferencia), modelos en 3D generados por un TAC y un chat.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es factible utilizar una herramienta en la cual los médicos estén visualizando y
expresando con su voz lo que piensan acerca del diagnóstico de lesiones óseas?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Qué tan complicado será el uso del entorno para los médicos sabiendo que se utilizará
realidad virtual?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Cuáles son los beneficios de las técnicas colaborativas de realidad virtual, comparadas
con las técnicas actuales, para el apoyo del análisis colaborativo de modelos
radiológicos en 3D de lesiones óseas?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El uso del ambiente virtual propuesto facilitará significativamente la interpretación
colaborativa de modelos radiológicos en 3D de lesiones óseas.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Resulta costoso y en ciertas ocasiones tedioso para los médicos trasladarse de un lugar a otro para estudiar un caso de lesión ósea, es por tal razón que resultaría de gran ayuda la utilización de un software en el cual un grupo de médicos no tengan que salir siquiera de sus consultorios (ahorrándose tiempo y dinero); sino simplemente encender su máquina, observar un modelo en 3D de la lesión, generada a partir de un aparato de Tomografía Axial Computarizado (TAC) y empezar a intercambiar puntos de vista mediante un sistema de audioconferencia o chat, para ofrecer un mejor diagnóstico y una solución más óptima al problema.
El uso de este entorno también será un medio por el cual se registren opiniones y diagnósticos en forma colaborativa.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Llevar a cabo toda la investigación resultó complicada en ciertos puntos y es interesante
conocer todos los comentarios finales que se tuvieron. Por eso en este capítulo se resumen los aspectos más importantes a los que se llegó como conclusiones.
Aunque en general los usuarios que realizaron la prueba no tuvieron muchos problemas en
interactuar dentro del ambiente virtual se observó que algunos aún se encuentran un poco
renuentes a usar este tipo de tecnología ya que el simple hecho de escuchar realidad virtual los asusta debido a que lo primero que se les viene a la mente es cascos, guantes, trajes, etc. Esto no debe ser así, ya que por ejemplo en este caso se utilizó realidad virtual de baja inmersión donde lo que se realizó fue el ambiente virtual; además de acuerdo a lo contestado en los cuestionarios la mayoría de los participantes ya habían utilizado la computadora para jugar video juegos lo cual les debió facilitar la interacción y prueba de ello fue lo observado al final de la prueba donde se vio que todos los participantes se encontraban interactuando entre si dentro del ambiente intercambiando puntos de vista y opiniones.
Para llevar a cabo el ambiente virtual en DIVE se tuvieron dos opciones: desarrollarlo en el lenguaje de programación nativo de DIVE o realizarlo en TCL (Raines y Tranter, 1999); se eligió realizarlo en el lenguaje de DIVE ya que al momento de descargar el software de la herramienta vienen incluidos ejemplos en los cuales nos basamos para aprender a programar en esta herramienta.
Se consideró que fue poco el número de participantes que realizaron la prueba de usabilidad, esto debido a diversas circunstancias, como los conocimientos que deberían tener y principalmente la disponibilidad de horario para participar.
Dando solución a los objetivos específicos que se plantearon al principio de la investigación y en base a la información recabada con los cuestionarios de usabilidad aplicados y a las entrevistas realizadas a los participantes durante y después de las pruebas piloto, se concluye lo siguiente:
Objetivo específico 1:
*	Ofrecer y adaptar una interfaz de apoyo de la cual se puedan auxiliar los
médicos para el diagnóstico de lesiones óseas.
El primer objetivo se cumplió, ya que los participantes comentaron que fue una buena idea
desarrollar un ambiente basado en realidad virtual que ayudara a los médicos a ofrecer un
mejor diagnóstico a través de un análisis en conjunto. Los participantes también mencionaron que sí utilizarían este entorno de realidad virtual como herramienta auxiliar para diagnosticar a un paciente además de recomendarlo con otros médicos.
Objetivo específico 2:
*	Apoyar en la reducción de tiempos y costos al evitar que los médicos se trasladen
de un lugar a otro a diagnosticar.
Con base en las entrevistas realizadas a los médicos al final de las pruebas se puede decir que les pareció algo muy interesante el poder diagnosticar desde el lugar en donde se encuentren sin la necesidad de salir de su lugar de trabajo, ya que esto les ahorraría mucho tiempo y en ciertos casos dinero si así fuera el caso; por lo tanto el segundo objetivo sí se cumple.
Objetivo específico 3:
*	Lograr una interfaz que sea amigable y fácil de usar para los usuarios.
Basándonos en lo contestado en los cuestionarios y en lo observado durante la realización de las pruebas se puede concluir que este objetivo también se cumple aunque con ciertos detalles.
Los participantes comentan que la interfaz del ambiente les pareció buena en general y que
interactuar dentro del entorno virtual se les hizo un proceso fácil ya que los métodos de
comunicación que se tenían así lo permitieron. Los detalles que nos hicieron saber fue que tal vez había que hacer un análisis de los colores que se estaban utilizando así como de los objetos (avatares) que representan a los médicos dentro del ambiente virtual para tratarlo de hacer más inmersivo.
La conclusión a la que se llega en cuanto a la hipótesis es la siguiente:
Hipótesis:
*	El uso del ambiente virtual propuesto facilitará significativamente la
interpretación colaborativa de modelos radiológicos en 3D de lesiones óseas.
Se concluye que la hipótesis se cumplió, ya que los participantes comentaron que tuvieron la posibilidad de ver la lesión ósea desde diferentes puntos de referencia debido a que se trataba de un modelo en 3D; y que como se tuvieron varios puntos de vista se redujo el tiempo del diagnóstico haciéndolo más completo. Además todos concordaron en que el uso del ambiente virtual ayudó a ofrecer un mejor diagnóstico. La afirmación de la hipótesis también se puederespaldar de acuerdo a los resultados obtenidos en las escalas de Likert de los cuestionarios que fueron aplicados.
Respondiendo a las preguntas de investigación se concluye:
Pregunta de investigación 1:
*	 ¿Es factible utilizar una herramienta en la cual los médicos estén visualizando y
expresando con su voz lo que piensan acerca del diagnóstico de lesiones óseas?.
Durante la prueba se observó que los participantes al principio utilizaron más la ventana de chat para intercambiar información pero conforme avanzó la misma, empezaron comunicarse
por los micrófonos. Al final los participantes comentaron que se sintieron más cómodos
usando como medio de comunicación el audio, lo cual demuestra que fue una buena idea
implementar su voz para que intercambiaran puntos de vista sobre el diagnóstico. En lo que
respecta al modelo de la lesión ósea presentado les pareció algo muy bueno debido a que
tenían la posibilidad de rotar el modelo y visualizarlo desde diversas perspectivas. Por lo que se concluye que sí es factible utilizar una herramienta de este tipo.
Pregunta de investigación 2:
*	 ¿Qué tan complicado será el uso del entorno para los médicos sabiendo que se
utilizará realidad virtual?
Los participantes comentaron en el cuestionario de usabilidad, que ya habían usado la
computadora para jugar videojuegos, por tal razón les resultó sencillo interactuar dentro del ambiente virtual.
Pregunta de investigación 3:
*	 ¿Cuáles son los beneficios de las técnicas colaborativas de realidad virtual,
comparadas con las técnicas actuales, para el apoyo del análisis colaborativo de
modelos radiológicos en 3D de lesiones óseas?
El principal beneficio es que no solamente un médico va a poder diagnosticar sino que se
tienen varias opiniones sobre la lesión. Además no es necesario que los médicos se trasladen de un lugar a otro o se pierda tiempo en enviar el modelo en 3D de la lesión de un paciente de una lugar a otro.
Debido a todo el análisis que se realizó en cuanto al hardware y software que se necesita para poner en funcionamiento el entorno virtual y a una prueba que se realizó con el ambiente en una computadora con características de hardware no muy altas, se concluye que no resultaría excesivamente costoso implementar este ambiente en un hospital.
Para concluir este capítulo, como se pudo constatar todos los objetivos propuestos al inicio de la tesis se cumplieron, así como también la hipótesis. La mayor parte de este capítulo se obtuvo de los resultados que arrojaron las pruebas, tanto de los cuestionarios como de las entrevistas a los participantes.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÍTICA
APLICACIÓN DE UNA HERRAMIENTA DE REALIDAD VIRTUAL
COLABORATIVA Y ADAPTACIÓN DE MODELOS GRÍFICOS EN 3D
PARA EL APOYO DEL DIAGNÓSTICO MÉDICO DE LESIONES ÓSEAS
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS, ÍREA TELEMÍTICA
PRESENTA:
ING. LENIN ALEJANDRO CERVANTES MEDINA
ASESOR:
D. en C. MIGUEL ÍNGEL GARCÍA RUIZ</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ALREF Algoritmo de administración de memoria con restricciones físicas en flujos de datos</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Para mejorar el índice de detección de ataques y conductas intrusivas sobre redes de comunicación, es necesario explorar nuevos modelos que se puedan aplicar a IDS que utilizan redes neuronales en su funcionamiento. Dado que en trabajos previos los mejores resultados se han obtenido utilizando redes neuronales recurrentes resulta natural entonces que en este trabajo se haga un énfasis especial en la investigación de este tipo de arquitecturas.
El uso de redes neuronales junto con el procesamiento wavelet busca aprovechar las características de ambos conceptos, es decir, utilizar las características de reconocimiento de patrones y clasificación de las redes neuronales junto con las propiedades de representación y localización que poseen las funciones wavelet. Al definir un nuevo tipo de unidad de procesamiento es necesario describir su funcionamiento, es decir, la forma en la que produce la salida a partir de un estimulo o entrada dado, además de indicar las restricciones en sus parámetros. Las unidades de procesamiento junto con otros elementos como unidades de entrada y salida, forman parte de un esquema de interconexiones que definen el mapeo entrada - salida descrito por medio de una ecuación.
Una vez definida la parte estructural del modelo es necesario definir los parámetros libres de la arquitectura y sus propiedades. Durante la etapa de aprendizaje estos parámetros ajustarán sus valores para minimizar el error producido por el modelo, dichos ajustes son descritos mediante un algoritmo de entrenamiento.
Ésta tesis tiene como propósito modelar y simular un sistema de detección de intrusos que use redes neuronales recurrentes con wavelets, obteniendo resultados cuantitativos que permitan llevar a cabo un análisis comparativo respecto a otros trabajos previamente realizados. Las redes neuronales basadas en wavelets reemplazan la función sigmoide utilizada por las unidades en las capas de ocultas por una función wavelet, dando lugar a un nuevo tipo de unidad de procesamiento que implementa sus funciones de transferencia a partir de una wavelet madre [SUN09].
Este trabajo pretende investigar la utilización de una red neuronal recurrente basada en wavelets para detectar conductas intrusivas o ataques sobre una red de comunicación. Una de las principales funciones del modelo será categorizar tráfico de red como dañino o inofensivo, adicionalmente, dentro de los patrones clasificados como intrusivos describir la clase de ataque a la que pertenece. Durante la investigación se explorará el desempeño de las redes neuronales basadas en wavelets como complemento a tecnologías de seguridad existentes como sistemas de autentificación, seguridad de ruteo y firewalls para mejorar la precisión de detección de nuevos ataques y de esta manera incrementar el nivel de seguridad en el dominio donde se implemente el sistema.
La principal característica del enfoque adoptado para la presente investigación es el cambio de la función de transferencia sigmoide que utilizan las redes neuronales tradicionales por una función wavelet, y de este modo determinar si el modelo de red neuronal propuesto obtiene mejoras en la velocidad de convergencia e índices estadísticos comparativos para los sistemas de detección de intrusos. La construcción del modelo planteado anteriormente implica una etapa de diseño donde se establezcan las características generales de la arquitectura así como la deducción del algoritmo de entrenamiento a usar, una etapa de implementación donde se llevará a cabo la programación mediante un lenguaje de alto nivel del diseño y finalmente una etapa de simulación donde se podrán obtener resultados cuantitativos sobre el desempeño de la red neuronal que permitan concluir la viabilidad de este modelo. El resultado final contemplará un esquema de interconexiones con distintas capas que después de una etapa de entrenamiento tendrá la capacidad de reconocer amenazas dentro del segmento de red donde se ubica el sistema de detección de intrusos.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Proponer un modelo de red neuronal recurrente con wavelets aplicado a la detección de intrusos en una red de comunicación de datos, dicho modelo contempla la definición de la arquitectura y el algoritmo de entrenamiento. El modelo planteado será capaz de analizar distintos patrones en la capa de aplicación del modelo OSI3 y llevará a cabo las tareas de detección, identificación y clasificación de ataques.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>A lo largo del tiempo diversas técnicas han sido propuestas para mejorar la detección de conductas dañinas en las redes de comunicación, en este contexto el uso de redes neuronales aplicadas en los sistemas de detección de intrusos demuestra ser una alternativa eficiente y una área de investigación promisoria.
En términos de velocidad de convergencia el modelo basado en la red neuronal auto recurrente SRWNN - MRW necesitó 60 épocas de entrenamiento contra 81 del modelo recurrente totalmente conectado, dejando en último lugar al modelo Elman con 160. Sin embargo el costo computacional requerido por el modelo recurrente totalmente conectado es del orden de O(n^4) catalogánd-olo como el más costoso en términos de recursos computacionales. Es decir que tomando en cuenta los índices de clasificación e identificación, el modelo auto recurrente con wavelets SRWNN - MRW con una complejidad computacional de O(n^2) ofrece una aproximación cercana al recurrente totalmente conectado a un costo computacional menor.
Las medidas en las tasas de falsos positivos y falsos negativos indican que aunque el modelo Elman obtuvo una menor tasa de falsos positivos emitidos, éste también posee la tasa de falsos negativos más alta de todos los modelos. Este comportamiento se puede analizar a través de los indicadores estadísticos de especificación y sensibilidad. La especificación se define como la probabilidad de que un patrón inofensivo de entrada sea catalogado por el IDS como tráfico de red normal. La sensibilidad es la probabilidad de que el IDS catalogue a un patrón dañino de entrada como un ataque al sistema. Una vez señaladas estas dos probabilidades tenemos que el modelo Elman posee una alta probabilidad de especificación, alrededor de 96%, lo que explica el bajo índice de falsos positivos. Por otra parte, el modelo Elman posee una sensibilidad muy baja, 78%, lo que explica el alto porcentaje en su tasa de falsos negativos. En un IDS ideal los indicadores de especificación y sensibilidad deben ser altos, aunque en aplicaciones prácticas nunca se obtiene un 100%, resultando en la emisión de falsos positivos y falsos negativos [FAW04].
Tomando en cuenta lo anterior, el modelo recurrente totalmente conectado y el auto recurrente con wavelets (SRWNN) resultan con tasas más equilibradas de falsos positivos y falsos negativos siendo estas 7.24%, 6.53% para el primero y 7.24%, 8.52 para el segundo. Es de recalcar que las tasas obtenidas por el modelo propuesto (SRWNN - MRW) fueron de 5.43% y 9.65%, estableciendo un mejor resultado en la tasa de falsos positivos que las dos arquitecturas mencionadas anteriormente.
El gráfico ROC demuestra que la aplicación de redes neuronales a los modelos de detección de intrusos funciona de manera significativa debido a que los 4 modelos contemplados se ubican en la región triangular superior, lo que permite establecer que su desempeño se aleja del comportamiento aleatorio definido por la línea y = x. En el gráfico se puede apreciar que respecto al costo, mientras el modelo Elman se ubica más al oeste de la gráfica, los modelos restantes están orientados más al este señalando una emisión mayor de falsos positivos. Si se considera el eje de los verdaderos positivos tenemos claramente que los modelos recurrente totalmente conectado y auto recurrente con wavelets (SRWNN y SRWNN - MRW) superan a Elman considerablemente, estableciendo sus ventajas al momento de la detección de patrones dañinos. El modelo Elman se dice que es más conservador que los restantes debido a que lleva a cabo una clasificación positiva de los patrones solo con evidencia firme y como consecuencia posee un índice bajo de falsos positivos, pero al mismo tiempo con la desventaja de tener una tasa baja de verdaderos positivos. Por el contrario los modelos recurrente totalmente conectado y auto recurrentes con wavelets (SRWNN y SRWNN - MRW) pueden ser llamados liberales debido a que llevan cabo una clasificación positiva con evidencia más ligera resultando en una mejor clasificación de ataques con el costo de tener una tasa de falsos positivos más alta [FAW04].
Como se puede observar en los resultados del capítulo anterior, la red neuronal recurrente totalmente conectada y la auto recurrente con wavelets (SRWNN - MRW) obtuvieron los mejores resultados en los indicadores de clasificación e identificación, siendo estos 93.15%,84.23 y 92.19%, 84.23% respectivamente. Estas medidas ubican a los modelos anteriores como los candidatos más factibles en la implementación del sistema de detección de intrusos que pueda cumplir con niveles de seguridad requeridos en una red de comunicaciones.
Las redes neuronales wavelet son un nuevo tipo de redes neuronales desarrolladas en los últimos años que combinan la teoría de las redes neuronales y el procesamiento wavelet. La principal característica de estas arquitecturas es que las funciones de transferencia de las unidades de procesamiento son cambiadas por funciones derivadas de una wavelet madre, para dar lugar a lo que se conoce como neurones wavelet o wavelons. De igual manera que en las redes neuronales convencionales, dentro del campo de redes neuronales wavelet existen distintas estructuras como la red con alimentación hacia adelante o la recurrente, el presente trabajo ha centrado su atención en las topologías recurrentes debido a que en trabajos anteriores éstas han obtenido las mejores medidas de desempeño [SAN05]. Durante el desarrollo de este proyecto se ha presentado el modelo auto recurrente con wavelets (SRWNN - MRW) para conocer su desempeño aplicado en los sistemas de detección de intrusos, obteniendo excelentes resultados que comprueban la eficiencia del procesamiento wavelet en las redes neuronales.
Esta tesis ha desarrollado un nuevo modelo de detección de intrusos basado en una arquitectura recurrente con funciones de transferencia wavelet. El proceso de construcción de dicho modelo abarcó el diseño de la topología, deducción del algoritmo de aprendizaje de la red neuronal, pre procesamiento de los datos, implementación y finalmente la evaluación de resultados. Después de una evaluación de los parámetros de desempeño, los resultados obtenidos permiten establecer la factibilidad de su implementación debido a que los valores obtenidos en esta arquitectura se ubican en las mejores posiciones respecto a otros modelos estudiados. Los resultados comparativos obtenidos serán considerados dentro de un artículo de divulgación científica que sirva a los investigadores en el área de seguridad informática como referencia en la aplicación de redes neuronales recurrentes wavelet en los IDS.
Una de las características más positivas del modelo auto recurrente con wavelets (SRWNN - MRW) es su velocidad de convergencia ya que puede alcanzar ciertas medidas de eficiencia con un número menor de épocas de entrenamiento y de esta manera facilitar el proceso de aprendizaje cuando se tiene un conjunto de muestras grande, todo esto haciendo un uso menor de recursos computacionales que otros modelos como el recurrente totalmente conectado.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de las Américas Puebla
Escuela de Ingeniería
Departamento de Computación, Electrónica y Mecatrónica

Modelos y Algoritmos para redes neuronales recurrentes basadas en wavelets aplicados a la detección de intrusos

Tesis profesional presentada por
Yaser García González
como requisito parcial para obtener el título en
Maestría en Ciencias de la Computación

Jurado Calificador

Presidente: Dra. Ingrid Kirschning Albers
Vocal y Director: Dr. Vicente Alarcón Aquino
Secretario: Dr. Oleg Starostenko
Cholula, Puebla, México a 13 de mayo de 2011

Derechos Reservados ©2011.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Construcción de recursos de aprendizaje a partir de patrones de interacción</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Desde el punto de vista científico, el principal problema consiste en la inexistencia
de plataformas que permitan a usuarios sin experiencia computacional desarrollar
recursos utilizables en el área de la educación aprovechando tecnologías
existentes tales como los pizarrones electrónicos.
Existen espacios educativos que cuentan con la tecnología necesaria para desarrollar diferentes actividades de aprendizaje. Estos espacios están siendo desaprovechados, ya que aún no existen suficientes recursos de aprendizaje. También existen tecnologías capaces de integrar los recursos a estos espacios, sin embargo aún no existe una forma sencilla de construir nuevos recursos.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Proponer una metodología basada en patrones de interacción, que facilite la construcción de recursos de aprendizaje a utilizar en pizarrones electrónicos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible automatizar la construcción de recursos de aprendizaje, aún si los usuarios no cuentan con la experiencia computacional necesaria. Esto se puede lograr presentando patrones derivados de recursos existentes a través de una plataforma.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Estudio de recursos educativos
Definición de patrones
Desarrollo de la plataforma
Integración de patrones a la plataforma
Pruebas y correcciones
A partir de la plataforma creación de nuevos recursos</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se desarrolló una plataforma a partir de una metodología basada en patrones de
interacción, que facilitó la construcción de recursos de aprendizaje a utilizar en
pizarrones interactivos. Esto se llevo a cabo mediante el estudio de recursos
interactivos, y al observar que eran aplicables en diferentes contextos educativos,
se definieron como patrones que son reutilizados para la elaboración de más
recursos.
Para comprobar que a partir de esta plataforma se pueden generar recursos de
aprendizaje, se les pidió a profesores de nivel básico, medio y superior que
desarrollaran recursos de aprendizaje haciendo uso de la plataforma siguiendo
una serie de tareas, las cuales arrojaron que el desarrollo de recursos es más
sencillo y rápido, así como también, que aunque algunos de ellos aun no utilizaban
tecnologías para desarrollarlos no tuvieron problema al interactuar con la
plataforma y que seguirían usándola como apoyo a sus materias.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Desarrollo de un Reconocedor Robusto de Voz de Niños</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Durante la década pasada y principios de esta, los sistemas que utilizan reconocimiento de voz han tenido un mayor auge en sistemas de telefonía, educación, entretenimiento y militares. En cada uno de los campos mencionados han existido problemas y nuevos retos al utilizarlos en tiempo real. Algunos de estos problemas son: ruido (en el ambiente, señal), variabilidad temporal y espectral dependiente de la edad del locutor, entre otros. La mayoría de la investigación se ha situado en sistemas que interactúan con adultos. Para enfrentar algunos de estos problemas se han investigado algoritmos o formas que minimicen el error en los reconocedores automáticos de voz y mejoren su desempeño [3].
Normalmente el reconocimiento automático de voz no había sido tan utilizado en los sistemas orientados a la educación y menos hacia niños debido a la falta de investigación en esta área. El laboratorio de tecnologías de voz TLATOA de la Universidad de las Américas-Puebla en conjunto con el CSLR* de la Universidad de Colorado en Boulder han trabajado en la versión en español de tutores inteligentes y libros interactivos para mejorar la lecto-escritura en escuelas públicas [4], los cuales utilizan tecnologías de reconocimiento de voz para niños.
Se requiere desarrollar un reconocedor de voz de niños que sea utilizado en este tipo de sistemas. Para lograrlo se debe escoger un conjunto de herramientas que nos permitan investigar y desarrollar reconocedores de voz utilizando técnicas para mejorar el desempeño en la tarea de reconocimiento de voz.
Actualmente no existe un reconocedor de voz de niños para el español hablado en México. Sistemas de lecto-escritura como los tutores y libros interactivos necesitan modelos de reconocimiento de voz para llevar a cabo algunas tareas como, dar seguimiento de la posición y ubicación de lectura de un niño, asistir detectando o leyendo errores, y para proveer información la cual puede ser usada para medir la fluidez de la lectura.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Los objetivos generales de esta investigación son:
* Desarrollar un reconocedor de voz que sea robusto.
* Presentar técnicas viables que ayuden a mejorar el reconocimiento de voz en niños en sistemas de lecto-escritura.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En esta tesis se ha construido un nuevo reconocedor de voz de niños utilizando SONIC. Además, se ha mejorado el nivel de reconocimiento del experimento base, aplicando métodos de adaptación y normalización.
Las limitaciones de este trabajo dependieron de la disponibilidad y calidad del corpus de voces de niños (problemas con las transcripciones o errores en la grabación) [5], de acuerdo a este último punto se decidió el porcentaje de locutores a utilizar para el entrenamiento y evaluación de los experimentos. Debido a la complejidad de algunas de las técnicas para mejorar reconocimiento de voz sólo se utilizaron algunas para nuestros experimentos.
Para estos experimentos, podemos decir que implementando enfoques como agrupamiento de clases nos puede llevar a ciertas mejoras en el desempeño. Se ha investigado y corroborado también el uso de la normalización de la longitud del tracto vocal (VTLN) en niños, obteniendo un porcentaje de error por palabra de 42.6% y una reducción del error de 8.76% respecto al experimento base. Para el caso de adaptación se han investigado dos métodos, el primero utiliza la estimación de las matrices para maximizar la probabilidad de los modelos transformados generando los datos de adaptación. Y una técnica de adaptación de los modelos ocultos de Markov usando regresiones lineales basadas en la estimación de la probabilidad máxima a posteriori.
En los experimentos de las regresiones lineales de probabilidad máxima (MLLR) se han obtenido mejores resultados que en la evaluación de los modelos base y los modelos normalizados con VTLN. El mejor porcentaje de este experimento fue de 41.2% y la reducción del error relativo con respecto a los modelos base en este experimento fue de 10.68%.
Para los experimentos desarrollados con la técnica de regresiones lineales de probabilidad máxima a posteriori (MAPLR), los resultados han sido los mejores en el desempeño general de los modelos desarrollados para esta tesis. La idea principal de este enfoque es restringir los valores de los parámetros de transformación usando una distribución previa, obteniendo así un porcentaje de error por palabra de 31.9%, mostrando una reducción del error relativo de 22.94% respecto a los experimentos base.
Las principales contribuciones de esta tesis son:
* El desarrollo de un reconocedor de voz de niños utilizando SONIC.
* Reducción de los porcentajes de error de palabra en cada uno de los experimentos con respecto a cada uno de los experimentos base, utilizando técnicas de adaptación y normalización para un reconocimiento robusto de voz usando SONIC.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Implantación de un medio electrónico de información y difusión para una institución educativa (Página Web de la DGEMS de la U. de C.)</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Facilitar la difusión de la información básica de la DGEMS de la U de C. y de sus bachilleratos.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un medio electrónico de difusión e información del nivel medio superior de la
Universidad de Colima.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Este proyecto es necesario para satisfacer las necesidades informativas y de difusión del nivel
medio superior de la U de C y las necesidades informativas del mismo nivel en el estado, a través del
uso de Internet y componentes multimedia.
La Universidad de Colima, cuenta con 31 bachilleratos repartidos en todo el estado y una
escuela de enfermería coordinados por la DGEMS.
Este proyecto se justifica por la falta de mecanismos apropiados para la difusión y promoción
del nivel medio superior de la Universidad de Colima. La justificación más amplia se puede encontrar en
un apartado de esta tesis llamado Definición del Proyecto.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este proyecto de tesis se resolvió el problema aplicando tecnología, la elaboración de
páginas web para promoción y difusión de una institución educativa, resultó ser una técnica muy
eficiente y con grandes ventajas, en esta investigación se elaboró la página web de la DGEMS teniendo
como resultado una página fácil de navegar (utilizar) en donde se puede encontrar información
relevante de la Dirección General y de los bachilleratos universitarios.
La realización o programación de un sitio en Internet, resulto ser una tarea laboriosa, tenemos
que tomar en cuenta muchos factores para llevar a cabo la elaboración de un sitio o una pagina web,
que resuelva eficientemente una problemática
Entre las conclusiones personales considero que para elaborar un sitio web eficiente se deben
tomar en cuenta los siguientes seis factores:
1. Definir los objetivos de la página
2. Planificar adecuadamente lo que se desea lograr
3. Formar un equipo de trabajo adecuado
4. Identificar la audiencia
5. Tener una descripción concisa de la información que se quiere mostrar
6. Contar con las herramientas necesarias (hardware y software)
El objetivo principal de la página se debe tener claro, y con base a éste, definir los objetivos
específicos. Cuando surgió en la DGEMS la necesidad de tener un medio más eficaz para la promoción
y difusión, se me consultó de cual era el medio más adecuado para este fin, llegué a la conclusión de
que por medio de Internet y a través de una página web se podría solucionar este problema, y es por
eso que el objetivo principal y los objetivos específicos se mostraron con bastante claridad.
La planificación, es el punto más delicado en la realización de cualquier proyecto, se debe tener
muy claro lo que se desea lograr, en este proyecto tomó un tiempo considerable llevar a cabo la planificación (aproximadamente 2 meses), principalmente se debió a que la elaboración de una página
web no es una tarea sencilla.
Cabe hacer mención que aun con la planificación, este proyecto tuvo varios retrasos en tiempo,
los resultados finales fueron satisfactorios, pero considero que el proyecto duró más tiempo en la
investigación, elaboración y publicación de resultados, que el que debió de haber tenido. Se detectaron
varios problemas e inconvenientes, éstos se puntualizan más adelante.
En un proyecto de esta magnitud se debe de contar con el equipo humano adecuado, para este
proyecto se contó con la participación de un equipo multidisciplinario, en el cual se incluyen: pedagogos,
comunicólogos, tecnólogos. El problema surgió cuando en ocasiones los especialistas de ciertas áreas,
ignoraban conceptos básicos en el uso de la tecnología como lo son: Internet, correo electrónico,
hipertexto, links, etc. Esto ocasionó, diversas discusiones en las que no se podía aterrizar o dar a
entender una idea o concepto.
Considero que al momento de crear un grupo multidisciplinario para la creación de un sitio o
página Web, es necesario, que a través de los especialistas en las tecnologías de información se
genere un contexto o ambiente que informe, enseñe o aclare ciertos conceptos básicos en el uso de las
tecnologías de información. Como sugerencia en este aspecto, se debe considerar la realización de un
curso de Internet al grupo de trabajo del proyecto que se desee realizar.
La identificación de la audiencia es tan necesaria como la realización de los objetivos, se debe
tomar en cuenta algunas interrogantes, como son:
-  ¿Quién usará la página?
-  ¿Qué problema trata de resolver quien consulta las páginas?
-  ¿Puede ver el usuario la página en diferentes plataformas?
-  ¿De dónde se puede conectar el usuario?
-  ¿Se actualizará constantemente la página?
En la realización de la página Web de la DGEMS, se tomaron en cuenta la mayoría de estos
aspectos, teniendo como resultado una descripción de la información que se muestra en la página. La
investigación histórica y documental se realizó minuciosamente, por lo que el usuario obtendrá con
facilidad la información que requiere.
Uno de los aspectos que generó mucha controversia en el desarrollo y evaluación de la página,
fue el diseño gráfico del sitio. Se generaron varias plantillas de diseño para su valoración, y mientras
algunas personas consideraban cierto modelo, a otras no les gustaba y viceversa. En esta experiencia
aprendí que el diseño es muy subjetivo, lo que a alguien le gusta puede que a otra persona no le guste,
mi recomendación es:
- Buscar una persona especialista en diseño gráfico para la integración del grupo de trabajo, de
preferencia una persona que sepa realizar diseños en Internet y en sitios Web
- De no contar con un especialista en el diseño, (este proyecto no contó con un especialista en
diseño gráfico ni en diseño Web) se deben visitar páginas Web ya elaboradas, navegarlas,
identificar los elementos gráficos, ver la gama de colores que utilizan así como sus
combinaciones, comparar páginas de diferentes instituciones educativas. Todo esto generará
una idea más amplia de cómo se debe diseñar una página web tomando en cuenta los
siguientes aspectos:
o Comprender el medio
o Establecer una jerarquía visual
o Ser sobrio y directo
o Manejar adecuadamente la distracción gráfica
o Ser consistente y uniforme
En conclusión a este punto puedo decir que en la página Web de la DGEMS, a pesar de no
contar con un experto en diseño, se realizó un sitio con estilo propio, considerando los principios
generales como son: jerarquía visual, uniformidad, atractiva, sobria y fácil de utilizar.
Contar con el equipo adecuado (hardware y software) facilita enormemente la realización y el
desarrollo de la creación de un sitio Web, en la DGEMS no se contaba con el equipo adecuado, por lo que se generó otro problema. Para la solución se adquirió el equipo adecuado así como el software
necesario para poder realizar la edición y elaboración del sitio, pero esto no fue fácil y retrazó la
finalización del proyecto. Sugiero que durante la planeación de cualquier proyecto de esta índole se
tome en cuenta el equipo que se tiene y verificar, si éste es el idóneo para la realización de páginas
Web.
La página desarrollada en este proyecto ofrece las funciones básicas para el funcionamiento
adecuado del sitio, esta permite a cualquier persona navegar y encontrar información sin ningún
problema, ya sean estudiantes, profesores, directivos administrativos o cualquier persona que desee
conocer los bachilleratos de la Universidad de Colima. Como cualquier sitio Web se pueden realizar
modificaciones para mejorarla.
Entre las mejoras que se pueden realizar están:
- Generar una mayor interactividad entre el usuario y la página.
Dentro de la página de la DGEMS la interactividad que existe es muy poca, solamente el
apartado de preguntas y sugerencias que se realiza a través del correo electrónico. Para la
realización de una página más atractiva se puede incluir encuestas, foros, chat y un sinnúmero
de opciones que hacen de una página web un sitio interactivo.
- Búsquedas de información
Para encontrar con facilidad la información referente a un tópico en específico es necesario
incluir un apartado de búsquedas de información, también se pueden generar búsquedas de
profesores, especialistas o personal que laborará en los bachilleratos universitarios o en la
DGEMS.
- Resolución de pantalla
La página de la DGEMS fue diseñada para una resolución de 1024 x 768 pixeles, si el usuario
cuenta con otra resolución como por ejemplo 800 x 600 píxeles, el diseño se muestra un poco
desfasado, y la página puede parecer menos atractiva. Se recomienda utilizar en el diseño una resolución más estándar y generar una página que se adapte a la resolución que el usuario
tenga en su computadora.
- Evaluación en otras plataformas
La plataforma que se utilizó para la generación de la página de la DGEMS fue la diseñada por la compañía Microsoft: Internet Explorer, sabemos que ésta es la más popular, pero esto no quiere decir que no debamos de probar la página dentro de otras plataformas como lo es el Netscape.
- Creación de un Apartado para Alumnos
Una de las ideas principales para la actualización de esta página es la creación de un portal para alumnos. Este permitirá llamar más la atención de los alumnos, en este apartado se
pueden realizar encuestas, foros, chats. También se sugiere incluir juegos didácticos así como reactivos de las diferentes materias del tronco común.
Las modificaciones y actualizaciones de una página Web, deben de ser constantes, se sugiere también, cada determinado tiempo hacer una modificación en el diseño de la página, esto para que no se estanque y quede en el olvido.
Como ya se comentó dentro del capitulo de desarrollo, la página tuvo aprobación institucional, pero esto no quiere decir que la página se dé a conocer por sí misma, se debe de realizar una campaña de publicidad acerca de la página Web de la DGEMS entre los alumnos, maestros, directivos. La publicidad del sitio Web es un aspecto fundamental para el logro de los objetivos de cualquier proyecto de este tipo.
La educación y las tecnologías de información cada vez están mas ligadas, el desarrollo de una página Web con fines educativos es una clara muestra de esto, debemos considerar el que cada día se siga avanzando más en este proceso de inducción de la tecnología a la educación, sabemos que no es fácil, requiere de gente calificada y de la colaboración de toda la comunidad educativa, es por eso que sugerimos la creación de más sitios Web que puedan influir en el desarrollo integral del estudiante, la Universidad de Colima, institución que cuenta con apoyo en tecnologías de información, nos ofrece la posibilidad de día a día ir generando una cultura informática. La Facultad de Telemática cuenta con personal e infraestructura adecuada para este desarrollo, y las escuelas, facultades, bachilleratos así como dependencias administrativas pueden generar proyectos en donde los estudiantes a través del servicio social y prácticas profesionales puedan ayudar a la creación de nuevos espacios virtuales para la generación y publicación de información y conocimientos.
Dejo entonces a consideración de alumnos, profesores y personal directivo de la Universidad de Colima, esta página de la Dirección General de Educación Media Superior, para su utilización en cualquiera de los ámbitos educativos que crean convenientes, y para la comunidad en general un claro un ejemplo de lo que se puede hacer utilizando nuevas tecnologías para generar un espacio de información y difusión para una institución educativa</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
Facultad de Telemática
Implantación de un medio electrónico de
información y difusión para una
institución educativa
(Página Web de la DGEMS de la U. de C.)
Tesis
Que para obtener el grado de
MAESTRO EN CIENCIAS
ÍREA TECNOLOGÍA Y EDUCACIÓN
Presenta:
Jorge Alejandro Ochoa Grajales
Asesor:
M.C. Víctor Castillo Topete
Colima, Colima, noviembre de 2001</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Propuesta de innovación de los sistemas y equipos de datos, voz y video para un centro de investigación del I.P.N3"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>I.- OBJETIVO 
 
El rápido desarrollo de los equipos de cómputo y telecomunicaciones es sorprendente en el presente,
así un equipo adquirido hace sólo un par de años o algo más, resulta obsoleto, o al menos poco capaz,
comparado con lo que se ofrece en el mercado especializado al momento de su comparación.

Aquí se propone una valoración e innovación  del equipo existente en un Centro de Investigaciones del
Instituto Politécnico Nacional, ubicado en una pequeña población del noroeste de Michoacán, y que
debido a la implementación de estas tecnologías se mantiene en contacto estrecho con sus instancias
administrativas y superiores jerárquicas sin merma por la lejanía geográfica, así como, a través del
acceso a Internet mantiene intercambio de información y mensajería con otras instituciones y
especialistas. También la presentación de un documento técnico de administración para el mismo
equipo.

II.3 Objetivos Particulares 
 
1) Propuesta técnica de actualización del equipo.

Proponer con argumentación técnica, la alternativa de substitución de equipo modular existente, por 
componentes superiores en calidad y rendimiento

2) Elaboración de la documentación normativa a la operación regular y en contingencias, para los
equipos y los servicios de telefonía, transmisión - recepción de datos y la recepción de vídeo por
frecuencia satelital. Normalizar la operatividad del equipo con documentación suficiente para respaldar el funcionamiento imperecedero y sostenido de los servicios de voz, datos y vídeo; en concordancia a
las recomendaciones de la ITU-T (International Telecomunications Union, antes CCITT)</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El equipo actualmente operando en este centro de investigación fue instalado en los años 1992-93 y ha
siete años de su implementación, se considera que aunque funcional se tiene una obsolescencia por la
aparición de mejores componentes modulares. Por otro lado, no existe una ‘carpeta técnica’ con las
especificaciones y procedimientos fundamentales de operación del sistema y del  equipo, y de los
procedimientos en caso de contingencias. Una de estas contingencias que ejemplifica los retos a
presentarse es la problemática del impacto del año 2,000, y colateralmente la incorporación a Internet 2,
todo esto tanto en el software como el hardware existente, además de las frecuentes fallas o
intermitencias de enlace y el desperfecto ya permanente de un multiplexor a nivel de la conexión central
(multipunto).</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>II.8 Metodología  
 
Creación y conformación de las ideas en los cursos de Seminario de tesis y desarrollo de proyecto de
telemática del programa académico de la Maestría correspondiente.

Acopio de información para los casos de estudio, situación actual y  propuestas por acción personal en
las sedes correspondientes, adquisición de materiales relativos y viajes de traslado a ubicaciones de la
información, hablamos prácticamente de ubicarnos en Colima, Col., Jiquilpan, Mich. y Ciudad de
México.

La escritura del presente documento, la conformación de la carpeta técnica y el CD que contenga toda
la información que se acopia es el resultado conclusivo y final de esta propuesta. </Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>5.1 Conclusiones
 
La propuesta del presente trabajo tiene dos vertientes:

La primera es señalar, describir y ubicar las circunstancias y estado de los servicios que existen  en una
determinada dependencia del sector público en un área foránea y específicamente del ramo educativo y
de investigación aplicada en apoyo a la región donde se encuentra. Hacer un comparativo con otra
institución de vanguardia y con características similares para derivar en propuestas de mejoramiento y
remediar la obsolescencia y el rezago tecnológico.

La segunda es conformar un documento de utilidad administrativa y técnica para el encargado en turno
de los sistemas de datos y voz, que no es necesario argumentarlo, se han vuelto indispensables para la
buena y eficiente marcha cotidiana de todas y cada una de las actividades tanto de nivel protocolario o
burocrático – administrativo, como de docencia, búsqueda e intercambio de información y comunicación
remota.

A continuación de manera puntual y objetiva se marcan las conclusiones que generan los hechos y
perspectivas encontrados en el desarrollo del presente trabajo:
 
            I. El actual enlace satelital tiene una vida activa de servicio de mas de seis años, lo que lo
            acerca a una inminente conclusión de su funcionalidad, específicamente por el uso y
            estado de la tecnología del equipo. 

            II. Este enlace satelital en su momento fue la alternativa idónea al entorno de ubicación 
             
            geográfica del Centro de Investigación, una población pequeña, de menos de 100,000
            habitantes y en una zona semi – rural y de índice de marginación medio; con servicios de
            comunicaciones en general de calidad media y media baja. Pero no lo es en el presente. 

            II. Se propone el cambio del medio de enlace punto multipunto (Centro foráneo – Dirección
            de Cómputo y Comunicaciones del IPN), de satelital a terrestre y suministrado por una
            compañía de las denominadas Telcos, pues existe la infraestructura necesaria (fibra
            óptica, centrales telefónicas próximas, otras instituciones locales: públicas y privadas
            demandando el servicio), oportunidad que se aproveche para incrementar un ancho de
            banda ya insuficiente y que pudiera abarcar transmisiones de videoconferencia. 

            IV. Se sugiere tratar directamente con la compañía UniNet; por experiencia personal e 
            informaciones obtenidas, Telmex no es la opción práctica de principio, sólo como
            proveedor del enlace de suscriptor o abonado al servicio. 

            V. El equipo existente en ambos extremos del enlace punto – multipunto, el cual consta de 
            módems, multiplexores, concentradores, conmutadores de datos y voz; de un alto costo
            en dólares, no necesariamente se deberá sustituir totalmente, se sugiere adquirir en stock
            o póliza de servicio con proveedores directos y serios de los fabricantes entre los que
            podemos mencionar: Cisco, New Bridge adquirida por Nortel Telecom y después por
            Alcatel , y 3Com,  (se sugiere además la compañía Cabletron). 

            VI. El crecimiento de la demanda de conexión de red LAN y WAN, así como a INTERNET de 
            este Centro de Investigaciones a incrementado de un 400 a 500 por ciento desde su
            instalación en 1993 de ocho computadoras IBM con tecnología de procesado 80286 y un
            servidor de la misma marca con un procesador 80386 y DD de 1.2 Mb. 

            VII. El rezago existente no es sólo a nivel del Centro, por si mismo, la Institución en general 
            (IPN), se ha visto superada de manera preocupante dada su participación protagónica en
            el Proyecto nacional de INTERNET 2, esto se comprueba de la manera más próxima al
            observar el proyecto llevándose a cabo por la Dirección General de Estudios
            Tecnológicos e Industriales (DGETI), de la misma Secretaría de Educación Pública;
            quienes cuentan con enlaces de tipo E1 a Institutos Tecnológicos clave, para de esos a
            su vez enlazar otras dependencias  como Centros de Bachillerato y otros institutos en los
            alrededores, contando con el soporte de UniNet y TelMex, y adquiriendo los equipos necesarios como tarjetas de bajada, concentradores y conmutadores y servidores y
            software encaminado al uso en red de bases de datos distribuidas y arquitectura cliente –
            servidor.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad  de  Colima. 
Facultad  de   Telemática
 "PROPUESTA DE INNOVACIÓN DE LOS SISTEMAS Y EQUIPOS DE DATOS, VOZ Y VÍDEO, PARA UN CENTRO DE INVESTIGACIÓN DEL I.P.N."

Tesis 

Que para  obtener el grado de: 

Maestro en Ciencias, Área Telemática
 
PRESENTA
 

 
Biól. JUAN MANUEL CATALÁN ROMERO

 
ASESORES
 
M. C. OMAR ÁLVAREZ CÁRDENAS
M.C. MARGARITA MAYORAL BALDIVIA 
Colima, Col.,  Junio  de  2001.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"LA HIPERMEDIA COMO ALTERNATIVA DE ENSEÑANZA DEL IDIOMA INGLÉS EN LA ESCUELA ENRIQUE CORONA MORF-N2"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Actualmente se vive en un mundo globalizado en constaste revolución
tecnológica, grandes avances en ciencia y educación se hacen presentes cada día
más. Ahora, las escuelas de cualquier nivel pueden apoyar su labor con e auxilio de
las computadoras.

Los avances tecnológicos ponen al alcance, multitud de herramientas que
multiplican notablemente las posibilidades en el diseño de material didáctico y la
mejora de la educación, que cabe aclarar, el auxilio de las computadoras no está en
contradicción con la enseñanza tradicional.

La computadora ofrece un sinfín d opciones para el acceso a la información,
de forma inmediata la comunicación se hace presente. La red mundial WWW, a
través de protocolos como ftp, http, gopher, etc. Facilita el acceso e intercambio de
información desde cualquier parte del mundo. Existen además entornos diseñados
para crear redes virtuales de comunicación (Moo, Mud, etc.); asimismo, la conexión
vía red tiene alcances mayores al simple intercambio de datos, ofrece verdaderas
líneas abiertas a la comunicación oral y escrita, tales como: correo electrónico, listas
de distribución, video conferencia, audio conferencia, etc.

Para efectos de este trabajo, el propósito es resaltar las ventajas que implican
el uso de las herramientas tecnológicas en el aula y en la enseñanza de idiomas.
Son abundantes los cursos de idiomas que utilizan material audiovisual, se puede
decir que este tipo de enseñanza ha aprovechado el avance tecnológico desde sus
inicios. No obstante, los grandes logros en computación permiten ser optimistas en
cuanto a la ampliación de estos recursos. 

A través de la computadora el profesor y el alumno dispondrán de la imagen y
el sonido en un entorno manejable y flexible donde información y comunicación se
enlazan, multiplicándose así las posibilidades de la enseñanza tradicional. Es a este 
entorno al que hemos dado en llamar "educación asistida por computadora" (EAC), 
concepto que se refiere a multitud de sistemas diferentes y de métodos de trabajo 
que van más allá de los sistemas CALL tradicionales (del Inglés, Computer Aided
Language Learning). 

Se ofrece una posible definición de la EAC, se orienta sobre su aplicación en
distintos entornos y sus ventajas en la enseñanza y el aprendizaje de lenguaextranjeras, y por último, se describen algunos sistemas y proyecto existentesapuntando a futuras líneas de investigación.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>1. Difundir los apoyos multimedia y su uso en el ámbito educativo. 

2. Proporcionar a los docentes diferentes experiencias sobre los usos didácticos 
de las NT.
 
3. Demostrar la factibilidad de que una aplicación multimedia puede servir como
apoyo didáctico a la materia de Inglés</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Haber cursado en la máxima Casa de Estudios La Universidad de Colima, 
la Maestría en Ciencias Área Tecnología y Educación y de vivir en mi propia
persona un aprendizaje significativo a través del uso de la tecnología, aunado a la
experiencia de desempeñarme como docente de la materia de inglés en el nivel
secundario; son los motivos principales que arrojaron la propuesta plasmada en
esta tesis. 

Actualmente la materia de inglés como segunda lengua en secundaria, se
aborda bajo un modelo de enseñanza eminentemente tradicional. El maestro y el
texto son las fuentes de todo conocimiento y saber; copiar, repetir y memorizar 
son las principales actividades que el alumno realiza en sus clases. 
Con la cantidad de avances tecnológicos que  se tienen en la actualidad,
surge en la memoria el recuerdo y la vivencia de lo aburrido que es el transcribir
en la libreta el material escrito en el pizarrón, vaciar del libro de texto las lecciones,
conversaciones, ejercicios, debido a que el libro no se puede rayar porque hay que
usarlo el siguiente año escolar. Claro que hay maestros que a pesar de las
limitaciones que existen buscan la forma de estimular la creatividad y facilitar el
aprendizaje. 

La implantación en la sociedad de las denominadas "NT" de la
comunicación e información, está produciendo cambios insospechados respecto a
los originados en su momento por otras tecnologías, como fueron en su momento
la imprenta, y la electrónica. Sus efectos y alcance, no sólo se sitúan en el terreno
de la información y comunicación, sino que lo sobrepasan para llegar a provocar y
proponer cambios en la estructura social, económica, laboral, jurídica y política. Y
ello es debido a que no sólo se centran en la captación de la información, sino
también y es lo verdaderamente significativo, a las posibilidades que tienen para
manipularla, almacenarla y distribuirla.  

Sin lugar a dudas, estas denominadas NT crean nuevos entornos, tanto
humanos como artificiales, de comunicación no conocidos hasta la actualidad y
establecen nuevas formas de interacción de los usuarios con las máquinas donde
uno y otra desempeñan roles diferentes, a los clásicos de receptor y transmisor de información y el conocimiento contextualizado se construye en la interacción que
sujeto y máquina establezcan.

Los maestros, comprometidos con el aprendizaje de sus alumnos desean lo
mejor para ellos. Esta propuesta es una iniciativa para propiciar los cambios que
se requieran: en el  currículum, la docencia y los alumnos; con el objetivo principal
de facilitar el aprendizaje del idioma inglés. 

Las dificultades o deficiencias encontradas deben considerarse como parte
de un todo: de la necesidad de contar con un ambiente conducente al aprendizaje.
Parte de ese ambiente es el acceso a la tecnología. Aunque no se debe pensar
que con esto se encuentra una solución fácil y rápida a los problemas de 
aprendizaje en la escuela como cuando se compran pizarrones o pupitres nuevos. 
El uso de las NT en el salón puede ser una valiosa herramienta para el
aprendizaje del Inglés así como para otras materias, además de una fuente que
motive a los alumnos a amar la experiencia de aprender una nueva lengua. Cierto
es que por sí solas no harán ninguno de los cambios anteriores, la tecnología no
sustituirá al maestro. Pero serán un elemento que provocará se cambie su rol en
el aula para beneficio de los estudiantes.

La intención es motivar hacia nuevas experiencias de aprendizaje a las
instituciones de educación pública, en caso concreto en el estado y ciudad de
Colima en la Secundaria Enrique Corona Morfín. Se trataría de la escuela pionera
en incorporar las NT, otorgando a los estudiantes los beneficios con que se cuenta
en la educación privada. Es importante mencionar, que dicha propuesta no será el
único remedio para resolver los problemas educativos contemporáneos, pero sí
una estrategia para lograr un cambio global. 

Para una ubicación correcta dentro del contexto a analizar, es necesario
repasar un par de conceptos claves sobre hipermedia.  Que tal como se emplea
aquí, es un modelo de diseño de programas multimedia que se caracteriza por
organizar la información en pequeños paquetes con significado completo, de
diferente nivel de complejidad, unidos mediante enlaces que permiten navegar
coherentemente a través de los paquetes, siguiendo una idea o una línea lógica
de razonamiento. Cada paquete puede integrar información gráfica, textual,  
audiovisual, etc. y la navegación se realiza también por entre los diferentes
soportes. 

Los hipermedia son utilizados en diseño curricular con un planteamiento
muy similar a las enciclopedias, como fuentes de información que permiten
acceder a lo que resulta relevante de acuerdo con una guía o propuesta de trabajo
previa. Los hipermedia como recursos para el aprendizaje independiente no
siempre son un recurso adecuado. Ya desde 1989 en una investigación realizada
en la Universidad de Barcelona (Bartolomé, 1993) se comprobó que la hipermedia
resultaba poco adecuado para principiantes ("novices") ya que su falta de
conocimientos y experiencia en el campo les dificultaba la toma de decisiones 
inherentes a la navegación. 

Existen bastantes experiencias de aplicar programas hipermedia a
situaciones concretas de aprendizaje. Patricia Carlson utiliza un hipertexto para
crear un "espacio de trabajo inteligente" en el que los alumnos pueden desarrollar
sus habilidades literarias (Carlson y González, 1993). En otros casos se mezclan
diferentes modelos aunque en ocasiones más bien nos encontramos con un
auténtico desconocimiento del medio, como en un programa presentado en una
conferencia internacional que se autodenominaba "tutorial hipermedia" y que luego
se describía como un "estudio de casos". 

Hoy en día, el hipermedia por excelencia es el World Wide Web. Este es el
auténtico hipertexto por excelencia, en el hiperespacio creado en Internet, en el
que es posible navegar sin restricciones. Por ello, en un CD-ROM o un DVD más
bien lo que encontramos son multimedia que utilizan un diseño hipermedia, pero
con contenidos limitados. Es difícil resumir las posibilidades del Web con relación
al aprendizaje y la Educación, pero no deben ser olvidadas por ningún educador.
En cuanto a los programas hipermedia en CD-ROM pueden ayudar de forma
similar a las enciclopedias, especialmente para trabajos específicos en temas
concretos, en el marco de un diseño curricular más amplio.

Los documentos hipermedia integran multimedia e hipertexto, con la
característica importante de la interactividad; multimedia es la combinación de
distintos contenidos infográficos (informaciones de diversa naturaleza como texto,  
imágenes, animaciones, sonidos, video, etc.), mientras que el hipertexto es una
estructura de la información de carácter no secuencial.

Debido a su naturaleza, los documentos hipermedia pueden ser una fuerte
herramienta para facilitar el aprendizaje significativo de los estudiantes, pues la
opción de presentar al alumno los contenidos de una forma atractiva con la
adecuada combinación de colores, ilustraciones y animaciones, consigue
automáticamente la atención y el interés del estudiante. 

Numerosas experiencias tecnológicas han buscado diseñar aplicaciones
formativas apoyadas en la informática, y diversos los paradigmas que se han
utilizado para llevar a cabo desarrollos informáticos de apoyo al aprendizaje. La 
implementación de las NT, cada vez más sofisticadas, han permitido integrar la 
multimedia y el hipertexto, obteniendo las aplicaciones hipermedia, en las que la
información de múltiple formato (texto, imagen, sonido...) se encuentra organizada
en pequeños paquetes de diferente nivel de complejidad, unidos mediante enlaces
que posibilitan la navegación de forma coherente a través de ellos, para ir en
busca de cualquier tipo de información, y en el formato que se desee. 

También como enuncian los diferentes eruditos el cerebro es un
“procesador múltiple” de información.  Al recibir “un dato” de información el cerebro
inmediatamente inicia la construcción de conexiones cerebrales que involucran a
millones de neuronas en este proceso, estableciendo nuevas redes o senderos
neuronales. Esta adquisición de datos es fragmentada en pequeños trozos de
información que forman parte de otros más grandes, rápidamente se combinan y
recombinan después de haber encontrado elementos y patrones similares
provenientes de experiencias anteriores, y son almacenados en forma permanente
en las redes neuronales que contienen la información 'relacionada', garantizando
así la disponibilidad de la nueva información para ser empleada en el futuro. Por el
contrario, una presentación de información lineal y secuencial, detallada y
fragmentada es de hecho "aburrida" para el cerebro.  

Como se puede observar, el aprendizaje humano no sigue una trayectoria
lineal, sino que trabaja asociando ideas, reconociendo patrones y recordando
trayectorias, de ahí que los sistemas hipermedia integren las potencialidades de las tecnologías hardware y software con objeto de presentar la información de una
forma más próxima a la que emplea nuestra mente para procesar cualquier tipo de
datos, de ahí que una de las prioridades a la hora del diseño de producciones
hipermedia se centre en dotarle de interactividad, para propiciar la gestión, el
acceso y la libre navegación por la información contenida. 

Hoy en día, la mayoría de las aplicaciones hipermedia están
fundamentadas en el aprendizaje cognitivo, superan los modelos conductistas de
los tradicionales E-A-O, y gozan de una gran aceptación, debido a que sus
características contribuyen a la optimización del aprendizaje mediado por la
computadora. Su potencial interactivo, la posibilidad de acceso a grandes bases 
de información multimedia; junto a la representación asociativa del conocimiento, 
que pretende emular la forma de procesamiento de la información del aprendiz,
son factores que pueden favorecer el proceso de aprendizaje (Beltrán, 1992). 

Como se ha señalado, los sistemas hipermedia han contribuido a cambiar la
concepción del conocimiento y presentan una nueva visión de la realidad y de las
estrategias de enseñanza-aprendizaje que rompen con la unidireccionalidad
tradicional. La conjugación de potentes herramientas, cada vez más intuitivas,
para la creación de entornos de aprendizaje y simulación en donde se combine
audio, vídeo, gráficos, texto e imágenes es ya una realidad que se está aplicando
en el ámbito de la formación tanto presencial como a distancia.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Es necesario definir algunas acepciones para mejorar la comprensión del tema
a tratar y explicar  la metodología que se utilizó, así como también de los
fundamentos teóricos de los cuales parte este proyecto.

Existen diversas teorías del aprendizaje. En los últimos años ha crecido la
preocupación por encontrar el lenguaje educacional y de comunicación propios en el
uso de las NT; y en la búsqueda de esquemas adecuados de presentación de la
información que orienten al alumnos hacia la construcción de su conocimiento.

El Constructivismo es la aplicación a la educación de las teorías del desarrollo
cognitivo. Es un intento por integrar una serie de enfoques que buscan motivar la 
actividad constructiva del alumno en su proceso de aprendizaje. 

El alumno es el responsable último de su aprendizaje, construye el
conocimiento por sí mismo, relaciona la información nueva con los conocimientos
previos para la construcción de su conocimiento, su aprendizaje debe ser
significativo.

El profesor debe ser el orientador que guía el aprendizaje del alumno y  quien
acompañe al estudiante en su construcción del conocimiento considerado como
verdadero.

Los conocimientos adquiridos en un área se ven potenciados cuando se
establecen relaciones con otras áreas. 

Lo anterior aplicado en los entornos multimedia, inicia con los trabajos de
Papert y su relación con lo que fue su materialización tecnológica, el Logo, hacia los
entornos constructivos de aprendizaje (Hannafin, 1992) haciendo hincapié en la
dimensión de actividad educacional (dirigida a objetivos – exploratoria). En las
aportaciones de (Morrison y Collins, 1995) con sus juegos epistémicos que ya
consideran las diferencias culturales y las experiencias contextualizadas en los
modos de conocimiento, nos encontramos con la misma dificultad: no parecen
abarcar con la suficiente amplitud todos los puntos de vista ni la pluridimensionalidad
de los casos concretos que exige el conocimiento experto.  
 
La teoría de la flexibilidad cognitiva propuesta por (Spiro and Cols, 1991) y
desarrollada en posteriores publicaciones, aporta nuevos elementos que permiten
afrontar los objetivos de este proyecto. 
Por estas razones se elige esta teoría y sus postulados como la más viable
para evaluar de los sistemas hipermedia su capacidad para integrar múltiples
esquemas y puntos de vista diversos en la transferencia y adquisición de
conocimientos, motivo principal de este trabajo.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Concluida esta investigación es evidente la escasez de estudios en tecnología,
y los sistemas hipertexto e hipermedia como una herramienta de acceso a la
información, no son la excepción. De cierto modo, es una respuesta natural a su
proceso continuo de evolución y es claro también que aún no existe una
fundamentación teórica lo suficientemente clara. Dicho proceso es complejo, sin
embargo los hallazgos sobre adquisición de conocimiento en hipertexto e
hipermedia, cada vez cobran mayor importancia y por supuesto validez. 

La hipermedia es un apoyo para el aprendizaje, siempre y cuando el uso que
se le de parta de unos objetivos claramente específicos y de una metodología bien
estructurada. Para ello debe considerarse y no dejar de lado, que los recursos
tecnológicos no son el único elemento que interviene en el proceso de enseñanza -
aprendizaje. Se requiere de una metodología adecuada, contar los materiales
necesarios (laboratorios, computadoras), docentes comprometidos con su labor: "el
aprendizaje de sus alumnos", y la condición más importante: orientar al alumno para
que sea crítico, analítico y reflexivo de su propia construcción del conocimiento. 

Los sistemas hipermedia no mejoran el aprendizaje por si solos. El requisito
principal es la capacidad mental con que todos los estudiantes cuentan, seguido de
la guía del profesor como el orientador para la construcción del conocimiento y del
uso adecuado de las NT. La computadora y los sistemas hipermedia, resultan ser
asistentes muy calificados para el proceso de enseñanza - aprendizaje.  

El papel de estos sistemas debe orientarse a facilitar o promover procesos de
pensamiento y el uso de estrategias de aprendizaje cognitivas y metacognitivas, pero
también se debe cuestionar la determinación a enseñar únicamente destrezas
cognitivas. Como señala (Crook, 1998) "La riqueza de la acción inteligente surge
como consecuencia de la participación cognitiva del sujeto en episodios de
interacción organizados en diversos medios culturales”. En este contexto, el rol de
los sistemas hipermedia debe orientarse a estimular el pensamiento, partiendo de 
estrategias diseñadas para que el alumno adquiera destrezas cognitivas. 
 
En cualquier caso, todo esto demanda mucha más investigación que permita
comprobar numerosas hipótesis todavía en el aire. Así, cuestiones como, qué
principios y métodos subyacen a estos sistemas, cómo desarrollar la autorregulación
del proceso de aprendizaje en los alumnos, cómo se puede evitar el sentimiento de
desorientación del usuario, qué procesos cognitivos y metacognitivos subyacen a
estos procesos, cómo diseñar tareas efectivas, etc., requieren de investigaciones que
ayuden a mejorar estos sistemas. 

El campo de las NT abre un sinfín de posibilidades de cara a la investigación y
el efecto que puede llegar a tener sobre la educación incumbe a los educadores
considerarlo a la hora de pensar en la tarea de reestructurar las escuelas. Aunque 
como decía (Papert, 1995): La escuela no llegará a utilizar las computadoras 
“correctamente” sólo porque los investigadores le digan cómo debe hacerlo. Llegarán
a utilizarlas bien (si es que ocurre algún día) como parte integral de un proceso de
desarrollo coherente.

Por otro lado, siguen presentes en las escuelas problemas y limitaciones que
afectan el uso de las computadoras en el aula, tales como su elevado costo y la falta
de presupuesto, y la  capacitación de los profesores y alumnos sobre el uso de la
PC. También es cierto que con el tiempo desaparecerán, pues la competencia entre
fabricantes de computadoras provocan que los costos se abaraten y que los equipos
sean cada vez más completos.

Asimismo, cada día se hace más cotidiano el uso de la tecnología y es más
fácil escribir textos, calcular datos y presentar ideas; con lo anterior, sería más
familiar el uso de las computadoras y profesores como alumnos utilizarán estos
medios para estar a la vanguardia.  

Con relación al papel docente, se cuenta con la responsabilidad de decidir si
aprovechar los beneficios y ventajas que ofrecen las NT o de optar por dar
continuidad al método tradicional o convencional. A la luz de las ventajas e
inconvenientes anteriormente descritos, se justifican ambas opciones, siempre y
cuando responda a una convicción personal del maestro de conformidad con unos
principios y valores superiores representados por el respeto de  la dignidad intrínseca
del ser humano. 

 
Este trabajo se ha centrado en descubrir la potencialidad del hipermedia y en 
aplicar algunos de los postulados de la "Teoría de la Flexibilidad Cognitiva".
Partiendo de esta última surgen variadas cuestiones: cómo los estudiantes navegan
o exploran estos sistemas, qué esquemas siguen, cómo recorren las pantallas, qué
diferencias de resultados hay entre un programa que incluye videos con otro que
incluya imágenes, qué grado de control puede llegar a tener el usuario sobre el
entorno de aprendizaje, cómo influye la interfaz de usuario y el modo de presentación
de la información en el usuario, cómo se puede intervenir en el aula partiendo de los
presupuestos de la Teoría de la Flexibilidad Cognitiva y desarrollando este tipo de 
sistemas.  

El alcance de este proyecto es amplio, pues busca desarrollar un ambiente de
aprendizaje desde las bases del uso de la computadora, que otorgue al maestro (a)
un apoyo para facilitarle el proceso enseñanza – aprendizaje, con la característica de
ser posible su aplicación a diversos niveles escolares y la actualización de
contenidos. Más aún, que pueda ser configurable por maestros y alumnos y
adaptable a las necesidades específicas de enseñanza tanto individual como
colectivamente.  

Un factor sustancial para obtener buenos resultados, es el contar o crear las
condiciones adecuadas para su aplicación. Lo cual implica: concientización de
autoridades educativas, alumnos y docentes; presentación más atractiva para el
estudiante de la materia de inglés, con materiales didácticos que despierten un
mayor interés en su diseño y contenidos. Lo interesante es flexibilizar la formación
del educando, como consecuencia de combinar métodos de enseñanza (tradicional –
tecnológico) y no aferrarse a un solo esquema de trabajo permitiendo al docente
contar con más herramientas y a apoyos en la enseñanza. 

Cabe destacar que la hipermedia no es la panacea para solucionar los
problemas educativos del idioma inglés en la secundaria, pero sí es un instrumento
de apoyo que facilita al profesor organizar y presentar los conocimientos que el
alumno debe aprender, de una manera más rápida y menos vertical en la relación  
entre docente – alumno, que genere esa interacción entre ambos y propicie al
alumno, ser creador en su búsqueda del conocimiento. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA 


 
“LA HIPERMEDIA COMO ALTERNATIVA DE ENSEÑANZA DEL
IDIOMA INGLÉS EN LA ESC. ENRIQUE CORONA MORFÍN” 
T E S I S  
QUE PARA OBTENER EL TÍTULO DE: 
 
MAESTRO EN CIENCIAS 
 
ÁREA: TECNOLOGÍA y EDUCACIÓN 
P R E S E N T A:
L.A.P. CARLOS ALBERTO MONTES CARBAJAL 
 

A S E S O R: 
M.C. PEDRO DAMIÁN REYES

 
 

 
Colima, Col. Febrero de 2004 </Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“LA RADIO COMO TECNOLOGÍA EDUCATIVA: NATURALEZA, USOS, EXPERIENCIAS Y REFLEXIONES" </Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Viejas y nuevas tecnologías. Globalifobias y globalifilias.
Era un día de julio del año 2001. En un diario llamado “EL COMENTARIO"  de
la pequeña ciudad de Colima, México, una nota daba cuenta de la detención de
José Bové5, agricultor francés, mientras protestaba junto con unos 100 palestinos y
franceses por el cierre de una carretera que discurre a lo largo de Cisjordania, el
principal territorio por extensión de Palestina. La detención como es de suponerse
fue realizada por la policía israelí. El periódico explicaba también que los
argumentos que esgrimían los manifestantes consistían en señalar que esa medida
afecta los derechos de los agricultores de la zona, quienes han sido desplazados de
sus tierras.
 ¿Por qué un grupo de franceses reclaman por lo que sucede a miles de kilómetros
de su tierra?...  ¿Por qué la noticia atraviesa Asia, Europa y llega a América para que
un diario de una provincia mexicana de cuenta de ello?...  ¿Por qué la detención de
los manifestantes provoca reacciones en varias partes del mundo?" ¦ La respuesta:
globalización.  ¡Vivimos en la aldea global!  ¡El mundo está globalizado y se lo
debemos a las tecnologías de información y comunicación que han penetrado en
todos los aspectos y procesos de la vida social comunitaria!. O al menos eso se
argumenta en los medios de comunicación, en los discursos, en las conferencias, en
los congresos, en los estudios de expertos, aún cuando la realidad pudiera ser
mucho más compleja y diversa.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Conocer la identidad, evolución, y experiencias de la radio educativa en Iberoamérica.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es la radio una tecnología educativa?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Cuáles son sus características, posibilidades y limitaciones cuando se usa como herramienta pedagógica?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Para qué niveles educativos y áreas del conocimiento es más adecuada?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿En Iberoamérica quiénes han llevado a cabo esfuerzos educativos exitosos utilizándola?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿En dónde y bajo qué condiciones surgieron sus proyectos?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>De motivaciones personales y algunas circunstancias.
La decisión de escoger un tema de investigación tiene que ver con la historia, los
retos y los intereses propios de quien toma la determinación. Las implicaciones y
elementos que intervienen son numerosos: el tiempo del que disponemos, la pasión
que le vamos a inyectar, la maduración del conocimiento que se va acumulando, la
trascendencia y aplicaciones prácticas que le encontramos. Este trabajo es mi grano
de arena en la gran playa en la que me desenvuelvo desde hace varios años de
manera un tanto intuitiva: la radio como herramienta para educar.
En 1996 terminé la licenciatura en Comunicación Social, sin embargo, considero que
no soy sólo comunicadora de profesión. Desde el primer año de la carrera, esto es,
desde 1992, por azares y fortunas del destino me convertí en productora de
programas de radio y televisión educativos. En realidad, amo y disfruto
profundamente el privilegio que tengo cada vez que estoy frente a la gente para
entrevistarla con una grabadora portátil, frente a la computadora para hacer un
guión, delante del micrófono para transmitir un programa. En cada uno de esos
momentos tengo la oportunidad de decir muchas cosas, y sin embargo no puedo
darme el lujo de olvidar que llevo sobre mis hombros la gran responsabilidad de
hablar a nombre de una institución que pretende contribuir en el mejoramiento de los
niveles de vida de la sociedad a quien debe su existencia, la Universidad de Colima.
Buscando obtener conocimientos y experiencias que me permitieran plantear con
bases más sólidas, proyectos de series radiofónicas educativas para la casa de
estudios antes citada, cursé la Maestría en Ciencias, área Tecnología y Educación,
ofrecida por la misma universidad, a través de su Facultad de Telemática. Fue allí
donde, a través de las materias que se nos impartieron y a la bibliografía que nos
recomendaron, me di cuenta que sólo las aplicaciones en multimedia, los sitos web
interactivos, la Internet y sus servicios de comunicación, entre otras herramientas
informáticas, eran consideradas como tecnología educativa vigente, no solo en dicho
plantel educativo, sino en el mundo "moderno". A pesar de ello, elegí como trabajo
de tesis a una "vieja tecnología": decidí seguir trabajando en torno a la radio y su
potencial educativo. En esta decisión intervienen dos motivaciones fundamentales.
Motivación número uno: entre viejas y nuevas tecnologías.
La primera de ellas la sustenta mi convicción personal de no “casarme"  con la idea
de clasificar, dividir y hasta marginar, a la tecnología en “vieja"  y “nueva". La
tecnología educativa es entendida por algunos autores como Ogalde (2000), como el
conjunto de procedimientos, métodos, técnicas, instrumentos y medios, derivados
del conocimiento científico, que al ser organizados sistemáticamente en un proceso
educativo, pueden facilitar y mejorar el camino para conseguir los objetivos de la
enseñanza. Es así como puede considerarse tecnología educativa a un libro, una
revista, un folleto, el pizarrón y los gises, hasta los discos, la radio, el audio, los
carteles, las fotografías, los diaporamas, la televisión, el video, el cine, los manuales,
la computadora, el CD-ROM, el hipertexto, un sito web, un simulador, la realidad
virtual, etc.
El problema surge, como bien lo señala Lion (1999), porque se han ido
incorporando al aula según han sido considerados novedades, (de hecho el pizarrón
es el único sobreviviente generación tras generación, el único permanente),
artefactos ideales para superar los problemas de la educación. No tomamos en
cuenta que cada medio tiene sus propias fortalezas y debilidades, que según sus
características pueden ser utilizados en diferentes modelos y ambientes de
aprendizaje, con diferentes objetivos, en diferentes situaciones, para diferentes
metas. Esto lo olvidamos quizá porque estamos más interesados en alcanzar e
imitar a los países desarrollados, que en estudiar nuestros propios problemas y
necesidades, nuestras características y posibilidades.
Nuestras autoridades educativas no están muy interesadas en conocer, como lo
menciona Castañeda, (1998) los fundamentos psicológicos, pedagógicos y
comunicacionales de cada herramienta didáctica, sólo se piensa en lo que nos da
status, en lo más nuevo, en encontrar un nuevo programa, un nuevo nombre, una
nueva campaña, una nueva tecnología que solucione los problemas de siempre. Si
bien la palabra tecnología aparece de forma constante y casi obsesiva en libros,
ensayos y artículos, discursos y reportes, nos olvidamos constantemente que su
significado etimológico es muy sencillo y esclarecedor: "Téckne" era el vocablo que
los griegos utilizaban para designar aquello que sabían hacer con conocimiento de
causa, cuando resolvían problemas concretos con eficacia. Si a ese término le
añadimos “Logos" , estudio o tratado, encontramos que tecnología no solo se refiere
al saber hacer sino a la reflexión teórica de ese saber.
Motivación número dos: nace una estación de radio en la
Universidad de Colima.
La segunda proviene de una situación que me brinda la emoción y la energía de los
retos nuevos. Después de mucho tiempo de espera, por fin el 23 de noviembre del
año 2000, la Secretaría de Comunicaciones y Transportes otorgó a la Universidad
de Colima el permiso1 para operar una estación radiofónica en el 94.9 de la
Frecuencia Modulada, con 3000 watts de potencia y un indicativo de llamada de
XHELT. La responsabilidad de poner en marcha a la emisora, la cual deberá tener un
perfil educativo y cultural2, recayó sobre un comité designado por el Rector de la
Universidad de Colima, en el que se cuentan a los directores de las diferentes
dependencias del área de comunicación de la casa de estudios y a los trabajadores
de la Dirección General de Radio Universitaria. Es en esta dirección, en donde se
han producido los promocionales y programas radiofónicos institucionales desde
___________________________________________________________________
(1)En México, se otorgan permisos para operar estaciones sin afanes lucrativos a gobiernos de estados,
instituciones educativas y organismos como el Instituto Mexicano de la Radio. Las concesiones se otorgan a los
particulares que deseen operar una emisora con fines comerciales. De ahí los términos “estación permisionada" 
para emisoras culturales, educativas o comunitarias y “estación concesionada"  para las comerciales.
(2) Decisión de las autoridades universitarias, relativamente obvia si tomamos en cuenta que la Universidad de
Colima es una institución que genera nuevos conocimientos, difunde el arte y la cultura, e invierte numerosos
esfuerzos en el ámbito de la docencia.
hace más de 3 lustros, y es ahí donde yo trabajo desde hace 5 años. En lo
personal, participar en la génesis de un nuevo medio de comunicación ha sido una
experiencia muy enriquecedora, he tratado de participar activamente en las primeras
etapas: el diseño de un edificio moderno y funcional, la selección y cotización de
equipo técnico, la planeación y desarrollo de objetivos estratégicos, actividades y
metas, la propuesta de barras programáticas, entre otras cosas. Sin embargo, debo
reconocer que aún falta una de las tareas más importantes, la del diseño de una
propuesta congruente y sólida que permita utilizar a la radiodifusora universitaria
como una herramienta pedagógica que contribuya a ampliar, mejorar o
complementar la oferta de educación formal y no formal3 de la Universidad de
Colima, tal y como lo han hecho ya numerosas instituciones educativas en América
Latina.
Para afrontar esa responsabilidad, no bastan las buenas intenciones y el deseo de
responder a un compromiso. Considero que antes de plantear propuestas y
proyectos, es preciso conocer, comparar, dominar ese ámbito de la tecnología
educativa, estudiar otras experiencias. Para lograrlo, inicio la aventura de conocer el
potencial de la radio para educar, el resultado de este esfuerzo, es lo que el lector
podrá encontrar en el presente documento.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Nos tocó presenciar el arribo de un nuevo siglo, y con él, el inicio de un nuevo
milenio cargado de júbilo y esperanzas. En el 2000 no se acabó el mundo como
algunas creencias populares y profesías lunáticas hacían temer, pero tampoco se
solucionaron problemas añejos como los de la desigual distribución de la riqueza, la
educación y la tecnología entre los seres humanos que habitamos en el planeta. Por
desgracia, algunos incluso se acentuaron, como los nacionalismos y
fundamentalismos que ponen en duda, con su ola de violencia, terrorismo y odio, a
la globalización" ¦  ¿Será que realmente no queremos ser iguales ni formar parte de
una misma aldea, la tan mencionada “aldea global" ?" ¦ Todo parece indicar que
seguirán conviviendo en la Tierra, esas dos tendencias que determinan fronteras de
acción contrapuestas: por un lado, la globalización, que significa atender los
fenómenos, sobre todo del campo económico, de las comunicaciones y la
información más allá del límite de cada nación y cuyas palabras clave parecen ser
libre mercado, tecnología, homogeneización y consumo. Por otro lado, está la que se
empeña en preservar las manifestaciones locales, en entender a cada región y
respetar las diferencias. En lo personal, lejos de apoyar una u otra, pienso que se
complementan. Determinados problemas como la ecología, los derechos humanos,
los medios de comunicación, la educación y la tecnología, pueden y deben
analizarse desde ambos planos, desde lo estrictamente local a lo planetario. De ahí
que la frase “pensar global y actuar local"  me parezca tan adecuada.
En ese contexto, de realidades globales y realidades locales, pero también en el de
las denominadas viejas y nuevas tecnologías aplicadas a la educación, surgieron las
inquietudes para realizar el presente trabajo. Exploré las posibilidades y
experiencias de una tecnología que de tan apropiada y consumida por la población
ya casi no está en la mesa de discusión ni bajo la mirada de quienes pretenden
plantear propuestas “innovadoras"  y “revolucionarias"  que resolverán los grandes
problemas de la educación, en términos de cobertura, calidad y pertinencia, en una región del mundo que no marcha al ritmo que imponen las grandes potencias
globalizadas.
Me arriesgué a realizar un trabajo aparentemente tardío y ajeno a las corrientes
actuales, en las que se apuesta por la computación y las tecnologías de información,
porque estoy convencida de que en el lugar en donde yo vivo, en nuestros países,
ciudades y pueblos latinoamericanos la prioridad es todavía la supervivencia antes
que la inversión en estrategias de desarrollo. La mayoría no usa computadoras
porque todavía no están presentes en su entorno ni requieren de competencias
informáticas en la mayoría de los empleos a los que pueden aspirar. Todos esos
ensayos sobre nuevas tecnologías y educación, las aulas virtuales, los simuladores,
la internacionalización de enseñanza, el aprendizaje masivo a través de la web,
etc., nos muestran nuevas perspectivas y son bastante ilustrativas, pero siguen
dejándome con la sensación de que sólo pueden dirigirse y aplicarse a una población
muy reducida en los países que están en vías de desarrollo. A nivel local, cuando
deposito la mirada en mi estado, en mi pequeña ciudad y los pueblos que la rodean,
me doy cuenta que todavía nos falta mucho por hacer realidad esos sueños, y ésta,
considero, no es una percepción individual y aislada: Zermeño (2000.p.139) nos dice
“[...] veo que gran parte del tejido social de Colima no tiene contacto con las
tecnologías de comunicación e información en el entorno cotidiano" , esto es, nos
enteramos de que las sociedades se están informatizando pero todavía no lo
percibimos ni en nuestro sistema educativo básico, ni en los hogares, ni en la
mayoría de nuestras actividades económicas, laborales y sociales. González J.
(1996.p.78) afirma que
“[...] uno de los principales obstáculos que tiene nuestro país para hacer suya
esa cultura de la información que tanto se proclama en el mundo, no sólo está
en el derecho de tenerla y el acceso a ella, sino que reside en la desigual
distribución de habilidades, los recursos y los conocimientos para poder
generarla y aprovecharla cabalmente."  Aquí es válido recordar la mención que
hace Didriksson (2000) sobre el "síndrome del casillero vacío" que sufre nuestro país y seguirá sufriendo mientras no tengamos la capacidad para darle el valor agregado
del conocimiento a los productos y servicios que podemos ofrecer.
Ahora bien, el presente trabajo y algunas investigaciones de expertos en desarrollo
social, me permiten afirmar que si se nos quita el sueño porque pensamos que en
las nuevas tecnologías de información y comunicación se encuentra nuestro pase
de abordar para ese tren globalizador, es probable que estemos invirtiendo en
ilusiones vanas: cito a Granville (2001.p.3):
“Se dice que a menos que los países en desarrollo y los poscomunistas se
pongan al día en cuanto a la revolución de las computadoras, sus economías
se seguirán atrasando. Es cierto que la integración a la economía mundial
ofrece las mejores esperanzas para el crecimiento. No obstante, la integración
no puede alcanzarse a través de la tecnología informática" . Considero que
nuestros países pobres necesitan estrategias de desarrollo, no un salto gigantesco al
ciberespacio, debemos reconocer que la pobreza y el subdesarrollo son el resultado
de políticas macroeconómicas e industriales, de la distribución desigual del ingreso y
de infraestructuras de mercado inadecuadas. Pero no somos los únicos en
problemas, la brecha digital excluye todavía a la mayor parte de la población
mundial. La demanda interna en los países en desarrollo y postcomunistas se queda
atrás porque la gente generalmente es demasiado pobre para adquirir bienes y
servicios relacionados con la tecnología informática. Granville (2001) acota que en la
India el 57 % de la población no sabe leer y escribir. En todo el continente africano
existen apenas 14 millones de líneas telefónicas, cifra más o menos común en
cualquiera de las grandes ciudades del mundo.
El fracaso de otras curas mágicas para el desarrollo y la educación debería hacer
que nos detuviéramos a reflexionar antes de poner una fe ciega en lo que las
“nuevas tecnologías"  pueden lograr. Confundimos aparatos con conocimientos, con
cambios. Didriksson (2000) nos señala que la educación y el conocimiento son parte
sustancial de las estrategias económicas para alcanzar un nuevo desarrollo.
Organismos Internacionales como la UNESCO, el Banco Mundial, la Universidad de
las Naciones Unidas y organismos regionales y nacionales han puesto a la educación y al conocimiento en el eje de sus propuestas para llevar a cabo las
alternativas que conduzcan a un nuevo desarrollo económico y social, pero no
debemos confundirnos y caer en la trampa de creer que la educación y el
conocimiento cumplirán su cometido únicamente si logramos “modernizarlos"  a
través de computadoras, multimedia e Internet. No considero válido el dejarse
deslumbrar por las telecomunicaciones y la informática y darle la espalda al
aprovechamiento de otras herramientas más accesibles y económicas, que ya están
ahí, que la gente sabe manejar casi en automático. No se trata de tener lo más
nuevo, no se trata de llegar primero y obtener lo que da status. En educación como
en otras facetas de la vida, se trata de aprender a seleccionar y usar lo que
realmente aporte cambios y mejoras cuantitativas y cualitativas.
Tecnologías como la radio, a la cual dirigí la mirada y estudié en el presente trabajo,
están mucho más difundidas, posicionadas, consolidadas y asimiladas entre la
población de nuestras regiones. Su utilización es recomendable siempre y cuando se
emprendan proyectos con conocimiento de causa, habiendo explorado su
naturaleza, sus características, sus fortalezas y debilidades, las experiencias ya
existentes, distinguiendo ciertos modelos y metodologías para su aplicación.
Considero que el presente trabajo ha cumplido con los objetivos que lo gestaron,
quedando demostrado con la teoría y las prácticas de las diferentes instituciones y
organismos cuyas experiencias se describieron, que la radio es una tecnología
educativa sumamente valiosa, siempre y cuando se combine con otros medios y
mecanismos, y se inserte en un conjunto de acciones más amplias, fuera de los
medios, como el compromiso del individuo para el estudio independiente y
autoregulado, la participación comunitaria, el apoyo de fundaciones, gobiernos e
instituciones, entre otras cosas, para que favorezca realmente el proceso educativo
de la región o país en donde se incorpore.
Entre las ventajas claras que este estudio reconoce en la radio cuando se utiliza
como herramienta pedagógica, destacan las siguientes:
 * Elimina distancias y llega a comunidades inaccesibles, aisladas y remotas.
 * Amplía las oportunidades educativas de la población, no solo por multiplicar el
número de “aulas"  disponibles, sino también por apoyar y favorecer el trabajo de
los maestros, alfabetizadores e instructores, sobre todo en aquellas regiones o
países con poco personal con el adecuado nivel de escolaridad y capacitación.
 * Multiplica o acelera el factor tiempo en el proceso de aprendizaje.
 * Aporta continuidad al proceso educativo, mediante su acción diaria e
ininterrumpida.
 * Facilita al auditorio su utilización, por la simplicidad de su manejo.
A lo anterior puede agregársele el reducido costo del aparato radiofónico que lo
pone al alcance de las mayorías, su tamaño cada vez más reducido, que permite
que un trabajador, ama de casa estudiante o campesino puedan transportarlo
mientras realizan otras actividades. Así, el escucha puede realizar paralelamente
actividades manuales e intelectuales que requiera su propio proceso de enseñanzaaprendizaje.
Por otro lado, mientras que el libro y las publicaciones periódicas
requieren una población alfabetizada y acostumbrada a leer, así como con una
capacidad de consumo suficiente y sistemas de distribución extendidos, la televisión
requiere un desembolso mayor y servicios eléctricos, y ya no se diga una
computadora con acceso a Internet, la radio llega en América Latina a todo tipo de
audiencia, y gracias a los transistores, sin necesidad de energía eléctrica, hasta los
lugares más apartados.
Dichas ventajas no implican necesariamente que la implementación de proyectos
educativos que utilicen a la radio sea un asunto sencillo en nuestro país y más
específicamente en Colima. Afirmarlo no sería una conclusión responsable de mi
parte. Estudios como los de González (1996) señalan que si bien nueve de cada
diez mexicanos escuchan la radio, y ésta es un canal omnipresente en la vida de
nuestro país, también, es cierto que es un canal profundamente descuidado para
efectos de políticas culturales y educativas. En radio abierta, esa que se transmite
por ondas electromagnéticas a nuestros aparatos receptores, contamos con una capacidad instalada mejor o igual que la de cualquier país desarrollado, pero su uso
se ha estereotipado al diseño de emisoras y programas orientados a los escuchas
como consumidores de mercancías. México es un país con una enorme tradición
radiofónica, según Velasco (2001) existen ya más de 1200 radiodifusoras, pero
desafortunadamente esa vocación es más de cantidad que de calidad. No bien se
consolidó la radio como invento plenamente acabado, allá por los años veinte,
cuando nuestro país entró de lleno en lo que pronto sería una poderosa industria. En
los inicios tomamos la delantera incluso a naciones que hoy son potencias
económicas como Francia, Japón y otras. Sin embargo, la responsabilidad social
para utilizar a la radio como herramienta educativa se ha centrado en un escaso 9 %,
según el mismo Velasco (2001) de esas 1200 radiodifusoras, las cuales son
administradas por universidades, instituciones y organismos de gobierno. El resto
es meramente comercial y se encuentra en manos de poderosos empresarios.
En estos momentos, en los que la Universidad de Colima está preparando el
surgimiento de una radiodifusora de corte educativo y cultural, debe asumir dicha
responsabilidad, la cual no sólo demostrará el compromiso que tiene con la sociedad
a la que debe su existencia, sino congruencia con sus planes y proyectos
estratégicos, los cuales la han llevado a contar con una amplia oferta de educación
formal, no formal e informal, así como productos y servicios diversos que la
fortalecen: bachilleratos, licenciaturas, posgrados, bibliotecas, un programa de
educación continua, centros de investigación, centros de producción de multimedia,
televisión y video educativos, un periódico de circulación estatal, servicios
telemáticos que facilitan el acceso a la Internet y al correo electrónico, y
próximamente la radiodifusora, demuestran que tiene todos los recursos humanos y
tecnológicos, así como el potencial para estructurar proyectos de educación a
distancia en el marco de un verdadero programa de integración tecnológica.
La experiencias aquí estudiadas demuestran que con menos recursos se pueden
implementar proyectos exitosos y duraderos, es de esperarse que con toda la
experiencia y la infraestructura de la citada casa de estudios pueda crearse un sistema que combine sus numerosos medios, adecuado a sus posibilidades y a las
necesidades de una población con grandes rezagos educativos, que se manifiesta
abiertamente interesada y deseosa de recibir este tipo de apoyo.
Si me atrevo a afirmar lo anterior es porque me apoyo en los resultados28 de la
encuesta que la empresa colimense PROYECTA: encuestas de opinión pública
realizó del 12 al 17 de mayo de 2001, en la zona conurbada de Colima y Villa de
Alvarez, gestionada por la rectoría de la Universidad de Colima. Gracias a dicha
encuesta se pretendía valorar la frecuencia con la que las personas escuchan la
radio, el tiempo que dedican a este medio de comunicación, sus preferencias, su
grado de interés en programas educativos entre otras cosas, para así, poder
justificar la inversión que estaba por hacerse. La empresa aplicó un muestreo
aleatorio sistemático por etapas, seleccionando primero las secciones electorales,
posteriormente las manzanas y finalmente los electores tomando en cuenta la su
proporción. Se realizaron 600 entrevistas en la zona conurbada antes mencionada,
éstas fueron personales y en los hogares. El margen de error de ese estudio fue de
3.5% para un nivel de confianza del 95%.
Los resultados que me parecen más interesantes y relacionados con el presente
trabajo y mis conclusiones son los siguientes:
*	La gente de Colima escucha la radio un promedio de 4.6 horas al día. El 73.4%
afirma escucharla diariamente, el 15.7% manifiesta escuchar la radio varias veces
por semana y solo 1.5% de los entrevistados manifiesta no escucharla nunca.
*	De entre los medios de comunicación locales, el 34.3% de los colimenses
encuestados le otorga una mayor credibilidad a la radio. El 32.7% a la televisión y
el 8.2% a los periódicos. Un 2.5% les creen a todos.
*	El 18.3% prefiere los programas musicales, el 16.4% se inclina por los noticieros,
el 7.7% gusta de los deportes, al 7.4% le agradan más los programas de
entrevistas, un 5.2% prefiere los de espectáculos, el 5.1% prefiere los culturales,
un 4.8 % elige los educativos, 3.8% escucha los de política y un 2.4% las
radionovelas.
*	A la pregunta de qué tipo de programas no deberían faltar en una nueva
radiodifusora, contestaron que en primer lugar los noticieros, en segundo los
programas musicales, y en tercero los educativos.
*	El 87.8 % de las mujeres entrevistadas se manifiesta interesada en escuchar
una estación de radio educativa; en hombres la cifra es del 72.3%.
Es así como, al menos de acuerdo con los resultados de la encuesta, la gente de
Colima se encuentra interesada y expectante en torno a la oferta educativa que la
Universidad de Colima pueda brindarles por radio. Finalmente, y aunque no es
objetivo del trabajo que ahora concluye, construir una propuesta de proyecto
educativo radiofónico para la institución en cuestión, me atrevo a sugerir, después de
haber concluido un estudio como el que ofrezco en este documento, algunos
lineamientos que debieran seguir las autoridades de la casa de estudios colimense,
así como los profesionales involucrados, a la hora de plantear un proyecto educativo
que utilice a la radio como herramienta pedagógica:
 * Cuidar que la promoción de los nuevos proyectos partan de una definición clara
de los objetivos y una planeación cuidadosa.
 * Diseñar dichos proyectos con un enfoque de participación multidisciplinaria y
medios diversos, no únicamente los radiofónicos.
 * Esforzarse para obtener costos reales de las diferentes estrategias diseñadas.
 * Tomar en cuenta que una significativa cantidad de tiempo, dinero y esfuerzo
debe dirigirse al entrenamiento de monitores y supervisores de campo, elementos
claves tanto en la instrucción como en la evaluación de los alumnos.
 * Convocar la mayor participación local posible, para garantizar el buen
funcionamiento de todo nuevo proyecto educativo.
 * Cuidar que las emisiones radiofónicas se coordinen con las otras acciones del
programa educativo.
 * Invertir los recursos suficientes para lograr una buena capacitación de los
encargados de la acción educativa directa.
 * Contar con un sistema de retroalimentación bien organizado.
 * Relacionar estrechamente los materiales, tanto formales como de contenido, con
la vida diaria de los beneficiarios del programa.
 * Utilizar mecanismos que logren, de los usuarios potenciales, una participación práctica y
efectiva en todos los aspectos del programa (planeación, organización, diseño y
producción de materiales).
 * Establecer mecanismos de cooperación y consulta con otros organismos
nacionales e internacionales relacionados con la educación y el desarrollo.
 * No pretender que los mensajes emitidos, por sí solos, lleven todo el peso de la
tarea educativa, sin una infraestructura que soporte adecuadamente el proceso.
 * No descuidar la capacitación y entrenamiento del personal educativo y operativo
involucrado en el proyecto.
 * Lograr que un equipo interinstitucional e interdisciplinario trabaje en forma
conjunta, consensada y concertada en el diseño, producción y operación de los
medios.
Por último, me atrevo a sugerir que la Universidad de Colima no debiera olvidar que
en el estado de Colima nunca ha existido una oferta radiofónica verdaderamente
educativa, ni siquiera cultural. La población está habituada a la programación con
fines comerciales de las 18 emisoras de Amplitud Modulada y Frecuencia Modulada
que se encuentran en el estado. En general los contenidos que ofrecen requieren
poca atención y comprensión de los mensajes. Por lo tanto para que la radio
educativa despierte el interés y la atención de sus posibles receptores-alumnos,
debe utilizar formados adecuados para atraer y sostener el interés del escucha.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
Facultad de Telemática
Maestría en Ciencias,
Írea Tecnología y Educación
“LA RADIO COMO TECNOLOGÍA
EDUCATIVA: NATURALEZA,
USOS, EXPERIENCIAS Y
REFLEXIONES" 
Tesis que para obtener el grado de Maestra en Ciencias,
en el área Tecnología y Educación presenta:
Ana Karina Robles Gómez.
Asesor:
M.C. Edgar Gómez Cruz.
Colima, Col., marzo de 2002.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>IMPLEMENTACIÓN DE UN SISTEMA DE INFORMACIÓN PARA EL MANEJO DE EXPEDIENTES CLÍNICOS UTILIZANDO EL ESTÍNDAR HL7 EN EL HOSPITAL REGIONAL UNIVERSITARIO</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo de este proyecto fue identificar el diseño, facilidad, beneficio y desempeño
que percibe el personal del hospital acerca del sistema de información propuesto
para la administración de expedientes clínicos electrónicos. Para tal caso, se asumió
que los pacientes cuentan con un nivel de familiarización de nulo a bajo, en el área
de computación, y el personal del hospital cuenta con un nivel de básico a
intermedio en este aspecto.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Las pruebas de uso se realizaron en dos etapas: la etapa uno se realizó antes de
implantado el sistema de información y la etapa dos al momento de su implantación.
En la primera se obtuvo información de exploración y diagnóstico; la segunda
permitió evaluar el desempeño, confiabilidad de información y diseño del sistema.
Una vez concluida cada etapa se compararon los resultados obtenidos en ambas.
A fin de obtener conocimiento relevante de la población en cuanto al objetivo
fundamental del sistema de información aquí propuesto se usó la teoría del muestreo
no probabilístico selectivo, el cual permite establecer un diseño de muestra tal que
sea representativa de la población dada (Rojas Soriano, 1995).</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La información constituye una valiosa herramienta en la atención médica, la salud
pública y privada, la capacitación y la administración de los servicios de salud. Por
ello, las instituciones de salud en todo el mundo se han propuesto sistematizar,
estandarizar y alcanzar la interoperabilidad de la información generada en sus
prácticas cotidianas.
Es imperante además la modernización y fortalecimiento de los sistemas
automatizados de información en el área de la salud, a fin de lograr la homologación
de conceptos y necesidades de información, para que faciliten la toma de decisiones
y generen información de calidad para niveles estratégicos.
El sistema de información aquí presentado permite la creación y manejo eficiente del
ECE en instituciones de salud cuyas poblaciones de atención tienen bajos niveles
socioeconómicos. Éste ha obtenido desde sus inicios comentarios muy positivos por
parte del personal y directivos del HRU, quienes identificaron el beneficio que brinda
y se encuentran en la mejor disposición por continuar con el proyecto para
sistematizar integralmente todas las funciones que se realizan internamente en el
hospital.
Se reconoce y se espera que en futuras investigaciones, la fase de prueba del
sistema puede mejorarse agregando más participantes, ya que no fue posible contar
con mayores recursos humanos y económicos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
Facultad de Telemática
Maestría en Ciencias en el Írea Telemática
IMPLEMENTACIÓN DE UN SISTEMA DE INFORMACIÓN PARA EL
MANEJO DE EXPEDIENTES CLÍNICOS UTILIZANDO EL ESTÍNDAR
HL7 EN EL HOSPITAL REGIONAL UNIVERSITARIO
Presentado por:
Lyz Edemi Martínez Angulo
Asesor:
M.C. José Román Herrera Morales
Coasesora:
Dra. Ana Rosa Mosqueda Ramírez
Colima, Colima, julio de 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DISEÑO E IMPLEMENTACIÓN DE UNA INTERFAZ DE COMUNICACIÓN EN INTERNET, ORIENTADO A PERSONAS CON DISCAPACIDAD VISUAL2</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Brindarle a la población con discapacidad visual del CREE una herramienta de comunicación
especializada (mensajería instantánea), con capacidad de conversión Text To Speech en
idioma español; con miras a expandir la utilización de esta aplicación a poblaciones que
compartan características similares.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El desarrollo de herramientas informáticas para personas que presentan algún tipo de
discapacidad, constituye un signo evidente del avance tecnológico existente en esta nueva era; lo cual proporciona mayor autosuficiencia a las personas que tienen capacidades diferentes y que buscan estar actualizadas a través de la información que se encuentra en línea.
Dicha población desarrolla mejor otras facultades y sólo necesitan una herramienta de apoyo o de base para lograr sus objetivos y aspiraciones de una manera satisfactoria; de tal manera, que se incremente su formación profesional y su plena incorporación en la sociedad.
Por lo tanto, la inquietud por realizar este proyecto surgió debido a que es de suma
importancia aportar a la sociedad invidente un medio de comunicación (mensajero
instantáneo) a través de Internet; ya que presentar esta debilidad, no implica que sea una
limitante para aprender a utilizar la computadora, dispositivos periféricos, software o
aplicaciones y por consiguiente la red más importante de información “Internet" .
Por último, se considera que la elaboración de esta herramienta permitirá que la sociedad
invidente de México y de otros lugares, puedan satisfacer sus necesidades de comunicación en línea; las cuales se ven muy limitadas por la falta de estandarización, accesibilidad y creación de aplicaciones especializadas para dichas personas.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>¿Cuál es la problemática existente en la
población invidente del CREE?
Carecen de una interfaz de comunicación a través de
Internet, para que ellos interactúen con otras
personas a través de la red.
Revisión de la literatura
* Falta de accesibilidad a la información en línea
(limitación para los invidentes)
* Herramientas de síntesis de voz (TTS), como
apoyo básico para el desarrollo de la aplicación
* Teorías HCI y directrices de la W3C, para el
diseño de aplicaciones orientadas a personas con
discapacidades
Desarrollo del software
Diseño e implementación del sistema
Evaluación de la Usabilidad
* Ejercicio básico con la herramienta (prueba
piloto)
* Procedimiento del ejercicio:
- Observación
- Cuestionarios
Análisis de los datos
(resultados y conclusiones)</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La accesibilidad es el resultado de acciones conjuntas que se llevan a cabo con el objeto de
romper las barreras que impiden a todas las personas, incluyendo las afectadas por algún tipo
de discapacidad, que obtengan los beneficios a los cuales todos tenemos derecho. Por esta
razón la accesibilidad que ofrecen los servicios bancarios, comerciales, sociales, de diversión
y entretenimiento, los sitios web, dispositivos y aplicaciones de computadora y la tecnología
en general, definen una mejor calidad de vida para todo individuo; ya que las oportunidades de
aprendizaje que esta tendencia está implantando, permite que todos podamos desempeñar
nuestros roles en cualquier entorno y circunstancias sin mayores complicaciones.
La herramienta SCI es una interfaz de comunicación orientada a usuarios con discapacidad
visual, la cual se basa en la transmisión de información en línea (mensajería instantánea), en
donde los datos que el emisor envía de manera escrita a través del canal de comunicación
llegan al receptor transformados en voz. Su importancia radica en que es un sistema
especializado, ya que el análisis, diseño y construcción de esta herramienta se ha
fundamentado en las técnicas del UML, los conceptos básicos propuestos por las metodologías
HCI, entre otros aspectos de igual relevancia.
Lo que ha permitido obtener una aplicación que ofrece una buena interacción de estos usuarios
con la misma y por lo tanto con otras personas que presenten o no las mismas debilidades; ya
que también puede ser utilizada por un individuo que no sea invidente, porque la interfaz
ofrece tanto una guía hablada como escrita de cada una de las funciones que realiza.
Finalmente, a lo largo de todo el proceso de desarrollo de este proyecto se analizó
cuidadosamente cada uno de los parámetros que se debían tomar en cuenta para lograr
alcanzar los objetivos que se definieron inicialmente; de lo cual se pudo concluir que cada uno
de dichos objetivos se cumplieron satisfactoriamente. A continuación, se muestra un análisis
de los resultados obtenidos:
- Para lograr el objetivo referente a la facilidad de uso que debe ofrecer la
herramienta mediante la utilización de ayudas que guíen adecuadamente al
usuario, se procedió a analizar y evaluar ciertos parámetros que se consideraron relevantes al momento de implementar dicha herramienta, tales como los
niveles de aceptación, usabilidad y utilidad; los cuales obtuvieron un porcentaje
altamente satisfactorio en los resultados de la prueba piloto que se aplicó a los
participantes con discapacidad visual de dicho estudio.
- El segundo objetivo acerca de mantener un bajo costo de implementación del
sistema también se logró, ya que el SCI podrá ser adquirido por la población
invidente del CREE de manera gratuita; además de que el mismo se encuentra
disponible al público en la dirección electrónica
http://alumno.ucol.mx/~al029397.
- Por último, a través de la evaluación de usabilidad del sistema también se
obtuvo una buena puntuación en la escala SUS, con respecto a fomentar e
incrementar la utilización de este tipo de herramientas en la población con
discapacidad visual del CREE.
Como experiencia personal, el incursionar en este tipo de proyectos ha proporcionado al autor
una inmensa experiencia de aprendizaje entorno a como detectar una problemática existente en
determinada área de estudio, como resolver la misma y llevar a cabo los correspondientes
análisis para lograr alcanzar de manera satisfactoria los objetivos planteados; todo esto
apoyado en las innovaciones de la tecnología, las cuales proporcionan los medios y
herramientas necesarios para realizar correctamente el trabajo designado y de esta manera
enfrentar las situaciones que se nos presentan en nuestra diario vivir.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMATICA
DISEÑO E IMPLEMENTACION DE UNA INTERFAZ DE COMUNICACION EN INTERNET, ORIENTADA A PERSONAS CON DISCAPACIDAD VISUAL.
TESIS
QUE PARA OBTENER EL GRADO DE 
MAESTRO EN CIENCIAS, AREA TELEMATICA
PRESESNTA:
ING. NOEMI LISETTE GUERRA GONZALES
ASESOR:
M. en C. ARMANDO ROMAN GALLARDO 
COLIMA, COLIMA JULIO 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“ANÍLISIS E IMPLEMENTACIÓN DE UN ESQUEMA DE SEGURIDAD EN REDES PARA LA UNIVERSIDAD DE COLIMA"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Actualmente la Universidad de Colima se ha preocupado por la seguridad e integridad de la información y servicios contenidos en su Intranet, por lo que ha invertido una fuerte cantidad de dinero en equipos y aplicaciones que ayudan a disminuir las vulnerabilidades de seguridad. Esta inversión a logrado incrementar el nivel de seguridad pero no al grado que la institución requiere y esto se debe principalmente a que estos equipos y aplicaciones no están funcionando a un cien por ciento, además la falta de algunos elementos entre los que se pueden mencionar:
" * Políticas oficializadas
" * Procedimientos,
" * Reglas de seguridad y
" * Monitoreo constante de la información que proporcionan los equipos y aplicaciones.
Por lo tanto DIGESET requiere de una planificación que involucre los puntos anteriormente expuestos a través de un plan de táctico de seguridad informática para las funciones críticas.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Analizar el esquema actual de seguridad informática utilizado para la Intranet de la Dirección General de Servicios Telemáticos, con el fin de mejorarlo a través de un plan táctico que de solución a los puntos expuestos en la problemática.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los sistemas de información de hoy en día están relacionados en un 100% con la exposición de la información a entidades externas que son ajenas a la organización o delegación a la que pertenecen. La Universidad de Colima esta compuesta por muchas delegaciones y dependencias, cada una de ellas tienen sistemas de información independientes que hacen uso de la red externa (Internet) e interna (Intranet). Esto los hace vulnerables a eventos de amenazas que conllevan a riesgos que pueden provocar perdidas tangible e intangibles, para evitar estas perdidas que en muchos de los casos serán monetarias, se requiere del planteamiento de un esquema de seguridad que no solo contemple elementos técnicos que comúnmente encontramos en el mundo de la seguridad informática, sino que establezca una base sólida para el establecimiento de controles, políticas y proyectos de seguridad que administren el ciclo de vida de la seguridad informática. DIGESET es la dependencia encargada de la distribución y administración de la red externa e interna de la Universidad de Colima, por lo tanto, será el objeto de estudio de la investigación.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En la Universidad de Colima no existe un documento oficial de políticas de seguridad informática que controle todas las eventualidades de la infraestructura de la Intranet. El campo de seguridad informática cambia constantemente, y de la misma manera cambia las políticas, por lo tanto, la seguridad informática es un proceso que requiere de una administración constante, para alcanzar el éxito en la integración de la seguridad a la cultura organizacional de DIGESET. En otras palabras, la seguridad informática es un proceso cíclico que requiere de la definición, implementación y verificación constante.
El establecimiento de un esquema de seguridad, no es tarea de una sola persona, todos los integrantes de la institución deben ser involucrados y las políticas de seguridad deben implementarse gradualmente, e ir acompañadas de un programa de concienciación.
DIGESET es la dependencia encargada de la distribución y administración de la red externa e interna de la Universidad de Colima, por lo tanto, debe incorporar la toma de decisiones basada en el análisis de riesgos para responder a tres preguntas básicas sobre la seguridad, estas son:  ¿Qué quieren proteger?,  ¿Contra quién (o qué) quieren protegerse? y  ¿Como lo quieren proteger? Para ello se implementó la metodología de la empresa SCITUM que permitió obtener resultados importantes para el esquema de seguridad actual de la Universidad de Colima
El primer resultado que se obtuvo de la implementación de esta metodología fue el análisis de vulnerabilidades que involucró los dispositivos (Servidores, ruteadores, firewall e IDS) críticos de la Intranet para llegar a obtener hallazgos a nivel de infraestructura, tal es el caso se los servidores Windows 2000 que cuentan con vulnerabilidades en la administración de las cuentas de usuarios; ya que las cuentas “guest"  y “TsInternerUser"  nunca han sido utilizadas y por ende nunca han sido desactivadas, además existen cuentas de usuarios que contienen contraseñas que nunca caducan, también estos servidores hacen uso de la aplicación FTP, por lo que se requiere que el personal responsable lleve acabo las actualizaciones necesarias para el buen funcionamiento de la aplicación Serv-U. Por otro lado, los servidores Unix evaluados cuentan con diversos tipos de vulnerabilidades de riesgo alto y medio, a través de las cuales un intruso podría ejecutar comandos remotamente o ganar acceso a los equipos con privilegios de administrador, en este conjunto de servidores es importante llevar acabo la actualización de versiones de algunas aplicaciones como OpenSSL, Apache y SSH entre otros, así como la planificación para la aplicación de los cambios necesarios; con estos hallazgos actualmente los administradores de los servidores se encuentran implementando las mejoras a corto plazo de los equipos para apoyar la etapa de preparación, que conllevará a la implementación del modelo de operación propuesto en el capitulo 4.
Se pudo establecer a partir de todos los hallazgos que se encontraron en la etapa de análisis de vulnerabilidades, que los dispositivos de comunicación (conmutadores, ruteadores y acceso remoto), cuentan con relativamente pocas vulnerabilidades, en el caso de los ruteadores solo se debe de considerar la utilización de algún tipo de encriptación para la configuración remota. La infraestructura utilizada para los servicios de acceso remoto vía MODEM a la intranet universitaria no establece algún método de autenticación en especial, por lo tanto es deseable especificar un método seguro.
Por otro lado, se encontró que:
" * La seguridad perimetral cuenta con una configuración básica.
" * Aun no se aplican las políticas específicas para cada servicio de la red.
" * Los servidores aún no están en la zona que les corresponde en el firewall.
" * No existe un protector de intrusos para cada delegación de la Universidad, que le permita determinar a los administradores de la intranet que es lo que se esta transmitiendo en dicho campus.
Se debe considerar que con la investigación se logró recopilar la información necesaria para la configuración adecuada de la seguridad perimetral.
La metodología no solo permitió encontrar hallazgos a nivel de infraestructura, sino que también a obtener hallazgos en función de procesos y gente, estas dos categorías se obtuvieron a través de entrevistas, visitas a las instalaciones de DIGESET e información proporcionada por el personal. Además los estudios de madurez nos permitió obtener unamedición del nivel de seguridad de la Intranet en algunos dominios con respecto al estándar ISO/17799 (ver tabla 3.3).
Con los hallazgos encontrados se llevó a cabo el análisis de riesgos que nos ayudo a determinar los controles necesarios para mitigar los riesgos y establecer una prioridad de las actividades a realizar de acuerdo a su tiempo de implantación y costo. Estas actividades se encuentran descritas en el capitulo 4.
Finalmente se formuló un plan táctico que consistió en el establecimiento de lineamientos, que incluyen todas las líneas tácticas vistas en el capitulo 4, por ejemplo la definición de funciones de seguridad y la consideración de un programa de concienciación y establecimiento de las políticas de seguridad de la Intranet universitaria, que debe de seguir el personal de DIGESET para adoptar el modelo de operación que se plantea. El proceso de mejoramiento de la seguridad es gradual y por fases, por lo tanto, las líneas tácticas permiten definir una ruta crítica de ejecución que elevará el nivel de seguridad, dependerá entonces, del comité de seguridad de DIGESET el llevar acabo la implementación del plan táctico. Al final del capitulo 4 se presentan los factores críticos de éxito que son resultado de un análisis minucioso de la investigación con los cuales la Dirección puede iniciar la implementación, esto les permitirá reducir tiempos y por ende obtener resultados a corto plazo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
Facultad de Telemática
“ANÍLISIS E IMPLEMENTACIÓN DE UN ESQUEMA DE SEGURIDAD EN REDES PARA LA UNIVERSIDAD DE COLIMA" 
TESIS
Que para obtener el grado de
MAESTRO EN CIENCIAS, AREA TELEMATICA
Presenta
Ing. Aaron Clemente Lacayo Arzú
Asesor
M.C. Raymundo Buenrostro Mariscal
Colima, Colima Julio de 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“Documento Curricular de la Maestría en Automatización y Control"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo de este trabajo es con la finalidad de apoyar a la
Escuela de Ingeniería Electromecánica en abrir la Maestría en
Automatización y Control, desarrollando el Plan de Estudios de la
Maestría y cumplir con todos los requisitos necesarios para abrir dicha
Maestría.
Uno de los objetivos importantes que tendrá la Maestría será
fortalecer a los alumnos de ingeniería de un amplio conocimiento de la
teoría de control, la automatización y sus elementos, con la finalidad de
que estos participen del desarrollo científíco y tecnológico en el campo
de la automatización y control industrial.
El Programa de Maestría en Automatización y Control tiene
como objetivo principal buscar atraer a los mejores alumnos de la FINE
y de la Escuela de Ingeniería Electromecánica; y así mismo de otras
partes del país, brindándoles la oportunidad de formarse
académicamente, así como realizar investigación en la planeación, la
operación y el control de los sistemas eléctricos de potencia, en el
análisis y el diseño de sistemas de control y robótica, así como en el
diseño y la exploración de los sistemas de comunicación modernos.
La misión de la Maestría en Automatización y Control será:
I Preparar recursos humanos altamente calificados
Í¬ Desarrollar Investigación tebrica y aplicada de alto nivel
i Realizar actividades de cooperación científica y
tecnológica con el sector productivo</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La Escuela de Ingeniería Electromecánica fue inaugurada
en el año de 1986, ofreciendo las carreras de Ingeniero Mecánico-
Eléctrico y de Electrónica y Comunicaciones, utilizando al inicio las
instalaciones de la Central Termoeléctrica Manzanillo 1, donde se
realizaban las prácticas de campo.
Cuando la Escuela fue trasladada a la Facultad de Ciencias
Marinas no se contaba con instalaciones propias y tampoco se tenía
edificios de laboratorios.
En el año 1992 se terminaron de construir los Laboratorios
de Electricidad y Magnetismo y Electrónica y posteriormente el
Laboratorio de Mecánica los cuales fueron entregados sin equipamiento.
En mayo de 1999 se terminaron de construir una sala de usos
múltiples, un centro de computo, la dirección de la Escuela y salón de
cúbiculos para los catedráticos, dos laboratorios (telecomunicaciones y
el de instrumentación y control).
Los laboratorios se han venido equipando lentamente y
actualizando con las nuevas tecnologías con los proyectos de Fomento
para el Mejoramiento de Escuelas Superiores (FOMES) 92, 93, 94, 95,
96y97.
La Escuela de Ingeniería Electromecánica ha venido
creciendo paulatinamente, y en su proyecto de Programa para el
Mejoramiento del Profesorado (PROMEP) en el cual se plasma el
crecimiento a futuro hasta el año 2006, este crecimiento es tanto para su
catedráticos como para la Escuela, En el PROMEP se tiene contemplado
la creación de un nuevo Posgrado, la Maestría en Automatización y
Control y por consiguiente, convertir la Escuela a Facultad, también este
proyecto de corpus_proyectos se encuentra plasmado en el Plan de Institucional
de Desarrollo (PIDE).
Uno de mis objetivos importantes junto con la Escuela de
Ingeniería Electromecánica a mediano plazo es la creación del posgrado
de Maestría en Automatización y Control, con la finalidad de formar
maestros de alto nivel que cubran las necesidades del país, fortalecer la
investigación científica y de desarrollo tecnológico; y formar maestros
universitarios e investigadores de alto nivel.
Por otra parte, en toda la Universidad de Colima sólo se
tiene un posgrado del área de ciencias exactas que es la corpus_proyectos en
Ciencias con especialidad en Computación que se encuentra en la
Facultad de Ingeniería Mecánica y Eléctrica ubicada en Coquimatlán,
Col. Por esta razón se optó por crear otro posgrado de ciencias exactas en
Manzanillo, ya que en el puerto de Manzanillo el crecimiento
tecnológico es muy acelerado y se tiene la necesidad de tener
profesionistas con posgrados, para ir con el crecimiento de la nueva
tecnología.
Para cumplir con los objetivos trazados por la Escuela como
es la Maestría propongo la Tesis llamada Documento Curricular de la
corpus_proyectos en Automatización y Control, para obtener el Grado de Maestro
en Ciencias Area Computación.
Con la apertura de esta nueva Maestría la Universidad de
Colima aportará una opción más para sus egresados a nivel Licenciatura
puedan estudiar un posgrado de alto nivel.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Como se puede observar en este trabajo de tesis cumple con
las requisitos necesarios para abrir la Maestría en Automatización y
Control con la modalidad semiescolarizada; y así mismo se fortalecerá la
Escuela de Ingeniería Electromecánica para la creación de nuevas las
Líneas de Generación y Aplicación del Conocimiento (LGAC) que los
alumnos de posgrado desarrollen y además de las que se trazarón en el
Promep (Programa de Mejoramiento de Profesores).
Este trabajo de tesis es muy interesante debido a que la
automatización es una disciplina que reúne una gran cantidad de áreas del
conocimiento y las sintetiza para lograr su objetivo: el adecuado control
de un proceso. Y es muy interesante para el desarrollo de la región
debido a que todo tiende a automatizarse.
Aunque los sistemas de control automático existen desde
hace ya varios siglos, el desarrollo de la tecnología desde los años 50
hasta nuestros días ha permitido una expansión enorme del ámbito de
acción de la automatización; al extremo que el mundo moderno
prácticamente no podría existir sin el uso de controladores automáticos.
Analizando el presente trabajo de tesis, los resultados obtenidos
son favorables para abrir la Maestría en Automatización y Control; el
estudio de mercado fue alentador para la apertura de la Maestia en la
Escuela de Ingeniería Electromecánica, se observó una buena demanda
por los profesionistas de la región y asimismo la Escuela se encuentra en
condiciones para llevar a cabo este posgrado.
Este trabajo sirve para generar líneas de investigación tanto
para la región occidente como para el país; creando vínculos con las
industrias que se encuentran en el estado.
Esta tesis sirve para elevar la Escuela a Facultad, y a su vez
proporcione beneficios a todos aquellos profesionistas que deseen
emprender estudios de posg-rado sin necesidad de salir de la región
enfrentando los retos que implica acudir a efectuar estudios a otro lugar,
asimismo otorgará grandes beneficios a nuestra sociedad, debido a que
proveerá de recursos humanos capaces de desempefiarse en la
investigación, en la industria y en la docencia.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
Facultad de Ingeniería Mecánica y Eléctrica
Campus Coquimatlán Colima
“Documento Curricular de la
Maestría en Automatización y Control" 
TESIS
Que para obtener el Grado de
Maestro en Ciencias
Area Computación
Presenta:
Ing. Jorge Gudiño Lau
Asesorado por:
M.I. Ricardo Fuentes Cobarruvias
Coquimatlán, Colima Agosto 1999</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Detección automática de plagio en texto</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El problema de la detección automática de plagio puede verse como un problema de
búsqueda y/o clasificación (en el análisis intrínseco de plagio, enfoque abordado en la sección 2.1, la tarea es abordada como un problema de clasificación pura).
El primero de los elementos a considerar en esta tarea es el corpus de referencia D, el cual
está conformado por un conjunto de documentos de referencia
3
. Por extraño que parezca en principio, no existe ninguna condición de que dichos documentos de referencia sean realmente
originales. La razón es que basta con que un fragmento de un texto sospechoso sea hallado en
otro para determinar que se trata de un caso de plagio. La extensión y cobertura que pueda
alcanzar el corpus de referencia es uno de los factores clave en el éxito de los sistemas de
detección de plagio basados en corpus. Cada documento d  D es una potencial fuente del
texto incluido en un documento sospechoso, es decir, un texto plagiado. El segundo elemento
a considerar es precisamente el documento sospechoso, el cual será conocido en adelante
como s. Este documento puede ser original, contener fragmentos plagiados o, de hecho, estar
enteramente plagiado. Estos dos elementos son suficientes para describir el planteamiento de
la tarea de detección de plagio:
Sean s un documento sospechoso y D un conjunto de documentos de referencia,
el objetivo de la detección automática de plagio es encontrar aquel documento
de D que haya sido utilizado como fuente para obtener el documento s, el
cual presumiblemente es un caso de plagio. Dicha búsqueda puede llevarse a un
nivel más específico: Sea si es un fragmento plagiado, el objetivo es encontrar
aquel fragmento dj de tal que dj es la fuente del fragmento plagiado si
.
Encontrar la fuentes de un fragmento sospechoso es una prueba adecuada para determinar
que se trata de un fragmento plagiado.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Los objetivos generales de esta investigación se detallan continuación:
1. Estudiar la problemática de la detección del plagio desde el punto de vista estadístico
con el afán de observar cuáles son las ventajas y carencias de los métodos existentes
hasta ahora.
2. Analizar los recursos existentes que puedan cubrir dichas carencias y comenzar con su
adaptación y aplicación en la mejora de los resultados obtenidos en esta tarea.
3. Sentar las bases para la creación de los recursos necesarios para la futura generación de
metodologías que ataquen la problemática dentro de entornos tanto monolingües como
translingües.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Detección automática de plagio en texto
Luis Alberto Barrón Cedeño
Departamento de Sistemas Informáticos y Computación
Director: Paolo Rosso
Tesis desarrollada dentro del Máster en Inteligencia Artificial,
Reconocimiento de Formas e Imagen Digital
Valencia, noviembre de 2008</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>USO DE TECNOLOGÍAS DE HARDWARE GRÍFICO EN EL APOYO AL REALISMO EN ENTORNOS VIRTUALES ARQUITECTÓNICOS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Las alternativas (Quicktime VR, VRML, etc.) con las que actualmente cuentan los arquitectos para mostrar diseños arquitectónicos virtuales interactivos, no ofrecen un alto nivel de realismo, tienen muy poca capacidad de interacción con el usuario y no logran crear una buena inmersión en el mismo. Como ejemplo de VRML se puede mencionar el proyecto de maqueta virtual de la Casa Santa (http://www.unirioja.es/Prensa/Noticias/CasaSanta.html) creado por el equipo de investigación Calagvrris.Ivlia, el cual no muestra buena calidad en texturas que componen la arquitectura y el ambiente. Con respecto a las vistas de 360 ° el problema puede no ser la calidad del ambiente, sino más bien se refiere a la interactividad del usuario, ya que este se encuentra limitado a estar en la misma posición todo el tiempo, sin poder interactuar con los objetos que le rodean, debido a que sólo son imágenes.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un entorno arquitectónico virtual, el cual sea superior en realismo, inmersión e interacción a las opciones con las que se cuenta actualmente, además de que dicho entorno será capaz de explotar al máximo las capacidades de visualización tridimensional presentes en las tarjetas aceleradoras de gráficos actuales.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Será posible incrementar el nivel de realismo en ambientes virtuales arquitectónicos mostrados en PCs de escritorio mediante el uso de nuevas tecnologías de hardware gráfico?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Un alto nivel de realismo en los entornos virtuales arquitectónicos es un factor clave para que éstos sean mejor asimilados y aceptados por el usuario?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El alto nivel de realismo facilitado por las tecnologías de hardware gráfico, incrementará significativamente el nivel de inmersión en un entorno virtual arquitectónico visualizado en una computadora de escritorio.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Actualmente existe software (3D estudio VIZ, Archicad, 3D MAX, etc.) y técnicas que sirven de apoyo a los arquitectos para visualizar los diseños arquitectónicos creados por ellos mismos, pero en ocasiones sólo se limitan a imágenes renderizadas, vistas de 360° y, en ocasiones, a entornos virtuales de poca calidad visual. Tampoco ofrecen una verdadera interacción con el entorno, ni efectos visuales que aporten un mayor realismo, tales como luces, sombras dinámicas, efectos de texturas en tiempo real, física de objetos, agua realista, etc. Estas limitaciones hacen que los entornos virtuales sean muy poco realistas y de poco interés para los clientes.
Es por todo lo anterior y el poco uso que se le da a la enorme capacidad de procesamiento con el que se cuenta actualmente en las tarjetas aceleradoras gráficas en la visualización de modelos arquitectónicos, que se optó por desarrollar esta tesis, la cual pretende lograr (mediante una computadora de escritorio y una buena tarjeta aceleradora gráfica) una representación virtual de entornos arquitectónicos que sea capaz de mostrar un alto nivel de realismo e inmersión a costos accesibles.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Al término del análisis de los resultados obtenidos en la evaluación de los entornos y con la ayuda de las gráficas, se observa claramente que el entorno virtual creado durante el desarrollo de esta tesis fue el que más aceptación tuvo por parte de los usuarios, ya que ofreció mayor realismo e inmersión al obtenido usando otro tipo de visualización y a pesar de que era una versión prototipo que no mostraba la máxima calidad y detalle que podría ser capaz de mostrarse en un entorno virtual arquitectónico, usando la tecnología empleada en esta tesis.
Finalmente, al terminar este proyecto se concluyó que cada uno de los objetivos planteados en la sección 1.2 y 1.3 se cumplieron satisfactoriamente. A continuación se muestra una breve descripción de los resultados obtenidos:
En lo referente al primer objetivo específico, el cual menciona la posibilidad de ofrecer una mejor representación visual de entornos arquitectónicos virtuales, éste se cumple satisfactoriamente, dado que al 100% de los usuarios les agradó más el entorno prototipo creado en esta tesis que los otros, además de que todos respondieron que sí les gustaría utilizar este tipo de visualización en su trabajo. En lo concerniente al aumento de realismo e inmersión, también fue un éxito, ya que cinco de los participantes contestaron que los objetos en el entorno parecían completamente reales y uno contestó que parecían casi reales.
En lo que concierne al segundo objetivo específico, el cual comenta el funcionamiento del entorno en computadoras no muy costosas, también fue cumplido, ya que, como se mencionó en la sección 6.1, el costo promedio para un equipo que cumpla con los requerimientos mínimos para la visualización del entorno virtual prototipo, es de 8 mil pesos, costo promedio de cualquier equipo de cómputo de hoy en día. El equipo sobre el cual se hicieron las pruebas es semejante al mínimo propuesto en requerimientos (excepto por la tarjeta de video) y costo, dicho equipo fue capaz de mostrar el entorno virtual a una velocidad promedio de 70 FPS y a una resolución de 1024x768 con 32 bits, la cual es superior a la mínima propuesta para este tipo de visualización y puede ser mejorada optimizando la programación de los elementos en el entorno. Por todo lo anterior se cumple el segundo objetivo.
El tercer objetivo específico, el cual comenta la capacidad del entorno virtual para ser accedido por Internet, también se cumple siempre y cuando se cuente con una buena conexión a Internet, ya que el entorno virtual prototipo ocupa un tamaño de 4.2 MB debido a la calidad de las texturas empleadas y a la complejidad de los modelos usados.
Cabe mencionar que el prototipo desarrollado en esta tesis se diseñó para ejecutarse en el equipo en el cual sería visualizado, o para ser accedido por página Web dentro de una red de área local; de haberse enfocado en su uso exclusivo por Web, éste se podría haber reducido en tamaño mediante la disminución de texturas, modelos menos complejos y eliminación de la multitextura en los mismos.
Con respecto a la interacción en el entorno arquitectónico virtual, todos los usuarios contestaron con la mayor calificación aprobatoria; además comentaron que lo que más les gustó del entorno es la capacidad de interacción con los objetos, ya que, el poder encender las luces de la casa y la libertad de navegación, los hacía sentir como si estuviera en una casa real.
Por todo lo anterior, se pude concluir que tanto el objetivo general como los específicos se cumplieron satisfactoriamente al término de esta tesis, comprobando que los entornos virtuales arquitectónicos que usan tecnologías de hardware gráfico, se ven enormemente favorecidos en el incremento de realismo e inmersión, el cual se refleja en una mayor aceptación por parte de los usuarios.
Sin embargo, debido a que fue solo una prueba exploratoria y con pocos participantes en su totalidad mujeres (debido a que fueron las únicas que se presentaron), se sugiere que para que los resultados de esta tesis sean más concluyentes, es necesario de un grupo de arquitectos y personas mucho más numeroso, en el cual, se incluyan personas de ambos sexos, y poder así apreciar si el sexo influye en los resultados finales.
Se cree que las pruebas arrojarían mejores resultados, si éstas, se realizaran con personas más relacionadas en el área de graficación por computadora, o con los juegos de video 3D de primera persona, ya que les seria más fácil la interacción con el entorno y la apreciación de los efectos gráficos.
Con respecto a la experiencia previa de las personas, esta resultó favorable, ya que la mayoría de éstas, ya tenía conocimientos previos sobre programas de 3D para arquitectura, tales como Autocad, Archicad, etc., esto les permitió hacer una mejor comparación entre el entorno virtual realizado en esta tesis, y los entornos virtuales creados con los programas actuales de diseño arquitectónico, dando como resultado, que el 100% de los participantes contestaron con la máxima puntuación a las preguntas 10 y 11 de la Sección B del cuestionario, las cuales se refieren a la preferencia en uso y la superioridad de la visualización mostrada en esta tesis, frente a las alternativas actuales. Además, el 100% de los participantes contestaron afirmativamente a las preguntas 6 y 7 de la sección C del cuestionario, las cuales cuestionan el gusto por usar y recomendar, en el ámbito laboral y personal, el tipo de visualización de entornos virtuales mostrada en esta tesis.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
FACULTAD DE TELEMÁTICA
USO DE TECNOLOGÍAS DE HARDWARE GRÁFICO EN EL APOYO AL REALISMO EN ENTORNOS VIRTUALES ARQUITECTÓNICOS
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS, ÁREA TELEMÁTICA
PRESENTA:
ING. MARIO EZEQUIEL GUZMÍN GARCÍA
ASESOR:
DR. MIGUEL ÍNGEL GARCÍA RUIZ
COLIMA, COLIMA. SEPTIEMBRE 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>GENERAClÓN DE CÓDIGO PARA EL ANÍLISIS DE SEÑALES ANÍLOGAS Y DIGITALES MEDIANTE EL TMS320C2X DSP</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Con este proyecto pretendo dar a conocer a los alumnos del área de
Comunicaciones y Electrónica, así como a los de Maestría en Ciencias Írea:
computación que existen nuevas alternativas de estudio basados en dispositivos
como PROCESADORES DE SEÑALES DIGITALES (DSP).
Que contamos con equipo especializado con los micro controladores, que
son dispositivos de propósito específico y que los podemos utilizar para nuestro
trabajo académico.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El procedimiento a seguir para realizar este proyecto se divide en los
siguientes puntos:
ANÍLISIS DE SEÑALES
1.- ESTADO DEL ARTE
2.-ANÍLISIS ARMÓNICO EN LA FRECUENCIA
3.- APLICACIÓN
4.- CÓDIGO DEL ANALIZADOR DE SEÑALES
5.-CONCLUSIONES
6.- BIBLIOGRAFÍA</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La finalidad de este proyecto es la de mostrar diversas herramientas
matemáticas alrededor del análisis de FOURIER que faciliten el análisis de
señales eléctricas.
Presentar otra alternativa para conocer las señales eléctricas. Las armónicas
que lo forman, su magnitud y su frecuencia y así poder predecir su
comportamiento.
Haciendo uso de un dispositivo electrónico Micro controlador TMS320C2X de la
Texas Instrument con un lenguaje de bajo nivel usado por el micro controlador,
podremos analizar el espectro de frecuencia de cualquier señal análoga o
digital.
Lo interesante de este proyecto es que a partir de un proceso matemático
como lo es el ANÍLISIS DE FOURIER se pudo desarrollar un algoritmo que es
conocido como algoritmo de la FFT. Base para la creación de los dispositivos
DSP.
Estos dispositivos con la increíble velocidad de procesamiento, con la
versatilidad de mezcla de señales, dieron paso al desarrollo de los analizadores
de espectros. Estos han evolucionado con la tecnología, pueden procesar y
mostrar información espectral de la señal que se está procesando conforme
esta varía.
En realidad los analizadores de espectros actuales son micro controladores con
una arquitectura y diseño específico orientados a realizar tareas de
procesamiento de señales digitales.
Para finalizar quiero dejar constancia de que este trabajo es solo un punto en
todo un universo en los que se desarrollan los DSP.
Para mi este trabajo representa el inicio de una nueva área de investigación
donde muchos se encuentran trabajando y donde falta mucho por hacer.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
FACULTAD DE INGENIERÍA
MECÍNICA Y ELÉCTRICA
GENERAClÓN DE CÓDIGO PARA EL ANÍLISIS
DE SEÑALES ANÍLOGAS Y DIGITALES
MEDIANTE EL TMS320C2X DSP
TESIS PROFESIONAL
QUE PARA OBTERNER EL TITULO DE:
MAESTRO EN CIENCIAS ÍREA:
COMPUTACIÓN
PRESENTA:
JOSE FRANCISCO PEÑA VERDUZCO
ASESOR:
MC. GERARDO FUENTES COVARRUBIAS
COQUIMATLÍN, COLIMA;
AGOSTO DE 1999.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>TÉCNICAS DE PROTECCIÓN CONTRA PIRATERÍA EN DISCOS COMPACTOS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La veloz marcha de los avances tecnológicos representa un gran negocio
para aquellos que encuentran un filón de oro en la elaboración de productos
piratas, desde una cinta de audiocasette hasta sofisticados programas de
cómputo, es por ello que se busca encontrar la (s) herramientas(s) tecnológicas
más adecuados para proteger los productos que genera el Cenedic (Centro
Nacional Editor de Discos Compactos ) de la Universidad de Colima, pues se
tienen algunos casos de discos compactos que ha desarrollado el Cenedic y se
han pirateado tanto en su información como el disco completo, y esto lleva a tener
perdidas considerables.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Identificar y analizar las herramientas existentes para la protección de datos y
software contra la piratería, con el propósito de encontrar las más adecuadas para
los productos que genera el Cenedic (Centro Nacional Editor de Discos
Compactos).</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La proliferación de equipo técnico y el abaratamiento del mismo convierte en
una "tentación" para todos la adquisición de productos piratas existentes en el
mercado, ya sean de música, información e incluso, de software; por ello, resulta
imperioso encontrar e implementar medidas de seguridad que garanticen su
integridad.
Un ejemplo claro que puede suscitarse es que cualquier persona que
adquiera algún disco, puede modificar esta información ya procesada (con todo lo
que esto representa) y realizar una actualización o hasta una edición "pirata". Esto
tiene una solución inmediata con la implementación de técnicas como el cifrado,
que evita que la información sea legible al usuario aunque se realice la copia; y la
protección contra ejecución del disco a través de software y hardware a pesar de
tener un duplicado.
En el Cenedic se tienen ejemplos de productos (Artemisa, Embriología
Humana 1 y 2) que han sido copiados y que actualmente se venden en mercados populares a un precio mucho más bajo; del mismo modo algunos discos
(Jurisprudencia ) se editaron por vez primera y las posteriores se cancelaron
debido a que la información ya estaba procesada, sólo bastaba pagar por su
edición, tarea que contrataron con otra empres aa precio mucho más bajo. Esto
genera pérdidas para el Cenedic, ya que invierte en personal y en procesos para
que la información finalmente sea "pirateada", es por eso que resulta imperioso
que todos los productos generados por el Cenedic estén protegidos, de esta
manera, y así evitar pérdidas.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>De acuerdo al tipo de investigación que se plantea y a las posibilidades de
aplicación que se vislumbran, se presenta la siguiente metodología:
*  Conceptualizar términos que intervienen en el encriptamiento de datos
*  Organizar y análizar la información.
*  Seleccionar los métodos elegidos para su experimentación (software y
hardware)
*  Analizar el comportamiento de los métodos elegidos
*  Documentar la información, experiencias y conocimientos adquiridos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>A lo largo de este trabajo se analizaron algunos de los diversos productos
que ofrece el mercado para la protección de datos. Estas tareas fueron realizadas
de acuerdo a determinadas características que posibilitaran su posterior
aplicación, es decir que fueran una suma de adquisición, puesta en marcha y
efectividad; esto encaminado a asegurar la aplicación de los candados (sentinel)
Para lograr estos resultados, primeramente, me di la tarea de investigar los
productos en el mercado, analizar sus características y usos, para finalmente,
proceder a seleccionar aquellos que resultaran más viables de implementar en un
futuro cercano, con esto puedo concluir que los productos de la empresa
Raimbows Technologies satisfacen nuestras necesidades, aclarando que
dependiendo del proyecto (disco compacto) y del costo es el nivel de seguridad
con el que se protegerían.
Este trabajo no culmina aquí, deseo hacer un llamado a los estudiosos y
desarrolladores de software para continuar esta tarea de investigación con el fin
de encontrar nuevos productos que impidan la piratería y den batalla a quienes se
dedican a lucrar con los resultados de quienes verdaderamente se esfuerzan.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÍTICA
TÉCNICAS DE PROTECCIÓN CONTRA PIRATERÍA
EN DISCOS COMPACTOS
TESIS
Que para obtener el grado de:
Maestra en ciencias, área telemática
Presenta:
ING. ANA AZUCENA EVANGELISTA SALAZAR
Asesor:
M. C. PEDRO DAMIÍN REYES
Colima, Col ., junio de 2001</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SISTEMA DE INFORMACION COMPUTARIZADA PARA LA CLINICA SAN FRANCISCO</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El problema de la clínica radica en la facturación debido a que no se tiene la
actualización constante de los precios de los medicamentos, ni de que medicamento
o servicio se le proporcionó a cada paciente, de tal manera que al facturar se tiene
que solicitar la tabla de precios de medicamentos a la farmacia, así como el
expediente del paciente para conocer que es lo que se le proporcionó y los días de
internamiento, ocasionando con esto tiempo de espera por parte del paciente.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Con el diseño del nuevo sistema se pretende que la elaboración de facturas
sea en un tiempo menor y con menos recursos humanos y materiales, así mismo de
tener el control de medicamentos y material existentes para en un momento dado
saber cuando se tiene que hacer solicitud de los mismos para no ocasionar un
desabasto o tener que recurrir a comprarlos a una farmacia corriendo el riesgo de no
encontrarlos o a un precio mayor; Así con el sistema se lograría un mayor prestigio
para la Clínica por la atención que se esta brindando.
Los objetivos se pueden numerar como se muestra a continuación:
+ Optimizar tiempos y movimientos
+ Tener un mayor control de los medicamentos y el material de curación utilizados.
+ Satisfacción del cliente.
+ Modernización.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con el desarrollo de El Sistema de información Computarizada para la Clínica
San Francisco, he obtenido una gran experiencia en varios aspectos. Experiencia
que va tanto del aspecto técnico, la aplicación de la metodología misma, como del
aspecto político, el contacto y organización de las diferentes personas involucradas.
En el aspecto técnico me pude percatar que el uso de la metodología no solo
es necesaria sino fundamental; el hecho de crear un modulo esencial permite fijar las
metas funcionales del sistema, la organización de los diferentes procesos para
cumplir con esas metas funcionales y de funciones complementarias fijadas tanto en
el análisis como en el diseño. Por otro lado, las técnicas de impiementación
estudiadas permiten una mejor estructuración y una división tangible de las tareas
producidas por el sistema dejando a su vez testimonio confiable del desarrollo del
sistema para su futuro mantenimiento.
Con el desarrollo de El Sistema de Información Computarizada para la Clínica
San Francisco se obtuvo un mejor control de los servicios y medicamentos que se le
suministran a cada paciente, así como el control de medicamentos que entran y
salen de farmacia y la actualización de precios de cada uno de ellos.
A lo largo de la evolución del presente proyecto se ha podido vislumbrar el
gran potencial que un desarrollo como el Sistema de Información computarizada para
la Clínica San Francisco representa, ya que es capaz por sí mismo de mejorar hasta
en un 90% el tiempo de creación de una factura -comparando como se habían
estado elaborando anteriormente.
Además este sistema logro la optimización de los recursos humanos,
materiales y financieros, debido a que se logró un registro pormenorizado de cada uno de los elementos en el momento preciso en que se generaba un movimiento; en
forma colateral el sistema propició una reorganización de la empresa, pues al inicio
del sistema, no existía un organigrama y tampoco un análisis de puestos formal. Por
lo que esta tesis puede dar pie al menos de una tesis referente a la administración de
Recursos Humanos que se refleje en un manual de organización que en un futuro
reciente será indispensable dado el incremento en el volumen de trabajo detectado.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
corpus_proyectos EN CIENCIAS AREA: COMPUTACION
SISTEMA DE INFORMACION COMPUTARIZADA
PARA LA CLINICA SAN FRANCISCO
T E S I S
Que para obtener el grado de
MAESTRA EN CIENCIAS
AREA: COMPUTACION
PRESENTA
Martha Patricia Ibarra Ramírez
A S E S O R
M.C. Ricardo Fuentes Covarrubias
Coquimatlán, Col., Febrero del 2000</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SISTEMA DE NOMINA BASADO EN LA METODOLOGIA DE COAD &amp; YOURDON</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>a) Maximizar la calidad del modelo actual
b) Maximizar la calidad de la especificación con respecto a los
requerimientos de la función de los usuarios.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Entre las metodologías de análisis de sistemas y técnicas de especificación de
requerimientos más relevantes, se tiene:
a) Structured Requeriments Definition (SRD): metodología creada por K.
Orr y definida como: “Output- Oriented" .
b) Diseño Lógico de Sistemas de Información Administrativos: metodología
desarrollada por: O. Barros.
c) Análisis Estructurado de Sistemas: desarrollada inicialmente por:
Yourdon.
Esta última metodología es la adoptada para nuestro caso de estudio y análisis.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Como primera instancia se puede decir según SEEN que el análisis estructurado
“permite al analista conocer un sistema o proceso (actividad) en forma
lógica y manejable" . Por lo tanto, el objetivo final de éste tipo de análisis es
organizar las tareas asociadas con la determinación de los requerimientos y así
tener una comprensión completa y exacta de una situación dada.
A finales de los años 60's y los inicios de los 70's el análisis estructurado surge de
la necesidad de buscar una forma interpretativa más rápida y eficiente, en donde
se puedan definir los requerimientos del usuario y del sistema. Sin embrago esto
todavía no se daba, debido a que existían grandes volúmenes de información que
había que leer los cuales a su vez, traían como consecuencia problemas de tipo:
monolítico, redundancia y ambigüedad.
Como consecuencia de esto, surge una gran variedad de diagramas que permiten
representar las especificaciones funcionales en forma sencilla y de manera rápida,
aumentando con ello el grado de comunicación entre las especificaciones
funcionales y el usuario final (analista, diseñador y programador).
Posteriormente, a mediados de los años 70's estando el análisis estructurado en
su apogeo y de acuerdo a los avances tecnológicos, trae una serie de
consecuencias que limitan al analista hacer un buen desempeño de sus
actividades. Entre estos problemas según Yourdon están los siguientes:
* Distinción poca y difusa entre los modelos lógicos y físicos.
* Limitación para modelar sistema en tiempo real.
* El modelo de datos se hacía de una manera muy primitiva.
Estas y otras razones dieron el nacimiento a grandes mejoras en cuanto al
desarrollo estructurado clásico tales como:
* Diagramas de entidad - relación
* Diagramas de transición - estado
* División de eventos
* Modelos esenciales
* Modelos de implantación
Con la incorporación de estas mejoras surge el Método del Sistema Yourdon,
el cual en su momento vino a revolucionar todo lo referente en materia de Análisis
y diseño estructurado ya que a partir de los años 90's y en la actualidad, se sigue
aplicando para el desarrollo de cualquier tipo de sistema, sin importar los cambios
tecnológicos que se dan día con día.
Para corrobar lo expuesto anteriormente sobre este método, es el estudio que se
hizo del Sistema de Nómina aplicando dicha metodología la cual vino a
comprobar que en la actualidad todavía es operante, ya que no le pide nada a
ningún otro tipo de paradigma que exista en el mercado del Análisis y diseño de
sistemas.
Pero a pesar de todo esto según Yourdon, se siguieron dando problemas tales
como:
* Tras la segunda y tercera corrección de un diagrama, el analista estaba
expuesto a hacer más cambios.
* Debido a la cantidad de trabajo que se requería, el analista dejaba a veces de
dividir el modelo del sistema en los de menor nivel, trayendo como
consecuencia funciones primitivas.
* A menudo no se incorporaban en el modelo del sistema los cambios en los
requerimientos del usuario, sino hasta después de la fase de análisis del
proyecto.
Para subsanar esta serie de dificultades surgen las herramientas CASE las cuales
se utilizan para dibujar diagramas de flujo y de datos entre otros. En la actualidad
muchas de estas herramientas se están utilizando para facilitar la fase de análisis,
disminuyendo con esto la serie de errores que se cometían anteriormente.
Los diversos aspectos del análisis estructurado han cambiado gradualmente a lo
largo de los últimos años. Estos principales esquemas de cambio contemplan lo
siguiente según Yourdon:
* Cambios de metodología
* Partición de acontecimientos
* Herramientas de modelado en tiempo real
* Integración más cercana del modelado de procesos y datos
Y en un futuro no muy lejano se darán si es que ya no se están dando, los
siguientes cambios:
* Mayor difusión del análisis de sistemas en niveles
superiores y organizaciones gubernamentales.
* Impacto sobre la industria del software.
* Mayor proliferación de las herramientas automatizadas.
* Integración del análisis estructurado con la inteligencia
artificial.
Finalmente un enfoque nuevo y diferente al análisis y diseño de sistemas es el
Orientado a objetos (O-O). Los cuales están basados en los conceptos de la
programación orientada a objetos.
Estas a su vez pueden ayudar a responder las demandas organizacionales para
nuevos sistemas que requieran mantenimiento, adaptación y rediseño contínuo.
Básicamente, en este tipo de programación los objetos son creados e incluyen no
solamente código acerca de los datos, sino también instrucciones sobre las
operaciones que se realizan con ellos.
Usando el enfoque de Coad y Yourdon al análisis O-O, empleamos cinco capas
las cuales consisten en lo siguiente:
1. Capa de clase / objeto
2. Capa de estructura
3. Capa de atributos
4. Capa de servicios
5. Capa de áreas
El análisis y diseño estructurado proporciona un enfoque semántico para el diseño
y construcción de sistemas de calidad. A lo largo de las fases el analista debe
proceder paso a paso para obtener retroalimentación de los usuarios buscando
omisión de errores, ya que al pasar demasiado rápido de una fase a otra, puede
provocar que el analista regrese a corregir partes anteriores del diseño.
El objetivo final que nos planteamos en un principio para nuestro caso de estudio
cumplió con las expectativas reales, ya que se logró un mejor y más eficiente
diseño o rediseño del mismo, con respecto del sistema que actualmente se
encuentra funcionando.
Con base al estudio realizado, esperamos obtener los mejores y más eficientes
resultados de nuestro sistema, ya que vendrá a subsanar todas aquellas
deficiencias que se tenían en cuanto a operación y funcionamiento.
Este tipo de metodología es aplicable a cualquier tipo de estudio, ya que un
problema por más complejo que sea, siempre podrá ser resuelto de una manera
fácil y sencilla.
Esta investigación se limitó en saber si el método del sistema de yourdon, era
aplicable a nuestro caso de estudio: El Sistema de Nómina. Con base a esto
podemos decir que sí cumplió con las expectativas finales, ya que se adaptó de
una manera fácil y rápida con respecto a lo que se planeó desde un principio.
Finalmente cabe hacer mención, que en un futuro no muy lejano es necesario
hacer uso de otra herramienta como es el caso del análisis orientado a objetos
(OOA/OOD), ya que es una metodología que hace uso de una descripción breve y
concisa, así como el uso de textos generales como fuentes para las definiciones;
de tal modo que éstas se enmarcan dentro del sentido común y por ende reducen
el empleo de modismos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
SISTEMA DE NOMINA BASADO EN LA METODOLOGIA DE COAD
&amp; YOURDON
Tesis para obtener el grado de
MAESTRO EN CIENCIAS AREA COMPUTACION
Presenta
Mario Alberto Lugo Rivero
Asesor
Dr. Nicandro Farías Mendoza
Colima, col. 27 de Agosto de 2003</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema Administrativo para Talentos Deportivos</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Estandarizar los sistemas de registro de los deportistas.
* Ofrecer seguridad en el manejo de la información.
* Administración de la información rápida y eficiente.
* Programación y calendarización de ciclos de entrenamiento
* Controlar el desarrollo y evolución de los deportistas
* Proporcionar información veraz y oportuna a la Coordinación de Natación que le permita
conocer los niveles actuales y proyectados de los nadadores del equipo representativo</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La necesidad de introducir técnicas administrativas y tecnológicas que de manera conjunta
permitan una permanente revisión del objetivo y control de actividades de los nadadores se hace
justificable por los siguientes motivos:
* No existe un estándar en los registros de los deportistas.
* No se tiene seguridad en el manejo de la información.
* Los procesos se hacen de forma manual lo que provoca que se apliquen a las tareas de
organización de datos a más personas de las asignadas, alejando a las mismas de su
tarea rutinaria.
La programación de actividades se realiza día con día, ofreciendo una visión menos clara
del objetivo final de los entrenamientos.
* La administración de la información es muy tardada, arrojando datos no actualizados de los
resultados y la toma de decisiones, por lo tanto, es errónea.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Las herramientas utilizadas para el desarrollo del Sistema Administrativo de Talentos Deportivos mejoraron conciderablemente el tiempo de cada uno de los ciclos de vida del sistemas.
El desarrollo del sistema con tecnicas internacionales permite a la Direccion de Deportes y Actividades Recreativas el intercambio deportivo con escuelas y clubes de otras partes del mundo incrementando asi la calidad en el desempeño de los nadadores, debido al intercambio de tecnicas y metodos cientificos utilizados en los lugares de origen de los entrenadores de intercambio.

Los nadadores mostraron un considerable aumento en su desempeño al mostrarse mas saludables con la alimentacion sugerida por el sistema y al explotarse de manera individual y cientifica cada uno de sus talentos, la generacion de entrenamientos se adapta a cualquier cambio en la fisionomia y desempeño del nadador. Los menus son creados segun las necesidades del deportista y cumplen con las caracteristicas de una nutricion eficiente, son generados para nutrir y agradar al paladar del nadador.

La planeacion de ciclos se realiza para cada nadador y se da seguimiento del mismo de manera individualizada, el entrenador ahora, se dedica a la depuracion de las tecnicas de los estilos de natacion y en la motivacion de los deportistas.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
FACULTAD DE INGENIERIA MECANICA Y ELECTRICA
corpus_proyectos EN CIENCIAS AREA COMPUTACION
Sistema Administrativo para Talentos Deportivos
TRABAJO QUE PRESENTA EL
Ing. José Porfirio González Farías
PARA OBTENER EL GRADO DE MAESTRO EN CIENCIAS AREA COMPUTACION
ASESOR: M.C. RICARDO FUENTES COVARRUBIAS</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Minería de Reglas de Asociación sobre Datos Mezclados"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Minar reglas de asociación en colecciones de datos mezclados, considerando la semejanza entre objetos y partes de ellos al contar las ocurrencias de los mismos, que permitan el uso de funciones de semejanza menos restrictivas que las permitidas por el algoritmo existente</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Diseñar un método para extraer reglas de asociación, en colecciones de datos mezclados,
incorporando el concepto de semejanza entre descripciones y subdescripciones de objetos, que permita el uso de funciones de semejanza menos restrictivas que las permitidas por el algoritmo existente.

Objetivos particulares
*Extender los conceptos de frecuencia y confianza, patrón frecuente y regla de asociación, incorporando el concepto de semejanza entre descripciones y subdescripciones de objetos con datos mezclados.
*Desarrollar algoritmos de búsqueda de patrones similares frecuentes4 en colecciones de datos mezclados para funciones de semejanza binaria y no binaria, a partir de las extensiones de los conceptos frecuencia y patrón frecuente.
*Definir propiedades de poda del espacio de los posibles patrones similares frecuentes.
*Diseñar algoritmos eficientes de búsqueda de patrones similares frecuentes en colecciones de datos mezclados para funciones de semejanza binaria y no binaria, que incorporen en su estrategia de búsqueda las propiedades de poda encontradas.
*Proponer un algoritmo eficiente de búsqueda de reglas de asociación, a partir de los patrones similares frecuentes encontrados por los algoritmos anteriores.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cómo extraer eficientemente reglas de asociación, en colecciones de datos mezclados, incorporando el concepto de semejanza entre descripciones y subdescripciones de objetos?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Metodología propuesta
Para cumplir con los objetivos particulares propuestos en la sección anterior, se propone la siguiente metodología:
1. Extender los conceptos de frecuencia, confianza, patrón frecuente y regla de asociación, considerando la semejanza entre subdescripciones de objetos con datos mezclados.
a. Extender el concepto de frecuencia considerando la semejanza entre subdescripciones de b. Proponer un algoritmo de recorrido exhaustivo sobre el espacio de combinaciones de
atributos que haga uso efectivo de la estructura propuesta en la búsqueda de
subdescripciones frecuentes.
c. Implementación de la estructura de datos y el algoritmo de recorrido exhaustivo sobre el
espacio de combinaciones de atributos.
d. Realización de pruebas y comparación de resultados con algoritmo ObjectMiner.
e. Análisis de los resultados obtenidos y retroalimentación.
3. Proponer un algoritmo de búsqueda de patrones similares frecuentes en colecciones de datos
mezclados, para funciones de semejanza no binaria.
Para esto se seguirán pasos análogos al los mostrados en el punto 2.
4. Definir propiedades de poda del espacio de posibles patrones similares frecuentes. Estas
propiedades de poda serán definidas de mayor a menor restricción.
a. Diseño de una propiedad de Clausura Descendente análoga a la propiedad Clausura
Descendente del soporte usada en el minado de conjuntos frecuentes de ítems.
b. Diseño de al menos 2 relajaciones de la propiedad Clausura Descendente.
5. Proponer para cada propiedad de poda, comenzando por las más restrictiva, algoritmos de
búsqueda de patrones similares frecuentes en colecciones de datos mezclados, para funciones
de semejanza binaria y no binaria que la satisfagan. Para cada algoritmo se seguirán pasos
análogos al los mostrados en el punto 2.
6. Proponer un algoritmo de búsqueda de reglas de asociación a partir de los patrones similares
frecuentes descubiertos por los algoritmos anteriores.
a. Extender el algoritmo de búsqueda de reglas de asociación propuesto en [3] o diseñar un
nuevo algoritmo, considerando las extensiones de los conceptos de frecuencia, confianza,
patrón frecuente y regla de asociación.
b. Implementación del algoritmo propuesto.
c. Realización de pruebas y comparación de resultados.
d. Análisis de los resultados obtenidos y retroalimentación
objetos al contar las ocurrencias de las subdescripciones.
b. Extender el concepto de patrón frecuente a partir de la extensión del concepto de frecuencia.
c. Extender el concepto de confianza a partir de la extensión del concepto de frecuencia.
d. Extender el concepto de regla de asociación a partir de la extensión de los tres conceptos anteriores.
2. Proponer un algoritmo de búsqueda de patrones similares frecuentes en colecciones de datos mezclados, a partir de las extensiones de los conceptos frecuencia y patrón frecuente, para funciones de semejanza binaria:
a. Diseño de nuevas estructuras de datos para el almacenamiento y recuperación eficiente de las subdescripciones y sus relaciones de semejanza, que aprovechen las repeticiones de las subdescripciones.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El concepto de semejanza es comúnmente usado en disciplinas como Medicina, Geología,
Sociología, etc., como herramienta para la toma de decisiones. El enfoque de minería de reglas de asociación en colecciones de datos mezclados basado en semejanzas, incorpora este conocimiento al proceso de minado de reglas de asociación y permite modelar las relaciones de semejanza entre los objetos y partes de ellos. Con este enfoque pueden ser obtenidos patrones frecuentes y por consiguiente reglas de asociación que no son obtenidas ni por el enfoque basado en discretización, ni por el enfoque basado en conjuntos difusos.
Antes de esta propuesta en el enfoque basado en semejanzas sólo se permitían funciones de
semejanza binaria basadas en los criterios de comparación de los atributos (también binarios) para modelar relaciones entre los objetos y partes de ellos. Además la misma función de semejanza debía ser usada para comparar tanto los objetos como sus partes y debía cumplir que si dos objetos no son semejantes respecto a un conjunto de atributos S1 entonces tampoco lo son respecto a cualquier conjunto S2, tal que S1 C S2. Estos elementos restringen la posibilidad de modelar otras relaciones de semejanzas y como consecuencia algunos patrones frecuentes y reglas de asociación pueden perderse; por lo cual se hace necesario desarrollar métodos para extraer reglas de asociación, que permitan el uso de funciones de semejanza menos restrictivas que las permitidas actualmente.
Como resultados preliminares de la investigación se extendieron los conceptos frecuencia, patrón frecuente, confianza, regla de asociación y la propiedad Clausura Descendente del soporte, considerando la semejanza no necesariamente simétrica, ni binaria, entre subdescripciones de objetos.
Adicionalmente se propusieron dos algoritmos (STreeDC-Miner, STreeNDC-Miner) que se centran en el minado de patrones frecuentes. Ambos algoritmos permiten el uso de funciones de semejanza binaria. El primer algoritmo sólo permite conjuntos de funciones de semejanza que cumplan la propiedad Clausura Descendente, mientras el segundo permite funciones de semejanza que no cumplan dicha propiedad.
Los resultados experimentales muestran que el comportamiento del primer algoritmo en cuanto al tiempo de ejecución es superior al del algoritmo existente (ObjectMiner), al emplear funciones que cumplan la propiedad Clausura Descendente; y que aunque este algoritmo no está diseñado explícitamente para funciones de semejanza que no cumplen esta propiedad, al emplear este tipo de funciones, obtiene patrones similares frecuentes imposibles de encontrar por el algoritmo ObjectMiner.
En el caso del segundo algoritmo, el cual encuentra todos los patrones similares frecuentes, al emplear funciones de semejanza que no cumplen la propiedad Clausura Descendente los resultados experimentales muestran que es factible para minar patrones similares frecuentes en colecciones de objetos descritos por un número pequeño de atributos.
Ambos algoritmos fueron presentados en el XIII Congreso Iberoamericano de Reconocimiento de Patrones, celebrado en la Habana, Cuba y publicados en las memorias de este evento [73]. Con base en los resultados preliminares podemos concluir que los objetivos planteados son alcanzables siguiendo la metodología propuesta.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Minería de Reglas de Asociación sobre Datos
Mezclados
Ansel Yoan Rodríguez González, José Francisco Martínez Trinidad,
Jesús Ariel Carrasco Ochoa, José Ruiz Shulcloper
Reporte Técnico No. CCC-09-001
31 de Marzo de 2009
© Coordinación de Ciencias Computacionales
INAOE
Luis Enrique Erro 1
Sta. Ma. Tonantzintla,
72840, Puebla, México.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Software de Control y Monitoreo de la Cámara Infrarroja del Telescopio de Cananea, Sonora."</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General.
Diseñar e implementar un sistema de control y monitoreo de la cámara infrarroja del telescopio tipo 2 m. de Cananea, Sonora. El sistema debe tomar en cuenta a las necesidades específicas que requieren los astrónomos como son:
- Los modos específicos de observación
- Ejecución de los comandos para el control del telescopio y el instrumento
- Monitoreo de la ejecución de los comandos enviados y del estado del telescopio.
- Obtención y almacenamiento de los datos obtenidos.
Todo lo anterior manteniendo al sistema sobre una interfaz amigable e intuitiva
que tome en cuenta las preferencias de los astrónomos para realizar observaciones de
objetos celestes.

Objetivos Específicos.
- Diseñar un sistema que sea capaz de generar automáticamente los elementos que
requerirá para el funcionamiento del mismo.
- Demostrar que mediante un lenguaje de definición de datos se pueden crear sistemas genéricos los cuales son capaces de agregar o retirar elementos del sistema sin requerir de reprogramar al sistema.
- Crear un sistema que sea fácil de desarrollar, modificar, mantener y extender.
- Diseñar un sistema abierto lo mas independiente posible de la plataforma, que sea
capaz de comunicar códigos en diferentes lenguajes de acuerdo al tipo de módulo,
desde un lenguaje base, que en nuestro caso es Java.
- Demostrar que haciendo uso de tecnología Orientada a Objetos se puede agregar
funcionalidad a las clases generadas por el sistema, realizando de esta forma
implantaciones especificas sobre un software genérico.
- Explícitamente promover la reutilización por el diseño y por la utilización de
tecnologías emergentes que faciliten la reutilización del software.
- Definir claramente la interfaz entre el hardware y la ingeniería de software.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los modelos propuestos durante la fase de investigación han sido implementados
en el software creando bloques lógicos en el sistema que facilitan su entendimiento para
posteriores investigaciones o modificaciones del software, cumpliendo así con el objetivo
de realizar un software fácil de mantener en operación y fácil de extender.
Los archivos de definición de datos XML han permitido crear una vía eficiente
hacia la automatización del sistema. Mediante estos archivos se simplifican las
actualizaciones, modificaciones, o rediseños del sistema de software, ya que de esta
forma no es necesario el tener conocimientos profundos sobre el lenguaje de
programación usado en el desarrollo del software ya que los diseñadores se limitan a
definir los objetos independientemente al lenguaje de programación usado.
La implementación del traductor "XmlToJava" demuestra que a partir de una
definición de datos se pueden obtener clases en cualquier lenguaje de programación, los
cuales pueden ser agregados al sistema automáticamente. La tecnología XML sin
embargo no es una tecnología orientada al desarrollo de sistemas de software, ya que
desde sus inicios ha estado ligada íntimamente a la Web. Sin embargo las facilidades que
ofrece como son la capacidad para describir objetos independiente de cualquier lenguaje
de programación, así como la gran interacción que tiene con el lenguaje de programación
Java hacen de XML un firme candidato como una tecnología que permita el desarrollo de
sistemas automáticos, ya que las facilidades de implementación del lenguaje permiten
que las aplicaciones se basen en XML para manejar las 3 capas de lógica, base de datos
y presentación, presentes en los sistemas actuales.
La implementación del sistema se centró en el instrumento Canica, sin embargo
para posteriores desarrollos de instrumentos, el compilador XmlToJava permitirá crear la
definición de las clases de los instrumentos en lenguaje XML evitando la programación
durante esta fase del desarrollo. Esto comprueba la hipótesis planteada sobre las
facilidades con las que XML contribuye en el desarrollo de sistemas.
El software de control y monitoreo es un sistema genérico que no está orientado
hacia un objeto en especifico por lo que puede ser implementado en cualquier instrumento
que pueda ser controlado por una computadora, esto gracias a la flexibilidad que le
proporciona el estar basado en XML. Sin embargo el control de cada instrumento es
específico al instrumento que se desee implantar, a la tarjeta de control usada para
controlar al instrumento, así como del lenguaje de programación usado para controlar el
hardware del instrumento, el cual varía de acuerdo al ámbito del hardware, por lo que el
sistema debe proporcionar un medio para realizar el enlace de la interfaz visual con el
control del hardware.
El esquema de comunicación de Java con el código nativo dinicamente fue
implementado usando JNI, es decir el sistema dinicamente puede encontrar métodos de
librerías nativas que se encuentren dentro de la misma computadora en la que se
encuentra el código en Java de la interfaz visual. La implementación de la comunicación
de Java con código nativo usando una red local con el esquema de Corba se deja abierto
para posteriores investigaciones.
El esquema de Corba no ha sido implementado por la limitante del tiempo, y
debido a que este no se encontraba dentro de los objetivos de la investigación, sin
embargo la flexibilidad del software permite agregar el esquema en Corba de manera
sencilla.
La implementación de las clases no se limita a enviar comandos de control de la
interfaz en Java hacia el control de la Canica en código nativo para su ejecución y control
del telescopio, es también un medio eficiente para obtener y mostrar datos referentes al
estado del instrumento que permitirán tener una certeza del estado real del telescopio y
de todos sus instrumentos. Por lo que la comunicación se realiza bidireccionalmente, en lo
que respecta al monitoreo del estado de los objetos del telescopio permite inicializar los
datos que se muestran en los paneles, conjuntamente muestra los resultados de ejecuciones de los comandos enviados.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Software de Control y Monitoreo de la
Cámara Infrarroja del Telescopio de
Cananea, Sonora.
Luis David López Gutiérrez.
Luis Carrasco Bazúa.
Jaime Muñoz Arteaga.
Aurelio López López.
Reporte Técnico No. CCC-04-003
4 de Marzo de 2004.
© Coordinación de Ciencias Computacionales.
INAOE
Luis Enrique Erro 1
Sta. Ma. Tonantzintla,
72840, Puebla, México.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>MISIÓN EMPRESARIAL DE LA RED UNIVERSITARIA DE CÓMPUTO DE LA UNIVERSIDAD AUTÓNOMA DE NAYARIT.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Conocer el funcionamiento de los servicios de Internet de la RUC-UAN y
comparar los servicios con otros proveedores de Internet en el Estado de
Nayarit.
Analizar el costo de los servicio de operación de Internet de la RUC-UAN.
* Realizar un estudio de las ventajas y desventajas que puedan tener los
servicios de Internet de las redes privadas y los de la RUC-UAN, así como
el valor comercial que representa internet en el Estado de Nayarit.
* Llevar a cabo un análisis de los costos de Internet de la RUC-UAN con los
costos de Internet de redes privadas.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Internet fue creada a partir de un proyecto del departamento de defensa de los
Estados Unidos llamado DARPANET (Defense Advanced Research Project Network) iniciado en 1969, cuyo propósito principal era la investigación y desarrollo de protocolos de comunicación para redes de área amplia para ligar redes de transmisión de paquetes de diferentes tipos, capaces de resistir las condiciones de operación más difíciles y continuar funcionando aún con la pérdida de una parte de la red (por ejemplo en caso de guerra).
Estas investigaciones dieron como resultado el protocolo TCP/IP
(Transmission Control Protocol/lnternet Protocol - Protocolo de control de transmisión) un sistema de comunicaciones muy sólido y robusto bajo el cual se integran todas las redes que conforman lo que se conoce actualmente como Internet.
Durante el desarrollo de este protocolo se incrementó notablemente el número de redes locales de agencias gubernamentales y de universidades que participaban en el proyecto, dando origen así a la red de redes más grande del mundo.
Trataremos de explicar cual es el funcionamiento de la red Internet, qué la compone y cómo se utilizan los recursos y la información que hay en ella, principalmente a aquellos que comienzan a explorar este mundo, además de que se pueden aportar comentarios y sugerencias, no se trata de explicar (es casi imposible) extensivamente todos los servicios que existen en la red.
ORGANISMOS PARA ESTABLECER ESTANDARES:
El consejo de Arquitectura de Internet, IAB (Internet Architecture Board ), toma las decisiones acerca de los estándares de comunicaciones entre las diferentes plataformas para que puedan interactuar máquinas de diferentes fabricantes sin problemas, determina las necesidades técnicas a corto y largo plazo y tomas las decisiones sobre la orientación tecnológica de la Internet. Este grupo es responsable de cómo se deben asignar las direcciones y otros recursos en la red, aunque no son ellos quienes se encargan de hacer estas asignaciones, para eso existe otra organización llamada NIC (Network Information Center) administrado por el departamento de defensa de los Estados Unidos.
El otro grupo importante es el Grupo de Ingeniería de Internet (Internet
Engineering Task Force- IETF) en el cuál los usuarios de Internet expresan sus opiniones sobre cómo se deben de implementar soluciones para problemas operacionales y cómo deben de cooperar las redes para lograrlo. La dirección de
Internet es en cierta manera una autocracia que funciona.
ORGANISMOS ENCARGADOS DE LA ADMINISTRACION DE INTERNET:
Los organismos encargados de administrar internet controlan entre otras cosas las direcciones, las operaciones y la seguridad de la red.
IANA (Internet Assigned Number Autority - Autorización de números asignados de
Internet) Se encarga de llevar el control de direcciones IP y la definición de los dominios en Internet.
Operación de Internet:
IEPG.- (Internet Engineering Plannig Group- Grupo ingenieril del planeación de Internet) Organismo que lleva el control de la operación de Internet.


Seguridad en Internet:
CERT.- Equipo de Respuesta para Emergencias Informáticas (Computer Emergency Response Teams, Respuesta de Emergencia de Computadora Agrupadas). Existen varios CERT en el Planeta, cada uno de ellos se encarga de auxiliar a usuarios y administradores de redes en cuestiones de seguridad de la información.

El enorme crecimiento de Internet se debe en parte a que es una red basada en fondos gubernamentales de cada país que forma parte de Internet lo que proporciona un servicio prácticamente gratuito. A principios de 1994 comenzó a darse un crecimiento explosivo de las compañías con propósitos comerciales en Internet, dando así origen a una nueva etapa en el desarrollo de la red. Area de aplicación de redes.
Dentro de las aplicaciones más elementales podemos destacar su uso para la transmisión, proceso y almacenamiento de datos que es consecuencia de la propia naturaleza de la red. También puede ser utilizada para ofrecer un canal multimedia; datos, voz, imágenes para dialogar entre los usuarios.
En la automatización de oficinas tiene su impacto en la administración de empresas. Las redes permiten igualmente una recuperación y organización más rápida de las informaciones, agilizando el proceso de decisión.
En el área comercial y bancaria, las redes de computadores pueden ser utilizadas para dar soporte a las transacciones. Las redes pueden ser asimismo utilizadas para dar soporte a cajas automáticas.
En el área médica han causado gran impacto principalmente en hospitales.
Los pacientes están continuamente monitoreados por equipos controlados por computadores. Otro uso de redes se destinan al acceso a bancos de datos sobre informaciones médicas.

En el área gubernamental, las redes pueden ser utilizadas para integrar los sistemas de información de prevención social, permitiendo una mejor y más rápida atención al usuario.

En el área militar, las redes tienen un papel fundamental en la organización de los sistemas de comando y control integrado, por ejemplo, los sistemas de observación y defensas aéreas.
La creciente popularidad de las computadoras personales está abriendo las puertas a la entrada de redes en los hogares, llevando el control de electrodomésticos, así como para la protección del hogar.
El área educativa se beneficia, con el uso de la información educativa apoyada por computador en locales remotos de difícil acceso.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La presente investigación se realizó con el uso de varias herramientas y técnicas de
recolección de información, algunas de ellas fueron el análisis de documentos,
análisis de costos, descripción de conceptos, entrevistas y análisis de los diferentes
servicios. Lo anterior es de suma importancia por lo valioso que resulta el manejo de
la información dado el objeto de estudio, misma que dá el sostén y valida las
, aportaciones.
Fue indispensable el análisis detallado para determinar qué empresa ofrece
los mejores precios del servicio de internet, sin embargo, el punto importante de
estudio después de analizada la metodología y habiendo evaluado el desempeño del
servicio de internet privado y de la RUC-UAN, es posible determinar objetivamente
con la comparación de varias alternativas quién ofrece menor precio pero no
necesariamente el mejor servicio, porque debemos aclarar que algunas empresas
tienen gastos extraordinarios que a simple vista no se aprecian. Por todo lo anterior
podemos concluir lo siguiente:
Estableciendo que no existe el contrato perfecto de los servicios de internet y
considerando la mayor ventaja posible hasta el momento para los usuarios de este
tipo de servicios, podemos decir que Megared resultó ser la opción más conveniente
de las empresas analizadas ya que tiene la ventaja de proporcionar un servicio
donde no se utilizaría línea telefónica conmutada aunque su mensualidad resultó ser
de las más elevadas, pero tiene a su favor que no presenta costos extras, como las
demás empresas.
Es importante señalar que si el tiempo de uso de la red es mucho se
recomienda contratar los servicios de la empresa Megared , conociendo además que
esta empresa utiliza un servidor con software Proxy debidamente instalado que le
proporciona mayor velocidad, con la salvedad de que algunos servicios de internet
no trabajan a través de Proxy.
Por otra parte podemos establecer que las empresas como EDI Homonet,
TELMEX y la RUC-UAN manejan costos extras, entendiéndose como costos extras
aquellos que se cubren por este servicio y que no se incluyen dentro de la renta; es
importante señalar además que en la gráfica 4 observamos que el grupo EDI tiene el
menor costo anual por el uso de internet, sin embargo a ello habría que agregar el
costo de las llamadas para cada uno de los enlaces. Después de realizada la
investigación encontramos en estas empresas que los usuarios de los servicios
exclusivamente de internet manifestaban problemas o reclamos que son recurrentes
como: número de usuarios en la red, una notable lentitud en velocidad con la que se
envía y recibe información, las desconexiones continuas de alguna empresa, costos
más altos por el pago del servicio telefónico entre otros.
Es así como se considera importante que la Universidad debe cubrir aquellas
operaciones que son consideradas como críticas de la administración mediante una
estrategia apoyada en las tecnologías de información, pero sobre todo cuantificar las
necesidades del usuario de los diferentes servicios de internet, para ser más
precisos, se debe conocer los volúmenes de información que maneja el usuario, el
tiempo disponible para trabajar en internet, tipo de información que requiere entre
otras consideraciones, sin embargo, la ventaja que ofrece la RUC-UAN como EDI y
Homonet es que proporcionan servicio medido por hora realizándose una sola
llamada con una sola conexión y con un costo extremadamente pequeño.
Las empresas que utilizan la línea telefónica conmutada como la RUC-UAN,
TELMEX, Homonet y EDI y que ofrecen sus servicios a través de conexión por
modem se dice que son convenientes para personas que se conectan poco a
internet, pero al realizar dicha conexión tiene un tiempo de duración prolongado, en
el caso específico de la RUC-UAN se pueden consultar los planes que ofrece la
misma a través de su página Web.
Para finalizar se recomienda a la RUC-UAN realizar su cambio de 
conexión que tiene en la Universidad de Guadalajara y generar su propia red de salida, además para que la RUC-UAN sea competitiva debe poseer su propia Reddorsal
de salida, independiente de la red tecnológica Nacional de CONACyT.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
MAESTRÍA EN CIENCIAS AREA: COMPUTACIÓN
MISIÓN EMPRESARIAL DE LA RED UNIVERSITARIA DE CÓMPUTO
DE LA UNIVERSIDAD AUTÓNOMA DE NAYARIT.
TESIS QUE PARA OBTENER EL GRADO
DE MAESTRO EN CIENCIAS COMPUTACIONALES
PRESENTA
AMADA CARRAZCO
ASESOR
M.C. ANDRÉS GERARDO FUENTES COVARRUBIAS
COQUIMATLÍN, COLIMA; ABRIL DEL 2000</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo> “Administración de Repositorios Institucionales Híbridos de Acceso Abiert"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En esta sección se plantea de manera específica la problemática que se aborda y se pretende resolver con esta investigación en particular.
Hoy en día cada institución educativa y científica se debe mantener en comunicación, con la finalidad de compartir, generar o renovar conocimientos. Los investigadores, por ejemplo, al momento de desarrollar proyectos se ven obligados a consultar información de diferentes fuentes (libros, revistas, tesis, memorias, etc.); este proceso de consulta resulta con frecuencia frustrante ya que el acervo inverosímil que transita en Internet expone, en su mayoría, suposiciones no comprobadas como datos importantes.
En contraste con lo mencionado anteriormente, el hecho de apoyarse en material científico respaldado por investigadores reconocidos garantiza que las ideas ahí expuestas han sido probadas y sus resultados y conclusiones son reales.
Por lo expuesto en los párrafos previos, resulta de gran importancia una colaboración constante entre instituciones y dependencias para compartir fuentes de información que incrementen el acervo bibliográfico y así dar un soporte significativo a los investigadores que buscan referencias bibliográficas para dar fundamento a sus proyectos. Sin embargo, al menos en la Universidad de Colima, se carece de un sistema o esquema de administración de repositorios eficiente que permita el intercambio y difusión de material bibliográfico de calidad científica en Internet.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un esquema de administración de repositorios institucionales híbridos capaz de ofrecer, intercambiar y difundir material bibliográfico en Internet con la participación de diferentes instituciones educativas y de investigación de forma libre y abierta.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Representar metadatos a través de XML.- Como lo establece el protocolo OAI-PMH y dado que XML es actualmente el lenguaje líder para la transferencia de datos en Internet, para compartir los metadatos propios del sistema es necesario estructurarlos y estandarizarlos implementando XML.
Para lograrlo, en el caso de esta investigación, se desarrolló un módulo capaz de generar un metadato Dublín Core por cada registro de material científico almacenado en la base de datos. Dicha conversión se llevó a cabo siguiendo las especificaciones marcadas por el estándar de generación de metadatos Dublín Core. Una vez estructurados los metadatos, estos son almacenados en un repositorio del servidor donde se encuentran disponibles para ser recolectados por otros cosechadores y recolectores de metadatos, también llamados proveedores de servicios.
Garantizar la conectividad entre diferentes RI.- Evidentemente en una red pueden surgir diferentes tipos de errores, uno de los más comunes es la desconexión o la pérdida de enlace entre dos o más nodos. Sin embargo este tipo de problemas están fuera de nuestro control y en estos casos, lo mejor que podemos hacer es implementar procedimientos a seguir en caso de que ocurra una eventualidad de este tipo para recuperarse con el menor daño o pérdida posible en casos en los que se está haciendo una cosecha remota, por ejemplo.
En estos casos resulta útil emplear especificaciones de control de flujo como las mencionadas en la descripción del protocolo OAI-PMH en el capítulo 2.
Cosechar y exportar metadatos entre diferentes RI.- Es en este punto donde realmente entra en acción el protocolo OAI-PMH. Es aquí donde los proveedores de datos y los proveedores de servicios interactúan entre sí conforme al protocolo OAI-PMH para brindar sus servicios respectivos. El proveedor de datos, quien es responsable de resguardar y garantizar la permanencia vigente de una determinada colección de metadatos, debe ser capaz de poner dicha colección de metadatos a disposición de los proveedores de servicios quienes se dedican a recolectar metadatos y usarlos para brindar servicios al usuario final.
Con base en lo antes mencionado, el sistema desarrollado en esta investigación tiene las siguientes funcionalidades:
* Proveedor de datos que ofrece a otros proveedores de servicios, material científico creado por investigadores de la Universidad de Colima.
* Proveedor de servicios capaz de acceder a los proveedores de datos de otras organizaciones e instituciones científicas.
Implementar un buscador de material bibliográfico.- Como se ha mencionado antes y dado que el sistema desarrollado es también un proveedor de servicios, éste es capaz de brindar funcionalidades al usuario final empleando los metadatos propios y los cosechados de otros proveedores de datos.
Uno de los servicios fundamentales con los que cuentan la mayoría de los proveedores de servicios es la posibilidad de brindarle al usuario final la oportunidad de buscar metadatos en base a determinados parámetros definidos por las necesidades particulares de cada uno de ellos. Es un hecho que sin la implementación de un buen buscador, un proveedor de servicios es incapaz de explotar todas sus posibilidades.
Dada la importancia que tiene en un proveedor de servicios, el sistema desarrollado fue provisto de un buscador donde el usuario final puede acceder y consultar los metadatos tanto de la Universidad de Colima como de otras organizaciones e instituciones científicas obteniendo información científica confiable de libre acceso y respaldada por investigadores reconocidos.
En la Figura 12 se muestra la ontología de material científico usada en este proyecto y su descripción se hace en los párrafos siguientes.
La ontología muestra como nodo jerárquico un repositorio institucional Green Access que ofrece un conjunto de servicios que una o varias instituciones ofrecen a los miembros de su comunidad para la gestión y la difusión de los materiales digitales creados por la misma institución y su comunidad de miembros, el cual se divide a su vez en dos tipos de trabajos de investigación: científica y académica.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El tema de los Repositorios Institucionales, aunque es un concepto relativamente reciente, está marcando nuevas tendencias y adquiere cada vez mayor fuerza conforme al continuo avance y desarrollo de Internet. Día a día, nuestras actividades cotidianas se involucran cada vez más con la tecnología y el intercambio de información y recursos a través de Internet se ha vuelto un factor clave, fundamental y crucial en numerosos sectores sociales. 
En este trabajo se desarrolló e implementó un esquema de administración de Repositorios Institucionales capaz de ofrecer, intercambiar y difundir material bibliográfico entre diferentes repositorios en Internet utilizando la filosofía de contextos abiertos del protocolo OAI-PMH. 
El sistema desarrollado en este trabajo cumplió con los objetivos planteados inicialmente. Se creó una interfaz amigable, intuitiva y relativamente fácil de usar para automatizar los procedimientos de registro y almacenamiento de material bibliográfico de calidad en formato digital. Así mismo, se desarrolló un módulo capaz de recolectar, intercambiar, difundir y compartir material y recursos científicos con otras instituciones educativas y científicas a través de Internet. Se diseñó también una interfaz de búsqueda para localizar y consultar documentos bibliográficos de manera rápida y eficiente. 
Los resultados manifiestan la importancia de la colaboración entre instituciones y dependencias para compartir fuentes de información que incrementen el acervo bibliográfico y así dar un soporte significativo a los investigadores que buscan referencias bibliográficas de calidad para dar fundamento a sus proyectos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
“Administración de Repositorios Institucionales
Híbridos de Acceso Abiert"
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN COMPUTACIÓN
Presenta:
Luis Enrique Rosas González
ASESORES:
D. en C. María Andrade Aréchiga
D. en C. Nicandro Farías Mendoza
Colima, Col. Febrero de 2012</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"APLICACIÓN DE LA USABILIDAD AL PROCESO DE DESARROLLO DE PÁGINAS WEB"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Bajo rendimiento en el desempeño de las labores de los usuarios del Gabinete Jurídico
de la DGAC debido a la poca funcionalidad que le brinda la herramienta Excel para el
ejercicio de su trabajo y para los resultados que desean obtener.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Los objetivos de esta tesis se enmarcan principalmente en lograr el buen y adecuado
desempeño de los usuarios directos de la Unidad Sancionadora del Gabinete Jurídico de
la Dirección General de Aviación Civil, para lo cual se aplicarán las técnicas d IPO a fin
de mejorar y eficientizar la usabilidad de la aplicación. De igual manera se persigue
elevar el grado de satisfacción del usuario en el uso de la herramienta.
Objetivos Generales: Rediseñar mediante la construcción de una herramienta Web, el
sistema de gestión de información del Gabinete Jurídico de la DGAC, Ministerio de
Fomento, Madrid, España.
Específicos: Mejorar y eficientizar el desempeño del trabajo de los usuarios del Gabinete
Jurídico de la DGAC, Ministerio de Fomento, Madrid, España, mediante la construcción rápido y sencillo de la información, la simplificación de su trabajo, la obtención de los
resultados de manera eficiente y eficaz, lo cual se reflejará en la satisfacción y agrado
ejerciendo su trabajo y en la fidelidad hacia el producto.
de una página Web.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Mediante el rediseño de una página Web se logrará eficientizar la labor y desempeño de
los usuarios del Gabinete Jurídico de la DGAC, que les permitirá un manejo más fácil,</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Como conclusiones finales del desarrollo de esta tesis de Master la autora de esta tesis
puntualiza lo siguiente:
* En base a lo presentado a lo largo del trayecto del desarrollo de esta tesis, se
utilizaron todos los recursos para el aprendizaje y aplicación del tema a investigar.
* Se realizaron pruebas, test, cuestionarios, observación de campo, reuniones con los
usuarios, entre otras actividades de usabilidad, que permitieron recabar los
elementos necesarios para lograr el objetivo planteado y revalidar la hipótesis.
* Los resultados se manifiestan en el agrado y satisfacción con que los usuarios de la
DGAC desarrollan su labor debido al uso satisfactorio del nuevo producto Web y
el fácil manejo de su interfaz.
* La autora continuará investigando en los lineamientos de usabilidad para seguir
eficientizando sistemas orientadas al humano.
* Se recomienda finalmente, utilizar en todos los ámbitos requeridos, la aplicación
de las técnicas de IPO para lograr productos que satisfagan las necesidades,
aumentar el desempeño laboral de las personas, y lograr la satisfacción total de los
usuarios.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>FACULTAD DE INFORMATICA
UNIVERSIDAD POLITECNICA DE MADRID
TESIS DE MÁSTER
TECNOLOGÍAS DE LA INFORMACIÓN
APLICACIÓN DE LA USABILIDAD AL PROCESO DE DESARROLLO DE PÁGINAS WEB
Autora: Hayser Jacquelín Beltré Ferreras
Asesor: Dr. Xavier Ferré Grau
Septiembre, 2008</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Framework para Elicitación automática de conocomientos"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Dado que hoy en día casi la totalidad del conocimiento de las organizaciones se
encuentra en formato digital, o bien es posible digitalizarlo con poco esfuerzo, sería deseable
disponer de algún mecanismo que analice tales fuentes de conocimiento y extraiga lo mas
significativo para ponerlo a disposición de quienes lo necesiten en un tiempo adecuado y en
forma apropiada.
De acuerdo a los estudios realizados en variados trabajos sobre LEL y Escenarios
(L&amp;E) se ha demostrado que estos son un eficiente medio para representar conocimiento
relacionado con el léxico de un lenguaje como si también a los sujetos, acciones y objetos
que forman parte de un sistema.
Con el fin de representar por este medio los conocimientos que habitualmente son
consumidos por los distintos participantes de un proyecto de software se presenta en este
trabajo un Framework para generación automática del Léxico Extendido del Lenguaje a partir
de información que se encuentre en documentación específica. A partir de tal léxico con sus
correspondientes descripciones1, el ingeniero de software podrá generar los escenarios
necesarios para representar el conocimiento que desee.
La utilidad de un framework como el que se presenta radica en la rapidez con la que se
pueden detectar los símbolos representativos de un Universo de Discurso (UdeD) 2 y
describirlos en base a la información que en el mismo se encuentra.
El modelo propuesto posee varias aplicaciones que permiten representar y transmitir
conocimiento en forma rápida y concreta.
Algunas aplicaciones son:
* En los casos donde los analistas deben aprender acerca de un dominio del
problema para luego realizar la Especificación de Requerimientos de Software
pueden obtener en forma rápida y sencilla una descripción de los principales
símbolos que forman parte del universo de discurso para luego representar tal
conocimiento en una forma entendible por todos los protagonistas del proyecto.
Basándose en el análisis de documentación relacionada con el universo de
discurso, puede alcanzarse este objetivo.
* Cuando las organizaciones que adquieren software "customizable"3 deben
analizar sus funcionalidades para decidir modificaciones o agregados sobre las
mismas, éstas deben aprender acerca de las prestaciones del producto y el
dominio sobre el cual éste trabaja analizando los manuales de producto y toda
documentación que brinde el proveedor en relación a su software.
Cabe agregar que el tipo de software "customizable" es de particular interés en este
trabajo para estudiar una aplicación diferente al desarrollo tradicional de software. La
particularidad de los procesos de implantación de estos productos es que los analistas
deben transmitir conocimiento a los usuarios y además deben recibir conocimiento de
ellos, a diferencia del desarrollo de software convencional en donde los analistas
solamente se limitan a recibir información.
En resumen, el principal objetivo que persigue el trabajo es generar un framework
que facilite a los ingenieros de software la representación de conocimiento en forma
sencilla y entendible por todos los miembros que forman parte de un proyecto de
software. La representación de conocimiento será asistida por una herramienta que
facilite y agilice las tareas del ingeniero de software en lo relacionado a la
representación y asimilación del conocimiento que se encuentra disperso en fuentes de
documentación.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>A la hora de pensar en incluir funcionalidad a un producto de software numerosas
fuentes de información deben ser consultadas y generalmente no se dispone del tiempo ideal
para el análisis en profundidad de todas ellas. Por tal motivo, un mecanismo de fácil
representación de conocimiento y entendible por todos los actores del proceso sería de
singular importancia a la hora de representar conocimiento que servirá para la toma de
decisiones en cuestiones relacionadas con la funcionalidad de un producto.
Si alguien debiera analizar variadas fuentes de información para decidir cuales serian
los tópicos mas apropiados para ser convertidos en funcionalidad de un producto a
desarrollarse, lo primero que debería hacer es comprender la mayor cantidad de cosas posibles
en el menor tiempo posible acerca del universo de discurso. Algo similar ocurriría si una
organización adquiere un producto "customizable" con posibilidades de ser adaptado a sus
necesidades. Esta debería aprender lo mas posible y en el menor tiempo acerca de las
funcionalidades incorporadas en el producto para decidir cuales deberán ser modificadas,
eliminadas o agregadas.
El tipo de situaciones planteadas precedentemente podrán ser soportadas representando
el conocimiento relevado mediante LEL y Escenarios cuya construcción será asistida por una
herramienta que permite extraer el conocimiento significativo de un universo de discurso
según se muestra en la siguiente figura (1).- Tomando como entrada un documento con un universo de discurso, una herramienta
desarrollada como parte de esta tesis ofrecerá una lista de símbolos candidatos a formar parte
del LEL.
(2).- Dada una lisa de símbolos candidatos, la herramienta asistirá al ingeniero de software
para descubrir las definiciones de los símbolos de la lista encontrando así el significado de
cada uno y su interacción con otros símbolos.
(3).- En base a las definiciones de los símbolos se confecciona el LEL y Escenarios de forma
manual o mediante alguna herramienta como TILS (Tools for the Implementation of LEL and
Scenarios) que permite asistir al ingeniero de software en el proceso de construcción del LEL
y Escenarios [GILÃ¢â‚¬â„¢03].
Se detallan a continuación algunos ejemplos de universos de discurso que suelen ser
fuente de información para los casos citados pudiendo los mismos ser tratados con la
metodología propuesta en este trabajo:
- Documentos con políticas y reglas internas de las organizaciones.
- Documentos con descripciones de productos de software.
- Legislación relacionada con el universo de discurso o con las políticas
organizacionales</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Al iniciarse las actividades relacionadas con el desarrollo de esta tesis se pretendió
realizar una contribución tanto al mundo académico como al profesional. Luego de las tareas
propias de estudio, análisis de antecedentes, implementación de algoritmos, análisis de casos
de estudios y evaluaciones de calidad se concluye que:
* Se incorporó la experiencia existente en el campo de Generación Automática de
Resúmenes para construir herramientas de resumen de texto y de la Ingeniería de
Requerimientos para representar conocimientos (L&amp;E); a fin aplicarla a lo que se ha
llamado en esta tesis Elicitación Automática de Conocimientos.
* Se establecieron heurísticas que permiten identificar la información relevante de un
documento para mostrarla en una forma ordenada y comprensible.
* Se elaboró un framework para trabajar con grandes volúmenes de información en la
elicitación de conocimientos asistida automáticamente en un universo de discurso que
se encuentra documentado.
* Se construyó una metodología de trabajo reproducible por ingenieros de software que
necesiten adquirir conocimientos a partir de cierta documentación de un universo de
discurso.
* Se desarrolló una herramienta, en base a las heurísticas establecidas, que permite
realizar el trabajo que habitualmente hacen los analistas aunque con una velocidad
mucho mayor y con una calidad comparable a la del trabajo de las personas quienes
son hoy en día las mejores analizadoras de información con el objetivo de extraer los
segmentos fundamentales de un universo de discurso.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Framework para Elicitación
Automática de Conocimientos
Tesis presentada para obtener el grado de Magíster en Ingeniería de software
Facultad de Informática
Universidad Nacional de La Plata
Argentina
Alumno: Lic. Daniel Demitrio
Director: Lic. Alejandro Oliveros
Mayo - 2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>INTERFAZ DE COMUNICACIÓN Y ADMINISTRACIÓN DE TAREAS (ICAT) PARA GRUPOS DE TRABAJO UTILIZANDO SOFTWARE LIBRE</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Las herramientas para dar soporte al trabajo colaborativo y comunicación instantánea (citadas en las Tablas 1 y 2) generaron la necesidad de crear una herramienta que integre ambos conceptos (administración de tareas y comunicación instantánea), dado que con esto se facilitaría la comunicación y el seguimiento de un grupo de trabajo cuyos integrantes se encuentren ubicados en lugares remotos y sin la necesidad de utilizar mayor cantidad de recursos, ya que esta mediante una grafica de Gantt mostrada en la ventana de comunicación en tiempo real presenta el estado de avance de las actividades pertenecientes al usuario receptor. Además con la particularidad de ser ampliamente disponible, por lo que el uso de la misma no representará un gasto para las organizaciones.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Crear una interfaz de administración y asignación de tareas para grupos de trabajo con un IM (mensajero instantáneo) integrado que permita la comunicación entre los miembros del grupo.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para el desarrollo de esta herramienta se decidió seguir la metodología tradicional, la cual consta de las etapas de análisis, diseño, implementación y prueba. Cada una de estas etapas serán definidas a continuación.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo se desarrolló un sistema que permite la interacción de personas trabajando en un entorno colaborativo y que se encuentran en ubicaciones geográficamente distantes. Este sistema se diseñó extendiendo el paradigma tradicional de que la información constituye una valiosa herramienta en la planeación de proyectos en entornos colaborativos, y mucho más la facilidad de contar con un medio que permite el intercambio de dicha información y la interacción en tiempo real de los integrantes del grupo.
El diseño del sistema se realizó a partir de las directrices citadas por Nielsen (2002) y partiendo del estudio previo de otras herramientas existentes para soporte del trabajo en entornos colaborativos, sin embargo, un aspecto que se tuvo siempre presente en el desarrollo de esta herramienta, es que la misma sirva como herramienta de trabajo a pequeñas organizaciones que de alguna manera se les hace difícil obtener las grandes herramientas existentes, ya sea por el costo o por los requerimientos de las mismas; para evitar esta limitantes para las pequeñas organizaciones se integran en el ICAT todas las opciones que conlleva desarrollar un proyecto en entornos colaborativos y la comunicación instantánea en una sola herramienta.
Como evaluación informal de la herramienta se llevó a cabo un caso de estudio en la facultad de Telemática de la Universidad de Colima, durando tres semanas. La recolección de los datos se realizó utilizando como instrumentos 2 cuestionarios, diseñados cada uno con objetivo diferente, pero siempre en la búsqueda de tomar en cuenta las respuestas obtenidas de ellos para mejorar o cambiar aspectos considerados. Además, sirvieron como instrumento de medición de resultados los registros en el sistema de las operaciones e interacciones de los usuarios con el sistema durante el caso de estudio, ya que los mismos proporcionaron material suficiente para explorar las respuestas de los usuarios a los cuestionarios y las interacciones reales que llevaron a cabo con el sistema.
En general, los resultados de los cuestionarios y el uso real del sistema muestran que el ICAT fue una herramienta para administración de tareas en grupo útil para los usuarios, estos resultados se hacen visibles en los resultados de las Tablas 24 a la 27. Como se pudo observar en la Tabla 26 la mayoría de los usuarios mostró un grado de interés al usar el sistema como herramienta para el desarrollo de su proyecto.
Decimos entonces que el objetivo de este trabajo que es crear una herramienta que integre el concepto de administración de tareas y comunicación instantánea en una sola aplicación, así como revisión y evaluación del prototipo se cumplió satisfactoriamente.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
FACULTAD DE TELEMÁTICA
INTERFAZ DE COMUNICACIÓN Y ADMINISTRACIÓN DE TAREAS (ICAT) PARA GRUPOS DE TRABAJO UTILIZANDO SOFTWARE LIBRE
TESIS
Que para obtener el grado de
MAESTRA EN CIENCIAS, ÁREA TELEMÁTICA
PRESENTA:
ING. IRONELIS VALDEZ BATISTA
ASESOR:
D. en C. JUAN JOSÉ CONTRERAS CASTILLO
COLIMA, COLIMA. AGOSTO 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"INTEGRACIÓN DE BASES DE DATOS EN INTERNET "</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Analizar las tecnologías para integración de bases de datos en
Internet, afín de instrumentar una aplicación práctica.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Considérese que por motivos de crecimiento de información y de
utilización ya no es posible tener almacenada en una PC única con una
base de datos, se decide comprar otra PC y repartir la base de datos
entre las dos máquinas, pero pasado un tiempo se decide unir otra PC
según las necesidades; en un momento se da cuenta que ya tiene 10 PC,
donde cada una cuenta con una base de datos especial y las cosas van
bastante lentas por que están cargadas de datos. En éste momento quizá
se actualice el sistema con Windows NT y un servidor SQL y se desarrolle
un aplicación lo cual subsanará en gran parte las necesidades de
crecimiento. Pero en dos años el servidor SQL ya no es suficiente, por lo
que decide por una base datos mayor con un sistema operativo Unix pero
las aplicaciones en Visual Basic ya no funcionaran por lo que tendrá que
invertirse mucho dinero y tiempo para actualizar los sistemas. Este
escenario podrá repetirse mucha veces, por lo que lo ideal es un lenguaje
estándar que funcione igual en cualquier máquina, sin importar hacia
dónde se transfiera la base de datos, ni desde qué máquina se trate de
accesar. El WWW es una solución, ya que se puede accesar a la
información desde cualquier parte del mundo o simplemente desde una
red local, así mismo permite una estructura de aplicaciones en un
lenguaje estándar: HTML, interfaz gráfica de usuario (GUI de las siglas en
inglés), soporte de diferentes plataformas, así como soporte de red; con lo
que da al usuario potentes herramientas para accesar sus datos con un
mínimo de gasto y esfuerzo y, utilizando herramientas que funcionan sin
importar el tipo de base de datos, sistema operativo o equipo de cómputo
que se utilice.
El actualizar una página de Internet o mantener actualizados los datos de
la información presentada suele ser una tarea fácil, ya que en ocasiones
sólo se requiere de modificar dos a tres líneas; pero cuando éstos
cambios son constantes, suele convertirse en una tarea repetitiva, ya que
implica en mantener una persona probablemente todo el tiempo de su
trabajo realizando tal tarea o si la actualización se realiza en lapsos de
tiempo muy breves durante todo el día esto resultaría imposible. En
consecuencia con la utilización de páginas creadas a través de consultas
a bases de datos es una elección que disminuye considerablemente el
tiempo de actualización de páginas de Internet</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>I. Investigación bibliográfica
II. Análisis de las tecnologías para manejo de bases de datos en
Internet.
III. Selección de la tecnología de manejo de bases de datos en el
Web.
IV. Selección de lenguaje de programación así como manejador de
bases de datos.
V. Análisis y diseño de la aplicación de control de inventario de
insumos informáticos sobre Internet o una Intranet.
VI. Desarrollo de la aplicación bajo las pautas establecidas en el
diseño.
VII. Pruebas y correcciones.
VIII. Implantación definitiva.
IX. Presentación de resultados y discusión de resultados.
X. Elaboración de material para presentación de los resultados de
la investigación.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Seleccionar una tecnología es una decisión en la que influyen muchos
factores. Es decir, se tendrá que decidir si desea ejecutar una aplicación
en Internet o en una Intranet, así como se debe verificarse el correcto
funcionamiento del servidor utilizando todas las versiones disponibles de
los navegadores a los que espera dar soporte. No se tendrá la seguridad
absoluta de que el servidor se comporta en la forma deseada si no realiza
una completa serie de pruebas basándose en múltiples plataformas.
? La seguridad es, ciertamente uno de los temas más importantes en un
servidor Web. Sin seguridad, la información a través de Internet es
susceptible al fraude y otros usos indebidos por parte de intermediarios.
? Con la integración de bases de datos en el Web, los usuarios de Internet
o Intranet pueden obtener un medio que puede adecuarse a sus
necesidades de información, con un costo, inversión de tiempo, y
recursos mínimos, disminución del tiempo de actualización de páginas de
Internet cuando la información presentada se localiza en bases de datos,
así como, mejorar la capacidad de publicación de información presentada
en Internet cuando esta es creada dinámicamente por consultas a bancos
de información.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
Facultad de Telemática
"INTEGRACIÓN DE BASES DE DATOS EN INTERNET "
TESIS
PARA LA OBTENCIÓN DEL GRADO DE:
MAESTRO EN CIENCIAS, ÁREA TELEMÁTICA
PRESENTA
ING. CÉSAR EUGENIO OLMOS DÍAZ
ASESOR
M.C. JESÚS ALBERTO VERDUZCO RAMÍREZ
COLIMA, COL. NOVIEMBRE DE 1999.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>JAVA- Una Tecnología para Desarrollo Múltiplataforma de Bases de Datos VI</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>“Identificar plenamente las bondades y carencias que ofiece el lenguaje
de programación Java para el desarro3o mulfiplatafoma de bases de
dato".</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Al momento de iniciar a escribir esta tesis, había muchas interrogantes
respecto del tema que se quería plantear, La mayoría de ellas se centraba
en el problema de la delimitación del estudio. Conforme se realizó la
revisión bibliográfica fueron clarificándose algunas cuestiones, que luego
permitieron proponer la hipótesis.
Luego de la revisión bibliográfica, teniendo en cuenta los alcances que
pretende este estudio, y el objetivo que se plantea, se propone la siguiente
como hipótesis principal:
“El lenguaje de programación Java, es una herramienta adecuada para
desarrollo multiplataforma de bases de dato".
Además de la anterior, a continuación se listan otras hipótesis de trabajo:
l Java permite acelerar los ciclos de desarrollo de aplicaciones de
sistemas de información y de bases de datos,
l Java es capaz de gestionar datos haciendo uso de sentencias SQL,
l Java brinda una excelente soporte de Infetiuz Grúñca pura Usuario,
l El código de aplicaciones multiplataforma de bases de datos en Java
es 100% portable.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La hipótesis principal está probada
Dado que fue posible desarrollar el pequeño modelo de bases de datos
multiplataforma, el lenguaje Java es una herramienta adecuada para este
tipo de aplicaciones.
Las hipótesis de trabajo fueron probadas
Porque Java acelera ciclos de desarrollo para estas aplicaciones,
haciendo uso de sentencias SQL, brindando una excelente interfaz gráfica
para el usuario, y siendo su código 100% portable entre plataformas
heterogéneas.
Se permite el intercambio de datos multiplataforma
Esta conclusión es de gran importancia, pues permitiría que todas las islas
de información pudieran ser integradas, llevadas a otras plataformas, o
simplemente usadas para presentación de datos, siendo de utilidad para
empresas, negocios, y el mismo gobierno, cuando sus necesidades
demandan emigrar hacia un nueva DBMS, o una nueva plataforma.
Sobre Java
Es un lenguaje de programación pequeño, simple, seguro, orientado a
objetos, interpretado, optimizado dinámicamente, multiproceso, de
arquitectura neutral, con colector de deshechos, y fuertes mecanismos de
tratamiento de errores, que permite escribir programas en ambientes
distribuidos, y dinámicamente extensibles.
Tocante a la GUI, los desarrolladores de Java se encuentran trabajando en
un proyecto denominado sk&amp;g, el cual tiene como objetivo mejorar las
capacidades de esta, al proveer objetos qw permitan manejar barras de
herramientas, botones con gráficas inte =:adas, entre otros. Esto con la
intención de que quienes están acok -,brados a utilizar este tipo de
objetos, puedan hacer uso de ellos también en Java.
Sobre los manejadores de las fuentes de información
Existen infinidad de manejadores de fuentes de información, estas para
diversidad de plataformas, y cada una tiene diversas carácterísticas y
potencialidades. Las aplicaciones que sean creadas para interactuar con
las fuentes de información tendrán que sacar todo el provecho a estas
características, todo esto dentro del contexto de las instrucciones SQL.
Si alguno de los manejadores de las fuentes de información no satisface las
necesidades de la aplicación, quizá, se tiene que pensar seriamente en un
cambio de manejador, a alguno que sí cumpla con las expectativas y
necesidades de los proyectos específicos con que se esté trabajando.
Un solo lenguaje
En un futuro no muy lejano posiblemente se hable sólo un lenguaje, Java.
5.2
Los desarrolladores seguramente, poco a poco irán encontrando que
Java, es el lenguaje más versatil y potente que existe en la actualidad, y
que también cada vez será mejor. Entonces, se convertirá en la
herramienta de desarrollo preferida, que desplazará a otras existentes, y
entonces habrá llegado el dia en que se hable un solo lenguaje, Java.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Univeraiad de Colima
Facultad de Telemátia
JAVA- Una Tecnología para
Desarro Múltiplatdorma de
Bases de Datos VI
TESIS
PARA LA OBTENCIÓN DEL GRADO DE
MAESTRO EN CIENCIAS, ÁREA TELEMÁTICA
PRESENTA
LIC. JORGE RAFAEL GUTIÉRREZ PULIDO
ASESORES
M.C. JESÚS ALBERTO VERDUZCO RAMíREZ
M.C. RAÚL AQUINO SANTOS
Gokma, GoL2Ziyo 2999</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SEGUIMIENTO DE INDICADORES DE RENDIMIENTO ESCOLAR Y ADMINISTRACIÓN DE PROGRAMAS ANALÍTICOS EN LA FACULTAD DE TELEMÁTICA DE LA UNIVERSIDAD DE COLIMA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la Universidad de Colima cada coordinador académico de los planteles de
educación superior lleva su propio control tanto de los programas analíticos como de
los indicadores de rendimiento escolar, estos datos los almacena cada uno en sus
archivos y con su propio formato. Estos archivos corren el riesgo de perderse por
causas naturales1 como el fuego, inundación o terremotos, por causas intencionales
como el robo del equipo o causas involuntarias por virus electrónicos.
Los indicadores de rendimiento escolar permiten obtener una estadística de los
alumnos que egresaron y desertaron en una generación de una determinada carrera,
esta estadística la llevan a cabo en hojas de cálculo y a veces es tal la cantidad de
archivos generados para una sola generación muy difícil de administrar manualmente
por parte de los coordinadores.
El proceso para determinar los indicadores de rendimiento escolar inicia
cuando se abre una nueva generación para algún plan de estudios. En este punto se
registra el número de alumnos inscritos de dicha generación, por cada semestre que
dure el programa de estudios se actualiza este indicador (ingreso generación), si en
algún momento deserta un alumno, se actualiza otro indicador para ese semestre
(deserción), si hay alumnos rezagados se actualizan los rezagados y el ingreso de
otras generaciones.
Al 2do año de cursado el plan de estudios, es decir, al 3er. Semestre, es
calculado otro indicador, la tasa de retención, se obtiene dividiendo el número de
alumnos que ingresan al semestre mencionado, entre el número de alumnos que
iniciaron en el programa.
Una vez que la generación egresa de la escuela, se hace un corte y se
registran los titulados, los egresados se cuentan una vez que termine el periodo de ordinarios en el último semestre del programa de estudios. Existe la posibilidad de
actualizar hasta en fecha posterior a los exámenes de regularización. Por último, se
calcula la eficiencia terminal, la cual está definida como "la proporción de estudiantes
que concluyen un programa en determinado momento, frente al total de los que lo
iniciaron un cierto número de años antes" (Mendoza, 2004).
Sin embargo, La Universidad de Colima no tiene un control centralizado de sus
programas analíticos, ni de sus indicadores de rendimiento divididos por plantel. Los
programas analíticos contienen información detallada de los planes de estudio de
cada programa ofrecen las escuelas.
Esta información es muy útil para dar a conocer su oferta educativa, tanto a
los estudiantes de la universidad, como a los futuros aspirantes a ella. Estos son
modificados individualmente por el profesor que imparte la asignatura, a petición del
presidente de academia. Estas modificaciones generalmente son anuales. Una vez
que el profesor finaliza las modificaciones, se deben remitir al coordinador de la
carrera para que se almacenen en sus archivos, sin embargo, su disponibilidad para
consulta esta limitada a la computadora del coordinador.
El Comité Curricular, en cada plantel, tiene la responsabilidad de evaluar los
planes de estudios de nivel licenciatura globalmente y semestralmente, la Tabla 1
muestra el tiempo que debe de transcurrir para que los planes de estudio sean
evaluados globalmente, respecto a la duración de dichos planes en semestres.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Automatizar los procesos de generación y administración de indicadores de
rendimiento escolar y programas analíticos de la Facultad de Telemática de la
Universidad de Colima.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Un desarrollo de software, incluye una serie de pasos que van desde la gestión
hasta la implementación, Giorgini y Henderson-Sellers (2005) definen lo siguiente
“Una metodología apunta a prescribir todos los elementos necesarios para el
desarrollo de un sistema de softwar", existen pues diferentes enfoques de las
metodologías de desarrollo, entre ellas se encuentra el modelo de cascada que fue
propuesta por Wiston (1970), otras más recientes como la programación extrema
(Baird, 2003).
Para el desarrollo del prototipo funcional se utilizó el modelo en cascada, el
cual incluye ciclos de retroalimentación, las fases del modelo se muestran a
continuación:</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El prototipo del sistema de información para el seguimiento de los indicadores
de rendimiento escolar y administración de programas analíticos en la facultad de
Telemática de la Universidad de colima, es una herramienta de software diseñada
para facilitar el proceso de gestión de estas dos actividades que desempeña el asesor
pedagógico en la facultad, pero proponiendo un sistema capaz de implementarse a
nivel institucional.
Esta herramienta ofrece a la facultad, un repositorio de información de la
evolución de los programas analíticos que esta ha manejado, y a su vez un medio de
difusión masivo para la oferta educativa del plantel. Ofrece el seguimiento de los
indicadores de rendimiento académico de todas las generaciones de estudio de la
facultad.
Sin embargo los programas analíticos solo se pueden consultar mediante Web,
mas no se encuentran disponibles para su descarga en formato electrónico, lo cual se
puede implementar para una versión futura.
En la navegación del sistema se detectó que no es suficiente el menú de
opciones, y es recomendable adaptar un control de navegación de tipo breadcrumbs9
(migas de pan), para informar al usuario en qué parte del sitio se encuentra.
El prototipo funcional no contiene una sección de búsquedas y los expertos en
usabilidad coincidieron en que el sistema no necesita esta sección, ya que en general
la información es fácil de encontrar mediante los menús.
La ayuda que se integró en el prototipo, no es lo bastante extensa según los
evaluadores de la usabilidad, aunque los usuarios finales no tendrán mayores
complicaciones debido a que el sistema está hecho para apoyar a sus funciones que
ejercen con regularidad, y que dominan en cuestiones de términos.
La retroalimentación de los errores a pesar de que se informa al usuario de
estos, se detectó que no explican lo sucedido, esto se puede mejorar en una
siguiente versión del sistema.
En general, se puede disminuir el uso de la red y de respuesta del sistema
usando más componentes de AJAX, los cuales aportan la sensación de estar usando
una aplicación de escritorio en lugar de una aplicación Web.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
SEGUIMIENTO DE INDICADORES DE RENDIMIENTO ESCOLAR Y ADMINISTRACIÓN
DE PROGRAMAS ANALÍTICOS EN LA FACULTAD DE TELEMÁTICA DE LA
UNIVERSIDAD DE COLIMA
TESIS
QUE PARA OBTENER EL TITULO DE
MAESTRO EN TECNOLOGÍAS DE INFORMACIÓN
P R E S E N T A:
VICTOR HUGO RODRÍGUEZ CASTAÑEDA
ASESORES
D. EN C. RICARDO ACOSTA DÍAZ
D EN C. JUAN JOSÉ CONTRERAS CASTILLO
COLIMA, COL., MAYO DE 2009.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"IMPLEMENTACIÓN DE UN SISTEMA PROTOTIPO DE TRANSMISIÓN DE PAQUETES DE INFORMACIÓN VÍA RADIOFRECUENCIA, UTILIZANDO EL PROTOCOLO AX25, PARA LA RED DE COMUNICACIONES DE LA CRUZ ROJA ESTATAL COLIMA"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El Objetivo general del presente trabajo es presentar la estructuración de un sistema
prototipo de transmisión de paquetes de información por radio utilizando el protocolo Ax.25.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Este sistema tiene como ventaja principal, el ser controlado por computadoras personales
con un enlace vía radio (donde no es necesario tener una super computadora), el cual
permitirá quitar los tiempos de transmisión de grandes volúmenes de información por la red
de fonía (voz), lo cual hace que el sistema de emergencias sea saturado por dicha ocupación
del canal de radiocomunicación.
Utilizando este sistema, la red de fonía no se vería afectada por saturación mientras ambos
sistemas pueden operar simultáneamente pasando más información y eliminando (no
sustituyéndolo) el empleo del fax para envió de grandes volúmenes de información en el
estado.
* La información, en el caso de eventos meteorológicos, se obtendrá de manera mas rápida de
cómo usualmente se recibe (fax), ya que se cuenta con un área de información directamente
ligada a instituciones serias de meteorología y seguimiento de huracanes (Miami, Florida),
lo cual permitirá que la información se obtenga de manera inmediata con una diferencia de
tiempo muy notoria de cómo se trabaja actualmente.
* La asesoría se presta de manera desinteresada, ya sea en la configuración de los sistemas o
en el manejo de los mismos. Los encargados de los sistemas y los radioaficionados que los
apoyan, muchos de ellos son personas desde las más humildes hasta personas con estudios
de doctorado, lo cual contribuye a la constante mejora de los sistemas y el control sobre los
mismos, lo cual va de la mano con el soporte y apoyo, todo con un fin común: contribuir a
la mejora de la radioafición y prestar el servicio a la nación cuando lo requiera.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>I. Investigación bibliográfica.
II. Análisis preliminar.
III. Planteamiento del problema.
IV. Análisis de la propuesta de solución.
V. Solicitud de equipo para la prueba del sistema (Universidad de Colima y
radioaficionados).
VI. Obtener por medio de la Comisión Federal de Telecomunicaciones, la licencia de
radioaficionado para fungir como administrador de la red en el estado de Colima.
VII. Solicitar las direcciones de Internet de uso exclusivo para radioaficionados.
VIII. Instrumentación del prototipo.
IX. Compilación del sistema a emplear por parte del coordinador de la zona norte de la
república Mexicana, Ing. Alejando Pereida (xe2bss).
IX. Convenio de colaboración con la Universidad de Colima para la instalación del sistema en
la red de fibra óptica de la Biblioteca de Ciencias.
X. Implantación y pruebas.
XII. Implantación definitiva
XIII. Presentación de resultados.
XIV. Publicación de resultados.
XV. Recopilación, en un libro de tesis, de la información y uso del sistema.
XVI. Integración a grupos de desarrollo sobre estos sistemas en Internet.
XVII. Participar en el establecimiento de la red Mexicana de comunicaciones digitales para
casos de desastre.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Mencionados los casos de aplicación, utilidad y asesoría sobre la red alterna de
comunicaciones para casos de desastres, la actividad de las instituciones involucradas en el
proyecto, se concluyen los siguientes puntos:
* El vivir en una región de la república Mexicana afectada por varios fenómenos naturales que
producen un desastre (volcán activo, fallas tectónicas, huracanes y productos tóxicos) es
latente por lo cual, la conciencia de Protección Civil es una de las preocupaciones
primordiales del gobierno del Estado de Colima. Con el fin de apoyar a lograr ese fin, el
trabajo de este documento de tesis tiene una aplicación directa para lograr que la
información, control y manejo del desastre se realice de una manera más rápida sin sustituir
ningún sistema de comunicación existente, sino por lo contrario, fortalecer los ya existentes.
* Al ser un sistema en constante evolución y con apoyo de la red mundial de radioaficionados
en el país y en el extranjero, lo convierte en una red de comunicaciones de gran magnitud,
ya que los sistemas de emergencia en la atención de un desastre son limitados. Por lo cual el
trabajar el sistema propuesto (JNOS), permite abarcar áreas no cubiertas por las redes de
comunicaciones establecidas, así, la red JNOS se apoyaría en la red establecida de
radioaficionados, los cuales tienen sus redes distribuidas a lo largo y ancho del planeta
logrando que la respuesta, envío y obtención de información sea coordinada por la unión de
varias instituciones y agrupaciones (radioaficionados, Universidad de Colima y Cruz Roja
Mexicana), permitiendo que Protección Civil y su personal se concentren de manera total a
la atención del desastre.
El constante trabajo por parte de los radioaficionados, para mejorar estos sistemas de
comunicaciones digitales es continuo, por tal motivo, aplicando esta línea de investigación a
las aulas y aprovechando la gran infraestructura de comunicaciones con que cuenta la
Universidad de Colima (mas aun por el apoyo de sus directivos), se pretende la instalación
de mas Nodos JNOS en las escuelas relacionadas al área, las cuales apoyadas por alumnos,
permitan su integración a grupos de investigación sobre el mejoramiento del software y
hardware para el incremento de las velocidades de transmisión (más de 56Kbps), y aun más
importante seria involucrar este grupo de investigación estudiantil a las actividades de
Protección Civil antes, durante y después de un desastre.
* Para lograr la cobertura Estatal ideal, se propone la instalación de un repetidor JNOS
(repetidor de TCP/IP), en el volcán de Colima o el cerro de Tecolapa, para de esta forma
poder tener desde cualquier punto del estado acceso a este sistema y a todas sus terminales
distribuidas en el Estado de Colima. También, implementar el acceso a Internet por equipos
de altas frecuencias (H.F. del inglés: High Frecuency) para los casos en los cuales la salida
por fibra óptica (a Internet y otros sistemas JNOS en el mundo), que esta proporcionando la
Universidad de Colima fallara. Con este equipo se establecería una conexión de más baja
velocidad (600 bps) para no interrumpir el envío y recepción de información generada en
Internet.
* Debido a la versatilidad de este software, se prevé en un periodo no muy largo (6 meses),
emigrar al sistema operativo Linux, donde las aplicaciones TCP/IP son mas fáciles de
implementar y desarrollar ya que se aprovechan las características propias de este sistema
operativo junto a los beneficios de comunicación inalámbrica del JNOS lo cual constituye la
plataforma de operación mas empleada por los radioaficionados en todo el mundo.
El estudio y factibilidad de la red se encuentra documentado, su aplicación y beneficio es
de gran importancia, ignorarlo o no apoyarlo seria un error.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
Facultad de Telemática
"IMPLEMENTACIÓN DE UN SISTEMA PROTOTIPO DE
TRANSMISIÓN DE PAQUETES DE INFORMACIÓN VÍA
RADIOFRECUENCIA, UTILIZANDO EL PROTOCOLO AX25,
PARA LA RED DE COMUNICACIONES DE LA CRUZ ROJA
ESTATAL COLIMA"
TESIS
PARA LA OBTENCIÓN DEL GRADO DE:
MAESTRO EN CIENCIAS, ÁREA TELEMÁTICA
PRESENTA
ING. OMAR ÁLVAREZ CÁRDENAS
ASESOR
M.C. JESÚS ALBERTO VERDUZCO RAMÍREZ
COLIMA, COL. ENERO DE 1999.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>INGENIERÍA DE SOFTWARE Y CIENCIAS DEL AMBIENTE: MÉTODO TECNOLÓGICO ÓPTIMO PARA LA RECUPERACIÓN DE PERFILES ATMOSFÉRICOS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En México no existen suficientes estudios sobre estrategias tecnológicas que
permitan el procesamiento y recuperación de perfiles atmosféricos, desconociéndose en
gran parte de qué manera se comportan las distintas capas de la atmósfera frente a ciertos
fenómenos tales como el efecto invernadero o las UHI. Por lo anterior, no es factible
pronosticar o evaluar de qué forma afectan éstos fenómenos de manera vertical, es decir con
relación a la altura a nuestra atmósfera.
Considerando lo anterior, es necesaria la implementación de estudios que utilicen los
perfiles atmosféricos con la finalidad de apoyar la tomar decisiones de impacto trascendental
para el bienestar de nuestra sociedad y nuestro planeta. Debido a que la recuperación de
perfiles atmosféricos facilita el estudio de estos fenómenos, resulta necesario la
implementación de un método que mediante el uso de herramientas de tecnología permita
llevar a cabo dicha tarea.
En esta investigación se definirán los métodos actuales y opciones viables para
optimizar el procedimiento con que se lleva a cabo la recuperación de perfiles atmosféricos,
con la colaboración del Centro Universitario de Investigación en Ciencias del Ambiente
(CUICA) de la Universidad de Colima Campus Coquimatlán; responsable de desarrollar
investigaciones relacionadas con las ciencias del Ambiente y de la Tierra, así como, registrar,
almacenar y procesar la información recibida de satélites de Órbita baja como son NOAA-N,
GOES-N y FEN YUN. Haciendo uso de las herramientas con las que cuenta el centro
actualmente y tomando en consideración las necesidades que requiere para hacer uso de
estos datos sobre perfiles atmosféricos.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Optimizar un método para la recuperación de perfiles atmosféricos de manera
eficiente, óptima y confiable utilizando nuevas tecnologías y la ingeniería de software
basada en componentes.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Mediante la aplicación de ingeniería y nuevas tecnologías de desarrollo de software
se optimiza y acelera el proceso de obtención y recuperación de perfiles
atmosféricos al mismo tiempo que se aligeran los métodos actuales.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En todo el mundo se realizan diversas iniciativas para prescindir de algunos fenómenos
meteorológicos causados por la acción antrópica tales como el efecto invernadero o la
mitigación de las UHI, estudiando su comportamiento y tendencias, pretendiendo la
reducción de los efectos que esta acción causa. Entre estas iniciativas podemos nombrar el
programa de las Naciones Unidas para el medio ambiente oficina regional para América
Latina y el Caribe. Así mismo, La Agencia de Protección al Medio Ambiente de los Estados
Unidos donde la ciudades como Chicago evalúan cómo aminorar el fenómeno de las Islas de
Calor Urbanas (Galindo Estrada, 2009).
Sin embargo, en nuestro país son pocos los estudios que pretenden solucionar, aminorar o
mitigar fenómenos como estos pues lo que nuestra investigación tiene como objetivo
generar información con tendencia a apoyar la creación de análisis, pronósticos y estrategias, para la reducción o moderación de estos fenómenos y su impacto en la atmósfera así como en sus habitantes. Del mismo modo, esta información es utilizable en varios campos como son las predicciones Meteorológicas, la Climatología y la investigación en Ciencias del Ambiente y de la Tierra.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En base al desarrollo, evaluación y análisis de esta investigación concluye con
múltiples ideas, primero que nada y lo más importante es que la hipótesis que se formuló al principio de este proyecto, el cual menciona que “La aplicación de Ingeniería optimiza y acelera procesos de obtención y recuperación de perfiles atmosféricos, influyendo y
aligerando los métodos actuales, mediante el uso de nuevas tecnologías" se corroboro al
demostrar que la información recuperada por la herramienta desarrollada reduce
considerablemente los tiempo de recuperación de perfiles atmosféricos y, por ende, acelera
los procesos consecutivos a su obtención como son su análisis y evaluación permitiendo que
investigadores y profesionales del área se den a la tarea de pronosticar el comportamiento
de la atmósfera, de una manera rápida y eficiente, eliminando los tiempos inmensos con los
que anteriormente se trataba logrando esto mediante un método que a través de esta
investigación se optimizó.
Seguido de esto también podemos aclarar que el objetivos de este trabajo de tesis fue
aprobados en su totalidad, pues el objetivo principal fue “Optimizar un método para la
recuperación de perfiles atmosféricos de manera eficiente, óptima y confiable utilizando
nuevas tecnologías y la ingeniería de basada en componentes" lo cual se llevó a cabo en la práctica mediante la optimización de un método que utiliza herramientas tecnológicas
integradas en una herramienta de software denominada (REPEAT) que genera a partir de
datos crudos satelitales la recuperación de perfiles atmosféricos, información que presenta una tendencia a la creación de estrategias para la reconocer el impacto antrópico en la atmósfera, mediante el análisis y evaluación de la esta información, así mismo la herramienta al ser evaluada demostró tener una mayor usabilidad que antiguas técnicas para la obtención de perfiles atmosféricos.
Una vez mencionado el objetivo principal quiero remarcar que, los objetivos
específicos también fueron cubiertos en su totalidad, a continuación los menciono, el
primero de ellos fue “Investigar métodos de sondeo meteorológico vertical para la recuperación de perfiles atmosférico". Lo cual se investigó y se encontraron varios métodos para esta tarea, tras esto, se optó por utilizar un método que utiliza los pases satelitales que se obtienen en una estación terrena los cuales mediante un procedimiento permiten recuperar perfiles atmosféricos.
A partir de aquí se comenzó a “Desarrollar un método capaz de acelerar el proceso
de recuperación de perfiles atmosférico", para el cual se utilizaron algoritmos de licencia GNU libre como son el AAPP desarrollado por la EUMETSAT y el IAPP desarrollado por la Universidad de Wisconsin siendo el primero de ellos el que procesa el pase y el segundo el que interpreta los datos del pase convirtiéndolos en información de perfiles atmosféricos,integrándolos en una herramienta de software mediante Ingeniería basada en componentes.
Tras el desarrollo del método se continuo con “Implementar este método como una
herramienta tecnológica con un alto grado de usabilidad para el usuari". Se utilizaron los algoritmos analizados del AAPP para su procesamiento y el IAPP para su interpretación
generando una herramienta capaz de integrar ambas tareas y ejecutarlas mediante una
aplicación que incluye una interfaz grafica.
El último objetivo fue “Evaluar el método desarrollado, demostrando su usabilidad y la
mejora que implica su uso". Lo cual se llevó a cabo utilizando cuestionarios con una escala de usabilidad de sistemas y, tras lo cual, se demostró que es usable y que la interfaz es amigable con el usuario.
En conclusión el desarrollo de una herramienta como esta, la cual convierte
procedimientos lentos y tediosos en tareas fáciles y ágiles, acelerando el proceso de
obtención, análisis, evaluación y pronóstico de información, además, presenta una línea de
trabajo en la que se unen dos ramas académicas cooperando entre ellas, las cuales son la
computación, específicamente la ingeniería de software y las ciencias del ambiente. Aunque
esta herramienta no pretende dar solución directamente a las problemáticas estudiadas por
las ciencias del ambiente, la meteorología o la climatología, es seguro que acelera procesos en los cuales el uso de esta información sea relevante y lleve a una mejora en los procedimientos llevados a cabo por estas áreas, brindando finalmente un beneficio para 
nuestro planeta Tierra.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>FACULTAD DE TELEMÁTICA
INGENIERÍA DE SOFTWARE Y CIENCIAS
DEL AMBIENTE: MÉTODO TECNOLÓGICO
ÓPTIMO PARA LA RECUPERACIÓN DE
PERFILES ATMOSFÉRICOS
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN COMPUTACIÓN
Presenta:
Jorge Fernando Galindo Núñez
Asesores:
D. en C. Pedro Damián Reyes y D. en C. Ignacio Galindo Estrada
Colima, Col., México, Enero 2012</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“ANÁLISIS E IMPLEMENTACIÓN DE UN ESQUEMA DE SEGURIDAD EN REDES PARA LA UNIVERSIDAD DE COLIMA"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Actualmente la Universidad de Colima se ha preocupado por la seguridad e integridad de la información y servicios contenidos en su Intranet, por lo que ha invertido una fuerte cantidad de dinero en equipos y aplicaciones que ayudan a disminuir las vulnerabilidades de seguridad. Esta inversión a logrado incrementar el nivel de seguridad pero no al grado que la institución requiere y esto se debe principalmente a que estos equipos y aplicaciones no están funcionando a un cien por ciento, además la falta de algunos elementos entre los que se pueden mencionar:
*  Políticas oficializadas
*  Procedimientos,
*  Reglas de seguridad y
*  Monitoreo constante de la información que proporcionan los equipos y aplicaciones.
Por lo tanto DIGESET requiere de una planificación que involucre los puntos anteriormente expuestos a través de un plan de táctico de seguridad informática para las funciones críticas.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Analizar el esquema actual de seguridad informática utilizado para la Intranet de la Dirección General de Servicios Telemáticos, con el fin de mejorarlo a través de un plan táctico que de solución a los puntos expuestos en la problemática.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los sistemas de información de hoy en día están relacionados en un 100% con la exposición de la información a entidades externas que son ajenas a la organización o delegación a la que pertenecen. La Universidad de Colima esta compuesta por muchas delegaciones y dependencias, cada una de ellas tienen sistemas de información independientes que hacen uso de la red externa (Internet) e interna (Intranet). Esto los hace vulnerables a eventos de amenazas que conllevan a riesgos que pueden provocar perdidas tangible e intangibles, para evitar estas perdidas que en muchos de los casos serán monetarias, se requiere del planteamiento de un esquema de seguridad que no solo contemple elementos técnicos que comúnmente encontramos en el mundo de la seguridad informática, sino que establezca una base sólida para el establecimiento de controles, políticas y proyectos de seguridad que administren el ciclo de vida de la seguridad informática. DIGESET es la dependencia encargada de la distribución y administración de la red externa e interna de la Universidad de Colima, por lo tanto, será el objeto de estudio de la investigación.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En la Universidad de Colima no existe un documento oficial de políticas de seguridad informática que controle todas las eventualidades de la infraestructura de la Intranet. El campo de seguridad informática cambia constantemente, y de la misma manera cambia las políticas, por lo tanto, la seguridad informática es un proceso que requiere de una administración constante, para alcanzar el éxito en la integración de la seguridad a la cultura organizacional de DIGESET. En otras palabras, la seguridad informática es un proceso cíclico que requiere de la definición, implementación y verificación constante. 
El establecimiento de un esquema de seguridad, no es tarea de una sola persona, todos los integrantes de la institución deben ser involucrados y las políticas de seguridad deben implementarse gradualmente, e ir acompañadas de un programa de concienciación. 
DIGESET es la dependencia encargada de la distribución y administración de la red externa e interna de la Universidad de Colima, por lo tanto, debe incorporar la toma de decisiones basada en el análisis de riesgos para responder a tres preguntas básicas sobre la seguridad, estas son: * ¿Qué quieren proteger?, * ¿Contra quién (o qué) quieren protegerse? y * ¿Cómo lo quieren proteger? Para ello se implementó la metodología de la empresa SCITUM que permitió obtener resultados importantes para el esquema de seguridad actual de la Universidad de Colima 
El primer resultado que se obtuvo de la implementación de esta metodología fue el análisis de vulnerabilidades que involucró los dispositivos (Servidores, ruteadores, firewall e IDS) críticos de la Intranet para llegar a obtener hallazgos a nivel de infraestructura, tal es el caso se los servidores Windows 2000 que cuentan con vulnerabilidades en la administración de las cuentas de usuarios; ya que las cuentas “guest"
 y “TsInternerUser"
 nunca han sido utilizadas y por ende nunca han sido desactivadas, además existen cuentas de usuarios que contienen contraseñas que nunca caducan, también estos servidores hacen uso de la aplicación FTP, por lo que se requiere que el personal responsable lleve a cabo las actualizaciones necesarias para el buen funcionamiento de la aplicación Serv-U. Por otro lado, los servidores Unix evaluados cuentan con diversos tipos de vulnerabilidades de riesgo alto y medio, a través de las cuales un intruso podría ejecutar comandos remotamente o ganar acceso a los equipos con privilegios de administrador, en este conjunto de servidores es importante llevar acabo la actualización de versiones de algunas aplicaciones como OpenSSL, Apache y SSH entre otros, así como la planificación para la aplicación de los cambios necesarios; con estos hallazgos actualmente los administradores de los servidores se encuentran implementando las mejoras a corto plazo de los equipos para apoyar la etapa de preparación, que conllevará a la implementación del modelo de operación propuesto en el capítulo 4. 
Se pudo establecer a partir de todos los hallazgos que se encontraron en la etapa de análisis de vulnerabilidades, que los dispositivos de comunicación (conmutadores, ruteadores y acceso remoto), cuentan con relativamente pocas vulnerabilidades, en el caso de los ruteadores solo se debe de considerar la utilización de algún tipo de encriptación para la configuración remota. La infraestructura utilizada para los servicios de acceso remoto vía MODEM a la intranet universitaria no establece algún método de autenticación en especial, por lo tanto es deseable especificar un método seguro. 
Por otro lado, se encontró que: 
*  La seguridad perimetral cuenta con una configuración básica. 
*  Aun no se aplican las políticas específicas para cada servicio de la red. 
*  Los servidores aún no están en la zona que les corresponde en el firewall. 
*  No existe un protector de intrusos para cada delegación de la Universidad, que le permita determinar a los administradores de la intranet que es lo que se está transmitiendo en dicho campus. 

Se debe considerar que con la investigación se logró recopilar la información necesaria para la configuración adecuada de la seguridad perimetral. 
La metodología no solo permitió encontrar hallazgos a nivel de infraestructura, sino que también a obtener hallazgos en función de procesos y gente, estas dos categorías se obtuvieron a través de entrevistas, visitas a las instalaciones de DIGESET e información proporcionada por el personal. Además los estudios de madurez nos permitió obtener una medición del nivel de seguridad de la Intranet en algunos dominios con respecto al estándar ISO/17799 (ver tabla 3.3). 
Con los hallazgos encontrados se llevó a cabo el análisis de riesgos que nos ayudó a determinar los controles necesarios para mitigar los riesgos y establecer una prioridad de las actividades a realizar de acuerdo a su tiempo de implantación y costo. Estas actividades se encuentran descritas en el capítulo 4. 
Finalmente se formuló un plan táctico que consistió en el establecimiento de lineamientos, que incluyen todas las líneas tácticas vistas en el capítulo 4, por ejemplo la definición de funciones de seguridad y la consideración de un programa de concienciación y establecimiento de las políticas de seguridad de la Intranet universitaria, que debe de seguir el personal de DIGESET para adoptar el modelo de operación que se plantea. El proceso de mejoramiento de la seguridad es gradual y por fases, por lo tanto, las líneas tácticas permiten definir una ruta crítica de ejecución que elevará el nivel de seguridad, dependerá entonces, del comité de seguridad de DIGESET el llevar acabo la implementación del plan táctico. Al final del capítulo 4 se presentan los factores críticos de éxito que son resultado de un análisis minucioso de la investigación con los cuales la Dirección puede iniciar la implementación, esto les permitirá reducir tiempos y por ende obtener resultados a corto plazo.

48.-
En base al desarrollo, evaluación y análisis de esta investigación concluye con múltiples ideas, primero que nada y lo más importante es que la hipótesis que se formuló al principio de este proyecto, el cual menciona que “La aplicación de Ingeniería optimiza y acelera procesos de obtención y recuperación de perfiles atmosféricos, influyendo y aligerando los métodos actuales, mediante el uso de nuevas tecnologías."
 se corroboro al demostrar que la información recuperada por la herramienta desarrollada reduce considerablemente los tiempo de recuperación de perfiles atmosféricos y, por ende, acelera los procesos consecutivos a su obtención como son su análisis y evaluación permitiendo que investigadores y profesionales del área se den a la tarea de pronosticar el comportamiento de la atmósfera, de una manera rápida y eficiente, eliminando los tiempos inmensos con los que anteriormente se trataba logrando esto mediante un método que a través de esta investigación se optimizó.
Seguido de esto también podemos aclarar que el objetivos de este trabajo de tesis fue aprobados en su totalidad, pues el objetivo principal fue “Optimizar un método para la recuperación de perfiles atmosféricos de manera eficiente, óptima y confiable utilizando nuevas tecnologías y la ingeniería de basada en componentes."
 lo cual se llevó a cabo en la práctica mediante la optimización de un método que utiliza herramientas tecnológicas integradas en una herramienta de software denominada (REPEAT) que genera a partir de datos crudos satelitales la recuperación de perfiles atmosféricos, información que presenta una tendencia a la creación de estrategias para la reconocer el impacto antrópico en la atmósfera, mediante el análisis y evaluación de la esta información, así mismo la herramienta al ser evaluada demostró tener una mayor usabilidad que antiguas técnicas para la obtención de perfiles atmosféricos.
Una vez mencionado el objetivo principal quiero remarcar que, los objetivos específicos también fueron cubiertos en su totalidad, a continuación los menciono, el primero de ellos fue “Investigar métodos de sondeo meteorológico vertical para la recuperación de perfiles atmosféricos"
. Lo cual se investigó y se encontraron varios métodos para esta tarea, tras esto, se optó por utilizar un método que utiliza los pases satelitales que se obtienen en una estación terrena los cuales mediante un procedimiento permiten recuperar perfiles atmosféricos.
A partir de aquí se comenzó a “Desarrollar un método capaz de acelerar el proceso de recuperación de perfiles atmosféricos"
, para el cual se utilizaron algoritmos de licencia GNU libre como son el AAPP desarrollado por la EUMETSAT y el IAPP desarrollado por la
Universidad de Wisconsin siendo el primero de ellos el que procesa el pase y el segundo el que interpreta los datos del pase convirtiéndolos en información de perfiles atmosféricos, integrándolos en una herramienta de software mediante Ingeniería basada en componentes.
Tras el desarrollo del método se continuo con “Implementar este método como una herramienta tecnológica con un alto grado de usabilidad para el usuario"
. Se utilizaron los algoritmos analizados del AAPP para su procesamiento y el IAPP para su interpretación generando una herramienta capaz de integrar ambas tareas y ejecutarlas mediante una aplicación que incluye una interfaz gráfica.
El último objetivo fue “Evaluar el método desarrollado, demostrando su usabilidad y la mejora que implica su uso."
. Lo cual se llevó a cabo utilizando cuestionarios con una escala de usabilidad de sistemas y, tras lo cual, se demostró que es usable y que la interfaz es amigable con el usuario.
En conclusión el desarrollo de una herramienta como esta, la cual convierte procedimientos lentos y tediosos en tareas fáciles y ágiles, acelerando el proceso de obtención, análisis, evaluación y pronóstico de información, además, presenta una línea de trabajo en la que se unen dos ramas académicas cooperando entre ellas, las cuales son la computación, específicamente la ingeniería de software y las ciencias del ambiente. Aunque esta herramienta no pretende dar solución directamente a las problemáticas estudiadas por las ciencias del ambiente, la meteorología o la climatología, es seguro que acelera procesos en los cuales el uso de esta información sea relevante y lleve a una mejora en los procedimientos llevados a cabo por estas áreas, brindando finalmente un beneficio para nuestro planeta Tierra.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
Facultad de Telemática
“ANÁLISIS E IMPLEMENTACIÓN DE UN ESQUEMA DE SEGURIDAD EN REDES PARA LA UNIVERSIDAD DE COLIMA"

TESIS
Que para obtener el grado de
MAESTRO EN CIENCIAS, AREA TELEMATICA
Presenta
Ing. Aaron Clemente Lacayo Arzú
Asesor
M.C. Raymundo Buenrostro Mariscal
Colima, Colima Julio de 2004</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"GDesign 1.0 Una aplicación de diseño basado en las gramáticas generativas y en la vida artificial "</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Este proyecto tiene como objetivo desarrollar un paquete de diseño digital que ofrezca al diseñador este conjunto de procesos entre los cuales resultan particularmente útiles las gramáticas generativas, los fractales y la autosimilaridad y la vida artificial.
Este software tiene como objetivo principal, en una primer fase, apoyar y fortalecer educativamente la enseñanza del diseño en los aspectos relativos al estudio de la forma, sin que dicha impostación inicial perjudique su expansión y la implementación de funciones más complejas.
El objetivo general de la herramienta es ofrecer un entorno de diseño generativo facil de utilizar, interactivo, personalizado y en grado de ofrecer un apoyo al estudio de formas, al desarrollo de diseños de varias naturalezas en 2 o 3 dimensiones y a la experimentación arquitectónica.

Desarrollar una herramienta de diseño generativo basada en dos técnicas: las gramáticas generativa (L-Systems) y algunos aspectos de la vida artificial (Cellular Autómata). 
Dicha herramienta tiene como finalidad principal el estudio de la forma por medio de procesos emergentes. Se trata entonces de una herramienta académica, pensada, por ahora,  por fines educativos, formativos y de investigación
Aplicaciones: se puede utilizar dentro los cursos de diseño en especial modo aquellos que implican las cuestiones formales y estéticas del diseño, y puden ser de mucho interes para varias instituciones académicas, no solo de nuestro medio.
Funciones y características principales:
Diseño de modelos arquitéctonicos bi y tridimensionales 
Diseño de gramáticas generativas personalizadas
Diseño e implementación de primitivas geométricas
Editing interactivo de los procesos generativos 
Gestión de base de datos de lenguajes y gramáticas
Gestión de colores y texturas
Compatibilidad con VRML y paquetes freeware de rendering y animación 3D
Gestión de imagenes bitmap y compatibilidad con los formatos BMP y TGA en alta resolución
Características técnicas
Compatible Windows/Linux
Implementación en VisualBasic, C++ o en Java
Interfaces gráfica
Expandibilidad
Compatibilidad</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Una de las ventajas del diseño generativo se refiere a la metodología emergente, abierta e interactivas que las herramientas generativas proporcionan, facilitando así una aproximación sistemica a los problemas del diseño. Dicha posibilidad permite integrar, en modo organico, las complejidades inherentes a los diferentes contextos a los cuales el diseño se refiere. Elementos prexistentes al diseño como las estéticas, las restricciones urbanísticas y ecológicas, los factores sociales y culturales pueden, gracias a la tecnología de la información, ser aprovechados más facilmente.
En segundo lugar, las herramientas generativas relacionan el diseño con los procesos naturales, tanto en los aspectos constructivos (bioingeniería, bioarquitectura) cuanto en los aspectos formales. Aqui las simulaciones y los modelos matemáticos permiten aprovechar creativamente de la complejidad y de la belleza de la naturaleza, ampliando notablemente las posibilidades creativas del diseñador.
Finalmente, la velocidad operativas de los modernos ordenadores permite al diseñador concentrarse en los aspectos estéticos y conceptuales y de experimentar rapidamente diferentes soluciones proyectuales ya que el proceso generativo se encarga de generar las diferentes variaciones de los diseños.
El diseño con la vida artificial y con otros sistemas generativos se puede hacer a mano, pero las caracterísitcas recursivas de estos algorítmos y la grande complejidad geométrica que se puede generar con ellos hace que un software sea la solución más indicada, tanto desde el punto de vista productivo y profesional cuanto educativo y experimental.
Los costos son bajos porque no se requiere de periféricos especiales ni especiales exigencias de memoria, velocidad de procesamiento y visualización en pantalla</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>PONTIFICIA UNIVERSIDAD CATÓLICA DEL PERÚ
ESCUELA DE GRADUADOS

 


GDesign 1.0
Una aplicación de diseño basado en las gramáticas generativas y en la vida artificial




Tesis para optar al Título de Magister en Ciencias de la Computación
Umberto Luigi Roncoroni Osio
Lima- Perú
Febrero 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Estimación de Proyectos Para Sistemas Basados en Conocimiento"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los sistemas basados en conocimiento (SSBBCC) son desarrollados y
aplicados en universidades y organizaciones dedicadas a la investigación y en
las empresas de tecnología de información. Estas últimas han comenzado a
pensar en ellos con una visión comercial, provocando su expansión y su
inclusión en proyectos donde ambos coexisten, lo cual amerita, a que sean
tomados en cuenta al momento de realizar una estimación global.
En base a un estudio preliminar efectuado en la etapa de elaboración del
anteproyecto de tesis, se han analizado someramente las técnicas enunciadas
en el punto 2.3, con el fin de verificar si son aplicables directamente a los
SSBBCC. Una vez finalizado estos estudios preliminares, es posible enunciar
que no se identifica expresamente la aplicabilidad de dichas técnicas a
SSBBCC, lo cual lleva a la realización de estudios más profundos cuyo objeto
es el de determinar si es posible aplicar los FP directamente a los tipos de
sistemas en cuestión, o si es necesaria alguna adecuación para lograr
estimaciones más precisas.
Una vez hechas las justificaciones del caso, es posible pensar en un
objetivo para el trabajo de tesis, el cual sería precisamente: determinar la
aplicabilidad de puntos de función a SSBBCC y construir una herramienta
o producto software que los incluya.
En este tratado solamente van a ser sometidos a estudio los métodos que
se exhiben en el punto 2.3, dejando abierta la posibilidad del estudio de otros
métodos o técnicas para futuras investigaciones.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Como se ha manifestado en el capítulo V (dedicado íntegramente a los
SSEE) este tipo de software ha comenzado a ser visualizado como una opción
para resolver problemas donde se requiere experticia que debe ser aplicada en
uno o varios lugares al mismo tiempo o cuando la presencia de uno o varios
expertos resulta un tanto crítica debido a la hostilidad de las instalaciones.
El uso de los SSBBCC, con fines comerciales, hace posible que en muchas
empresas se comiencen a gestionar, administrar y controlar los recursos
previstos frente a los recursos utilizados. Este trabajo constituye un aporte
precisamente para ayudar al encargado de proyecto a gestionar los recursos
que va a necesitar a la hora de iniciar un proyecto con las características de los
desarrollos de SSBBCC.
Los proyectos de desarrollo de de SSBBCC, al igual que cualquier otro tipo
de desarrollo, deben ser controlados y medidos, de lo contrario su gestión y
seguimiento no serían correctos.

CONSIDERACIONES PREVIAS A LAS CONCLUSIONES
Algunas consideraciones previas a las conclusiones propiamente dichas se
exponen a continuación:
i) De acuerdo al estudio y análisis efectuado en el capítulo IV, no
existe en el mercado o al menos no se ha identificado
explícitamente la presencia de una herramienta que sea aplicable a
los SSBBCC.
ii) Esto ha llevado a tomar como base de experimentación algunos de
los métodos existentes y sacar provecho de ciertos aspectos.
iii) El método de estimación que se presenta en este trabajo es
experimental; es decir, harían falta una serie de estudios empíricos
como para dar un soporte ciento por ciento fiable. A pesar de ello,
en el capítulo XII, se han sometido a medición sendos trabajos
académicos del Instituto Tecnológico de Buenos Aires (ITBA) con
resultados altamente satisfactorios y con márgenes de error
tolerables.
iv) Los datos de los valores del coeficiente de ajuste lógico (CAL) que
en un principio son 0.800 para entradas y salidas y 1.200 para
conceptos, puede variar en función de los resultados que se vayan
obteniendo y a medida que se recopile información de proyectos
basados en esta tecnología.
v) Otro tanto pasa con los valores de las características técnicas de la
aplicación que se quiere medir. En un principio la cantidad de
parámetros de ajuste técnico es de diez (10), pero se pueden
agregar o eliminar en función de los resultados y de las
características particulares de la instalación donde se desarrolla el
software.
vi) El mismo tratamiento del punto anterior se realiza para los
coeficientes de ajuste y variación técnica.
vii) Si bien es cierto este tipo de métodos se aplican en etapas
tempranas del desarrollo, conviene utilizarlo en el momento de
tener definida la tabla de concepto-atributo-valor (etapa de
conceptualización), de manera de poder agrupar en transacciones
lógicas los elementos de entradas, salidas y conceptos.
Teniendo en cuenta estas consideraciones, se puede concluir lo siguiente:
a) El método de estimación para proyectos que incluyan a
SSBBCC, ha resultado útil cuando se lo ha aplicado a sendos
trabajos del ITBA (capítulo XII). Además ha arrojado
resultados coherentes y con márgenes de error aceptables.
b) Representa un aporte para el encargado de proyectos que
incluyen a SSBBCC, especialmente a la hora de estimar los
recursos necesarios para continuar o no con dicho proyecto.
c) El método sirve como línea base y permite detectar desvíos
en cuanto a los tiempos de duración total del proyecto.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>TESIS DE MAGISTER
EN INGENIERÍA DEL SOFTWARE
Estimación de Proyectos Para
Sistemas Basados en Conocimiento
AUTOR : ING. JOSÉ DANIEL OVEJERO
DIRECTORES
DR. DANTE CARRIZO (UPM)
M.ING. EDUARDO DIEZ (ITBA)
BUENOS AIRES, 2006</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Extensión de un lenguaje formal (LCARS) para especificar problemas de clasificación y de selección de rasgos, mediante la construcción de un intérprete optimizado"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Aportar una solución para aliviar la escasez de herramientas tecnológicas (software) de apoyo
a la labor de investigadores, docentes y estudiantes de RP, en particular aquellos que trabajan
bajo el enfoque lógico-combinatorio (RPLC).

Objetivos específicos
"Adaptar y extender la definición existente del lenguaje LCARS.
"Crear un intérprete para el lenguaje
"Proponer un producto de software que permita reducir el tiempo de desarrollo de
aplicaciones para investigaciones, estudios, ejercicios, etc en RP.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En el marco del RPLC se busca modelar problemas sin incurrir en limitaciones acerca del
tipo de variables usadas para representar los atributos de los objetos estudiados, así como suponer distribuciones de probabilidad, topologías de clase, etc [57].
Esto implica estudiar objetos descritos por atributos tanto de naturaleza cualitativa como cuantitativa, lo que a su vez complica considerablemente el proceso de evaluar la semejanza entre dos o más patrones, dificultando así el proceso de clasificación o reconocimiento.
Debido a esto, los diferentes intentos por desarrollar herramientas para modelar el proceso de reconocimiento según este enfoque carecen, en general, de la flexibilidad necesaria. Puesto que exigen al usuario ajustarse a lo que ofrece la herramienta sin dar oportunidad de que el usuario defina nuevas funciones de semejanza, tipos de datos, etc.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>reconocimiento de sus bloques constituyentes y un proceso de aprendizaje que se cree
corto debido a la escasez de estructuras propias de los lenguajes de programación.
LCARS mantiene la especificación de un problema independiente de su solución, y es
posible utilizarlo desde cualquier aplicación en WIN32. El lenguaje es suficientemente
flexible para expresar, de diversas maneras, funciones de semejanza y criterios de
comparación, facilidad que hasta hoy no era disponible.
El lenguaje, aunque busca abarcar la mayoría de los fenómenos comunes y posibles en
la especificación de problemas de RP, no es forzosamente completo y, por tanto, es
factible de ser extendido para insertar nuevas funcionalidades.
El producto de esta investigación extiende los trabajos pasados sobre lenguajes y
herramientas para RP, en el marco del enfoque lógico-combinatorio dando una primera
aproximación al producto final SHELL-STUDIO. LCARS es el código que para ese
proyecto se genera, y se entiende que puede ser utilizado por la comunidad que
maneja este enfoque sin ninguna ambigüedad ya que está basado en un modelo solido.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Extensión de un lenguaje formal (LCARS) para
especificar problemas de clasificación y de selección
de rasgos, mediante la construcción de un
intérprete optimizado
TESIS
Que para obtener el grado de:
Maestro en Ciencias de la Computación
Presenta
Juan Evencio Guzmán Trampe
Director de tesis
Dr. Salvador Godoy Calderón
México D.F. marzo 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Aprendizaje Incremental en Sistemas Multi-Agente BDI"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La carencia de una propuesta formal de aprendizaje [35] en agentes especificados por el
modelo BDI. El paradigma de agentes sapientes propuesto en [59] presenta razones fuertes
para incorporar aprendizaje basado en la experiencia en los agentes inteligentes. El aprendizaje
incremental es un aspecto que debe ser seriamente considerado en el desarrollo de
agentes inteligentes. El protocolo SMILE reduce el envío de mensajes en la comunicación,
y envía aquella evidencia que aporta información relevante para el aprendizaje, lo anterior
se traduce a un menor uso de recursos computacionales, y en una convergencia más
rápida de la hipótesis aprendida. Además, para el caso de los ejemplos vistos en esta tesis,
no siempre resultó posible aprender localmente, debido principalmente a la carencia de
informaciónTowards-BDI-SA, Jason-Smiles.
En esta tesis atacamos:
1. Teoría de Aprendizaje Formal: extendemos la semántica operacional de AgentS-
peak(L), presentando así, una primera aproximación formal del proceso de aprendizaje.
2. Aprendizaje Distribuido Incremental: el aprendizaje por SMILE se categoriza en el nivel 1 y nivel 2 de circunspección para agentes, bajo la suposición de que el
conocimiento se encuentra distribuido en el SMA.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Esta tesis se inspiró inicialmente en la caracterización de Agentes Sapientes hecha por
el paradigma de Sapiencia Computacional [59]. Se destaca la importancia que tiene el
aprendizaje incremental en los sistemas inteligentes, a través de la caracterización hecha
para los agentes sapientes, los cuales son vistos como sistemas que aprenden tanto su
estado cognitivo como sus habilidades, dentro de un ambiente social, a través de la experiencia
[65]. Aquí, conectamos la idea de efecto-causa, con la de función inversa [59], y a su
vez con la de aprendizaje. Una primera re
exión para incorporar aprendizaje por experiencia
en agentes BDI, contempla una extensión a la arquitectura BDI [31]. Se argumenta
cómo la función directa puede ser caracterizada por todos los componentes comunes a una
arquitectura BDI y, se propone la extensión en forma de una meta-función que trabaja de
forma paralela, cumpliendo dos tareas: i) emphauto-observación sobre las acciones resultantes;
y ii) como un regulador entre la función directa y la inversa, dónde ésta última se
incorpora mediante un mecanismo de aprendizaje inductivo que retroalimenta el sistema.
Basándonos en el trabajo hecho sobre aprendizaje intencional [36{39], discutimos algunas
extensiones al modelo BDI para incorporar tanto aprendizaje intencional como social,
en donde:
El elemento de aprendizaje retroalimenta al sistema (función inversa), utilizando
aquellas creencias relevantes al plan ejecutado que falló para corregir su situación.
Lo anterior ha sido, actualmente implementado en esta tesis mediante mecanismos
de aprendizaje inductivo (TILDE y FOL-SMILE) [40, 41] .
La meta-función es planteada inicialmente, como un meta-plan que cumple la función
de recolectar información sobre los planes ejecutados [40]. El cual es incorporado
mediante la detección de fallo de planes [41].
Como resultado, nuestro agente BDI, capaz de aprender de forma incremental, y se
puede aproximar a la caracterización de agente sapiente.
Aterrizamos esta tesis partiendo del trabajo de [35], en el cual A. Guerra-Hernández
aborda exitosamente el problema de aprendizaje SMA en términos de una arquitectura
BDI (programada en Lisp). Se ha utilizado como hilo conductor para esta tesis el hecho
sobre aprendizaje intencional en [36{39],
En una primera contribución [40], logramos además, reforzar el fundamento pragmático
de [35] mediante una implementación en Jason (intérprete bien conocido de AgentS-
peak(L)). También, pasamos de una implementación en un nivel de diseño, a una hecha en
un lenguaje formal orientado a agentes BDI. Al respecto A. Rao advierte que ha existido
una divergencia entre práctica y teoría en el área de investigación de agentes BDI [68].
En [40] disminuimos considerablemente dicha brecha para [35].
Al implementar el mecanismo de aprendizaje con el algoritmo de TILDE [40], no es
posible garantizar un aprendizaje verdaderamente incremental dentro del SMA. Con el
propósito de sanear dicha carencia y asegurar un aprendizaje incremental en el SMA, en
una segunda implementación, se escala SMILE a lógica de primer orden, para poder ser
adoptado como protocolo de aprendizaje por los agentes BDI. Para este fin resultó de
gran importancia el trabajo llevado por A. Luna, de forma paralela a esta tesis, en la cual
aborda el problema de aprendizaje incremental en primer orden, y propone un algoritmo
para inducir árboles lógicos de decisión (ILDT) [41]. Este algoritmo hizo posible contar
con un Mecanismo-M localmente eficiente [11] compatible con lógica de primer orden.
Finalmente, al inclinarnos por la tendencia de pasar de arquitecturas de implementación
a lenguajes formales de programación, y elegir AgentSpeak(L) como formalismo para modelar
agentes BDI, dimos el primer paso para desarrollar una teoría sobre aprendizaje social:
mediante la definición del protocolo SMILE en términos del modelo BDI. En esta tesis, al
mismo tiempo que extendemos la semántica operacional de AgentSpeak(L) siguiendo las
especificaciones introducidas en Jason, presentamos una primera aproximación, de lo que
puede ser una teoría formal de aprendizaje.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Aprendizaje Incremental en
Sistemas Multi-Agente BDI
Gustavo Ortiz Hernández
Director de Tesis:
Alejandro Guerra Hernández
Tesis presentada para obtener el grado de
Maestro en Inteligencia Artificial
Departamento de Inteligencia Artificial
Fac. de Física e Inteligencia Artificial
Universidad Veracruzana
México
Septiembre 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“SIMULACIÓN DE UN ALGORITMO DE ENRUTAMIENTO PARA REDES DE SENSORES INALÁMBRICOS"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar y simular en OPNET, una red de sensores inalámbricos con LORA-CBF como protocolo de enrutamiento.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El protocolo de enrutamiento LORA-CBF es aplicable en redes de sensores inalámbricos con adecuaciones mínimas.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Las redes de Sensores Inalámbricos ofrecen un medio eficaz para el análisis y monitoreo de los fenómenos físicos y químicos que ocurren a su alrededor. Las características que las distinguen, les permiten desplegarse en una gran variedad de terrenos, ya sea bajo condiciones hostiles o bien en situaciones de peligro, motivos por los que su uso ha continuado aumentando en diferentes sectores como la industria, medicina, seguridad, domótica, agricultura, etc.
A pesar de las ventajas que suponen las redes de sensores inalámbricos respecto a la infraestructura física o el costo de implementación sobre las redes de comunicación convencionales, también encuentran limitantes en aspectos como la energía disponible y las capacidades de memoria y procesamiento, lo que ha desembocado en un gran esfuerzo de investigación para desarrollar métodos o algoritmos que optimicen las características de los nodos y amplíen el alcance de las redes de sensores inalámbricos.
El desarrollo de nuevos algoritmos para las redes de sensores inalámbricos busca mejorar los protocolos existentes y optimizar las características de los nodos sensores, además de ampliar el campo de aplicación de los mismos.
La simulación es una herramienta de gran valor para el análisis del comportamiento de una red bajo situaciones controladas. OPNET Modeler ofrece una amplia gama de modelos y librerías para la evaluación de redes de comunicación.
Este trabajo de tesis desarrollo y simuló un modelo para redes de sensores inalámbricos con LORA-CBF como protocolo de enrutamiento, protocolo originalmente creado para redes en entornos vehiculares o VANETs, con el objetivo de demostrar que el protocolo es aplicable a redes de sensores inalámbricos con adecuaciones mínimas.
Para cumplir con el objetivo del trabajo de investigación, se pensó utilizar el modelo de nodo ZigBee incluido en el modelador, sin embargo, debido a que el conjunto de protocolos ZigBee un es de libre distribución, únicamente es posible modificar las características de la capa de acceso al medio, ya que se deseaba desarrollar el protocolo LORA-CBF a nivel de capa de red, dicho modelo fue descartado. Luego de una larga revisión de modelos existentes desarrollados por diversos grupos de investigación, se seleccionó un modelo de nodo desarrollado por el grupo de investigación Open ZigBee. Dicho modelo permitió desarrollar LORA-CBF y modificar al mismo tiempo las capas de aplicación y de acceso al medio para satisfacer las necesidades del protocolo propuesto en este trabajo de tesis.
Para desarrollar el protocolo LORA-CBF se analizaron detalladamente los documentos publicados por los autores del algoritmo y las características que definían a una red de sensores inalámbricos, debido a que las características de las redes vehiculares son en su mayoría similares a las de las redes de sensores inalámbricos, la adaptación del protocolo a éstas últimas, no representó mayor problema. Sin embargo, fueron necesarias algunas modificaciones para asegurar el correcto manejo de los recursos de los nodos y evitar duplicación de información.
El modelo desarrollado en OPNET Modeler, permitió realizar un análisis del comportamiento de una red de sensores inalámbricos con el protocolo LORA-CBF como protocolo de enrutamiento. Con las métricas seleccionadas para evaluación y los diferentes escenarios de simulación creados, fue posible el análisis de aspectos como la escalabilidad que el protocolo ofrece, robustez de la red y diferentes métodos de envío de datos.
Se demostró que LORA-CBF es funcional en redes de pequeña y gran escala al simular escenarios con 25, 50, 75, 100 y 200 nodos y arrojar resultados positivos respecto a la tasa de entrega de paquetes y a los tiempos reducidos de retardo punto a punto y de descubrimiento de ruta.
La robustez de la red se analizó a través del envío de datos por más de un nodo sensor al nodo “sink"
 en escenarios con uno, dos y cuatro nodos emisores. A pesar de que la tasa de entrega de paquetes se redujo en el escenario con cuatro nodos emisores, el sobre-procesamiento de la red y sobre-procesamiento de enrutamiento se mantuvo en bajos niveles en los tres escenarios simulados.
LORA-CBF se adaptó para dar soporte a envío multidifusión, en el escenario desarrollado para evaluar esta característica, un nodo sensor fue el encargado de enviar tráfico de datos a más de un destino. Se obtuvieron excelentes resultados en la tasa de entrega de paquetes, alcanzando niveles de casi el 100%, mientras que los niveles de tráfico de enrutamiento así como los tiempos de retardo de transmisión y descubrimiento de ruta se mantuvieron en niveles bajos.
Se comprobó que es posible adaptar el protocolo de enrutamiento LORA-CBF a redes de sensores inalámbricos, que el algoritmo ofrece un rápido despliegue y convergencia a la red, garantiza altos niveles de entrega efectiva de datos y ofrece posibilidades de escalabilidad, robustez y diferentes formas de envío de datos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
“SIMULACIÓN DE UN ALGORITMO DE ENRUTAMIENTO
PARA REDES DE SENSORES INALÁMBRICOS"

TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRA EN COMPUTACIÓN
PRESENTA:
ING. MAYTHÉ GONZÁLEZ GUTIÉRREZ
ASESORES:
D. EN C. RAÃƒÆ’Ã†â€™Ãƒâ€¦Ã‚Â¡L AQUINO SANTOS
M. EN C. OMAR ÁLVAREZ CÁRDENAS
COLIMA, COL., FEBRERO DE 2012.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>APRENDIZAJE EN COLABORACIÓN DE UNA SEGUNDA LENGUA A TRAVÉS DE REALIDAD VIRTUAL</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Determinar si los entornos de realidad virtual de colaboración mejoran el aprendizaje de un segundo idioma, en comparación con los métodos presenciales convencionales utilizados normalmente en las aulas de la escuela.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Es el entorno de colaboración virtual más adecuado para el aprendizaje de una segunda lengua que un aula tradicional?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Cuál es el entorno de realidad virtual de colaboración preferido por los estudiantes a los cuales se dirige esta investigación?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Cuáles factores del trabajo en colaboración general y de los entornos virtuales mejoran el aprendizaje del inglés como segunda lengua?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Cuáles son los requerimientos de hardware y software necesarios para un trabajo óptimo dentro de un entorno virtual de colaboración?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La utilización de un entorno de realidad virtual en colaboración, como herramienta de aprendizaje de una segunda lengua, aumenta el desempeño de los estudiantes gracias al sentido de libertad desarrollado al ser representado por un avatar, permitiendo una mayor colaboración entre ellos en comparación al entorno presencial de la educación tradicional.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En la actualidad instituciones como la Organización de las Naciones Unidas para la Educación, la Ciencia y la Cultura, UNESCO (2008), plantean dentro de sus objetivos propiciar la construcción de sociedades de conocimiento donde la información y el conocimiento estén al alcance de cualquier individuo en el mundo.Para lograr este objetivo nuestro sistema de educación nacional se ve en la necesidad de integrar a sus estudiantes en estas sociedades a partir del desarrollo de una cultura informática la cual involucra elementos como la capacitación y educación a distancia así como el aprendizaje de una segunda lengua, específicamente el inglés.
Los planes de desarrollo nacional (Presidencia de la República Mexicana, 2007) y estatal (SP, 2004) plantean como parte de sus estrategias para lograr lo anterior promover el uso de nuevas tecnologías en el proceso de enseñanza, aprendizaje, mientras que en los programas de la asignatura de lengua extranjera (Inglés) en secundaria y bachillerato mencionan la importancia de que los estudiantes comprendan el papel de este idioma para construir conocimiento (SEP, 2004 y DGB, 2008).
Debido a la globalización, México requiere de ciudadanos con habilidades en el área de las tecnologías de la información y comunicación, además con conocimiento de una segunda lengua, por ejemplo el inglés, con la finalidad de ser competitivo en comparación con otras naciones, como lo expresó Bill Gates a Marcelo Ebrard el 21 de Marzo de 2007 (Campos et al. 2007), ya que de acuerdo a una encuesta realizada por Campos et al. (2007) de la empresa mexicana Consulta Mitofsky, en ese año sólo el 9% de la población mexicana mayor de 18 años declararon hablar un segundo idioma, del cual el 86% corresponde al inglés.
Los entornos de colaboración virtual presentan una opción bastante atractiva como medio para el aprendizaje del idioma inglés por varias razones: Jackson y Winn (1999) señalaron como ventaja del aprendizaje dentro de un entorno virtual la capacidad que muestran los alumnos de interactuar con los objetos dentro del ambiente para construir su propio conocimiento, por lo cual afirman que los principios de los ambientes virtuales coinciden con la filosofía del constructivismo, cuyas ideas principales establecen que el conocimiento debe ser construido en lugar de adquirido y la instrucción debe apoyar a dicho proceso de construcción en lugar de comunicar información.
Jackson y Fagan (2000) mencionan que debido a la importancia del contexto social del aprendizaje, la colaboración es un factor fundamental para enriquecer el conocimiento. Es decir, es a través del trabajo con otras personas que el individuo enfrenta nuevos retos, los cuales al ser resueltos generan experiencias de aprendizaje mucho más significativas que aquellas provenientes del trabajo individual.
La colaboración a través de los medios tecnológicos ha conseguido enriquecer el trabajo de las personas, por ejemplo, la disposición de recursos expertos existentes en otros lugares del mundo, así como la capacidad de integrar equipos con individuos de diferentes países teniendo como beneficio la creación de productos, académicos o comerciales, más heterogéneos (Purvis, Purvis y Cranefield, 2004).
Actualmente los entornos de colaboración virtuales (CVE por sus siglas en inglés) han probado ser una herramienta bastante útil en el campo de la educación y el aprendizaje del inglés como segunda lengua no es la excepción. Un ejemplo de esto es el caso descrito por Di Blas y Poggi (2006) en sus proyectos desarrollados desde el 2002 SEE, Stori@Lombardia y Learning@Europe. Las autoras reportaron dentro de los resultados que, al conseguir una estrecha colaboración entre estudiantes europeos e israelitas,se logró desarrollar un alto sentido de presencia virtual (sentirse físicamente dentro del ambiente), consiguiendo con esto una gran participación por parte de los estudiantes en las actividades del experimento, teniendo como consecuencia el escribir con mayor fluidez en inglés.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Una vez analizada toda la gama de resultados, concluimos que la hipótesis de este trabajo ha sido aceptada como verdadera. Al haber trabajado con un entorno de colaboración de realidad virtual, los 3 equipos de GV (Grupo Virtual)colaboraron en un nivel mucho mayor que el único equipo de GT (Grupo Tradicional) que entró en esta categoría, de acuerdo a las mediciones de colaboración realizadas durante las sesiones de trabajo. En los dos equipos restantes que trabajaron dentro de un aula tradicional de clase siempre existió un participante el cual determinaba las respuestas de las evaluaciones presentadas al final de cada práctica, por lo que la colaboración entre ellos era inexistente.
Tanto los objetivos particulares, como las preguntas de investigación, fueron respondidos durante el trayecto de esta tesis, ya que se logró establecer el MUVE preferido por los estudiantes de nuestro grupo de trabajo.A través de un estudio de usabilidad, se consiguió identificar los factores de los entornos de realidad virtual y del trabajo en colaboración que permitieron mejorar el desempeño de los estudiantes del grupo GV. Así mismo se elaboró un conjunto de recomendaciones sobre software, hardware y participantes para un desempeño óptimo dentro de un entorno de colaboración virtual
En el caso nuestra muestra, los alumnos del grupo GV prefirieron SecondLife a DIVE, como se mostró en los resultados del examen VRUSE, en gran parte como consecuencia a los gráficos superiores del primer entorno, así como sus características de personalización del avatar.
Al examinar los elementos que desencadenaron la colaboración entre los jóvenes, observamos que en el caso del equipo de GT la motivación detrás de esto era por completo ajena al ambiente de trabajo, es decir el entorno en donde trabajaron no tuvo impacto en su desempeño.
Los factores lúdicos y de conciencia, comunes en cualquier entorno virtual, propiciaron que los integrantes de GV colaboraran activamente en los ejercicios de evaluación de cada sesión, ya que al sentirse representados por una forma ideal de sí mismos (avatar), estos consiguieron un nivel de confianza mayor al expresado normalmente en su entorno. Especialmente en aquellos casos donde los estudiantes presentaban mayores deficiencias, la simulación les dio la oportunidad de salirse del rol de mal estudiante. Al sentir que se encontraban acompañados de personas verdaderas dentro del MUVE (factor copresencia), se consiguió el grado de libertad necesario para establecer una interacción activaentre compañeros.
Por medio de la aplicación de las pruebas de t-student y Mann-Whitney, comprobamos que estadísticamente, existió un avance significativamente mayor en el nivel de comprensión auditiva de inglés del grupo GV en comparación con GT. Esto se debió a la manera en que el entorno de colaboración virtual mantuvo el interés de los estudiantes en éste.
Los factores lúdicos, de presencia e interactividad del MUVE, les brindaron a los equipos de GV la sensación de encontrarse en un extenso ambiente a explorar, lo cual, además de despertar su curiosidad, les permitió interactuar con mejores materiales que los normalmente encontrados en un aula tradicional y así generar aprendizaje mucho más significativo y duradero.
Los datos necesarios para dichas pruebas, se obtuvieron a través de dos exámenes PET (Preliminary English Test) de la Universidad de Cambrige.Estos fueron presentados antes y después del tratamiento propuesto en este trabajo.
Al comparar la mejora obtenida en el nivel de comprensión auditiva de inglés, entre los 6 miembros de los equipos considerados como no colaborativos y los 10 pertenecientes a la categoría de colaborativos, observamos que todos los de la primera categoría realizaron sus prácticas en el ambiente tradicional (GT). De este primer grupo, sólo 2 personas lograron mejorar su capacidad de entendimiento del idioma inglés en un punto, 3 de ellos se mantuvieron y uno decreció en 7 puntos.
Por otro lado, 12 integrantes que entraron a la categoría de colaborativos, 9 pertenecían a GV y sólo 3 a GT. En el caso de los 3 miembros de GT, únicamente 2 de ellos mostraron una mejora en su nivel de comprensión auditiva del idioma inglés y esto gracias a su motivación personal, no al entorno donde realizaron sus prácticas.
En caso contrario, 8 de los 9 jóvenes de GV presentaron un aumento en la misma habilidad, encontrando a la persona con una mejora más significativa dentro de este grupo. A diferencia de su contraparte, los jóvenes de este grupo expresaron haber sido influenciados en todo momento por el entorno de colaboración virtual.
Durante los experimentos, los estudiantes de GV presentaron un nivel de confianza en sus habilidades mayor al de los integrantes de GT. Esto gracias al factor conciencia del entorno virtual, teniendo como consecuencia directa una colaboración más activa en los miembros del primer grupo.
Lo anterior, combinado con la sensación de encontrarse físicamente en el sitio explorado y rodeado por personas reales (copresencia), un ambiente lúdico y la interacción con una gran cantidad de materiales animados e interesantes, ayudó a los estudiantes de GV a conseguir un aumento significativamente mayor en su nivel de comprensión auditiva del idioma inglés, en comparación con los integrantes de GT.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
Facultad de Telemática
APRENDIZAJE EN COLABORACIÓN
DE UNA SEGUNDA LENGUA
A TRAVÉS DE REALIDAD VIRTUAL
Tesis para obtener el grado de
Maestra en Tecnologías de Información
Presenta:
Liliana Martínez Venegas
Asesores:
D. en C. Miguel Ángel García Ruiz.
Mtra. Verónica Haydée Medina Ortega.
Colima, Col. Junio de 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“DISEÑO DE UN GESTOR DE REDES DE PETRI"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Crear una Herramienta para el diseño de sistemas automáticos basado en las Redes de Petri como
herramienta de modelado, utilizando como plataforma el lenguaje de programación Visual Basic.
En ningún sentido se presenta esta herramienta como una competencia con respecto a las demás
ya existentes. Se presenta como una manifestación de sólo una parte del cúmulo total de
conocimientos adquiridos a lo largo de la maestría y como una opción ante las demás ya
existentes, a fin de que el potencial usuario tome la decisión final, en base a su comodidad y a sus
expectativas.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El rápido avance en la ciencia y la tecnología va proveyendo a los diseñadores de sistemas de
control discreto y de flujo de información, de herramientas cada vez más potentes para el
desarrollo de sus aplicaciones. En virtud de los convenios que la Universidad de Colima ha
suscrito con Microsoft, ha sido posible para quienes laboramos y estudiamos en esta universidad,
trabajar con las aplicaciones de esa empresa, tales como Visual Basic, permitiendo la creación de un Gestor de Redes de Petri, con lo cual este trabajo de tesis hace una aportación a la producción de material intelectual de nuestra alma mater, la cual a la vez podrá ser aplicada como un recurso didáctico, así como un recurso de aplicación profesional.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología aplicada al desarrollo de esta herramienta consistió en tres fases:
Durante la primera fase, en recabar información referente a las Redes de Petri. Asimismo,
paralelamente, consistió en recabar toda la información posible acerca del tratamiento de datos en
ambiente gráfico, mediante el trazado de curvas y el almacenamiento de datos.
La siguiente fase consistió en elaborar prototipos cada vez más completos del proyecto global, comprobando en cada uno de los avances la integridad de la información, utilizando el método evolutivo del desarrollo de software, el cual se expone en la Figura 1.1.
La tercera fase consistió en la elaboración de la documentación relacionada al programa, con las correspondientes sugerencias de los asesores.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El programa GRP es una herramienta en proceso de desarrollo constante que proporciona un soporte gráfico interactivo tanto a quienes participan en el proceso de enseñanza-aprendizaje de áreas de conocimiento tan separadas como el diseño de autómatas, de toma de decisiones por medio de la Investigación de Operaciones, o la evolución de una coreografía, los cuales puedan ser representados mediante Redes de Petri.
Es evidente que existe una gran cantidad de aplicaciones (tanto en la industria como en los diversos procesos en los que existe un flujo de información y una secuencia ordenada de eventos) cuyo diseño y análisis son factibles tomando como base las Redes de Petri y su implementación se lograra de una manera sencilla y segura, si se cuenta con una herramienta automatizada que permita conocer de forma rápida y confiable los diferentes estados de la aplicación en estudio.
La herramienta que se obtuvo como resultado de este trabajo de tesis nos permite llevar a cabo los análisis referidos. Ésta ha sido denominada "Gestor de Redes de Petri GRP". Sus aplicaciones, en el entorno de Manzanillo, se pueden extender desde el diseño de un proceso de trámites en una agencia naviera, hasta el diseño de un programa institucional de construcción de un edificio.
Asimismo, la aplicación GRP será de gran apoyo en los cursos en los cuales se analicen las Redes de Petri, porque permitirá al alumno comprender de una manera sencilla, confiable e interactiva, las bases mismas de su teoría, en virtud de poseer una Interfaz Gráfica del Usuario fácil de comprender y utilizar.
El código fuente del programa ofrece la oportunidad de empleo como recurso didáctico en los cursos de Visual Basic.
Se proporciona una colección de Herramientas de Redes de Petri gratuitas para que el interesado, si así lo desea, los descargue, utilice y compare.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE INGENIERÍA ELECTROMECÁNICA
“DISEÑO DE UN GESTOR DE REDES DE PETRI "
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRO EN COMPUTACIÓN
PRESENTA:
ING. JOSÉ REFUGIO MARTÍNEZ REYES
ASESORES:
M.C. EFRAÍN HERNÁNDEZ SÁNCHEZ
M.C. PEDRO RAMÓN GÓMEZ LÓPEZ
MANZANILLO, COLIMA. OCTUBRE DEL 2003</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SISTEMA DE INFORMACIÓN PSICOPEDAGÓGICA DEL NIVEL MEDIO SUPERIOR DE LA UNIVERSIDAD DE COLIMA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de esta investigación es determinar el grado de aceptación
por parte del personal orientador educativo y alumnos del nivel medio superior de la
Universidad de Colima, en la sistematización del seguimiento psicopedagógico
mediante el uso de un sistema de información cliente / servidor que repercuta de
manera positiva en el desempeño de los procesos de orientación educativa.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿La obtención de la ficha psicopedagógica por parte del Orientador
Vocacional, haciendo uso de un sistema de cómputo en el Nivel Medio
Superior de la Universidad de Colima, mejorará la atención personalizada con
los alumnos?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Con la sistematización electrónica de las actividades para la obtención de la
información psicopedagógica, se disminuirán tiempos en la generación de
expedientes y la obtención de resultados de los inventarios aplicados?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Con la sistematización por medios electrónicos de la ficha psicopedagógica
de los alumnos, se propiciará un seguimiento más adecuado para la elección
de carrera?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿Con la sistematización por medios electrónicos de la ficha psicopedagógica
de los alumnos, se propiciará un seguimiento más adecuado para la elección
de carrera?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Los Orientadores Vocacionales de la Universidad de Colima muestran buena
disposición para utilizar nuevas herramientas tecnológicas en su quehacer
laboral?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Básicamente el estudio está pensado en la obtención y seguimiento de la ficha
psicopedagógica de los alumnos del nivel medio superior de la Universidad de
Colima, por medio de la sistematización electrónica de los procesos de obtención,
recuperación y almacenamiento de información. Para ello se consideraron dos
contextos en la obtención de resultados. El primero previo a la sistematización, tiene
como base los procesos que se llevan a cabo mediante el uso de consumibles como
papel, lápices, etc. Además de que la recuperación de resultados de los distintos
instrumentos captadores de la información es mediante la revisión manual de cada
uno de los expedientes o fichas de los alumnos y sus distintos inventarios. En el
segundo, se tiene la implementación sistematizada de los procesos de creación,
captura, recuperación y seguimiento de la información, mediante el uso de un
sistema diseñado para tal fin.
Con base en los argumentos anteriores se propone la siguiente hipótesis:
Ho. Mediante la sistematización electrónica de los procesos de obtención de la
ficha psicopedagógica de los alumnos del nivel medio superior de la Universidad de
Colima, no se tendrá un impacto favorable entre sus actores principales, ni acortará
los tiempos en la administración de la información.
H1. La sistematización electrónica de los procesos de obtención de la ficha
psicopedagógica de los alumnos del nivel medio superior de la Universidad de
Colima, derivará en un servicio más favorable y disminuirá los tiempos en la
administración de la información.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Este trabajo tiene la intención de mostrar el impacto positivo del uso de la
tecnología en los procesos de Orientación Educativa en el nivel medio superior de la
Universidad de Colima, específicamente en el seguimiento y generación de la ficha
psicopedagógica de los estudiantes. Nuestro indicador de impacto es el tiempo de
recuperación, divulgación y almacenamiento de información relevante entre los
actores principales en los planteles de educación media superior de la Universidad
de Colima. Esta investigación proporcionará información de utilidad para padres de
familia, maestros, tutores y alumnos; concerniente a los procesos de formación
educativa y elección de carrera que conlleve a la generación de un proyecto de vida.
Así también este estudio con el tiempo puede servir como referencia a otras
investigaciones estadísticas, como: el sistema de indicadores de rendimiento
académico de los planteles de educación media y superior, índice de deserción en el
nivel medio y superior, y cambios de carreras profesionales en el nivel superior. Por
otra parte, este soporte tecnológico ofrece un panorama de posibilidades al personal
orientador en el manejo de tecnología actualizada en su medio laboral, que coadyuve
en la calidad de la orientación educativa y se refleje tanto en la formación integral
como en la toma de decisiones del alumno.
La investigación es considerada viable por contar con el apoyo de las
dependencias involucradas y el personal, así como los recursos necesarios para
desarrollarla y llevarla a cabo.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se ha publicado en repetidas ocasiones en diversidad de documentos, los beneficios que ha traído en los últimos años el desarrollo de las tecnologías de información y comunicación aplicados a casi todas las áreas del quehacer humano, como la industria, el comercio, la milicia, o simplemente dentro de los hogares de las familias, pero sobre todo, un campo en que se ha percibido un crecimiento acelerado es, en la educación.
Las escuelas le apuestan a la tecnología como una herramienta transformadora de estereotipos establecidos, específicamente la computadora y el uso del internet, han acelerado los procesos en generación, publicación, edición, eliminación y consulta de la información, mediante herramientas de software diseñadas para tales fines.
El Sistema de Información Psicopedagógica de la Universidad de Colima, es una herramienta de software diseñada para facilitar al orientador educativo la realización de sus funciones, como es, la administración de la información concerniente a la ficha psicopedagógica personal, los resultados de inventarios de orientación, y el seguimiento oportuno del alumno durante su estadía en el nivel medio superior. Esta herramienta ofrece al orientador educativo un repositorio de información relevante y puntual sobre distintos aspectos como información general, psicosocial, escolar, de intereses y aspectos vocacionales del alumno.
Al hacer uso del SIPSUC, se puede afirmar en base a los resultados obtenidos, que se agilizan las actividades con los alumnos, como el llenado de la ficha psicopedagógica o la realización de algún inventario vocacional, al reducir los tiempos de intervención, el orientador educativo dispone de un lapso de tiempo extra, para dedicarse a actividades de intervención más personalizadas, que coadyuva en un mejor servicio y satisfacción por parte de los alumnos, con las actividades de orientación educativa y la interacción con la misma herramienta informática.
La aceptación del SIPSUC por parte de los orientadores educativos es favorable, porque dinamiza el trabajo monótono y mecanizado que requieren algunas actividades, como la evaluación de los inventarios aplicados a los alumnos o la generación de distintos reportes de las intervenciones, así como el de formar el expediente personal de cada uno de ellos; considerando que la matrícula de ingreso de la población estudiantil de ese nivel educativo va en aumento (DGEMS y DGPDI,
2007), se requiere de una herramienta como el SIPSUC para responder a las exigencias actuales sobre asesoría e intervención al alumnado, que permita brindar un soporte confiable a orientadores educativos y alumnos, para encauzar sus aspiraciones, intereses, aptitudes y habilidades, para forjar a corto y largo plazo sus metas y proyecto de vida.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE TELEMÁTICA
SISTEMA DE INFORMACIÓN PSICOPEDAGÓGICA DEL NIVEL
MEDIO SUPERIOR DE LA UNIVERSIDAD DE COLIMA
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN TECNOLOGÍAS DE INFORMACIÓN,
PRESENTA:
ARNOLDO GÓMEZ SANDOVAL
ASESOR: M.C. ARTHUR EDWARS BLOCK
COASESOR: DR. MIGUEL ANGEL GARCÍA RUIZ
COLIMA, COL., MARZO DE 2009.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SISTEMAS COMPUTACIONAL DE APLICACIONES PARA LA ENSEÑANZA AUTOMATIZADA BAJO TECNOLOGIAS DE CUARTA GENERACION</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La Informática Educativa ha sido la disciplina concreta de aplicación de los
procesos del manejo y organización de la información y su automatización en el
campo educativo, y es precisamente esta disciplina la que se ha convertido en la
impulsora del desarrollo de técnicas y metodologías propias para la instrucción y la
educación en sus más diversos niveles y propósitos.
Son atribuibles a la Informática Educativa (IE) la idea y las formas de utilizar la
potencialidad de la computadora como medio de enseñanza, cuestionando con
esto el propio papel del maestro o instructor, y por tanto planteando las nuevas
metodologías que debe desarrollar de frente a la computadora y en conjunto con
ella para el mejoramiento y desarrollo del proceso de enseñanza-aprendizaje.
Sólo desde esta disciplina fue posible plantearse el desarrollo de una concepción
nueva en el ámbito de unificar propósitos tanto de la educación como de la
informática, para lo cual tuvieron que confluir disciplinas como la Psicología
Cognitiva, Teorías del Aprendizaje, Pedagogía, Teorías de la Comunicación y las
propias disciplinas de las Ciencias Computacionales.
Podemos afirmar categóricamente que la Informática en general y todo el
desarrollo de las Ciencias Computacionales no resuelven en sí los problemas de
la educación. Sólo el análisis y la aplicación sistemática de todo lo que ofrecen
estas puede plantearse alternativas a los procesos de educación, diseño de
estrategias y metodologías para el aprovechamiento de aquellas en beneficio de la
enseñanza, la cultura, y la ciencia, tarea central y campo de estudio de la
Informática Educativa.
Con esta amplia perspectiva nos planteamos destacar la importancia del campo
de estudio y aplicaciones de la Informática Educativa, como la disciplina que nos permite conceptualizar la problemática educativa actual y de futuro, en su ámbito
tecnológico y metodológico.
En nuestras Universidades se requiere con cierta urgencia del desarrollo e
implementación de nuevas técnicas de enseñanza, de la modernización de la
instrucción y la formación profesional, sobre todo en lo tecnológico, lo que implica
la adopción de nuevas metodologías. Las ciencias computacionales e informáticas
están obligadas a dar pasos definitivos.
En esta perspectiva nos planteamos además analizar y desarrollar una
metodología informática adecuada para la integración de sistemas con aplicación
al campo educativo, tinto en lo general como en aplicación concreta. Dicha
metodología quedará desarrollada en lo que llamamos Sistema de Aplicaciones
para la Enseñanza Automatizada, mediante las actuales tecnologías de cuarta
generación.
El programa desarrollado y que se anexa, reúne el conjunto de técnicas de diseño
metodológico y programático que aquí se sustentan.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTADAD DE INGENIERIA MECANICA Y ELECTRICA
SISTEMA COMPUTACIONAL DE APLICACIONES PARA LA ENSEÑANZA AUTOMATIZADA BAJO TECNOLOGIAS DE CUARTA GENERACION
AUTOR 
ANSELMO ALVAREZ ARREDONDO
PARA OBTENER EL GRADO DE MAESTRO EN CIENCIAS COMPUTACIONALES
COQUIMATLAN, COLIMA
MAYO 1999</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DESEMPEÑO COSTO-BENEFICIO DE DOS SISTEMAS PASIVOS DE CLIMATIZACION EN CUBIERTAS PARA CLIMAS CALIDOS-SUBHUMEDO</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>DESEMPEÑO COSTO-BENEFICIO DE DOS SISTEMAS PASIVOS DE CLIMATIZACION EN CUBIERTAS PARA CLIMAS CALIDOS-SUBHUMEDO
Caso: Coquimatlan, Colima
Tesis para obtener el grado de Maestro en Arquitectura
Luis Francisco Fajardo Velasco
Directr de Tesis: 
Dr. Armando Alcantara Lomeli
Asesores:
Dr. Leandro Sandoval Alvarez
Cr. Luis Gabriel Gomez Azpeitia
Diciembre 2005</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“EMISIONES DE DIÓXIDO DE CARBONO POR LOS SISTEMAS CONSTRUCTIVOS DE CUBIERTAS DE VIVIENDA DE INTERÉS SOCIAL EN LA CIUDAD DE COLIMA"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Con el informe del Cambio Climático 2007 (IPCC,2007), resultado del Panel
Intergubernamental sobre este fenómeno, se concluyó que el calentamiento
exagerado de la atmósfera terrestre es producido por el impacto de algunas
actividades humanas; dicho impacto, al no ser asimilado por el entorno, ha
comenzado a causar estragos a nivel global.
Cabe destacar que esta tendencia conduce a la disminución acelerada de los
recursos mediante el consumo de energía desmedido, lo cual aumenta la emisión de
gases de efecto invernadero, principalmente dióxido de carbono, metano, óxido
nitroso y clorofluocarbonos. Todo esto provoca que la atmósfera atrape mayor
cantidad de radiación solar, la cual se emite desde la Tierra y ocasiona el aumento
de las temperaturas en océanos y en la superficie terrestre.
Los principales productores de los gases de efecto invernadero son el cambio
demográfico, el desarrollo socio-económico, y la rapidez del cambio tecnológico
(IPCC, 2000). Así, es indispensable tomar conciencia de los impactos humanos en el
medio ambiente y por esto, incorporar estrategias que aminoren su gravedad, pues
será imposible corregir los daños causados por la acelerada explotación permanente
de recursos y combustibles fósiles.
El aumento demográfico trae consigo más explotación de recursos para cubrir las
necesidades de los nuevos sectores, entre dichos requerimientos y uno de los más
fundamentales es la vivienda, lo cual ha ocasionado una producción masiva de
éstas. Tanto en México, como en el mundo, la construcción es uno de los sectores
que más gases de efecto invernadero emiten al medio ambiente.
A nivel global se destaca que el 8% de los gases de efecto invernadero emitidos a la
atmósfera, corresponden al sector de los edificios (Stern, 2007). En el año 2000, la
construcción en México se encontró dentro de los cuatro sectores que más
contribuyeron a la generación de emisiones de efecto invernadero, colocando al país,
dentro del 1.5 por ciento del volumen global en emisiones de gases.
El incremento de las emisiones, año con año, se debe principalmente al consumo
energético en donde según el Balance Nacional de Energía 2005 (SENER, 2006), el
sector residencial, comercial y público consume el 20.6% del total de Energía en
México. Aquí, el subsector residencial representó el 17.22% del total de energía -tan
sólo en etapa de operación-; a esto, es necesario agregarle lo que implica la
elaboración de materiales, construcción y demolición para aproximarse a la idea del
consumo energético que resulta de la vivienda.
En el informe presentado por la Comisión Nacional para el Fomento a la Vivienda
(2006) se elaboró el cálculo sobre las necesidades de vivienda en el periodo 2001-
2010, con base a esto se consideró el posible escenario para el 2030, año en el cual
se estima que habrá en México alrededor de 45 millones de hogares. Lo anterior,
representa la necesidad de edificar un promedio de 766 mil viviendas anuales.
De acuerdo a las estadísticas de incremento de población, necesidad de vivienda, y
a las notables consecuencias de perturbaciones climáticas a causa del calentamiento
global, existe un aumento significativo en la conciencia por la protección del medio
ambiente. De esta manera se manifiesta la necesidad de todos los sectores
productivos -incluyendo el de la construcción- para concentrar esfuerzos en controlar
y reducir su participación en las acciones que contribuyan a dañar al medio
ambiente.
La vivienda es parte fundamental dentro del sector de la construcción pues es el
principal instrumento que permite satisfacer las exigencias de confort adecuadas
para la población (Olgyay, V. 2002), por ello y ante su creciente emisión de dióxido,es importante implementar medidas más inocuas sobre todo en lo referente a los
elementos de cubierta: encargados de proteger de la radiación solar durante todo el
día a las habitaciones y cuya importancia radica en que “controlando la radiación
solar, se pueden mantener las condiciones climáticas de los espacios interiores
(Olgyay, V. 2002). Este beneficio implica el costo del impacto ambiental que se
produce considerando el total de viviendas actualmente en operación, y la necesidad
de cubrir la proyección de demanda.
A partir del incremento en la demanda habitacional es pertinente estudiar, mediante
la herramienta de análisis de ciclo de vida (ACV), el impacto de los sistemas de
cubierta de la vivienda de interés social en México. Con este uso, se podría
determinar el incremento o disminución en emisiones de gases a partir de la
sustitución del sistema constructivo tradicional por el sistema convencional de losa
de concreto o la reciente implementación de sistemas prefabricados.
Si se tiene en cuenta la energía requerida para fabricar los componentes con los que
se construyen, y considerando la gran cantidad de materiales involucrados en los
sistemas de cubierta, esto es tan eficaz para disminuir la energía requerida en la
vivienda como para menguar la energía que incorporan sus materiales, desde la
materia prima hasta antes de llegar a su vida útil.
La falta de estudios completos del ACV en el sector de la construcción hacen
importante y necesaria la intervención del investigador, quien evaluaría los efectos
medioambientales y también podría suministrar información necesaria para que los
fabricantes y constructores actúen controlando, disminuyendo o eliminando
determinados problemas, que reduzcan el impacto y con esto las emisiones de gases
y el efecto en el calentamiento global.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Determinar el sistema constructivo de cubierta que genera menores emisiones
de CO2 (dióxido de carbono), durante su ciclo de vida en la vivienda de interés
social en la ciudad de Colima.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cuál de los sistemas de cubierta utilizados en las viviendas de interés
social, en la ciudad de Colima, genera menores emisiones de dióxido de
carbono a lo largo de su ciclo de vida?</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El sistema constructivo industrializado de vigueta y bovedilla en cubiertas de
interés social es generador de la menor cantidad de emisiones de CO2 a largo
de su ciclo de vida.
Si bien la energía incorporada2 del sistema industrializado de vigueta y bovedilla es
mayor que en los sistemas convencionales, en su vida útil consume menor cantidad
de energía eléctrica en sistemas de climatización para mantener la vivienda a
temperatura de confort; esto por ser un sistema de mejor capacidad térmica. De esta
manera se convierte en el menor generador de emisiones de CO2 dentro de la
categoría de impacto del calentamiento global, al considerar el ciclo de vida
completo desde la extracción de materia prima, elaboración, puesta en obra, una
proyección de 50 años de vida útil, y demolición.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con los resultados obtenidos en el desarrollo de la investigación y contrastándolos
con la hipótesis inicial, la cual menciona que: “el sistema constructivo industrializado
de vigueta y bovedilla en cubiertas es generador de la menor cantidad de emisiones
de CO2 a largo de su ciclo de vida, tenemos que esta hipótesis queda comprobada.
Si bien, en la capacidad incorporada del material, durante toda la energía consumida
hasta su etapa de construcción, el sistema genera mayores emisiones, pero su
eficiencia en su comportamiento térmico en la etapa de operación, lo coloca como el
sistema de menor emisión de CO2, como parte de los sistemas constructivos
implementados en la ciudad de Colima para la construcción de viviendas de interés
social.
Esta conclusión quedó reflejada dentro de ambas comparativas en los sistemas
constructivos, donde la cubierta de vigueta y bovedilla, a partir del resultado del
procesamiento de datos y evaluación de impactos de SimaPro, se obtuvo la menor
cantidad de CO2, y a su vez, en los resultados de comparativas de temperatura,
tanto en el programa simulador Trysys, como en los casos monitoreados, presenta
una mejor capacidad térmica, reflejando menores temperaturas en la vivienda, con lo
cual es posible plantear la conclusión, de que a mayor capacidad térmicas de aporte
del material, esta aunado un ahorro de consumos energéticos y por coincidente un
ahorro en emisiones de dióxido de carbono.
Además, la investigación marca la diferencia de los sistemas de cubierta en sus
procesos de ciclo de vida, en donde la intervención de procesos estáticos como
extracción, elaboración y construcción, que no sufren transiciones una vez
dispuestos en la cubierta, pero es necesario a su vez, tener el conocimiento de los
procesos que impactan en el incremento de las emisiones, ya que influye en la toma
de decisiones, debido a que cualquier modificación de consumos, determina la
disminución de emisiones, no sólo las que repercuten al calentamiento global, sino también de otro tipo de impactos, en donde el nivel de ponderación cambia
dependiendo de la evaluación y la categoría.
Todo sistema trae consigo una serie de procesos que varían en cada etapa del ciclo,
desde que nace el producto hasta que muere. Esos procesos llevan consigo un
particular sistema de fabricación según recursos o tradiciones constructivas,
tecnologías aplicadas y demanda del sistema; todo esto genera mayor incorporación
de maquinaria, mano de obra y aprovechamiento de recursos, lo que dentro del
estudio de ciclo de vida tiene una participación al considerar 1 m2 del sistema
construido; en este caso es importante establecer los límites y estandarizar los
procesos, tener un panorama más real de la investigación (sensibilizar), por lo que
al seleccionar distintos lugares y métodos de fabricación, se obtendrán resultados de
impacto distintos
La comparativa entre los sistemas demuestra las considerables contribuciones que
tiene el elemento de cubierta en la vivienda, concluyendo que la aportación que
refleja el diferencial y la mayor contribución de impacto es la etapa de operación,
demostrándose mediante el diferencial de resultados en los sistemas sin incorporar
la vida útil, y surgieron distintos resultados una vez incluida, ya que anteriormente la
menor cantidad de emisiones era para la losa de concreto, y finalmente en los tres
sistemas una vez incorporada la etapa de operación en una proyección de vida útil
de 50 años se aporta el 99% de emisiones de dióxido de carbono.
También se observa cómo las intervenciones de procesos, que requieran traslado
fuera del sitio o aumento de productividad, mediante mayor maquinaria, aporta más
cantidad de emisiones dentro de cada sistema; detectando la importancia del ahorro
en el consumo de combustible mediante otras alternativas mas eficaces, que aunque
parezcan depreciables en la unidad funcional de 1 m2 de losa, en procesos como el
de extracción, construcción o demolición, para las necesidades de vivienda y el
aumento demográfico descontrolado que se tiene, cualquier disminución en los consumos marca la diferencia al considerar el total de viviendas que se construyen y
operan al año.
Porque el requerimiento de 3 a 4 hectáreas de especies forestales para capturar las
emisiones de una vivienda, con la necesidad de construcción actual, indica que las
hectáreas forestales son insuficiente para captar la cantidad total de emisiones por lo
se tiene que actuar, principalmente en su disminución, y en el cuidado y reforestación
de hectáreas de bosque.
El estudio muestra la influencia de cada uno de los procesos involucrados, lo que da
indicaciones sobre posibilidades de mejorar el perfil, e indica dónde es posible
disminuir la generación de consumo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
FACULTAD DE ARQUITECTURA Y DISEÑO
MAESTRÍA EN ARQUITECTURA
“EMISIONES DE DIÓXIDO DE CARBONO POR LOS SISTEMAS
CONSTRUCTIVOS DE CUBIERTAS DE VIVIENDA DE INTERÉS
SOCIAL EN LA CIUDAD DE COLIMA.
Tesis que para obtener el grado de
MAESTRA EN ARQUITECTURA
Presenta:
Ixchel Yadira Flores Velasco
Asesor:
Dr. Arq. Julio Mendoza Jiménez
Coasesor:
Dr. Alejandro Pablo Arena Granados
Coquimatlán, Colima, Mayo del 2010.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo> METODOLOGIA PARA LA IMPLANTACION DE UN SISTEMA ADMINISTRATIVO DE CALIDAD EN EMPRESAS DE SERVICIOS, BAJO LA NORMATIVIDAD DE ISO-9000</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>* Proporcionar a los directivos de las empresas de servicios en una forma sencilla y práctica,
la estructura de las normas ISO-9000, la esencia de los veinte requerimientos de la norma
ISO-9001, así como sugerencias de cómo interpretarla y como afrontarla de manera
efectiva
* Establecer una metodología para implantar un Sistema Administrativo de Calidad bajo la
Normatividad de ISO-9000, en las empresas de servicios Públicas, Privadas o
Gubernamentales, del Estado de Colima.
* Orientar a los empresarios prestadores de servicios, en el sentido de que un Sistema
Administrativo de Calidad bajo la normatividad de las normas de la serie ISO-9000,
fortalece las relaciones con sus clientes y mejora la productividad de sus empresas.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La implantación de un Sistema Administrativo de Calidad incrementa la eficiencia de
los procesos y la productividad de las empresas de servicios, y su certificación
conforme a la normatividad de las normas ISO-9000 fortalece su posición en el
mercado y la confianza de los clientes, reduce costos a mediano plazo, y previene
riesgos potenciales que afectan la productividad y calidad de los servicios.
VARIABLE INDEPENDIENTE: La implantación de un Sistema Administrativo de
Calidad incrementa la eficiencia de los procesos y
productividad de las empresas de servicios.
VARIABLE DEPENDIENTE: La certificación conforme a la normatividad de las
normas ISO-9000 fortalece su posición en el
mercado y la confianza de los clientes, reduce
costos a mediano plazo y previene a largo plazo
riesgos potenciales que afectan la eficiencia de los
procesos.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En la actualidad existen un gran número de empresas de servicios, públicas y privadas, que
proveen insumos y/o servicios a otras empresas que exportan sus productos a mercados donde
la calidad es fundamental, por ello éstas últimas están exigiendo a las empresas de servicios la
implantación de un Sistema Administrativo de Calidad bajo la normatividad de ISO-9000, y su
certificación por un organismo externo ya sea nacional o internacional, dependiendo del
mercado o giro de la empresa y, que sea reconocido por la Organización Internacional para la
Estandarización, quien es la encargada de emitir y revisar las normas ISO-9000.
Por las empresas de servicios portuarios y servicios turísticos del estado de Colima, que cada
día tiene un desarrollo tangible y un crecimiento sostenido, y que la expectativa a mediano y
largo plazo es prometedora, me siento motivado y quiero ayudar a contribuir con el presente
trabajo a proporcionarles una metodología con fundamentos sólidos y comprobados, que les
ayude y oriente objetivamente a implantar un Sistema Administrativo de Calidad bajo la
normatividad de ISO-9000.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>* Los resultados obtenidos del proceso de implantación de un Sistema Administrativo de
Calidad, bajo la normatividad de ISO-9000, en la Subgerencia Regional de Generación
Termoeléctrica Occidente, fueron satisfactorios, ya que se logró implantar y certificar su
Sistema Administrativo de Calidad en un lapso de 13 meses (Marzo 1998-Abril 1999),
desarrollándose en éste período todas las etapas de la metodología propuesta en esta tesis,
en donde se describe la forma de cual es el camino que se debe recorrer para lograr
establecer un Sistema Administrativo de Calidad.
* Durante el desarrollo del proceso se tuvieron momentos de incertidumbre y avances
mínimos, derivados de la problemática expuesta en el capítulo No. 9, por lo que el tener
como referencia dicha problemática, así como las estrategias de solución aplicadas, debe
de llevar a cualquier otra empresa de servicios a lograr implantar y certificar su Sistema
Administrativo de Calidad en un menor tiempo, siempre y cuando se cumplan todas las
etapas de la metodología desarrollada en esta tesis.
* La hipótesis general planteada al inicio de este proyecto de investigación aplicada ha sido
comprobada en lo referente a que se ha incrementado la eficiencia de los procesos, al
definirse métodos de trabajo documentados y actualizados, y se ha establecido un mejor
control del proceso de los servicios técnico-administrativo que se generan en la
Subgerencia Regional, tal como se mostró en el análisis de costo-beneficio, además se ha
fortalecido la relación con nuestro cliente al certificar el Sistema Administrativo de
Calidad.
* En lo referente a las hipótesis específicas todas se pudieron comprobar, ya que se logró
implantar y certificar el Sistema Administrativo de Calidad, en donde todo el personal (41   empleados) participó activamente al habérsele capacitado de acuerdo a lo establecido en el
punto 8.1 del capítulo IV de esta tesis, llegándose a integrar un equipo de trabajo con alto
desempeño, ya que se cuenta con un grupo de 22 auditores internos y 4 auditores líderes,
del cual el sustentante integra éste último grupo.
La expectativa a largo plazo es la tendencia hacia la Calidad Total, ya que a nivel institucional
se ha contraído el compromiso, estableciéndose un Programa Institucional de Calidad Total
(PICT).
Al cierre del año 1999, la Región de Producción Occidente en la etapa de Aseguramiento de
Calidad tiene un avance del 100%.
Cabe mencionar que la metodología propuesta para implantar un Sistema de Administrativo de
Calidad en empresas de servicios, bajo la normatividad de ISO-9000, y expuesta en esta
tesis, fue similar a la desarrollada en todos los centros de trabajo que integran la Región de
Producción Occidente, por lo que puedo afirmar que es exitosa y se puede desarrollar también
en cualquier tipo de empresas de servicios, de pequeña y mediana capacidad, públicas,
privadas o gubernamentales, con el mismo resultado satisfactorio.
La mejor prueba testimonial de que la metodología para la implantación de un Sistema
Administrativo de Calidad en empresas de servicios, bajo la normatividad de ISO-9000 es
efectiva, es una copia del certificado otorgado a la Subgerencia Regional, por el Instituto
Mexicano de Normalización y Certificación, A.C., el cual esta acreditado por la Dirección
General de Normas de la Secretaria de Comercio y Fomento Industrial, el cual se presenta a
continuación.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA
corpus_proyectos EN ADMINISTRACION
METODOLOGIA PARA LA IMPLANTACION DE UN SISTEMA ADMINISTRATIVO DE CALIDAD EN EMPRESAS DE SERVICCIOS , BAJO LA NORMATIVIDAD DE ISO-9000

TESIS
QUE PARA OBTENER EL GRADO MAESTRO EN ADMINISTRACION
PRESENTA
JESUS ALBERTO ALVAREZ SERRANO

COASESORES
M.C. GERARDO ALTAMIRA MURATALLA
M.C. ARMANDO LUNA ORNELAS
MANZANILLO,COL, SEPTIEMBRE DEL 2000</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo> SISTEMA INTEGRAL PARA EL MANEJO DE PROYECTOS OCEANOGRAFICOS "OCEANUS"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un programa de computo que funcione bajo Windows 95, para el proceso y analisis de proyectos oceanograficos, que cubra las necesidades de los centros de investigacion relacionados con las ciencias marinas, en los estados de Michoacan, Colima y Jalisco.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En función de las pláticas que se realizaron con investigadores de los
centros de estudios relacionados con las ciencias marinas en la región,
principalmente del Centro de Ecología Costera de la Universidad de Guadalajara,
el Laboratorio de Acuacultura de la Universidad Autónoma de Guadalajara, el
Centro Regional de Investigaciones Pesqueras dependiente de la SEMARNAP, el
Instituto Oceanográfico del Pacífico de la Secretaría de Marina, el Centro
Universitario de Investigaciones Oceanográficas y la Facultad de Ciencias Marinas
de la Universidad de Colima, se planteó la necesidad de contar con un programa
de cómputo con las características del que se presenta, ya que ayudara a un
rápido, sencillo y eficaz proceso de la información oceanográfica recabada en los
diferentes proyectos de investigación que se llevan a cabo en los centros antes
mencionados, disminuyendo considerablemente el tiempo de análisis, con el
consecuente aumento de la productividad científica.
Lo anterior será posible ya que las características del “Sistema Integral para
el Manejo de Proyectos Oceanográficos"  unifican el uso de software como son las
hojas de cálculo, graficado, estadística y programas propios de diversos equipos
electrónicos oceanográficos.
Desde otro punto de vista, el sistema servirá como auxiliar didáctico en la
Facultad de Ciencias Marinas, en las materias donde se describan procesos oceanográficos, ya que se podrá utilizar con datos reales obtenidos de estudios
realizados en la región, de igual manera ayudará a los estudiantes a
complementar sus prácticas de laboratorio y viajes de estudio en barcos
oceanográficos.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Una vez obtenidas las necesidades de los investigadores que laboran en
las instituciones antes mencionadas, y que fueron recopiladas por medio de una
entrevista dirigida, las necesidades fueron agrupadas en función de las
características que se solicitaron con mayor insistencia e importancia, por lo que
se obtuvo el marco de referencia respecto a las variables oceanográficas que el
sistema utiliza. Para desarrollar el programa se utilizó el diseño descendente y el
lenguaje de programación Microsoft Visual Basic, versión 4, contemplando las
siguientes características:
* Programación orientada a objetos, conducida por eventos. Con un entorno de
interfaz gráfica de usuario, (GUI).
* Enlaces dinámicos con Windows 95 (DLL), para el manejo del entorno del
sistema operativo.
* Manejo de objetos incrustados y vinculados (OLE) para la importación y
exportación de archivos.
* Utilización de Microsoft Access, para el uso de las bases de datos generadas,
de tipo relacional.
* Manejo del portapapeles, para el intercambio de información.
* Manejo de interfaz de múltiples documentos (MDI)
* Búsquedas y ordenamientos.
Las herramientas antes mencionadas son propias del Visual Basic, además se
diseñaron algoritmos para las siguientes tareas:
* Métodos estadísticos de tendencia central, Anexo III.
* Graficado, utilizando el método de interpolación Spline, Anexo III.
* Importación de imágenes de satélite, para mostrar condiciones superficiales del
océano.
* Cálculo de los parámetros oceanográficos, Anexo III.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Una vez concluido y probado el Sistema Integral para el Manejo de
Proyectos Oceanográficos (OCEANUS), y en función de las opiniones dadas por
los investigadores entrevistados y por los actuales usuarios, se encontró que el
OCEANUS cubre los requerimientos planteados originalmente, teniendo la
capacidad de que le sean incluidas en el futuro otras variables para el proceso de
datos oceanográficos.
El programa es de fácil manejo, encontrándose que durante el proceso de
la información capturada se disminuye entre un 50% y 60 % el tiempo que emplea
un investigador en utilizar otros programas para el mismo fin.
Por la capacidad de la base de datos, los proyectos de investigación
oceanográficos fácilmente pueden ser procesados con el OCEANUS, así como
usar la misma base de datos con otro tipo de programas que puedan importar
archivos *.MBD.
Los resultados que dan los cálculos realizados por el OCEANUS son
completamente consistentes con los resultados generados con programas de uso
general.
La importación de archivos tipo ASCII generados por equipo electrónico
oceanográfico son fácilmente incorporados en la base de datos del sistema, así
como la manipulación de archivos *.bmp y *.gif son 100% compatibles.
Con el uso didáctico del OCEANUS es posible que los estudiantes de
ciencias marinas tengan un aprendizaje significativo en las materias
relacionadas con la oceanografía, ya que por la modularidad del sistema el
alumno puede ir construyendo su conocimiento, lo anterior fundamentado con los
principios del Constructivismo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>SISTEMA INTEGRAL PARA EL MANEJO DE PROYECTOS OCEANOGRAFICOS "OCEANUS"
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS COMPUTACIONALES
PRESENTA 
ALEJANDRO RAFAEL MORALES BLAKE
COQUIMATLAN 
JUNIO DE 1999</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"PROPUESTA DE INNOVACION DE LOS SISTEMAS Y EQUIPOS DE DATOS,VOZ Y VIDEO, PARA UN CENTRO DE INVESTIGACION DEL I.P.N."</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El rapido desarrollo de los equipos de computo y telecomunicaciones es sorprendente en el presente, asi un equipo adquirido hace solo un par de años o algo mas, resulta obsoleto, o al menos poco capaz, comparado con los que se ofrecen en el mercado especializado al momento de su comparacion.
Aqui se propone una valoracion e innovacion del equipo existente en un Centro de Investigacion del Instituto Politecnico Nacional, ubicado en una pequeña poblacion del noroeste de Michoacan, y que debido a la  implementacion de estas tecnoligias se manteien en contacto estrecho con sus instancias administrativas y superiores jerarquicas sin merma por la lejania geografica, asi como, a traves del acceso de internet mantiene intercambio de informacion y mensajeria con otras insitituciones y especialistas. Tambien la presentacion de un documento tecnico de administracion para el mismo equipo.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El equipo actualmente operando en este centro de investigacion  fue instalado en los años 1992.93 y ha siente años de su implementacion, concidera que aunque funcional  se tiene una obsolescencia por la aparicion de mejores componentes modeluares. Por otro lado, no existe una "carpeta tenica" con las especificaciones y procedimientos fundamentales de operacion del sistema y del equipo, y de los procedimientos en caso de contingencias. Una de estas contingencias que ejempplifica los restos a presentarse es la problematica del impacto del año 2,000, y colateralmente la incorporacion a Internet 2, todo esto tanto en el software como el hardware existente, ademas de las frecuentes fallas o intermitencias de enlace y e desperfecto ya permanente de un multiplexor a nivel de la conexion central(mutlipunto).</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Creación y conformación de las ideas en los cursos de Seminario de tesis y desarrollo de proyecto de telemática del programa académico de la Maestría correspondiente.
Acopio de información para los casos de estudio, situación actual y propuestas por acción personal en las sedes correspondientes, adquisición de materiales relativos y viajes de traslado a ubicaciones de la información, hablamos prácticamente de ubicarnos en Colima, Col., Jiquilpan, Mich. y Ciudad de México.
La escritura del presente documento, la conformación de la carpeta técnica y el CD que contenga toda la información que se acopia es el resultado conclusivo y final de esta propuesta.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La propuesta del presente trabajo tiene dos vertientes:
La primera es señalar, describir y ubicar las circunstancias y estado de los servicios que existen en una
determinada dependencia del sector público en un área foránea y específicamente del ramo educativo y
de investigación aplicada en apoyo a la región donde se encuentra. Hacer un comparativo con otra
institución de vanguardia y con características similares para derivar en propuestas de mejoramiento y
remediar la obsolescencia y el rezago tecnológico.
La segunda es conformar un documento de utilidad administrativa y técnica para el encargado en turno
de los sistemas de datos y voz, que no es necesario argumentarlo, se han vuelto indispensables para la
buena y eficiente marcha cotidiana de todas y cada una de las actividades tanto de nivel protocolario o
burocrático , administrativo, como de docencia, búsqueda e intercambio de información y comunicación
remota.
A continuación de manera puntual y objetiva se marcan las conclusiones que generan los hechos y
perspectivas encontrados en el desarrollo del presente trabajo:
I. El actual enlace satelital tiene una vida activa de servicio de mas de seis años, lo que lo
acerca a una inminente conclusión de su funcionalidad, específicamente por el uso y
estado de la tecnología del equipo.
II. Este enlace satelital en su momento fue la alternativa idónea al entorno de ubicación
geográfica del Centro de Investigación, una población pequeña, de menos de 100,000
habitantes y en una zona semi-rural y de índice de marginación medio; con servicios de
comunicaciones en general de calidad media y media baja. Pero no lo es en el presente.
III. Se propone el cambio del medio de enlace punto multipunto (Centro foráneo, Dirección
de Cómputo y Comunicaciones del IPN), de satelital a terrestre y suministrado por una
compañía de las denominadas Telcos, pues existe la infraestructura necesaria (fibra
óptica, centrales telefónicas próximas, otras instituciones locales: públicas y privadas
demandando el servicio), oportunidad que se aproveche para incrementar un ancho de
banda ya insuficiente y que pudiera abarcar transmisiones de videoconferencia.
IV. Se sugiere tratar directamente con la compañía UniNet; por experiencia personal e
informaciones obtenidas, Telmex no es la opción práctica de principio, sólo como
proveedor del enlace de suscriptor o abonado al servicio.
V. El equipo existente en ambos extremos del enlace punto, multipunto, el cual consta de
módems, multiplexores, concentradores, conmutadores de datos y voz; de un alto costo
en dólares, no necesariamente se deberá sustituir totalmente, se sugiere adquirir en stock
o póliza de servicio con proveedores directos y serios de los fabricantes entre los que
podemos mencionar: Cisco, New Bridge adquirida por Nortel Telecom y después por
Alcatel , y 3Com, (se sugiere además la compañía Cabletron).
VI. El crecimiento de la demanda de conexión de red LAN y WAN, así como a INTERNET de
este Centro de Investigaciones a incrementado de un 400 a 500 por ciento desde su
instalación en 1993 de ocho computadoras IBM con tecnología de procesado 80286 y un
servidor de la misma marca con un procesador 80386 y DD de 1.2 Mb.
VII. El rezago existente no es sólo a nivel del Centro, por si mismo, la Institución en general
(IPN), se ha visto superada de manera preocupante dada su participación protagónica en
el Proyecto nacional de INTERNET 2, esto se comprueba de la manera más próxima al
observar el proyecto llevándose a cabo por la Dirección General de Estudios
Tecnológicos e Industriales (DGETI), de la misma Secretaría de Educación Pública;
quienes cuentan con enlaces de tipo E1 a Institutos Tecnológicos clave, para de esos a
su vez enlazar otras dependencias como Centros de Bachillerato y otros institutos en los
alrededores, contando con el soporte de UniNet y TelMex, y adquiriendo los equipos necesarios como tarjetas de bajada, concentradores y conmutadores y servidores y
software encaminado al uso en red de bases de datos distribuidas y arquitectura cliente-servidor.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>"PROPUESTA DE INNOVACION DE LOS SISTEMAS Y EQUIPOS DE DATOS, VOS Y VIDEO, PARA UN CENTRO DE INVESTIGACION DEL I.P.N."
 Tesis
Que para obtener el grado de:
Maestro en Ciencias, Area Telematica
PRESENTA
Biol. JUAN MANUEL CATALAN ROMERO
ASESORES
M.C. OMAR ALVAREZ CARDENAS
M.C. ARGARITA MAYORAL BALDIVIA
COLIMA, COL, JUNIO DE 2001</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo> SOFTWARE PARA LA CONSTRUCCION DE CADENAS PROTEICAS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La estructura química de los ácidos nucleicos es compleja, así como los mecanismos de
expresión genética. La ciéncia avanza cada día y se acumula el acervo teórico sobre este
tema, que cada vez alcanza dimensiones que no pueden ser asimiladas por las personas
comunes y corrientes. Por ello consideramos que es necesario crear un software que de
manera sencilla explique la estructura q@nioa de los compuestos en los que reside el
mecanismo de transmision genetica, y que ademas el usuario pueda interactuar con ella
para crear moléculas simples proteicas, y comprender los principios b6sieos funcionales de
la expresión genética.
Consideramos que este software ser4 de gran aplicabilidad, ya que facilita a los
profesores la enseiñanza de estos tópicos de manera amena pero con un alto índice de
confiabilidad; ademas, sirve de base para la creación de un sofIware hecho en México que
se utilice en la creación de modelos tridimensionales de protelnas y su proceso de síntesis,
con un amplio contenido técnico que puede ser utilizado tanto en docencia como en la
investigación y la industria.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo principal de esta tesis se basa en el desarrollo de software multimedia que incluye informacion teorica precisa referente a la biologia molecular, en especial la explicacion de la estructura quimica de los acidos nucleicos, parte de la genetica humana, donde se incluyen los siguientes rubros:
.Teoria introducida precisa acerce de la biologia molecular, explicada ademas por medio de imagenes y esquemas.
-Software demostrativo del contenido teorico acerca de la estructura quimica de los acidos nucleicos y los principios de la expresion genetica.
.Software interactivo donde el usuario puede crear moleculas proteicas simples en tercera dimension.
.Contenido del programa. Estos siete temas se encuentran englobados en un menu principal de facil manejo.
1.- Historia de la evolucion de los acidos nucleicos y su repercusion en la Genetica.
2.- Estrucutura quimica de los acidos nucleicos.
3.- Codigo genetico.
4.- Organelos que participen en la sintesis de proteinas.
5.- Mecanismos de sintesisi de proteinas: transcripcion, replicacion, trasnduccion.
6.- Glosario.
7.- Referncias</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Se pretende desarrollar un programa multimedia que englobe informacion teoricaacerca del codigo genetico y los acidos nucleicos, presentada en forma textual, asi como una seccion intenctiva de creacion de moleculas Debido a la gran cantidad de informacion generada, el programa y los datos estaran almacenados en un CD-ROM.
El desarrollo de esta apliacion multimedia permitira una mejor y mas rapida comprension por parte de los alumnos de la carrera de Medicina acerca de la biologia molecular en especial el código genético y los acidos nucleicos.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Primera etapa:
Acopio, compilación ykíición textual del material informativo proveniente de diversas
fuentes audiovisuales.
Porcentaje de tibdo: 20%
Segunda etapa:
Digitalización y retoque de las imágenes provenientes de libros, revistas e Internet.
Porcentaje de trabajo: 10%
AI final de esta etapa se contó con 40 imágenes referentes al proceso genético. Se
capturaron y editaron aproximadamente 10 cuartillas de información textual.
Tercera etapa:
Generación de gráficas en 31) de hcidos nucleicos y aminoácidos esenciales.
Porcentaje de trabajo: 20%
Al final de esta etapa se coartaron con 725 imhgenes que contienen g-rhficas en 3D.
Cuarta etapa:
Desarrollo de secuencias de animaciones acerca de ios ácidos nucleicos y los procesos
de replicwión, transcripción y transducción del ADN.
Cantidad de animaciones: 5
Porcentaje de trabajo: 20%
Quiuta tipa:
Desarrollo del programa interactivo donde el usuario podrá crear una molécula en 3D
a partir de una secuencia de ADN o viceversa (2 opciones), y el resto de los mófulos.
Porcentaje de trabajo: 30%</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con respecto al programa de aplicacion propuesto en el proyecto de tesis, este, se realizo y se completo tal como se habia diseñado con el compilador Delphi 3.0, de la empresa Borland. Este programa consta de ias siete opciones que se mostraron en a introduccion.
Tanto el software dearrollado como el programa de instalacion de la aplicacion POV-RAY se encuentran inclluidos en el CD-ROM que se edito para tal efecto, dada las maginitudes de dichos programas, ademas de que el CD.ROM presenta cierta facilidad para la distribucion y el uso de progrmas mutlimedia. El tratar de realizar un progrma de aplicacion multimedia de este tipo requiere de la inversion de mucho tiempo y esfuerzo.
El contenido de esta tesis puede en el futuro completarse con aplicaciones adicionales utilizando como base la informacion y programas de aplicacion obtenidos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>FACULTAD DE INGENIERIA MECANICA Y ELECTRICA
SOFTWARE PARA LA CONSTRUCCION DE CADENAS PROTEICAS
TESIS 
QUE PARA OBTENER EL TITULO DE:
MAESTRO EN CIENCIAS COMPUTACIONALES
PRESENTA:
MIGUEL ANGEL GARCIA RUIZ
COLIMA, COL
AGOSTO DE 1998</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo> MODELADO DE SISTEMAS SOFTWARE BASADO EN MDE
(Caso: SISTEMAS EXPERTOS DE DIAGNOSTICO)</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El desarrollo de sistemas software es cada vez más complejo debido a una serie de factores, como la aparición de nuevas tecnologías, la interconexión de varios sistemas y plataformas, la necesidad de integrar viejos sistemas aun válidos, la adaptación personalizada del software a cada tipo de usuario, las necesidades específicas de un sistema y las diferentes plataformas de implementación. Esteescenario lleva a necesitar, en cortos periodos, múltiples versiones de la misma o parecida aplicación. Por ello, la Ingeniería del Software debe proporcionar herramientas y métodos que permitan desarro-llar una familia de productos con distintas capacidades y adaptables a situa-ciones variables, y no sólo un único producto. Esta situación provocó la crea-ción de un diseño que puede ser compartido por todos los miembros de una familia de programas. De esta manera, un diseño hecho explícitamente para un producto, beneficia al software común y puede ser usado en diferentes productos, reduciendo los gastos generales y el tiempo para construir nuevos productos.
Ante esta situación, surge el concepto de Líneas de Productos Software (del inglés Software Product Lines y siglas SPL),con la finalidad de controlar y minimizar los altos costos del desarrollo de software, así como los largos tiempos de producción. Uno de los elementos clave para una SPL es la re-presentación y gestión de la variabilidad.
Para captar la semántica de la variabilidad de un dominio específico, i.e. para plasmar las características comunes y variantes de una SPL mediante mode-los que representen familias de sistemas software, se utiliza el denominado Modelo de Características. Este modelo es la representación clásica de la variabilidad en una SPLel cualpuede definirse como “una jerarquía de caracte-rísticas con variabilidad (K. Czarnecki , et al., 2006).
Asimismo, la Ingeniería Dirigida por Modelos (del inglés Model Driven Engi-neering y siglas MDE) (Kent, 2002)y el enfoque de la Programación Genera-tiva (Czarnecki y Eisenecker, 2000) proporcionan una base adecuada para apoyar el desarrollo de las SPL, facilitando el desarrollo de productos de software para diferentes plataformas y permitir su utilización en diferentes tecnologías. Pero este desarrollo de productos de software deberá hacerse bajo las técnicas ampliamente utilizadas durante el proceso de desarrollo de productos de toda la SPL, lo cual permite que el proceso sea correcto y ade-cuado.
Además, para enfrentar la complejidad de los sistemas software, la Ingenie-ría de Software promueve la automatización del desarrollo del software y otros mecanismos. Sin embargo, la automatización de estos sistemas sólo es posible mediante un framework que soporte este proceso. Ante ésto, el Gru-po Gestionador de Objetos (del inglés Object Management Group y siglas OMG) (http://www.omg.org) propone para la MDE: i) La Arquitectura Dirigi-da por Modelos (del inglés Model Driven Architecturey siglas MDA) (http://www.omg.org/mda), la cual aboga el uso de estándares e indepen-dencia de plataforma en el proceso de desarrollo de software, como un nue-vo método de producir aplicaciones; y ii) La Facilidad de Meta-Objetos (del inglés Meta Object Facilityy siglasMOF) (http://www.omg.org/mof), que iden-tifica conceptos básicos como modelo, metamodelo y meta-metamodelo, y las relaciones entre ellos.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo de esta tesis, derivada de la hipótesis es : Generar modelos de sistemas software (Sistemas Expertos de Diagnostico), aplicando estrategias de la Ingeniería Dirigida por Modelos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Con base en lo anteirormente descrito, la hipótesis planteada en esta tesis expresa que: “Los sistemas software pueden modelarse aplicando estrategias de la Ingeniería Dirigida por Modelos“</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Por otro lado, una clase de sistemas que está cobrando interés en los últimos tiempos son los Sistemas Expertos (del inglés Expert Systems y siglas ES)(Giarratano y Riley, 2004). Pero el desarrollo de este tipo de sistemas es complejo, dado que los elementos básicos que conforman su arquitectura varían tanto en su comportamiento como en su estructura (Cabello,2008). Por consiguiente, existe la necesidad de soportarlos adecuadamente. Las metodologías y aplicaciones desarrolladas de los SEforman una amplia cate-goría de productos de investigación, ofreciendo ideas y soluciones a dichos sistemas en dominios específicos. Estos sistemas emulan el razonamiento humano para la solución ante un problema. Cabe señalar que es notable la importancia que han cobrado en los últimos años los SE que realizan tareas de diagnóstico(Cabello, 2008). Por ello, para ilustrar la tesis, se ha elegido como dominio a los Sistemas Expertos de Diagnóstico (del inglés Diagnostic Expert Systems y siglas DES).
Por todo lo anterior, en esta tesis se sigue el enfoque MDA, para poder gene-rar código desde modelos (que capturan la variabilidad), utilizando progra-mación generativa y técnicas de SPL. Explícitamente, se propone capturar la variabilidad de la SPLde los DES, en términos de un modelo de característi-cas.Con la configuración de dicho modelo y la información del comporta-miento de un sistema específico (mediante un diagrama de secuencias), se transforma un modelo modular en modelos de componentes y conectores. La transformación entre estos modelos es definida mediante mapeos, que se caracterizan por tener una correspondencia sintáctica y semántica entre sus respectivos metamodelos.Para realizar dicha transformación de modelos será utilizado el lenguaje estándar de la OMG para la transformación de modelos, denominado “Query/View/Transformations Relational (QVT-Relational) (OMG, 2001).</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Esta tesis básicamente consistió en modelar sistemas software mediante téc-nicas de transformación de modelos. Para ilustrar esta estrategia de modela-do, se eligió a los Sistemas Expertos de Diagnóstico. Específicamente esta estrategia consistió en transformar el modelo modular de los Sistemas Exper-tos de Diagnósticoen modelos componente-conector “ad-hoc  al tipo de diagnóstico. Para llevar a cabo dicha tarea, se plasmó la variabilidad de la Línea de Productos Software de los Sistemas Expertos de Diagnóstico, me-diante un modelo conceptual de la variabilidad (el modelo de características), se representó el comportamiento de dichos sistemas mediante diagramas de secuencia de UML (el modelo de interacción), para generar las arquitecturas de los Sistemas Expertos de Diagnóstico medinate modelos de componentes-conectores con comportamiento.
El trabajo realizado en la tesis tiene como fundamento el plan de producción contemplado en la aproximación BOM-Lazy: un marco para desarrollar apli-caciones basado en Línea de Productos Software y técnicas de transforma-ción de modelos, la cual ha sido publicada en diferentes artículos internacio-nales. Específicamente la investigación realizada en esta tesis contempla sólo la primera transformación (i.e. del modelo modular al modelo de C-C esque-letos), pero con una mejora: la incorporación del comportamiento de todos los elementos arquitectónicos de los sistemas mediante la especificación de modelos de componentes y conectores con comportamiento.
Con ello se propone una solución posible a la problemática del desarrollo de software, permitiendo:

* Utilizar diferentes espacios tecnológicos, que están en la vanguardia internacionalmente y que son estándares de la OMG (MDA, MOF, QVT, UML, XMI, SPEM).
* Trabajar en altos niveles de abstracción (i.e. con modelos no con pro-gramas) y de esta manera trabajar en el espacio del problema y no de la solución.
* Aplicar la técnica de la transformación de modelos, especificando rela-ciones de mapeo que identificaron patrones de los metamodelos que conforman los modelos utilizados en la transformación.
* Desarrollar familias de productos que comparten partes comunes (re-utilización del software), mediante técnicas de Línea de Productos Software, bajando los costes y reducir los tiempos de mercado así como los múltiples esfuerzos que se llevan a cabo en el desarrollo del software.
* Realizar tareas de diagnóstico, emulando el comportamiento de un experto para resolver un problema, ilustrando nuestra propuesta con el dominio específico de los Sistemas Expertos de Diagnóstico, dado el interés de los últimos tiempos en desarrollar sistemas inteligentes y dada la importancia de la tipología de dichos sistemas al resolver ta-reas de diagnóstico en diversas áreas.
En esta tesis se tuvieron en cuenta las siguientes consideraciones:
* La especificación de la variabilidad de una SPL y la funcionalidad de dichos sistemas software, es manejada en modelos independientes y separados. La funcionalidad de una SPL se plasma en modelos arquitectónicos, mientras que la variabilidad de la SPL se plasma en modelos de las características del dominio. Por ello se definen una serie de metamodelos utilizando MOF para su especificación.
* La aproximación BOM-Lazy (Cabello, 2008) esutilizada como una aproximación MDA basada en SPL que utiliza técnicas de transformación de modelos, para transformar un modelo modular en varios modelos de componente-conector.
* El modelo de características es utilizado para representar la variabilidad. El modelado de las características de una SPL es una de las técnicas más utilizada en el desarrollo de las SPL. Esta técnica tiene una sintaxis 
clara utilizada para construir un modelo conceptual.

* La transformación de modelos aplica el principio de la transformación de modelos dirigido por metamodelos.
* La transformación entre los modelos se define por medio de mapeos, que se caracterizan por las relaciones sintácticas y semánticas de correspondencias entre sus metamodelos.
* La identificación de diferentes patrones de mapeo entre el modelo modular y el modelo C-C, es especificado usando el estándar QVT-Relations.
* El comportamiento de los modelos C-C es especificado mediante un metamodelo híbrido denominado metamodelo C-C-C (metamodelo de componentes, conectores y comportamiento) que consiste en unir el metamodelo C-C con el metamodelo de interacción (metamodelo de secuencias de UML).
* El comportamiento es introducido a través de un refinamiento de los modelos C-C resultantes de la transformación (i.e. estructura de los modelos C-C) instanciando mensajes, argumentos y secuencias, obteniendo así los modelos C-C con comportamiento (i.e. estructura y comportamiento) también denominadas arquitecturas esqueleto.
Se ha establecido un marco de trabajo para el diseño y análisis de arquitec-turas software (en particular de los Sistemas Expertos de Diagnóstico). Este marco consiste en la especificación de los modelos de las vistas arquitectóni-cas (Modelo Modular, Modelo de Componentes-Conectores, Modelo de Inter-acción), de la vista de variabilidad (Modelo de Características), así como de la propuesta de un proceso del modelado de estas vistas.
El proceso de modelado está basado en la propuesta MDA de la OMG, el cual incorpora dos principales ventajas del metamodelado: i) poder crear los me-tamodelos de las vistas para generar modelos, y ii) mantener consistencia entre los modelos.

De esta manera, el proceso propuesto en esta tesis, contempla las entradas, actividades, secuencia y productos en cada uno de estos dos niveles de abs-tracción (metamodelos y modelos). Dicho proceso ha sido aplicado al caso particular de los Sistemas Expertos de Diagnóstico. Pero puede ser usado para otros casos donde se requiera modelar una arquitectura software.
En un metamodelo se representan elementos de los modelos que conforman dicho metamodelo y cómo están relacionados entre sí. La formalización de las diferentes relaciones entre los metamodelos sirven de guía para la im-plementación de las reglas de transformación, que han sido realizadas usan-do el lenguaje QVT-Relational, pero son útiles también para guiar otro tipo de 
implementación, por ejemplo con QVT-Operational y ATL.
En la implementación, algunos de las conclusiones obtenidas al momento de transformar modelos son los siguientes:
*	El lenguaje QVT-Relational tiene poco uso en el medio de las trans-formaciones siendo QVT-Operational el estándar más utilizado: Prueba de esto son las tres herramientas que implementan QVT-Relational, dejando de lados varios aspectos funcionales de este lenguaje y obte-niendo sólo cierto porcentaje de fiabilidad con implementaciones par-ciales.
*	El plug-in elegido, en este caso Medini, presenta diferentes dependen-cias que deben ser resueltas al momento de instalar en la plataforma Eclipse.
*	Aunque formalmente existen cuatro niveles de abstracción en el mo-delado, en EMF sólo es posible trabajar en dos niveles: metamodelo y modelo (como una instancia del metamodelo).
La estrategia de transformación que se ha seguido aquí, apegada a la pro-puesta MDA de la OMG, ha permitido llevar acabo la generación de modelos de una vista en función de otro modelo, específicamente de la vista modular a la vista de componentes-conectores, con lo cual se logra una transitividad entre modelos vía transformaciones.
Además se ha establecido un vínculo entre la estructura y el comportamiento que existe entre algunos elemento arquitectónicos, mediante la incorporación de un modelo de interacciones que captura la interrelaciones entre los com-ponentes de la vista de componentes-conectores, de ahí que dicho modelo sirva de puente entre esta vista y la vista modular.
Con la estrategia de la transformación de modelos se mantiene consistencia entre los modelos de las dos vistas aquí consideradas, cuando alguna de ellas cambie por razones demantenimiento o evolución.
Los metamodelos sirven como plantillas para crear los modelos, así que al crear un modelo, esto contiene sólo lo que está especificado en su plantilla, es decir no hay elementos ni relaciones sorpresas.
La vista modular se fue conformando al ir aplicando el proceso de descom-posición iterativa, obteniéndose al final del proceso, el modelo correspon-diente de esta vista (especificación de módulos y relaciones). Luego, al usar este modelo como fuente, y al aplicar la transformación, se obtuvo la vista de componentes-y-conectores, la cual gracias al modelo de interacciones que se incorporó, se llegó a refinar la segunda vista a través del refinamiento de esta transformación, llegando a obtenerla en forma precisa.
Existen otras estrategias de modelado de los sistemas software, como la que modela cada uno de los sistemas de forma individual e independiente. Pero si utilizamos estrategias basadas en Línea de Productos Software, podemos aprovechar para generar variasmodelos bajando costos y reduciendo tiempos de producción. Asimismo existen otras técnicas de Línea de Productos Soft-ware para generar modelos, como la aproximación BOM-Eager la cual es conveniente aplicar cuando existe poca variabilidad de la Línea de Productos Software, y que utiliza técnicas de árbol de decisión. Pero si existe mucha variabilidad, es más conveniente utilizar la estrategia utilizada en esta tesis, que se corresponde con la aproximación BOM-Lazyy que utiliza técnicas de transformación de modelos, la cual permite generar modelos a partir de otros modelos.
Como trabajo futuro se plantea crear un prototipo, el cual capture las instan-cias a través de la configuración del Modelo de Características en tiempo de ejecución, y con ello poder transformar automáticamente el Modelo Modular en un Modelo de Componentes-Conectores.
Cabe hacer énfasis en que en esta tesis se obtienen los modelos de los sis-temas software, quedando de trabajo fututo obtener los sistemas ejecuta-bles, i.e. implementar completamente la aproximación BOM-Lazy, pero con las mejoras implementadas en esta tesis, lo cual representa una aportación a las técnicas de modelado de software.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD DE COLIMA 
FACULTAD DE TELEMATICA
MODELADO DE SISTEMAS SOFTWARE 
BASADO EN MDE
(Caso: SISTEMAS EXPERTOS DE DIAGNOSTICO)
TESIS
PARA OBTENER EL GRADO DE 
MAESTRO EN COMPUTACION
PRESENTA:
Saul Ivan Beristain Petriz
ASESORES:
D. en C. Maria Eugenia Cabello Espinosa
D. en C. Jorge Rafael Gutirrez Pulido</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Estructura Didáctica y Reprobación  El caso de la carrera de sistemas computacionales del instituto tecnológico de colima</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de Colima
ESRUCTURA DIDACTICA Y REPROBACION
El caso de la carrera de Ingenieria en sistemas computacionales del Insituto Tecnologico de Colima
TESIS
QUE PARA OBTENER EL TITULO DE 
Maestro en Educacion 
PRESESNTA
Aurelio Sanchez Magaña
julio de 1997 </Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DISEÑO CURRICULAR DE LA LICENCIATURA EN SISTEMAS COMPUTACIONALES</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar una curricula acorde al contexto actual de la educacion a nivel nacional y que cumpla con el plan institucional de la universidad de Colima.
. cumplir las recomendaciones de la asociacion nacional de insitutciones de educacion en informatica ac. (aniei), haciendo enfasis en lo relativo a porcentajes de areas de conocimientos para el perfil profesional.
.factibilidad de incorpporar la opcion intermedia: tecnico superior universitario.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Para enfrentar los retos de la educación superior, la organización para la cooperación y desarrollo económico,
extendió al sistema educativo mexicano las siguientes recomendaciones:
* Mejorar la pertinencia de la formación profesional.
* Diversificar los niveles de salida de los estudiantes.
Dentro de este contexto internacional se justifica la incorporación del nivel 5 isced (internacional
standard classifícation of education) de técnico superior universitario en las carreras profesionales de las
universidades mexicanas.
En el contexto nacional, el plan de desarrollo 1995-2000, establece como una de sus estrategias dar un
impulso extraordinario a la educación superior y responder a la demanda de técnicos y profesionistas por
medio de planes y programas de estudio pertinentes y flexibles
El objetivo que se pretende sigan las instituciones de educación superior es ofrecer contenidos relevantes
para la vida técnica y profesional y una preparación competitiva; por medio de:
* Apertura de nuevas oportunidades educativas con base en criterios de calidad y a las necesidades
tecnológicas así como las perspectivas de desarrollo regional y local.
* Apoyo a las instituciones que tengan como fín la creación de nuevas modalidades educativas, así como la
reforma de planes y programas que consideren como criterios fundamentales los avances mas recientes
en el conocimiento y la pertinencia de los programas.
De esta manera, en el contexto nacional se justifica la incorporación del nivel 5 isced (técnico superior
universitario) en las carreras profesionales de las universidades mexicanas, entre las que se encuentra la de
Colima.
En el contexto estatal, la estrategia general de desarrollo, considera esencial la pertinencia de la educación
superior para enfrentar los siguientes escenarios estimados en Colima:
* Transformación por la globalización de las comunicaciones, la tecnología y la apertura de los mercados
internacionales.
* Arribo de empresas nacionales y transnacionales con poder para controlar el mercado regional.
asimismo se señalan dos oportunidades que tiene Colima para responder a los efectos previsibles del reto
de lograr el desarrollo económico:
* Grandes posibilidades en el manejo de la información.
* Incremento en la matricula en todos los niveles educativos.
El gobierno del estado considera que los esfuerzos en Colima deberán orientarse a fortalecer la educación
superior y la investigación científica, promover la coordinación de esfuerzos para que se difunda la
tecnología elaborada en el estado y se utilicen beneficio del sector productivo e impulsar la creación de
centros de innovación tecnológica que creen tecnología de punta y sean incubadores de proyectos
productivos.
Para estas tareas la universidad de Colima tiene alta capacidad de respuesta, respaldada por un prestigio ganado con tesón en los últimos años.
De acuerdo a su marco normativo, la universidad de Colima tiene como misión integral de recursos humanos altamente competitivos en el mercado laboral y capaces de incidir, con visión humanística en el desarrollo regional y nacional, comprometidos con su formación óvos y con habilidades das para el análisis científico y el desarrollo tecnológico;
* La generación de conocimientos y soluciones a problemas sociales.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con el presente trabajo sobre la curricula. Se innova el modelo educativo de la licenciatura en Sistemas Computacionales pertinente a las necesidades de la región, lo cual permite mejorar la calidad de la educación en el área de las ciencias computacionales en la Universidad de Coó curricula se considera que se incrementa la eficiencia de las secuencias de aprendizaje de los conocimientos de la computación, redes y sistemas al vincular prácticas con las materias que así lo requieran, por ejemplo arquitecturó redes, computación, programación etc.
Para la evaluación del curriculum es muy útil la participación de los estudiantes y egresados. La participación de los egresados en la investigación permite medir su eficiencia en el campo de trabajo, su posicionamiento en empleos afines al área, así como sus necesidades de formación durante la carrera.
La participación de los estudiantes en la investigación permite conocer la eficiencia inmediata de la implementación de la curricula en las aulas y laboratorios, sus méritos y los ajustes prácticos que sea necesario realizar. Como también la participación de los estudiantes en la investigación permite aprovechar su entusiasmo para realizar los estudios de demanda.
Es indispensable determinar los requerimientos de servicios en cuanto a sistemas se refiere, de las empresas e instituciones de la región, para conocer la evolución del empleo que es otro factor importante.
Se determinó una clasificación de las áreas de conocimiento acorde a las políticas institucionales.
Específicamente las áreas de conocimiento, en las que se tomaron en cuenta las recomendaciones de la ANIEI, en la que la investigación y el inglés son esenciales. Lo anterior para producir egresados capaces de participar en los niveles internacionales de cooperación en su área y especializarse o realizar estudios de posgrado.
La nueva curricula toma en cuenta las propuestas de contenidos de la ANIEI, esto se refleja en los conocimientos que deberá adquiriendo el estudiante hasta egresar.
Las áreas propuestas por la ANIEI se consideran flexibles ya que se pueden incluir dentro de los contenidos de los programas sintéticos. Sin embargo se estima que dicha flexibilidad obstaculiza precisar con mayor exactitud las diferencias entre distintas carreras. Otra conclusión al respecto es que los modelos curriculares son demasiado similares y en consecuencia producen profesionistas con conocimientos del mismo tipo en general, variando solo el nivel de profundización con que se abordan ciertos temas.
La metodología aplicada fue muy eficiente ya que tomando en cuenta criterios institucionales, nacionales y de axiologia propia del área, proporcionó claramente las posibilidades de diseño de la curricula. En conclusión se justificó la investigación, teniendo como objetivos el diseño de una licenciatura en sistemas computacionales acorde al contexto actual de la educación a nivel nacional.
La experiencia obtenida refuerza la cultura del diseño curricular dentro de la Universidad de Colima, especialmente en las áreas de ciencias de la computación, además de que se logró elaborar una curricula actualizada que podría incluir el nivel de Técnico Superior Universitario. Lo cual permitirá una mayor eficiencia en otros trabajos de investigación similares que se requiera el nivel Técnico Superior Universitario.
Las innovaciones del modelo educativo de la Licenciatura en sistemas, debe hacerse de manera periódica a corto plazo para ir obteniendo mejores criterios sobre el seguimiento de los objetivos para lograr profesionistas que satisfagan las necesidades de la región.
Para esto se propone determinar variables que sirvan de indicativo de los avances o retrocesos, por ejemplo estudios semestrales tanto de demanda de aspirantes como de demanda de empleadores, estudios de calidad de los servicios proporsionados por los estudiantes y egresados, acreditación nacional de los egresados y de la escuela o facultad u otros similares que proporcionen información sobre la calidad de la educación y del plan de estudios en general.
Para enfrentar los retos de la educación superior, la organización para la cooperación y desarrollo económico,
extendió al sistema educativo mexicano las siguientes recomendaciones:
* Mejorar la pertinencia de la formación profesional.
* Diversificar los niveles de salida de los estudiantes.
Dentro de este contexto internacional se justifica la incorporación del nivel 5 isced (internacional
standard classifícation of education) de técnico superior universitario en las carreras profesionales de las
universidades mexicanas.
En el contexto nacional, el plan de desarrollo 1995-2000, establece como una de sus estrategias dar un
impulso extraordinario a la educación superior y responder a la demanda de técnicos y profesionistas por
medio de planes y programas de estudio pertinentes y flexibles
El objetivo que se pretende sigan las instituciones de educación superior es ofrecer contenidos relevantes
para la vida técnica y profesional y una preparación competitiva; por medio de:
* Apertura de nuevas oportunidades educativas con base en criterios de calidad y a las necesidades
tecnológicas así como las perspectivas de desarrollo regional y local.
* Apoyo a las instituciones que tengan como fín la creación de nuevas modalidades educativas, así como la
reforma de planes y programas que consideren como criterios fundamentales los avances mas recientes
en el conocimiento y la pertinencia de los programas.
De esta manera, en el contexto nacional se justifica la incorporación del nivel 5 isced (técnico superior
universitario) en las carreras profesionales de las universidades mexicanas, entre las que se encuentra la de
Colima.
En el contexto estatal, la estrategia general de desarrollo, considera esencial la pertinencia de la educación
superior para enfrentar los siguientes escenarios estimados en Colima:
* Transformación por la globalización de las comunicaciones, la tecnología y la apertura de los mercados
internacionales.
* Arribo de empresas nacionales y transnacionales con poder para controlar el mercado regional.
asimismo se señalan dos oportunidades que tiene Colima para responder a los efectos previsibles del reto
de lograr el desarrollo económico:
* Grandes posibilidades en el manejo de la información.
* Incremento en la matricula en todos los niveles educativos.
El gobierno del estado considera que los esfuerzos en Colima deberán orientarse a fortalecer la educación
superior y la investigación científica, promover la coordinación de esfuerzos para que se difunda la
tecnología elaborada en el estado y se utilicen beneficio del sector productivo e impulsar la creación de
centros de innovación tecnológica que creen tecnología de punta y sean incubadores de proyectos
productivos.
Para estas tareas la universidad de Colima tiene alta capacidad de respuesta, respaldada por un prestigio ganado con tesón en los últimos años.
De acuerdo a su marco normativo, la universidad de Colima tiene como misión integral de recursos humanos altamente competitivos en el mercado laboral y capaces de incidir, con visión humanística en el desarrollo regional y nacional, comprometidos con su formación óvos y con habilidades das para el análisis científico y el desarrollo tecnológico;
Se recomienda poner especial énfasis en el análiis del plan vigente. Est demás que surjan con la implementacion de ésta nueva opción.
Un punto importante es la inversión en tecnología. Se recomienda establezcan políticas de adquisición y sustitución de equipo obsoleto.
En las instituciones que imparten educación en las áreas de computación la carrera tecnológicas implica la costosa carga de mantenerse a la vanguardia en equipo para cumplir con los fines de servir eficientemente a la sociedad.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Diseño curricular de la Licenciatura en Sistemas Computacionales
TESIS
QUE PARA OBTENER EL GRADO DE
corpus_proyectos EN CIENCIAS COMPUTACIONALES

PRESENTA
ING. CONRADO OCHOA ALCANTAR

ASESOR
M.C. ANDRES GERARDO FUENTES COVARRUBIAS

COQUIMATLAN; COLIMA AGOSTO DE 1999.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ALGORITMOS DE BALANCE DE CARGA CON MANEJO DE INFORMACIÓN PARCIAL</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de este trabajo es diseñar algoritmos de balance de carga con manejo
de información parcial.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Actualmente existen muchos trabajos en los que se presentan estrategias que permiten
balancear carga [3], [10], [14], [15], sin embargo, la mayoría de ellos requieren de conocer el estado de carga global del sistema. Esto implica que para balancear carga, se tenga que recolectar el estado de carga de todos los procesadores que integran el sistema, requiriendo una gran cantidad de comunicaciones y por ende aumento en el tiempo de respuesta (sobre todo cuando el número de procesadores participantes es considerable). En consecuencia este tipo de estrategias presentan problemas de escalabilidad al aumentar el número de procesadores.
Nuestro trabajo propone balancear carga utilizando información parcial, es decir que el
balance puede ser ejecutado sin necesidad de conocer el estado de carga de todo el sistema.
Para ello se propone implementar algunas topologías de comunicación lógica que permitan
organizar a los procesadores en un esquema de vecinos, en donde un procesador pueda balancear carga únicamente con los procesadores que se encuentren en su vecindad. El objetivo de utilizar información parcial es minimizar el número de comunicaciones requeridas en el balance, de esta forma se pretende reducir el problema de escalabilidad y el tiempo de respuesta de las aplicaciones.
Con el fin de verificar que el manejo de información parcial logra reducir el problema de
escalabilidad y el tiempo de respuesta, se han diseñado dos algoritmos que implementan dos topologías de comunicación lógica; toroide y árbol binario. Los algoritmos fueron adaptados en la herramienta DLML [5]. DLML (Data List Management Library) es una herramienta que permite desarrollar aplicaciones paralelas mediante el uso de funciones típicas sobre listas.
DLML ya incluye un algoritmo de balance de carga con manejo de información global llamado
subasta, dicho algoritmo lo substituimos por los nuestros para realizar las pruebas necesarias, y comparamos el desempeño del algoritmo original contra los desarrollados en esta tesis para verificar si efectivamente se reduce el problema de escalabilidad.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología para la realización de la tesis consistió de las siguientes etapas:
1. Revisión de literatura relacionada con los algoritmos de balance de carga.
2. Estudio del balance de carga en la herramienta DLML.
3. Elaboración y discusión de los algoritmos de balance con manejo de información parcial.
4. Adaptación de los algoritmos propuestos en la herramienta DLML.
5. Selección de aplicaciones e infraestructura de pruebas.
6. Evaluación de los resultados.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En esta tesis se presentaron dos algoritmos de balance de carga con manejo de información
parcial. Los algoritmos son una propuesta para balancear carga utilizando sólo una parte de la información del estado de carga del sistema. El uso de información parcial permitió reducir el número de mensajes requeridos para balancear carga y se logró el objetivo de reducir los tiempos de respuesta y el problema de escalabilidad.
Los algoritmos propuestos fueron implementados en la herramienta DLML con el fin
de verificar su funcionamiento y comparar su desempeño con el algoritmo con manejo de
información global que tiene de origen la herramienta.
Los experimentos fueron realizados utilizando dos aplicaciones: N-Reinas y Multiplicación
de matrices. La primera aplicación tiene como característica la generación dinámica de carga y la segunda es considerada estática pues no genera más carga a tiempo de ejecución. La
infraestructura utilizada estuvo compuesta por dos clusters: cluster heterogéneo Pacifico y cluster homogéneo Aitzaloa.
Los resultados obtenidos con respecto a la reducción de los tiempos de respuesta, escalabilidad
y distribución de carga fueron los siguientes:
Tiempos de respuesta
En el caso de la aplicación de las N-Reinas (aplicación dinámica), los algoritmos: toroide y árbol binario, presentaron menor tiempo de respuesta con respecto al algoritmo global.
Esta reducción fue más significativa cuando el tamaño del tablero se incrementa y es
independiente del cluster utilizado.
Para la aplicación de multiplicación de matrices (aplicación estática), los resultados
mostraron que el algoritmo global tiene mejores tiempos de respuesta cuando el número
de cores utilizados está entre 8 y 32. Sin embargo, al incrementar el número de cores sus tiempos de respuesta se incrementan considerablemente, mientras que los tiempos de
los algoritmos propuestos se reducen, manteniendo esta tendencia hasta 256 y 512 cores
en el algoritmo toroide y árbol binario respectivamente. Superando así los tiempos de
respuesta del algoritmo global.
Escalabilidad
Para la aplicación de las N-Reinas, se puede observar que los algoritmos propuestos
logran reducir el problema de escalabilidad del algoritmo global. En los resultados se
observa que para un tablero con N=16 y N=17, el desempeño del algoritmo global se
degrada a partir de 64 y 128 cores respectivamente, mientras que nuestros algoritmos
mantienen un buen desempeño. Sólo hasta 1024 cores se observa que se presenta ligeramente
un problema de escalabilidad para las pruebas en donde se considera el tablero
con N=16, mientras que para N=17, el desempeño de los algoritmos no se degrada.
Para la aplicación Multiplicación de Matrices, el problema de escalabilidad es notable
para el algoritmo global y se presenta a partir de 16 cores para tamaños de matrices
cuadradas 1000 y 1200. En el caso de los algoritmos propuestos, estos presentan mejor
desempeño. El problema de escalabilidad se presenta cuando se utilizan 256 cores, aún
cuando los algoritmos propuestos también pierden eficiencia a mayor número de cores,
esta no es tan significativa, contrario a lo que sucede con el algoritmo global que pierde
eficiencia de forma exponencial.
Distribución de carga
Para comparar la forma en que los algoritmos distribuyen carga, se ocupó la aplicación
de N-Reinas. Los resultados mostraron que los algoritmos propuestos tienen una mejor
distribución de carga. Cuando la plataforma de ejecución es el cluster heterogéneo Pacifico,
se observa que los cores con mayor capacidad de procesamiento son los que han
procesado más carga, sobre todo cuando el algoritmo utilizado es el toroide, seguido del
árbol binario y finalmente el algoritmo global. Además de que se observa una mayor
uniformidad en la cantidad de carga procesada por cada core. Cuando el cluster utilizado
es Aitzaloa, la cantidad de carga procesada por core presenta mayor uniformidad
con el algoritmo toroide, seguido del árbol binario y finalmente el algoritmo global.
Debido a que la implementación de los algoritmos se realizó en DLML, actualmente esta
herramienta cuenta con tres algoritmos de balance de carga que pueden ser elegibles en el
desarrollo de aplicaciones paralelas. A continuación se presenta el trabajo a futuro.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>ALGORITMOS DE BALANCE
DE CARGA CON MANEJO DE
INFORMACIóN PARCIAL
Para obtener el grado de
MAESTRO EN CIENCIAS
(Ciencias y Tecnologías de la Información)
PRESENTA
Ing. Juan Santana Santana
Asesores
Dr. Manuel Aguilar Cornejo
Dr. Miguel Alfonso Castro García
Sinodales
Presidente: Dr. Ricardo Marcelín Jiménez
Secretario: Dr. Miguel Alfonso Castro García
Vocal: Dr. José Oscar Olmedo Aguirre
México - D.F.
Enero de 2010</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Un enfoque MDA para el desarrollo de aplicaciones basadas en un modelo de componentes orientados a servicios</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Este proyecto formó parte de un proyecto más grande llamado Service Binder NG, cuya motivación
principal son las necesidades de los industriales participantes en el proyecto, que son, France Telecom
Recherche et Developpement (FTR&amp;D) y Schneider Electric.
El proyecto estuvo dividido en dos ejes principales: Eje infraestructura y eje desarrollo.
El eje de infraestructura está encargado de introducir mejoras al Service Binder [38] haciendo una
implementación de la especificación de los servicios declarativos de OSGi, además de implementar
soporte a dos nuevos tipos de conexión entre bundles, el servicio WireAdmin, basado en el enfoque
productor consumidor, y el servicio Event, basado en la publicación y suscripción de eventos
asíncronos.
El eje de desarrollo estuvo encargado de desarrollar herramientas que faciliten la construcción de
aplicaciones que utilicen la infraestructura del primer eje, étas herramientas siguen un enfoque
moderno que privilegia el modelado y la transformación de modelos (MDA). Es en este eje donde se
ubicó el proyecto  “Un enfoque MDA para el desarrollo de componentes orientados a servicios " 
presentado en éte trabajo.
El proyecto tenía entonces como objetivo principal construir una herramienta MDA de apoyo para
los desarrolladores de bundles bajo la plataforma OSGi que basan su desarrollo el modelo de los
servicios declarativos. Como segundo objetivo se deseaba definir un proceso de desarrollo que pudiera
ser fácilmente adoptado para la creación de otras herramientas MDA. Esto es debido a que se utilizará
este proceso en la creación de nuevas herramientas para apoyar a los desarrolladores en otros temas
dentro del eje de infraestructura del proyecto Service Binder NG.
Se planteó un tiempo de desarrollo de 3 trimestres, tiempo en el cual tenía que tenerse un resultado
palpable, tenía que desarrollarse un lenguaje de modelado específico para nuestro dominio y posteriormente implementarlo en alguna herramienta de software, por lo cual, se decidió hacer el proceso de desarrollo lo mas sencillo posible, utilizando frameworks como soporte para la construcción de la herramienta y siguiendo un enfoque que pudiera ser fácilmente replicable mediante el uso de estándares ampliamente conocidos como UML.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Los objetivos generales de este proyecto son:
a) Propuesta de un vocabulario visual de modelado. Uno de los objetivos de este proyecto era desarrollar un vocabulario que permitiera modelar una aplicación construida a partir de componentes 41 orientados a servicios de manera visual. Este vocabulario sería obtenido mediante la creación de un perfil UML específico al modelo de componentes orientados a servicios. Con este lenguaje se podrán hacer las descripciones de aplicaciones que sigan este modelo, y además, ser utilizado por alguna herramienta para por ejemplo, automatizar la implementación de los bundles.
b) Definición de un proceso replicable de desarrollo de herramientas MDA. Para poder cumplir con los objetivos planteados dentro del proyecto se debe de definir un proceso simple para la creación de una herramienta MDA. Éste debe favorecer el uso de estándares ampliamente aceptados y de frameworks de desarrollo que aceleren el proceso. Además se deseaba que éte pudiera ser fácilmente replicable por cualquier organización de desarrollo de software para crear sus propias herramientas
MDA de acuerdo a sus propias necesidades y su propio dominio.
c) Creación de herramientas de desarrollo. Además de los aspectos de modelado, en el proyecto se construyeron herramientas que facilitan el desarrollo de aplicaciones basadas en el lenguaje de modelado de componentes y el proceso de desarrollo propuestos. El proyecto buscó favorecer el desarrollo de plugins para la plataforma de Eclipse.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Una parte importante de este proyecto es la definición de un proceso de desarrollo de herramientas
MDA que sea simple de seguir y fácil de adaptar a cualquier dominio. Como punto de partida fue
establecido un proceso basado en el proceso unificado (RUP Rational Unified Process) [39], adecuándolo a las necesidades del proyecto, esto era debido a que no era un proyecto de desarrollo de
software tradicional, sino un proyecto de investigación.
Como parte del proceso unificado se realizaron tareas cuyo objetivo fue el control de los riesgos del
proyecto y los artefactos generados a lo largo del mismo. Se dividió el proyecto en cuatro fases
principales: Concepción, Elaboración, Construcción y Transición, cada una de estas fases subdividida
en iteraciones de 2 semanas, véase figura 22.
Durante la fase de concepción se plantearon los alcances y objetivos del proyecto, se planteó la
visión del proyecto y se especificaron los requerimientos de la herramienta que sería desarrollada.
Durante la fase de elaboración se creó el perfil UML para los servicios declarativos y se realizó el
diseño de la aplicación siguiendo el proceso unificado, que para éte proyecto en particular consistió en
una serie de modelos que fueron utilizados por herramientas de desarrollo para generar la aplicación. El
desarrollo mismo de la herramienta MDA se convirtió en un desarrollo MDA, y el enfoque cambió por
completo del propuesto en el proceso unificado.
Debido a que una parte importante de nuestro proyecto fue la definición de un proceso de
desarrollo que pueda ser utilizado por cualquier organización de desarrollo de software para crear
herramientas MDA, era importante descubrir herramientas que facilitaran el desarrollo de nuestro
producto. Así, la fase de construcción se dividió en cuatro actividades, primero se realizó una
investigación sobre frameworks de desarrollo que facilitaran el desarrollo del editor visual. La segunda
actividad consistió en crear el editor visual utilizando el framework encontrado. La tercera fue la
investigación de un framework que nos facilitará implementar la generación de código a partir de nuestros modelos, y la cuarta actividad se refiere a la implementación de la generación de código basándonos en este framework. La última parte de la construcción de la herramienta es la validación de la misma mediante pruebas sobre su funcionamiento.
La última fase del proceso unificado, transición, consistió en definir como será entregada y distribuida la aplicación a los usuarios finales, además del desarrollo de un manual de usuario y redacción de un artículo para comunicación de los resultados.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Al inicio de este proyecto se tenían planteados 3 objetivos principales: la definición de un lenguaje de modelado de aplicaciones basadas en componentes orientadas a servicios, la construcción de una aplicación que acelerara el desarrollo de éte tipo de aplicaciones y por último la definición de un proceso fácilmente replicable para la creación de aplicaciones MDA para otros dominios.
Se definió un lenguaje de modelado visual basado en un perfil UML para el desarrollo de bundles haciendo uso del enfoque de los servicios declarativos. Posteriormente se realizó una investigación sobre herramientas de soporte que nos ayudaran a desarrollar un editor visual dónde crear modelos basados en éte lenguaje visual, validarlos contra la especificación (mediante las reglas en OCL del perfil UML) y generar el código del bundle.
Se evaluó la herramienta haciendo pruebas sobre todas las reglas contenidas en la especificación verificando que todas fueran validadas de manera correcta por la herramienta, asegurando con esto que el código generado estuviera libre de errores semánticos.
Se estableció un proceso replicable para la creación de herramientas MDA. Este proceso fue
utilizado para la creación de dos aplicaciones más, de manera que el proceso fue probado. Las nuevas herramientas fueron creadas con éxito en un tiempo muy corto (véase apéndice IV). De esta forma una organización de desarrollo de software puede tomar el proceso definido para crear sus propias herramientas MDA de acuerdo a sus necesidades. Desde esta perspectiva, la solución propuesta, es un punto de partida para la adopción de MDA en un mayor número de organizaciones dedicadas al desarrollo de software.
7.1. Comparación con otros trabajos similares
Se encontraron durante este trabajo de investigación muchos esfuerzos para lograr introducir MDA en los procesos de desarrollo de software, sin embargo muchas de las herramientas están enfocadas a plataformas específicas y es complicado o imposible adaptarlas a un dominio en particular.
Durante la investigación se encontró solamente un trabajo similar, pero este no se refiere a una herramienta MDA enfocada al desarrollo de componentes orientados a servicios. Lo encontrado fue un trabajo desarrollado en la Universidad Politécnica de Valencia, en España [41], en el cual se plantea un metamodelo completo para OSGi, y a futuro la creación de herramientas MDA basadas en su metamodelo, sin embargo, al momento de redactar esta tesis, no existe ninguna implementación de su solución.
Cabe señalar que como parte de este proyecto, se realizó una estancia en la Universidad Joseph Fourier en Grenoble, Francia, donde se tuvo la oportunidad de observar el trabajo de otros investigadores en el desarrollo de herramientas para crear herramientas MDA. Se observó que a diferencia de el presente proyecto, donde se hizo uso de frameworks existentes para crear nuestra solución, en Grenoble los investigadores estaban construyendo soluciones prácticamente desde cero, complejas de manejar y adaptar. Por experiencia personal, en la industria actual de desarrollo de software se tiene una visión mucha mas práctica, sólo se modifican los procesos cuando las modificaciones facilitan o aceleran considerablemente el desarrollo de aplicaciones, sin llegar a ser complejas ni difíciles de implementar.
7.2. Perspectivas
Durante la fase final de este trabajo de tesis, se observó la posible continuidad de este trabajo en el desarrollo de una herramienta MDA completa, que no sólo genere el esqueleto de algunos componentes del código de la aplicación. El enfoque declarativo está siendo adoptado ampliamente en la actualidad, un ejemplo de ello es un framework llamado Spring [42] con el cual se pueden construir la base arquitectónica de una aplicación de forma declarativa de manera similar a como funcionan los servicios declarativos.
Utilizando el proceso definido, generar un archivo en formato XML o descriptor que represente los aspectos no funcionales de la aplicación, no representa ningún problema, si existiesen más frameworks como Spring que se encarga de tomar este descriptor y convertirlo en una aplicación funcional, entonces MDA sería completo, al menos en el aspecto no-funcional, esto es debido a que el desarrollo de los aspectos no funcionales de las aplicaciones es mas proclive a ser automatizado. La herramienta
desarrollada absorbería toda la complejidad relacionada con la construcción del descriptor de la aplicación, validando todas las reglas utilizando OCL, de acuerdo a las especificaciones de Spring.
Nuestra herramienta se planea será liberada como open source para la comunidad OSGi y MDA, de manera que cualquier persona que desee explotar nuestras ideas y extenderlas pueda hacerlo, de manera que el esfuerzo realizado en la construcción de la herramienta y la definición del proceso de desarrollo de la misma pueda ser retomado por otros proyectos y, de esta forma, poder contribuir a que la idea de MDA pueda ser adoptada en un futuro como parte fundamental de un proceso de desarrollo de software.
MDA ha sido desde su concepción una idea que ha quedado siempre más del lado de los sueños que de la realidad, sin embargo con la evolución de las herramientas de desarrollo actuales y el impresionante avance tecnológico en tecnologías MDA, éta se encuentra cada vez más cerca de volverse una realidad. Este proyecto significa un paso más en el proceso de adopción de MDA como parte del proceso de desarrollo de software en la actualidad y se espera pueda contribuir al lanzamiento de un nuevo proceso de desarrollo basado en herramientas de este tipo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad Autónoma Metropolitana
Unidad Iztapalapa
Ciencias Básicas e Ingeniería
Maestría en Ciencias y Tecnologías de la Información
Un enfoque MDA para el desarrollo de aplicaciones
basadas en un modelo de componentes
orientados a servicios
Idónea comunicación de resultados
Presenta: Nétor A. Riba Zárate.
Asesor: Dr. Humberto Cervantes Maceda
Julio 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Migración y nuevas características del sistema de votación electrónica del Instituto de Matemáticas de la UNAM"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD NACIONAL AUTÓNOMA DE MÉXICO
    
POSGRADO EN CIENCIA E INGENIERÍA DE LA COMPUTACIÓN 
 
“MIGRACIÓN Y NUEVAS CARACTERÍSTICAS DEL SISTEMA DE 
VOTACIÓN ELECTRÓNICA DEL INSTITUTO DE MATEMÁTICAS DE
LA UNAM” 
     
T     E      S      I      S   
                   
QUE PARA OBTENER EL GRADO DE: 

MAESTRO EN CIENCIAS(COMPUTACIÓN)
  

P      R      E      S      E      N      T      A:     
   

IVÁN CHRISTIAN CERVANTES CORONADO 
 
DIRECTOR DE TESIS:   DR. SERGIO RAJSBAUM GORODEZKY

México, D.F.        2009.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"EGRAI Espacio Grupal con Referencistas y Agentes como apoyo a la Investigación"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Muchas de las funciones de las bibliotecas tradicionales se ven representadas y en algunos
casos superadas en las bibliotecas digitales [Sánchez 2001], debido entre otras cosas, a la
introducción de tecnologías de información. Actualmente, las bibliotecas digitales constituyen
una de las principales áreas de investigación, donde convergen múltiples y variadas
disciplinas.

La facilidad de acceso, la disponibilidad de los materiales, el manejo y la administración de
los datos de forma electrónica y otras características propias de recursos de información en
formato digital, son algunos de los factores que han contribuido a su popularidad.
Entre las tareas que se realizan en las bibliotecas digitales pueden citarse la digitalización de
documentos, (ya sea para su preservación o para su manipulación electrónica), el manejo de
grandes espacios de información mediante el empleo de bases de datos y el intercambio de
material bibliográfico.

Junto con la clasificación, el almacenamiento, la búsqueda y la recuperación de información,
son aspectos dignos de consideración para incrementar la calidad de los servicios que ofrecen
las bibliotecas tradicionales y digitales. Para ello, se requiere también de medios que brinden
atención personalizada a los usuarios, y que al mismo tiempo, propicien la colaboración y la
formación de grupos de usuarios con intereses afines [Sánchez et al. 2001].
En particular, este trabajo pretende contribuir a la extensión del servicio de referencia que
actualmente se ofrece en la biblioteca tradicional y digital de la Universidad de las Américas

− Puebla, con el propósito de apoyar la investigación de la comunidad universitaria.
En este documento, se considera la función que tiene la biblioteca tradicional y digital al
ofrecer servicios de localización de información para los usuarios. Se investiga el papel que
juegan los referencistas en la investigación y las características principales de tecnologías
informáticas que facilitan su labor. Se analiza el empleo del paradigma de agentes en
bibliotecas digitales y se describe su incorporación a un sistema diseñado para fomentar la
cooperación en un grupo de referencistas, formando un ambiente donde se propicia la
colaboración entre el grupo y los agentes.

Los capítulos de este documento, están organizados de la siguiente manera: el capítulo 1
habla sobre la importancia del papel que desempeñan los referencistas en una biblioteca
tradicional o digital. El capítulo 2 trata en principio las técnicas más utilizadas en los sistemas
de búsqueda y recuperación de información, menciona las principales características de los
sistemas para grupos, describe el paradigma de agentes y su aplicación en bibliotecas
digitales. Posteriormente el capítulo 3 presenta trabajos desarrollados en el área y su relación
con el presente proyecto.

El capítulo 4 describe el diseño y la arquitectura de los agentes en EGRAI y su incorporación
al sistema VRef, en tanto que en el capítulo 5 se detallan las características técnicas de su
implementación. El capítulo 6 muestra los resultados de las pruebas preliminares aplicadas al
ambiente y finalmente, el capítulo 7 contiene las conclusiones y propone trabajo a futuro.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Tradicionalmente, un usuario continúa acudiendo a los referencistas cuando éstos le
proporcionan referencias bibliográficas útiles. Si se hace énfasis en la importancia de éstas
más que en quién o qué las proporcionó, los agentes podrían asistir a los referencistas y ser
incorporados en la biblioteca tradicional o digital para localizar referencias bibliográficas
relevantes para los usuarios.
En particular, en la biblioteca de la Universidad de las Américas − Puebla, se ha diseñado un
ambiente que pretende extender los servicios de referencia que actualmente se ofrecen. El
sistema se denomina VRef, (en el capítulo siguiente se encuentra una descripción del mismo),
aunque a grandes rasgos, se trata de un sistema que responde a las necesidades del trabajo en
equipo que realizan los referencistas [Sánchez et al. 2001].
En este documento se investiga la aplicación del paradigma de agentes en bibliotecas digitales
como medio para facilitar la disponibilidad de sus recursos a los usuarios. En particular, se
analiza la incorporación de agentes al sistema VRef. A este proyecto se le denomina "Espacio
Grupal con Referencistas y Agentes como apoyo a la Investigación" y se abrevia como
EGRAI.
Como un resultado de la investigación realizada sobre el trabajo de los referencistas, en
especial de los que laboran en la biblioteca de la Universidad de las Américas − Puebla, se
requiere que los agentes en EGRAI atiendan las consultas de los usuarios tomando en cuenta
los temas de su interés, que puedan buscar referencias bibliográficas en las diferentes
colecciones de la biblioteca y que presenten los resultados al usuario oportunamente. Para ello
se emplean diferentes tecnologías informáticas, las cuales se describen en el capítulo
siguiente.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>1. Puedo identificar las funciones principales del ambiente fácilmente.</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>2. "El vocabulario que se emplea en el ambiente es:"</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>3. El ambiente propicia y/o fomenta la colaboración y cooperación de los usuarios.</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>4. "Si tuvieras que utilizar el sistema hoy, te sería:"</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>5. "La interfaz del sistema evita que te pierdas"</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Desde el punto de vista del groupware, EGRAI permite que el trabajo realizado por los
referencistas y referencistas voluntarios esté al servicio los demás usuarios, lo cual fomenta la
colaboración y fortalece el sentido del trabajo en grupo, contribuyendo simultáneamente a la
construcción social del conocimiento [Sánchez et al. 2001].
Como las experiencias de las interacciones previas de los usuarios con el sistema construyen
un acervo de conocimientos, (el cual es consultado por los agentes referencistas y demás
usuarios), se evita la duplicidad de trabajo y se propicia que los usuarios puedan contar con
una buena lista de referencias en menos tiempo.
De acuerdo a los resultados obtenidos de las pruebas, se alcanzó el objetivo de proporcionar
una herramienta que asista en tareas de investigación y se demostró que los agentes pueden
ser utilizados como asistentes en la búsqueda de referencias bibliográficas relevantes
provenientes de diferentes colecciones.
La incorporación de los agentes referencistas al sistema VRef muestra la aplicación de
tecnologías informáticas que pueden ser implementadas para atender las necesidades de
ambientes híbridos, en particular las de este ambiente que reúne a usuarios de la biblioteca
tradicional y digital.
En este sentido, el agente referencista contribuye a extender los servicios de referencia,
aportando ventajas a los usuarios como el hecho de que no tenga que preocuparse por
limitaciones de lugar o tiempo para poder localizar recursos bibliográficos.
Finalmente, la representación del referencista resultó fundamental en términos de la
interacción entre el usuario y el sistema para que el usuario percibiera con facilidad a un
proceso que actúa en su beneficio porque conoce sus intereses.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de las Américas Puebla
Escuela de Ingeniería
Departamento de Ingeniería en Sistemas Computacionales

EGRAI: Espacio Grupal con Referencistas y Agentes como apoyo a la Investigación

Tesis profesional presentada por
María Auxilio Medina Nieto
como requisito parcial para obtener el título en
Maestría en Ciencias con Especialidad en Ingeniería en Sistemas Computacionales

Jurado Calificador

Presidente: Dr. Antonio Sánchez Aguilar
Vocal y Director: Dr. J. Alfredo Sánchez Huitrón
Secretario: Dr. David Ricardo Sol Martínez
Cholula, Puebla, México a 1 de agosto de 2001

Derechos Reservados ©2001.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Adaptación y Uso de Minería de Datos Espaciales y no Espaciales"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Las tecnologías de la información son herramientas que ayudan a las personas a tomar 
decisiones de forma eficiente y efectiva. Los Data Warehouse [16, 5], Minería de datos [9, 
1, 15], Sistemas de Apoyo a las Decisiones [5], y los Sistemas de Información Geográfica 
[3, 20] son ejemplos representativos. 
 
El trabajo de investigación descrito en  este documento plantea el análisis y aplicación de 
tecnologías de minería de datos. El contexto de prueba es una base de datos del volcán 
Popocatépetl desarrollada en el grupo Xaltal del CENTIA (Centro de Tecnologías de 
Información y Automatización). Ésta es una base relacional estructurada en el esquema
Open GIS (anexo A). La finalidad es recopilar datos, obtener información y generar 
conocimiento.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General. 

Adaptación, implementación y utilización de algoritmos de minería de datos espaciales y 
no espaciales para ser aplicados a la base de datos del volcán Popocatépetl. El propósito es 
contar con una herramienta que permita analizar, evaluar y optimizar la información del 
volcán, así como generar conocimiento que ayude a la toma de decisiones. 
 
Objetivos específicos. 

* Análisis de la información de la base de datos del volcán Popocatépetl. 
* Análisis de algoritmos de minería de datos espaciales y no espaciales. 
* Modelado de un almacén de datos geográficos. 
* Desarrollo de un sistema que aplique algoritmos de minería de datos espaciales y no 
  espaciales a la base de datos del volcán. 

Esta tesis está dividida en 7 capítulos. En el capítulo 1 se hace una introducción al trabajo 
de investigación. En el capítulo 2 se explica qué es minería de datos espaciales y se 
presentan trabajos relacionados. Minería de datos y el sistema SUBDUE son descritos en el 
capítulo 3. En el capítulo 4 se explican la base de datos del volcán Popocatépetl y 
arquitectura del sistema desarrollado. En el capítulo 5 se muestra la implementación del 
sistema. En el capítulo 6 se presentan algunos resultados obtenidos y en el capítulo 7 las 
conclusiones y el trabajo a futuro.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Este capítulo presenta algunos resultados obtenidos  con el sistema de minería de datos 
desarrollado. Para la aplicación de PAM se utilizó como ejemplo el agrupamiento de 
objetos geométricos en tres clusters. Recordemos que el número de clusters puede ser 
determinado por el usuario. La representación de cada cluster está determinado por los 
colores siguientes: cluster 1 color rojo, cluster 2 color azul y cluster 3 color verde. 
 
Una vez completado el agrupamiento de objetos geométricos, se procedió a aplicar 
SUBDUE a los datos descriptivos  asociados. La representación de los resultados generados 
está determinado por los colores siguientes: subestructura 1 color rojo, subestructura 2
color azul y subestructura 3 color verde. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de las Américas Puebla
Centro Interactivo de Recursos de Información y Aprendizaje

Adaptación y Uso de Minería de Datos Espaciales y no Espaciales

Tesis profesional presentada por
Manuel Alfredo Pech Palacio
como requisito parcial para obtener el título en
Maestría en Ciencias con Especialidad en Ingeniería en Sistemas Computacionales

Jurado Calificador

Presidente: Dr. Mauricio Javier Osorio Galindo
Vocal y Director: Dr. David Ricardo Sol Martínez
Secretario: Dra. María del Pilar Gómez Gil
Cholula, Puebla, México a 16 de mayo de 2002

Derechos Reservados ©2002.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Agentes Móviles en Bibliotecas Digitales"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>1.3.2 Objetivos del Proyecto
Una vez planteado el marco de trabajo en el que se desarrolla esta investigación, se puede
mencionar los siguientes objetivos:

*   Estudiar el paradigma de agentes móviles y su aplicación en bibliotecas digitales.
*   Analizar el diseño e implementación de sistemas de agentes móviles ya existentes.
*   Definir tareas que puedan ser ejecutadas por agentes móviles en el marco de
    bibliotecas digitales florísticas.
*   Introducir el concepto de agentes móviles a la arquitectura de una biblioteca digital
    florística distribuida.
*   Desarrollar agentes móviles que permitan mejorar el funcionamiento de bibliotecas
    digitales.
*   Definir las tareas especificas que realizaran los agentes móviles tanto en los nodos
    cliente como en los nodos servidores que componen una biblioteca digital.
*   Plantear los problemas de comunicación y seguridad involucrados al implementar
    código que migra entre maquinas y de que manera pueden atenuarse o eliminarse
    dichos problemas.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los mayores problemas de las bibliotecas digitales son la introducción masiva de datos y la
búsqueda y recuperación de información. Esta tesis se enfocó al estudio e incorporación de
tecnologías novedosas que permitan mejorar y optimizar el acceso y recuperación de datos en
el contexto de una biblioteca digital distribuida. Para lograr crear una aplicación capaz de
recuperar información desde un ambiente de repositorios de datos distribuidos, se incorporó
una de las tecnologías más novedosas y de más auge con que actualmente se cuenta en la
ciencia de la Computación la tecnología de agentes móviles .

Se diseño SAM (Sistema de Agentes Móviles), un sistema que permite la creación de agentes
móviles cuya tarea es migrar entre los nodos que componen a la biblioteca, en busca de
información relevante para el usuario. Estos agentes se comportan de manera autónoma
porque pueden migrar a voluntad entre los servidores que componen su itinerario, son
transportables por sí mismos ya que al migrar llevan consigo tanto el código necesario pare
realizar su tarea como su estado de ejecución y son capaces de comunicarse con su dueño y
con el ambiente a través de mensajes que permiten el envío de información tanto síncrona
como asíncrona.

Específicamente, SAM permite la recuperación de información relevante en la Biblioteca
Digital Floristica Distribuida FDL, mediante la aplicación de una técnica de recuperación de
información conocida como modelo booleano extendido [Salton 1983]. La característica más
importante de este modelo es que personaliza la búsqueda de documentos incorporando en
sus formulas los "pasos" asociados con los términos a buscar en base a un archivo de preferencias del usuario. 

Esto permite la recuperación de documentos clasificados en orden de
importancia para cada persona. Otra característica importante de este modelo es que permite
que los términos a buscar sean frases unidas por conectivos lógicos (AND/oRj,) y no
simplemente palabras aisladas y sin relación entre ellas.

Otro aspecto interesante que se logró incorporar en SAM es la retroalimentación, ya que el
sistema permite que el usuario califique cada uno de los documentos obtenidos asignandoles
mayor peso a los documentos relevantes y menor peso a los irrelevantes. Esta forma de
retroalimentación permite ir refinando las búsquedas posteriores ajustándose lo más posible a
los gustos del usuario.

SAM es una clase de agentes que migran a voluntad entre cada uno de los nodos que
componen la biblioteca de una manera transparente para el usuario, pues permite recorrer
diferentes computadoras sin tener que preocuparse sobre los protocolos de comunicación o
los paradigmas de programación utilizados. El usuario únicamente "instancia" al agente, y
éste se encarga del resto.El diseño planteado puede implementarse de varias maneras. Para demostrar su factibilidad,
se implemento un prototipo de SAM utilizando la plataforma conocida como Aglets
Workbench.

Se puede decir que el desarrollo de este sistema cumple con los objetivos propuestos en la
sección 1.4. Primero, porque se logró la incorporación de agentes móviles en una biblioteca
digital floristica y después porque este sistema permitió mejorar el funcionamiento de dicha
biblioteca. El diseño de SAM permitió cumplir de manera concreta con dos de los objetivos
planteados, tanto la definición de tareas especificas para los agentes móviles en el contexto de
la biblioteca digital floristica como la definición de tareas que realizan dichos agentes tanto en
los nodo cliente como en los nodos servidores que componen la biblioteca digital.
Otra meta importante de este proyecto ha sido estudiar a fondo el paradigma de agentes
móviles, esto se logró gracias a la exhaustiva revisión bibliográfica que se hizo y a la
clasificación de lenguajes de programación de agentes móviles que se planteo en el capítulo
2.

También se plantearon los problemas de comunicación y seguridad involucrados en el
desarrollo de agentes móviles, así como las posibles soluciones a dichos problemas (ver
capítulo 2, sección 2.9).

En resumen, el haber conjuntado tres áreas de investigación tan importantes como son:
bibliotecas digitales, agentes, y técnicas de recuperación permitió desarrollar e implementar
un sub−sistema de agentes móviles recuperadores de información en el contexto de
bibliotecas digitales distribuidas.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de las Américas Puebla
Escuela de Ingeniería
Departamento de Ingeniería en Sistemas Computacionales

Agentes Móviles en Bibliotecas Digitales

Tesis profesional presentada por
Claudia Verónica Pérez Lezama
como requisito parcial para obtener el título en
Maestría en Ciencias con Especialidad en Ingeniería en Sistemas Computacionales

Jurado Calificador

Vocal y Director: Dr. J. Alfredo Sánchez Huitrón
Cholula, Puebla, México a 22 de mayo de 1998

Derechos Reservados ©1998.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Análisis de algoritmos para reconocimiento de caracteres manuscritos de un solo escrito caso de Porfirio Díaz"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El problema de la mayoría de las personas e instituciones, es la conservación de sus 
documentos escritos, especialmente cuando éstos tienen un siglo de antigüedad. Es por esto 
que se ha intentado conservar tanto el documento, como la información que éste contiene. 
Un avance en la tecnología a finales del siglo XX, ha dado origen al desarrollo de las 
bibliotecas digitales, las cuales ponen a disposición de un mayor número de personas la 
información. Por medio de un escáner se puede digitalizar libros, revistas, documentos 
(cartas, telegramas, memorandas,...); pero falta un paso muy importante, interpretar el 
contenido e información de estas imágenes digitales. 

Existen en el mercado una variedad de software de reconocimiento de caracteres 
(OmniPage [OmniPage 03], Corel [Corel 03], Proyecto Clara OCR [CLARA 03], …), sin 
embargo todos poseen un margen de error muy alto cuando la entrada es un documento con 
caracteres manuscritos. Es por ello que se han propuesto proyectos de investigación acerca 
del reconocimiento de caracteres manuscritos. La Universidad de las Américas - Puebla 
(UDLAP), tiene actualmente un Proyecto de Investigación dedicado al reconocimiento de 
caracteres manuscritos, aplicado a los telegramas escritos por el Gral. Porfirio Díaz, dado 
que es una contribución importante a la historia del País [Gómez, Linarez, Spínola y Cortés 
01]. Se pretende eventualmente digitalizar los telegramas del Gral. Porfirio Díaz, que la 

Biblioteca de la Universidad de las Américas – Puebla actualmente maneja, estos se 
encuentran en microfilm, transcribiendo el texto que contiene. La figura 1.1 muestra la 
descripción general del sistema. El proyecto descrito aquí continua las investigaciones 
realizadas por Sergio Linares y Carlos Alberto Spínola [Linares&amp;Spínola 00], plasmadas 
en la tesis “Reconocimiento de Letra Manuscrita de Porfirio Díaz, Utilizando un Shell Neuronal ANNSYD” y los estudios de Jorge Navarrete reportados en su tesis “Mejora del  Algoritmo de segmentación para el reconocimiento de caracteres de telegramas escritos por el Gral. Porfirio Díaz” [Navarrete 02]. El trabajo de [Linares&amp;Spínola 00] encontró 
una solución básica a este problema, pero no obtuvo resultados suficientemente buenas para 
su implementación practica. Actualmente se ha divido al proyecto en dos partes: mejoras a 
la segmentación de letras en una palabra y mejoras al reconocimiento de caracteres. Se 
aplicarán técnicas diferentes que mejoren los resultados en cada parte. A la fecha en el caso 
de la segmentación se logró un 83.86% de éxito [Navarrete 02], coeficiente que puede ser 
mejorado con un archivo de entrenamiento mayor, ya que sólo se usaron 80 palabras.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>1.5 Objetivo General. 

Este proyecto consiste en analizar dos algoritmos para el reconocimiento de caracteres 
manuscritos antiguos.  
Para esto, se realizará un software de reconocimiento de caracteres, en donde se 
implementan dos algoritmos: una red neuronal basada en la arquitectura de Kohonen y el 
algoritmo de “el Vecino Más Cercano” 
 
 
1.6 Objetivos Específicos. 
 
· Implementar una Red Neuronal de Kohonen. Se prueban diferente arquitecturas 
para encontrar el mejor resultado de reconocimiento.  
· Implementar un Algoritmo de tipo Nearest Neighbors (Vecino más cercano). Se 
analizarán los diferentes algoritmos, y se implementará el que proporcione un 
patrón de clasificación óptimo a nuestro problema. 
· Probar nuevamente, los casos evaluados anteriormente por Sergio Linares y 
Carlos Alberto Spínola [Linares &amp; Spínola 00], con los algoritmos de Kohonen y 
Nearest Neighbors. 
· Comparar el índice de reconocimiento obtenido en las investigaciones de 
 
[Linares &amp; Spínola 00], contra el generado por la red neuronal de Kohonen y el 
algoritmo de Nearest Neighbors.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>4.4 Conclusiones. 
 
Se realizó el estudio y la implementación de dos algoritmos de reconocimiento de patrones 
(Vecino Más Cercano, Red de Kohonen), los cuales dieron resultados regulares alrededor 
del 71% de reconocimiento la red de kohonen y el 50% el Vecino Más Cercano; esto se 
debe principalmente a que las letras tienen muchas inconsistencias de una palabra a otra  
palabra, e incluso dentro de la misma palabra. 

El Vecino Más Cercano es un algoritmo supervisado, esto es, que tiene que saber de 
antemano cual es el prototipo que representa a cada clase. Para esto se implementó el 
algoritmo de K-Means, que se encarga de calcular los centroides, que son los representantes 
de cada clase, entrada del clasificador Vecino Más Cercano. 

En las pruebas se notó que las vocales “a” y “e”, confundían bastante al reconocedor, como 
se muestra en los resultados parciales. Cuando se eliminaron estas vocales, se incrementó el 
porcentaje de reconocimiento considerablemente, una posible solución sería crear 
subgrupos de cada letra, ya que existen muchas variantes de la misma letra. 

El reconocedor de la Red de Kohonen, obtuvo mejores resultados que el Vecino Más 
Cercano.  Se comporto mejor cuando se hicieron las pruebas con las mayorías de las letras 
dando un aceptable porcentaje de reconocimiento del 70%. Cabe hacer notar que 
observamos que los conjuntos de entrenamiento y pruebas tuvieron pocos ejemplos. Esto se 
debe a que en el conjunto de imágenes no es lo suficiente extensa, como puede observarse 
en la tabla “Lista de Archivos de Imágenes Retocada” en el Anexo C. En este se 
encuentra el conteo para cada letra, y cabe mencionar que no se cuenta con muestras de 
todas ellas y que algunas tienen muy pocos patrones. 

Consideramos que la red de Kohonen ofrece mejor reconocimiento que el Vecino Más 
Cercano porque la red de Kohonen puede almacenar más información (según el número de 
nodos de salida) y en el Vecino Más Cercano solo se tiene un representante para cada clase 
y existen clases muy parecidas. 
 
Cabe mencionar que esta es una investigación abierta todavía, y falta mucho por 
experimentar tanto con las técnicas desarrolladas como con otras más (por ejemplo aplicar 
técnicas de procesamiento de imágenes).  
 
 4.5 Trabajos Futuros. 
 
Cabe mencionar que todavía falta mucho por realizar en el sistema, empezando desde la 
unión de la herramienta realizada por [Navarrete 02] hasta los aquí implementados. En el 
proyecto completo falta de resolver el problema de la extracción de palabras, líneas del 
documento original, y la eliminación del ruido en las imágenes, ya que esto se realiza 
manualmente.  
También falta realizar más pruebas en la red de Kohonen, por ejemplo cambiando el 
tamaño de la matriz de cada letra, ya que actualmente es de 18  renglones por 12 columnas, 
a uno más grande por ejemplo 60 renglones por 40 columnas, se podrían probar con otras  
formas de organizar los nodos de salida (renglones por columnas) en la arquitectura de la 
red de Kohonen. 
 
Puede agregarse la comparación de la salida del reconocedor de caracteres contra un 
diccionario, esto  serviría para incrementar a un más el porcentaje de reconocimiento o en 
su caso implementar una gramática para la verificación de la sintaxis. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de las Américas Puebla
Escuela de Ingeniería
Departamento de Ingeniería en Sistemas Computacionales
Análisis de algoritmos para reconocimiento de caracteres manuscritos de un solo escritor: caso de Porfirio Díaz
Tesis profesional presentada por
Guillermo de los Santos Torres
como requisito parcial para obtener el título en
Maestría en Ciencias con Especialidad en Ingeniería en Sistemas Computacionales
Jurado Calificador
Presidente: Dra. Ingrid Kirschning Albers
Vocal y Director: Dra. María del Pilar Gómez Gil
Secretario: Dr. Oleg Starostenko
Cholula, Puebla, México a 18 de diciembre de 2003
Derechos Reservados ©2003.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Análisis Robusto en Sistemas Conversacionales con Iniciativa Mixta"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La demanda de un acceso inmediato, continuo, y libre, a la super_carretera de la
información, es cada día mayor; así mismo la necesidad de obtener interfaces que de una 
manera eficiente y confiable puedan proporcionar los medios suficientes para lograr esto;
hace preciso el fijar una meta en la cual se contemple la formación de una nueva generación 
de interfaces computacionales que soporten una interacción más compleja, pero sin que
estas pierdan los toques atractivos, y de la simplicidad en su uso que las caracterizan. 

El lenguaje hablado es un medio ejemplar para interactuar, que  proporciona  las ventajas
propias de su naturaleza humana, y que muestra la capacidad de ofrecer una interacción 
humano-computadora refinada. Sin embargo en la medida en que este medio proporciona
ventajas, surgen retos que implican una colección de esfuerzos, desarrollados por un 
conjunto de gente que en su diaria actividad investigadora intenta proporcionar las mejores
soluciones para esta clase de retos. 
  
En la Universidad de las Américas Puebla se encuentra establecido el Grupo de 
Procesamiento Automático de Voz “TLATOA”, el cual desde hace años atrás ha venido
desarrollando una actividad  de investigación y generadora de aplicaciones de Voz para el 
idioma Español de México. El presente trabajo de tesis es una pequeña aportación a este
conjunto de esfuerzos que realiza el grupo para lograr un mayor grado de entendimiento y 
aplicación del lenguaje hablado a interfaces computacionales.
 
En la presente tesis se describirá la metodología y la herramienta empleada para el
desarrollo de una aplicación que efectúa un proceso de análisis de Lenguaje Natural para el
idioma Español de México en un domino restringido de aplicación  que sirve de apoyo a 
otro módulo de esta herramienta. Esta herramienta es  muy importante para la aplicación  y
fue desarrollada originalmente en el  Center for Spoken Language Reseach (CSLR)  de la 
Universidad de Colorado EUA y  que lleva por nombre CU Communicator.  </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Como esta descrito en [Burstein&amp;Mc.Dermont 1996] “El objetivo total de la
investigación en la planeación con Iniciativa Mixta es el de explorar síntesis productivas de
fuerzas complementarias en los humanos y las máquinas para construir planes efectivos, 
más rápidamente y con una gran confiabilidad”.
 
Así mismo ellos también hacen  una clara distinción sobre la motivación de esta
investigación. “Nuestro más grande interés en  los sistemas de planeación con Iniciativa 
Mixta crece sobre algunas observaciones de las fortalezas y debilidades de los humanos,  de
los sistemas de planeación automática, y de cómo estos son usados. Los humanos son... 
mejores en la planeación de tareas y las máquinas son mejores en la búsqueda sistemática
de los posibles espacios para estos planes”.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En el presente trabajo de tesis se presento el contexto en que se desarrollan las
interfaces conversacionales, enfocado principalmente al módulo  de Entendimiento de 
Lenguaje Natural (Parser) contenido en ellas. Principalmente se busco obtener un grado
mayor de entendimiento de la forma en que funciona este módulo, sus alances y 
limitaciones.  
 
Para lograr una aplicación práctica de lo que se consiguió entender, se empleo una
herramienta desarrollada por la Universidad de Colorado, la cual funciona originalmente
para el idioma inglés. Para nuestro caso y el enfoque de este trabajo se busco que el Parser 
soportara una gramática en español, la cual se diseño de acuerdo a un contexto de altas
académicas.    

Para lograr lo anterior se debieron recabar los datos necesarios a través de la grabación de 
diálogos simulando una situación en la que se desarrolla este tipo de transacciones los
cuales fueron procesados con otra herramienta proporcionada por la Universidad de 
Carnegie Mellon, denominada CMU Statistical Modeling Toolkit que permite analizar un
volumen de información en forma de texto y obtener de él su modelo de lenguaje, 
vocabulario y estadísticas de comportamiento del modelo de lenguaje mismo. Con los
resultados de este procesamiento de muestras se formularon nuevos ejemplos para probar y 
evaluar el desempeño de la gramática integrada al Parser, a través de una simple interfaz
incluida en el mismo CAT que permite evaluar a este módulo de manera independiente a 
todos los otros módulos.
 
Dicho lo anterior dentro de las propuestas para trabajos futuros y que se desprenden del
presente trabajos podrían incluirse: 
                                                                                                              
*    Probar otras metodologías para la recolección de datos, para obtener un estándar para el 
     diseño de gramáticas que puedan ser integradas a este sistema. 
*    Aplicar este tipo de tecnología a situaciones reales para obtener datos reales,
     provenientes de usuarios reales. En caso particular de este caso de estudio integrar el
     módulo modificado del Parser del CAT para que funcione en conjunto con los demás 
     componentes del sistema, para una  aplicación de altas académicas y que se pruebe de
     una manera real, en un periodo escolar. 
     
*    Otro enfoque es de poder generalizar el conocimiento adquirido de la implementación y 
     prueba de esta aplicación a otros dominios.
 
*    Seguir de cerca el  desarrollo de la propuesta hecha por el CSRL para la integración de
     los módulos de reconocimiento y entendimiento del lenguaje en un solo componente, 
     para que sea empleado en aplicaciones para el español de México. 

*    Instalación y pruebas del sistema soportando el acceso vía telefónica, basado en un
     plataforma de tarjetas Dialogic y observar el desempeño los módulos anteriormente
     mencionados. 
     
Aunado a lo anterior el desarrollo del presente trabajo de tesis es una aportación al conjunto 
de esfuerzos que involucra el desarrollo de Tecnologías de Lenguaje Humano, las cuales
desempeñan un rol central en la provisión de interfaces que cambiaran drásticamente el 
paradde la comunicación humano-computadora, yendo de la programación a la
conversación con ellas. Esto permitirá a los usuarios un acceso, proceso, manipulación  y 
absorción de una basta cantidad de información de manera eficiente y confiable.
 
Todaveda mucho trabajo por hacer, sin embargo los logros alcanzados colectivamente,
por la comunidad investigadora de esta área, nos dan las razones suficientes para
mostrarnos optimistas, sobre el rendimiento efectivo dentro del campo de acción de algunos 
sistemas, aún con capacidades limitadas.
 
Con la conclusión del presente trabajo se muestra que el tiempo para el desarrollo de este
tipo de tecnología es oportuno. A partir de que la necesidad de proveer información a
cualquier persona, a cualquier hora y en cualquier lugar, es cada día mayor. También 
debido a que el mundo en su totalidad, se esta movilizando para desarrollar la
supercarretera de la información, la cual será la columna vertebral del crecimiento
económico en un futuro no muy lejano. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de las Américas Puebla
Escuela de Ingeniería
Departamento de Ingeniería en Sistemas Computacionales

Análisis Robusto en Sistemas Conversacionales con Iniciativa Mixta

Tesis profesional presentada por
Javier Velázquez Sandoval
como requisito parcial para obtener el título en
Maestría en Ciencias con Especialidad en Ingeniería en Sistemas Computacionales

Jurado Calificador

Presidente: Dr. Gerardo Ayala San Martín
Vocal y Director: Dra. Ingrid Kirschning Albers
Secretario: Dr. Antonio Sánchez Aguilar
Cholula, Puebla, México a 13 de agosto de 2001

Derechos Reservados ©2001.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Análisis y Desarrollo de Técnicas para la Exploración de un Ambiente Desconocido por un Robot Móvil"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Este trabajo de tesis reporta la investigación y el análisis realizado de algunas técnicas que
permiten controlar un robot móvil dentro de un ambiente desconocido, a fin de realizar una
exploración autónoma del entorno y construir un mapa de él, a partir de la información
recogida por los sensores (cámara de video y sensores de aproximación) del robot. Algunas
de las técnicas analizadas así como las aportaciones propias fueron probadas y valoradas en
un pequeño robot móvil (Khepera 213) puesto dentro de un ambiente desconocido con
obstáculos geométricos de colores homogéneos, como parte de este trabajo se le adaptó una
cámara de video al robot móvil que carecía de un sensor de este tipo.

Los robots móviles requieren de un sistema propio de navegación que les permita generar sus
trayectorias [1]. Con el problema de navegación está también el problema de evitación de
obstáculos para alcanzar el objetivo sin perder el rumbo [2]. Para lograr este fin el robot debe
contar con una estructura de sensores que le permita percibir el entorno que lo rodea, estos
sensores pueden ser externos o internos [3].

En la medida en que sea flexible el sistema de navegación de un robot móvil dependerá de un
mapa de su entorno, para la creación de estos mapas de manera autónoma por un robot
explorador, éste debe contar con sensores, exteroceptivos y un sistema que permita integrar
toda esta información de manera eficiente.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>1.2 Objetivos generales

*   Investigar, describir y analizar diversas técnicas aplicadas en los robots móviles con
    sensores externos para la navegación y exploración autónoma en ambientes 
    desconocidos con obstáculos geométricos .

*   Desarrollo de algoritmos para el reconocimiento e interpretación de escenas de un
    ambiente desconocido y modelado del comportamiento del robot, para la estimación
    de características del mundo explorado por el agente móvil, de tal manera que puedan
    ser utilizadas para la planificación de trayectorias en futuras incursiones de este u
    otros robots en dicho ambiente.
  
1.3 Objetivos Específicos

*   Con base en la investigación analizar, proponer e implementar métodos eficientes
    para el reconocimiento e interpretación de patrones geométricos de colores
    específicos.

*   Implementar algoritmos propuestos para el procesamiento de características espacio
    temporales basándose en una secuencia limitada de imágenes para obtener el mapa
    del mundo desconocido.

*   Desarrollar un sistema robusto para el control de movimientos e integración de la
    información de los sensores internos y externos del robot "Khepera 213".

*   Adaptar una cámara de vídeo al robot "Khepera 213" con el propósito de adquirir
    mayor información del ambiente a explorar, para satisfacer el modelado de escenas
    dinámicas con el robot en ambientes desconocidos.

*   Construir un sistema para probar el funcionamiento de los algoritmos propuestos y
    verificar la correcta visualización del ambiente desconocido con base en la
    información recopilada por el robot explorador.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>6.3 Conclusiones

En esta tesis se ha desarrollado un sistema robusto para el control e integración de
información de sensores externos e internos del robot khepera 213, con la finalidad de
controlar al robot en la tarea de exploración de ambientes desconocidos con obstáculos
geométricos de colores homogéneos. Para realizar esta tarea, se le adaptó al robot khepera una
cámara de video como sensor principal de un sistema de visión artificial acoplado a los
algoritmos modificados de Braitenberg, lo cual le permitió al sistema del robot tener una
mejor representación del ambiente por explorar con la finalidad de construir un mapa de él.

Se propusieron e implementaron mejoras al vehículo definido por Braitenberg en [4] llamado
vehículo con comportamiento explorador, dichas mejoras consistieron en adaptarle un
mecanismo que permite que el robot no quede varado dentro del ambiente desconocido al
nivelarse la percepción de los sensores con los que cuenta el robot, y por medio de los cuales
presenta repulsión hacia los obstáculos en el camino y atracción hacia espacios abiertos, otra
aportación más a los vehículos de Braitenberg consistió en que el comportamiento del agente
autónomo ahora se ve influenciado por un sistema de visión robótico y no solo de sensores de
proximidad, lo que le permite al robot tomar mejores decisiones sobre el rumbo que debe
tomar en la tarea de exploración, la influencia del sistema de visión en la toma de decisiones
puede ser ajustado por medio de un factor de sensibilidad, por lo que es posible controlar la
distancia de acercamiento del robot a los obstáculos.

En la construcción de los mapas de espacios explorados se obtuvieron representaciones
burdas del ambiente, debido a la poca exactitud que tiene el sistema odométrico interno del
robot kephera, además de que la velocidad de reacción del robot esta condicionada al tiempo
mínimo de captura (1 segundo) que permite el programa comercial que se utilizó para la
adquisición de video.

Se sugieren como trabajos futuros, construir un sistema propio de adquisición de la
información de video, debido a que el sistema desarrollado en esta tesis utiliza el software
comercial de la cámara de video Quick cam vc, por lo tanto la comunicación entre este
programa y el sistema del robot se realiza por medio de un archivo de imagen, lo que limita la
velocidad de respuesta del robot en su trabajo de exploración.

El sistema odométrico del robot es inexacto, lo que no permite conocer de manera confiable
en cualquier momento la posición y orientación del robot dentro del ambiente, lo que se ve
reflejado en la construcción del mapa del entorno, el cual es solo una aproximación burda de
un mapa real, entonces es necesario construir un sistema robusto para conocer la ubicación y
orientación del robot en cualquier momento y en tiempo real, se sugiere utilizar una
corrección de posición por medio de marcas, lo cual es permitido por el sistema de visión
propuesto en este trabajo .

Los algoritmos modificados de Braitenberg mostrados en este trabajo reciben un vector de
características compacto del ambiente y que contiene información tanto de los sensores
infrarrojos y del sistema de visión, por lo que se abre la posibilidad de utilizar este vector con
alguna técnica de inteligencia artificial, específicamente redes neuronales, para controlar la
incursión del robot en ambientes inexplorados.
Considero que se han cumplido los objetivos planteados al inicio de este trabajo, el cual es un
buen punto de partida para estudiar más a fondo y hacer aportaciones interesantes en el
fascinante mundo de la robótica.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de las Américas Puebla
Escuela de Ingeniería
Departamento de Ingeniería en Sistemas Computacionales
Análisis y Desarrollo de Técnicas para la Exploración de un Ambiente Desconocido por un Robot Móvil
Tesis profesional presentada por
José Alberto Chávez Aragón
como requisito parcial para obtener el título en
Maestría en Ciencias con Especialidad en Ingeniería en Sistemas Computacionales
Jurado Calificador
Presidente: Dr. Fernando Antonio Aguilera Ramírez
Vocal y Director: Dr. Oleg Starostenko
Secretario: Dr. Daniel Vallejo Rodríguez
Cholula, Puebla, México a 18 de mayo de 2001
Derechos Reservados ©2001.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Análisis y exploración de las tecnologías de Agentes Móviles y Jini en su contexto Cliente-Servidor para recuperación de información visual"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>1.1 Definición del problema

Existen escenarios en los cuales el manejo de informacion distribuida es muy
importante, especialmente de onformación  visual, tal como imágenes, documentos de texto,
secuencias de videos que requieren ser recuperados mediante algún patrón de comparación
con un cierto porcentake de aceptacion y de rapidez en la obtención de los tesultados
requeridos.

Existen muchos sistemas distrubuidos los cuales ofrecen cada vez más recursos que
pretenden mejorar el desempeño del manejo de información distribuida. El manejo de este
tipo de información, por ejemplo, en Bibliotecas Digitales y Sistemas de Información
Geográficos, entre muchos otros, son relevantes para que estps se desarrollen con un alto
grado de desempeño.

Empresas diseñadoras de software están cada vez mas dedicadas a ofrecer herramientas que permitan hacer de la iformación distribuida un manejo transparente y
fácil para el usuario final.

Sin embargo se requiere tener un estudio más profundo sobre las diferentes
aplicaciones que puedan existir para cada una de estas herramientas en recuperación de
información distribuida, ya que existen diferentes contextos de los cuales se pueden obtener
información, así como también, diferentes necesidades en intereses de usuarios finales, tales
como tiempo de transferencia, seguridad de sus datos, costos, etc.

Es por eso que se lleva acabo un estudio profundo de la tecnologia de Agente Móvil y
Jini en su contexto cliente/servidor, las cuales prometen un alto desempeño en la recuperación de información distribuida.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>1.2 Objejtivos generales.

Se realiza un análisis y exploración de dos tecnologías como son Agentes móviles y
Jini en el contexto de cliente/servidor, para la recuperacion de información visual, 
especialmente de imágenes distribuidas y documentos de texto. Obteniendo de esta manera
un estudio profundo sobre el desempeño de éstas dos tecnológías y una serie de
recomendaciones para determinar cual de estas es la que ofrece mejores desempeños en
cuanto a la recuperación de información, especialmente de imágenes, con el fin de apoyar a
diferentes comunidades que requieran de obtención de este tipo de información distribuida,
tales como Bibliotecas digitales, Educación a Distancia, Sistemas de Información Geográficas, entre otras.

1.3 Objetivos Especificos.

Se lleva a cabo un análisis y exploración de dos nuevas tecnologías poderosas en la
recuperación de información distribuida como es Agentes Móviles y Jini. Se obtiene
como resultado, un estudio más completo en cuanto a la recuperación de imágenes, y una serie de
recomendaciones sobre los puntos potencuales en su desempeño y uso, para poder
determinar en que aplicaciones ofrece mejores resultados cualquiera de estas dos
tecnologías y lograr en la aplicacion un mejor desempeño. Todo esto mediante pruebas
realizadas sobre dos plataformas: Unix y Windows, estudios de las capacidades de cada una 
de ellas, caparación entre las dos tecnologías para obtener el mejor desempeño en cuanto
a recuperación de información, manejo se seguridad, soporte de plataforma, complejidad de
implementación, asi como tambien la debilidad de la tecnología que pueda causar un mal
desempeño con su aplicación.

Se espera alcanzar un mejor desempeño de recuperación de información visual,
limitado por el momento el estudio, a la recupercaión de imágenes, utilizando para ello la
mejor tecnología, logrando optimizar la recuperación de las imagenes requeridas, con un
alto procentaje de información deseada. Esto es, solo recuperar los datos esperados con un
alto grado de aceptabilidad.

Para obtener los resultados requeridos del proyecto, se llevan a cabo las siguientes actividades.

*   Se desarrolla un agente móvil el  cual viaje a diferentes servidores buscando imágenes
    requeridas y recuperarlas al usuario final. Todo esto dentro de los laboratorios de
    cómputo de la UDLA.

*   Se desarrolla ua aplicación en Jini para la recuperación de imagenes y ofrecerlas al
    usuario. Aplicando sus pruebas solo dentro de los laboratorios de cómputo de la 
    UDLA.

*   Comparaciones cuantitativas de recuperación de los patrones deseados de las dos
    tecnologías.

    Estas comparaciones son:

        *   Velocidad de respuesta al encontrar un patrón con un porcentaje de similitud al
            deseado y traerlo al servidor local.

        *   Complejidad de los métodos que ofrece cada tecnología para desarrllo de
            aplicaciones een recuperación de imágenes.

        *   Seguridad de portabilidad de datos.

        *   Desempeño de las tecnologías ante la recuperación de dos formatos estándares de
            imágenes como son GIF o JPG, con diferentes tamaños de bytes.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>8.1 Conclusiones

Este proyecto fué dedicado al estudio de dos tecnologías de recuperación de información distribuida tal como Jini en su contexto cliente/servidor y Agententes Móvil.

Durante el estudio de cada una de ellas se fué descubriendo su potencial en diferentes
aplicaciones de la ingeniería en sistemas computaciónales. No obstante el objetivo de este proyecto fué descubrir de las dos tecnologías, la que obtuviera mejores resultados en la recupercaión de información visual, especialmente y dedicado a la nformación de imagenes y documentos de texto. Los resultados obtenidos a estas tecnologías fueron solo
basados dentro de este contexto de investigación. Reiterando que las dos tecnologías son
fuertemente confiables en la recuperación de información distribuida.

Como nuevas tecnologías, Agentes Móviles y Jini poco a poco van creciendo, ofreciendo cada vez más una recuperación exitosa de información distribuida.

Se pudo apreciar, que son dos tecnologías un poco diferentes en cuanto a su
aplicación, mas sin embargo, a lo largo de este proyecto se fue descubriendo que al igual
que los Agentes Móviles Jini ofrece servicios de recuperación de información, tanto general 
como personalizada.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de las Américas Puebla
Escuela de Ingeniería
Departamento de Ingeniería en Sistemas Computacionales
Análisis y exploración de las tecnologías de Agentes Móviles y Jini en su contexto Cliente/Servidor para recuperación de información visual
Tesis profesional presentada por
Sergio Fabian Ruiz Paz
como requisito parcial para obtener el título en
Maestría en Ciencias con Especialidad en Ingeniería en Sistemas Computacionales
Jurado Calificador
Presidente: Dra. María del Pilar Gómez Gil
Vocal y Director: Dr. Oleg Starostenko
Secretario: Dr. J. Alfredo Sánchez Huitrón
Cholula, Puebla, México a 10 de diciembre de 2002
Derechos Reservados ©2002.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Aplicaciones de la Videoconferencia en Bibliotecas Digitales"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>1.1 La biblioteca digital florística

La iniciativa de la Biblioteca Digital Florística (FDL) es un proyecto multi−institucional y
multidisciplinario que proporciona espacios de colaboración virtual sobre la base de
colecciones de información botánica actualizada y una variedad de servicios de biblioteca
digital. Los proyectos Flora de Norte América (FNA) y la Flora de China (FOC) forman parte
de esta iniciativa.

Construir una biblioteca digital florística requiere de la colaboración y comunicación de
autoridades especializadas en la materia y de procesos complejos de revisión. Sólo el
proyecto de FNA involucra mas de 800 científicos, incluyendo taxonomistas y biólogos. Se
pretende que la FNA sea una biblioteca digital florística, con información actualizada sobre
aproximadamente 20,000 espacies de plantas vasculares y briofitas recolectadas al norte de
México, en Estados Unidos, Cayos de la Florida, Islas Aleutianas, Canadá, Groelandia y las
Islas de Miquelon y Saint Pierre [FNA 1996; Schnase et al. 1997].

La biblioteca digital FDL consistirá de una gran colección de documentos en una gran
variedad de medios y formatos incluyendo texto, mapas e ilustraciones, e integrará
herramientas computacionales y servicios de biblioteca [Schnase et al. 1997].
Es de esperarse que un proyecto de tal magnitud ofrezca retos y dificultades. Algunas
interrogantes serían: ¿cómo mejorar la introducción masiva de datos? ¿cómo lograr una buena
digitalización de la información? ¿cómo recuperar información útil para el usuario? ¿cómo
mantener a los usuarios de la comunidad en constante comunicación?. Esta tesis está enfocada
a tratar el problema de comunicación entre los usuarios de una biblioteca digital distribuida.

Como se ha mencionado en secciones anteriores, una de las tareas cotidianas en el ambiente
de bibliotecas digitales es la comunicación entre los usuarios. Dada la gran cantidad y
diversidad de usuarios y las herramientas existentes para comunicación surgen las siguientes
interrogantes: ¿cómo lograr que los usuarios se mantengan comunicados entre sí para
compartir información, experiencias, puntos de vista, localización de documentos situados en
la biblioteca digital? ¿de qué manera se pueden incorporarse sistemas de videoconferencia a
una biblioteca digital de estas características?</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>1.2 Objetivos del proyecto

Una vez planteado el marco de trabajo en el que se desarrolló esta investigación podemos
mencionar sus objetivos:
*   Analizar el concepto de videoconferencia.
*   Analizar el diseño e implementación de sistemas de videoconferencia ya existentes.
*   Analizar las diferentes técnicas existentes de compresión para audio y video digital.
*   Definir las tareas que puedan ser ejecutadas con esta tecnología en los nodos de una
    biblioteca digital distribuida.
*   Plantear los problemas de comunicación y seguridad involucrados al implementar
    este tipo de tecnología, de manera que puedan atenuarse o eliminarse dichos
    problemas.
*   Plantear un modelo que permita incorporar la funcionalidad de videoconferencia a
    una biblioteca digital.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>7.1 Logros

Uno de los mayores problemas de las bibliotecas digitales es la comunicación y colaboración
visual entre los usuarios que la conforman. Esta tesis se enfocó al estudio e incorporación de
tecnologías novedosas que permitan mejorar la colaboración y comunicación visual entre los
usuarios en el contexto de una biblioteca digital botánica (FDL). Para lograr crear una
aplicación capaz de comunicar a los usuarios de forma visual desde un ambiente dinámico
que conforma la biblioteca digital botánica (FDL), se incorporó una de las tecnologías más
novedosas y de más auge con que actualmente se cuentan en la ciencia de la computación, la
tecnología de videoconferencia.

Se diseñó VicFDL (Videoconferencia en la FDL), un sistema basado en su totalidad en el
modelo conceptual propuesto, cuya tarea es tomar en cuenta las necesidades de
comunicación, colaboración y manejo de información de los miembros de una comunidad
botánica en el contexto de una biblioteca digital botánica (FDL). El sistema busca definir el
perfil de los individuos dentro de la biblioteca digital botánica, este perfil consiste en el grado
de interacción y nivel de experiencia de los individuos miembros de la comunidad botánica
para su posterior comunicación visual.

Específicamente, VicFDL provee servicios de comunicación con los integrantes de una
biblioteca digital botánica que se encuentran distribuidos en diferentes partes del mundo, así
como la sincronización de sus recursos permitiendo la creación de espacios o salones
virtuales para la discusión de temas de interés común, mediante la técnica de comunicación
planteada en el modelo propuesto. Las características más importantes de este modelo son que
permite la comunicación de los usuarios de la FDL de manera visual, empleando mecanismos
de comunicación tanto el ambiente WWW como en un ambiente de aplicación, logrando
liberar la carga de trabajo tanto para la biblioteca digital botánica (FDL) como para los
medios de comunicación de los usuarios. Otra característica de este modelo es que permite
tener un mecanismo de multicast como un servicio integrado de la biblioteca digital para
ofrecer videoconferencias por demanda por medio de los espacios o salones virtuales creados
por los usuarios miembros de la biblioteca digital.
Otro aspecto interesante que se logró incorporar a VicFDL es la retroalimentación por parte
de los usuarios, ya que el sistema permite que exista tanto en el módulo web como el módulo
de aplicación una interacción y una colaboración más natural, esta forma de retroalimentación
permite ir refinando las búsquedas posteriores de la información de acuerdo a los gustos del
usuario.

VicFDL es un sistema de videoconferencia que permite comunicar a los usuarios de una
comunidad botánica de una manera transparente, pues permite comunicar diferentes
computadoras sin tener que preocuparse sobre los protocolos de comunicación, los
requerimientos de hardware o los paradigmas de programación utilizados. El usuario
únicamente instanciará que tipo de comunicación requiere, que tipo de sistema operativo tiene
ejecutando una instrucción y el sistema se encargará del resto.

El modelo propuesto puede implementarse de varias maneras. Para demostrar su factibilidad,
se implementó un prototipo en la biblioteca digital botánica (FDL) llamado VicFDL con las
tecnologías existentes de videoconferencia.

Se puede decir que el desarrollo de este sistema cumple con los objetivos propuestos en la
sección 1.4. Primero, porque se logró la incorporación de mecanismos de comunicación
visual a la biblioteca digital botánica y después porque este sistema permitió mejorar la
comunicación de una forma más natural con los usuarios de dicha biblioteca. El diseño de
VicFDL permitió cumplir de manera concreta algunos de los objetivos planteados, como el
analizar a profundidad el concepto de videoconferencia, la revisión de los diseños e
implementaciones de sistemas ya existentes, las técnicas de compresión para la transmisión
de audio y video, tanto la definición de tareas que realiza en sistema de videoconferencia en
los nodos que componen a la biblioteca digital.

Otra meta importante de este proyecto ha sido estudiar el paradigma de los sistemas de
videoconferencia y optimizar los tiempos de respuesta empleando mecanismos que liberan a
la biblioteca de la carga de transferencia de audio y video que se requieren para la
implementación de esta tecnología; esto se logró gracias a la exhaustiva revisión bibliográfica
y las pruebas de los sistemas de videoconferencia existentes.
También se plantearon los problemas de comunicación en lo que se refiere a los anchos de
banda y seguridad involucrados en el desarrollo de sistemas de videoconferencia, así como las
posibles soluciones a dichos problemas (ver capítulo 3, sección 10.3.1).

En resumen, el haber conjuntado tres áreas de investigación tan importantes con son:
bibliotecas digitales, videoconferencia, y técnicas de compresión permitió desarrollar e
implementar un sub−sistema comunicación por videoconferencia en el contexto de bibliotecas
digitales distribuidas.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de las Américas Puebla
Escuela de Ingeniería
Departamento de Ingeniería en Sistemas Computacionales

Aplicaciones de la Videoconferencia en Bibliotecas Digitales

Tesis profesional presentada por
Raúl Morales Salcedo

como requisito parcial para obtener el título en
Maestría en Ciencias con Especialidad en Ingeniería en Sistemas Computacionales

Jurado Calificador

Presidente: Dr. Oleg Starostenko
Vocal y Director: Dr. J. Alfredo Sánchez Huitrón
Secretario: Dra. Ingrid Kirschning Albers
Cholula, Puebla, México a 4 de mayo de 1999
Derechos Reservados ©1999.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Aproximación y manipulación de objetos definidos en CSG mediante octrees"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La graficación, es una de las formas más fáciles de comunicarse con una computadora, es una
herramienta muy útil con la que el hombre puede crear arte, representar conocimiento y
comunicarse de una manera visualmente agradable y fácil de comprender.
Las gráficas por computadora se utilizan en muchas áreas, tales como: industria, negocios,
gobierno, educación y entretenimiento. La lista de aplicaciones es enorme y está creciendo
rápidamente [2].

Hay diferentes tipos de gráficas desde el punto de vista de las dimensiones. Por ejemplo las
gráficas de dos dimensiones. Este tipo de gráficas se pueden visualizar fácilmente en la
pantalla de una computadora, ya que dicha pantalla, se puede considerar como un mundo de
dos dimensiones. Sin embargo, el proceso de visualización de una gráfica de más de dos
dimensiones es mucho más complejo precisamente porque la pantalla de la computadora es
de dos dimensiones. Debido a esto, una gráfica de tres dimensiones o más, puede ser
únicamente proyectada en la pantalla.

Este proyecto enfoca la creación de gráficas de tres dimensiones, que serán tratadas cómo
objetos los cuales puedan ser manipulados por medio de la rotación y la translación. La
rotación, es útil para poder visualizar desde diferentes puntos de vista al objeto, y la
translación permite cambiar de posición a un objeto, lo que es visualmente agradable, por
ejemplo, si se considera que la pantalla esta formada con los ejes "X" (ancho) y "Y" (largo) y
al eje "Z" como punto de referencia desde donde se ve al objeto, entonces, al transladar un
objeto sobre el eje Z por ejemplo, produce una sensación de acercamiento o alejamiento del
objeto.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>1.2 Definición

El objetivo de este proyecto es poder hacer una representación aproximada de objetos sólidos
definidos en CSG (Geometría Sólida Constructiva) por medio de árboles octales, tomando en
cuenta que CSG es un esquema cuyos sólidos primitivos simples se combinan por medio de
un conjunto de operaciones Booleanas regularizadas, las cuales se incluyen directamente en la
representación [6]. En la figura 1.1 , se muestra un objeto definido por CSG.

Existe un método clásico en donde se genera cada Árbol octal de las primitivas que
conforman el objeto deseado, y después se ejecutan las operaciones Booleanas sobre estos
árboles generados. Llevando a cabo este procedimiento, se usa gran cantidad de memoria y
procesamiento.

Además, el número de octantes que forman a cada primitiva, generalmente es más grande que
el número de octantes que conforman al objeto final, y una vez que se ha creado el objeto, los
árboles octales de cada primitiva ya no se necesitan.
La representación del Árbol Octal de un objeto CSG deseado, se puede realizar, utilizando un
método Top−down, donde no se necesita generar cada Árbol Octal de las primitivas que
forman a un objeto, ni ejecutar las operaciones Booleanas sobre estos árboles. Ya que, no
todos los octantes que están incluidos en las primitivas que forman al árbol CSG forman parte
del objeto final [4].

Tomando en cuenta que el procesamiento, y en particular los recursos de memoria que se
requieren para generar un Árbol Octal deseado se gastan al examinar, generar y almacenar los
octantes y ejecutar operaciones Booleanas sobre estos [4]. Se ve la importancia del método
propuesto y su eficiencia puede ser apreciada, porque sólo involucra la generación,
almacenamiento de aquellos octantes incluidos en el objeto final.

1.3 Objetivos

1.3.1 Objetivos Generales .
 
*   Desarrollar un programa que sea capaz de hacer una representación aproximada de
    objetos sólidos, definidos en CSG por medio de árboles octales.
*   Aplicar al objeto construido, un conjunto de operaciones Booleanas: Unión,
*   Intersección, Diferencia y Complemento, con otros objetos sólidos definidos
    similarmente, para formar nuevos sólidos.
*   Realizar manipulaciones sobre el sólido, como rotación y translación

1.3.2 Objetivos Específicos

*   Desarrollar un método que directamente obtenga los nodos finales del Árbol Octal del
    objeto CSG final. Esto es, sin la necesidad de crear el Árbol Octal de cada primitiva
    del árbol CSG, ni realizar las operaciones Booleanas entre los Árboles Octales.
*   La finalidad es que el nuevo método sea más veloz y tenga menores requerimientos
    de memoria, es decir, que sea más eficiente.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>6.1 Conclusión

La construcción de objetos sólidos en 3 dimensiones es muy útil para gran cantidad de áreas,
y existen infinidad de métodos para crearlos, de acuerdo a cada necesidad, tanto de precisión,
de tiempo, de memoria, de facilidad entre otros requerimientos.
La creación de objetos sólidos definidos mediante la Geometría Sólida Constructiva requiere
de un conjunto de primitivas y operaciones Booleanas para unir estas primitivas. Los árboles
octales proporcionan facilidad para crear las primitivas y, valiéndose de las propiedades para
el manejo de operaciones Booleanas de los árboles octales, se puede definir un objeto
mediante Geometría Sólida Constructiva.
Un método al que se le llamó método clásico consiste de crear árboles octales de cada
primitiva y mediante operaciones Booleanas unirlas, creando al mismo tiempo árboles octales
de las operaciones Booleanas y generando así un objeto final, ya con el cual todos los demás
árboles creados quedan sin utilidad. Teniendo como base [4] se consideró la posibilidad de
crear un método Top Down el cual resultó más rápido y que al mismo tiempo ocupó menos
memoria.

El método Top Down efectivamente es más rápido que el método clásico, y entre más
precisión tenga el objeto más se nota la rapidez, por que ésta se acentúa más ya que hay
menos creación de octantes innecesarios, y de sub−árboles completos, esto se vio claramente
en la tabla 5.4. En donde el mismo objeto pero con más precisión gastaba proporcionalmente
menos memoria en el Método Top Down con respecto al Método Clásico.

Con respecto a la memoria se puede concluir que la mejoría es muy notoria, puesto que con el
Método Top Down únicamente se creará un solo árbol octal, el cual contiene al objeto
deseado; mientras que el Método Clásico, al construir varios árboles octales para crear un
objeto final, ocupa más memoria. Es muy claro notar que la memoria que ocupa cada objeto
depende del tipo de primitivas que lo forman y la cantidad de éstas, ya que entre más
complejo sea más, memoria ocupará. El ahorro de memoria en el método Top Down con
respecto al método clásico no depende de la precisión del objeto, como se puede ver en la
tabla 5.4 , ya que en promedio el ahorro por ejemplo entre un objeto con una precisión de 70
y el mismo objeto con una precisión de 100 es muy similar. No obstante aun cuando el ahorro
no depende de la precisión, un objeto entre más precisión tenga más cantidad de memoria va
a utilizar.

El método CSG vía árboles octales, es útil para crear objetos sólidos y es fácil manipular a un
objeto final mediante un conjunto de Rotaciones y Translaciones, ya que por la propiedad que
tienen los árboles octales de almacenar la información ordenadamente se puede visualizar
cada octante que forma parte del objeto final en un rotación específica.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de las Américas Puebla
Escuela de Ingeniería
Departamento de Ingeniería en Sistemas Computacionales

Aproximación y manipulación de objetos definidos en CSG mediante octrees (árboles octales)

Tesis profesional presentada por
Marva Angélica Mora Lumbreras
como requisito parcial para obtener el título en
Maestría en Ciencias con Especialidad en Ingeniería en Sistemas Computacionales

Jurado Calificador

Presidente: Dr. David Ricardo Sol Martínez
Vocal y Director: Dr. Fernando Antonio Aguilera Ramírez
Secretario: Dr. Rogelio Dávila Pérez
Cholula, Puebla, México a 17 de mayo de 2000

Derechos Reservados ©2000.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>"Desarrollo de un exportador de datos geográficos en una arquitectura de componentes GIS"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>1.1 Definición del problema

Muchas de nuestras actividades cotidianas requieren conocer información acerca de la
distancia entre lugares, tiempos de recorrido y ubicación de servicios. Sin embargo, cuando se
desconoce esta información los mapas se convierten en una herramienta importante en el
proceso de toma de decisiones. Para mejorar este proceso, actualmente se utilizan los GIS en
el análisis y la consulta de grandes cantidades de información geográfica llevando a cabo
estos procesos en tiempos muy cortos.
Actualmente varias empresas de SIG ofrecen productos comerciales basados en el Web.
Algunos productos que podemos mencionar son los siguientes: ArcView Internet Map Server
(IMS) y Map Objects IMS de ESRI Inc[1],. MapXsite y MapXtreme de MapInfo Corp. y Map
Guide 3.0 de Autodesk Inc entre otros. Aunque estos SIG basados en el Web parecen ser
similares se basan en diferentes arquitecturas, bases de datos, plataformas, formatos de datos
y diferentes metodologías como CGI (Common Gateway Interface), Plug−ins, HTML
extendido y Java.
Sin embargo a pesar de estas diferencias el problema más común que se tiene en el campo de
los GIS es la existencia de diferentes formatos para almacenar los datos geográficos, lo que
dificulta el intercambio de la información geográfica. Establecer un formato estándar de
almacenamiento permitirá un mayor acceso a la información de diferentes GIS sin la
necesidad de contar con algún producto en especial, sobre todo en el contexto de Internet.
Esfuerzos de este tipo son llevados a cabo por comités como el Consorcio Open GIS (Open
Geospatial Intereoperability Specification Consortium, o OGC)[10]. Sin embargo,
actualmente muchos de los datos de un GIS son almacenados en sistemas de archivos o bases
de datos con un formato especifico y sólo a través del respectivo GIS es posible consultar y
analizar estos datos. Como consecuencia los usuarios no pueden utilizar un solo producto GIS
para tener acceso a todos los datos que deseen, lo que resulta en un alto costo si se adquiere
alguno o varios productos GIS. Otro problema que se tiene es el tiempo que se requiere para
conocer y entender algún producto específico, además del tiempo y los recursos que se
utilizan para instalar y configurar cada producto. El problema aumenta en Internet ya que
existen datos geográficos distribuidos en todo el mundo, almacenados con diferentes formatos
y administrados por diferentes GIS.
Para resolver el problema mencionado anteriormente un número considerable de A Java
applets@ y software relacionado con los GIS han recientemente empezado a emerger en el
WWW. El rango va desde A applets@ simples, inmaduras y experimentales hasta
aplicaciones complejas. En esta campo Java ha ganado una rápida aceptación como un
lenguaje de programación para el web[7], por su independencia de plataforma y
programación orientada a objetos. Aunque muchas de las Java applets son hechas
principalmente en el campo de la investigación con el objeto de resolver problemas que
aparecen durante investigaciones o para ilustrar actividades educacionales también existen
aplicaciones complejas hechas en Java. Algunos trabajos que merecen ser mencionados son:

* Cambray, y Leclerc, C., Software Architectures Based on cartographical Products.
1999. [4]
* Kotzinos, D., y Prastacos, P., GAEA, a Java−based Map Applet , 1999.[5]
* Sorokine, A. y Merzliakova, I., Interactive Map Applet for Illustrative Purposes ,
1998.[6]
* Kim M., Kim K., Lee K., y Lee J., Pure Java−Based GIS for a advanced
Geo−Processing over WWW Environmental 1999. [7]

Estos trabajos resuelven algunos de los problemas que planteamos anteriormente pero no
cuentan con una arquitectura lo suficientemente abierta que permita el acceso a datos
geográficos sin depender de una formato particular (propietario) de almacenamiento.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>1.2 Objetivos

La propuesta de esta tesis es el diseño y desarrollo de un exportador de datos geográficos en
una Arquitectura de Componentes GIS . La arquitectura de componentes GIS permitirá a los
usuarios de Internet obtener datos geográficos a través de los Componentes Clientes y el
Exportador de datos geográficos.

Los Componentes Clientes pueden ser aplicaciones que a través de una interfaz gráfica o
applet obtienen las solicitudes de datos geográficos de los usuarios de Internet. Los datos
geográficos se obtendrán a través del Exportador que responderá a las solicitudes de los
diferentes Componentes Clientes. La ventaja de esta arquitectura es que los Componentes
Clientes no necesitan contar con algún software especial como un GIS o un DBMS y los
usuarios en Internet solo necesitaran un navegador para consultar los datos geográficos
obtenidos.
Tal arquitectura deberá tener las siguientes características: ser abierta, permitir compartir e
intercambiar datos, proporcionar una interfaz homogénea al usuario y contar con una forma
común de almacenar los datos espaciales y no espaciales. Para llevar a cabo esta propuesta se
identificarán y/o construirán los diferentes tipos de software que pueden ser usados como
componentes de dicha arquitectura: lenguajes de programación, Sistemas de Información
Geográfica y Administradores de bases de datos (DBMS)[4]. Los principales componentes
que conformarán esta arquitectura son:

*   Cliente : Aplicaciones que utilizan información geográfica de algún sitio que cuente
    con un exportador de datos geográficos. Los clientes pueden ser locales o encontrarse
    en otros sitios diferentes.
*   Exportador : Es el componente que permitirá el acceso a la información geográfica
    desde diferentes Clientes utilizando un formato estándar de almacenamiento de los
    datos.
*   Datos : Información espacial y no espacial almacenada con un formato de datos
    estándar, aunque su origen puede ser el de un GIS comercial pero traducida al
    formato estándar. 

1.2.1 Objetivo general

El objetivo de esta tesis es el diseño y desarrollo de un exportador de datos geográficos
dentro del contexto de una arquitectura de software basada en componentes GIS. Este
exportador permitirá el acceso a la información geográfica a los diferentes Clientes
componentes GIS localizados localmente o remotamente en otros sitios.

1.2.2 Objetivos específicos

*   Proporcionar la funcionalidad del Exportador de datos geográficos a cualquier sitio
    que cuente con un GIS.
*   Facilitar el acceso de la información de diferentes GIS utilizando un formato estándar
    para almacenar los datos geográficos.
*   Los Componentes Clientes podrán obtener información geográfica local o remota de
    forma transparente.
*   Acceso a la información geográfica a través de una arquitectura que elimine la
    necesidad de que el usuario final mantenga localmente el software (GIS, DBMS) o
    los datos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>6.1 Conclusiones de la arquitectura

El problema más difícil de resolver en el desarrollo de esta tesis fue el diseño de la
arquitectura. Para lograr este diseño se analizaron varios trabajos relacionados con el
problema y diferentes tipos de software (DBMS, GIS, Middleware) que sirvieron para su
desarrollo. Finalmente se obtuvo una arquitectura con las siguientes características:

*   Independiente de plataforma.
*   Abierta a la anexión de otros componentes. 
*   Con capacidad para intercambiar datos geográficos en Internet
*   Con un diseño y desarrollo de bajo costo. 
*   Utiliza un formato estándar para almacenar los datos geográficos.
*   Los Clientes no necesitan contar con un GIS o DBMS para obtener datos geográficos. 
 
Los usuarios de Internet podrán obtener datos geográficos utilizando solo un
navegador.

Por las características descritas anteriormente se puede concluir que esta arquitectura es
abierta para que en un futuro se pueden anexar más componentes, lo que significaría ampliar
el rango de funciones GIS para satisfacer las necesidades de los usuarios de información
geográfica en Internet.

6.2 Conclusiones del exportador

Con respecto al exportador de datos geográficos podemos concluir que el objetivo de servir
datos geográficos en un formato standard a los clientes componentes que lo soliciten se logro.
Como conclusión se puede decir que se obtuvo un exportador con las siguientes
características:
 
*   Proporcionar la funcionalidad del Exportador de datos geográficos a cualquier sitio
    que cuente con ArcView GIS. 
*   Facilita el acceso a los datos geográficos de diferentes GIS utilizando el formato
    estándar OpenGIS. Actualmente se pueden traducir archivos Shapefile a la
    especificación OpenGIS.
*   Los Componentes Clientes podrán obtener datos geográficos local o remotamente de
    forma transparente.
*   Acceso a la información geográfica sin la necesidad de que los clientes cuenten con
    un GIS, DBMS o los datos geográficos.
*   Los usuarios podrán tener acceso a los datos geográficos del exportador sin la
    necesidad de mantener localmente el software (GIS, DBMS) o los datos.
*   Capacidad para administrar los datos geográficos que se desean exportar con una
    interfaz de usuario gráfica.

6.3 Trabajos a futuro

En el desarrollo de esta tesis aprendimos que diseñar y desarrollar una arquitectura como ésta
implica un gran esfuerzo, por lo que varios trabajos a futuro se pueden enfocar para mejorar
la arquitectura y sobre todo para ampliarla. La arquitectura de componentes GIS se puede ver
como una colección de componentes que proporcionan la funcionalidad de un Internet GIS a
cualquier Web site que cuente con un GIS. La ventaja de esta arquitectura es que es abierta
para que en un futuro se pueden anexar más componentes. Los Componentes pueden
implementar un amplio rango de aplicaciones de acuerdo con las necesidades de los usuarios
de información geográfica. Como el componente Exportador de datos geográficos ya se
diseño e implementó en la presente tesis ahora se necesita que trabajar para crear más
componentes que utilicen el exportador. Trabajos que se tienen actualmente en desarrollo por
compañeros de la maestría son:

*   Picent Pedro, Creación de ligas a información geográfica , 2000. [26]
*   Loranca Olivia, Consultas espaciales , 2000. [27] 

Actualmente se tienen algunas necesidades que pueden ampliar la funcionalidad de la
arquitectura y del exportador de datos geográficos, entre las ampliaciones que se podrían
realizar en el futuro podemos mencionar las siguientes:
 
*   Ampliar el rango de formatos que se pueden traducir a la especificación OpenGIS.
*   Por que actualmente solo se traducen archivos Shapefile de ArcView.
*   Contar con un proceso inverso que convierta los objetos OpenGIS a archivos
*   ShapeFile y otros formatos propietarios de GIS comerciales.
*   Crear un Visualizador que despliegue gráficamente objetos OpenGIS y que utilice la
    invocación de métodos remotos (RMI) para recuperar los datos geográficos del
    exportador.
*   Crear una interfaz entre el Exportador y ArcView GIS para que ArcView procese la
    información que el exportador le solicite y posteriormente la traduzca a la
    especificación OpenGIS para que esta sea accesible a los clientes.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Universidad de las Américas Puebla
Escuela de Ingeniería
Departamento de Ingeniería en Sistemas Computacionales

Desarrollo de un exportador de datos geográficos en una arquitectura de componentes GIS

Tesis profesional presentada por
Germán Escobar Alonso
como requisito parcial para obtener el título en
Maestría en Ciencias con Especialidad en Ingeniería en Sistemas Computacionales

Jurado Calificador

Presidente: Dr. Gerardo Ayala San Martín
Vocal y Director: Dr. David Ricardo Sol Martínez
Secretario: Dra. María del Pilar Gómez Gil
Cholula, Puebla, México a 16 de mayo de 2000

Derechos Reservados ©2000.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Simulación paralela de los procesos de
intrusión y retracción de mercurio (Hg)
en medios porosos para clusters multicore</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En esta tesis se ha presentado la elaboración de un simulador paralelo para dos procesos de la porosimetría de mercurio, el proceso de intrusión y el proceso de retracción. Este simulador paralelo permite ejecutar los procesos de porosimetría de mercurio en una red modelada por computadora que representa un medio poroso. La manera en la que se representa la red porosa es utilizando el DSBM que es un modelo diseñado en la UAM-I, y que permite que las redes porosas se asemejen mucho a la realidad. En cuanto al simulador que proponemos, este fue diseñado siguiendo el modelo de memoria compartida y distribuida y ambos implementados con OpenMP y MPI respectivamente.
La versión inicial se realizó para una red que se procesa en un modelo multiprocesador con esquemas de memoria compartida. Esta versión se probó en diferentes nodos con diferente número de procesadores, y se notó que la tendencia hacia el mejor rendimiento se obtiene al usar el mismo número de hilos que procesadores. Se probó que en un nodo de 4 procesadores el número ideal de hilos fue 4, y al probarlo en un nodo de 8 procesadores la mejor configuración se obtiene al usar 8 hilos. Este resultado se obtuvo con una versión del simulador en donde se reparte equitativamente las caras de la red entre los hilos para realizar los procesos de intrusión y de retracción. Esto nos indica que entre mejor se reparten (de manera equitativa) los datos entre los hilos se obtienen los mejores resultados. De igual manera con el esquema equitativo se observa que los procesadores se encuentran trabajando en un 100 %, indicando que se explota al máximo los recursos paralelos del esquema multiprocesador. Con la versión de hilos no equitativa el tiempo es mayor en comparación con la versión de hilos equitativa.
Considerando los resultados de la primera versión se generó una segunda versión del simulador, en la que se particiona y distribuye la red porosa cúbica. Estas particiones de la red se hacen para crear sub-redes asignándolas a diferentes procesadores para así distribuir el uso de memoria y los requerimientos de procesamiento entre todo el conjunto de procesadores.
En esta versión los recursos de memoria compartida en cada nodo son explotados, pero es agregado el protocolo de paso de mensajes para permitir la comunicación entre los procesadores. En esta versión se buscó la mejor congelación en cuanto a procesos utilizados en diferentes sistemas de cómputo. En un cluster de 32 nodos (128 procesadores) la mejor congelación se obtuvo al usar 256 procesos, dos procesos por procesador.
En el cluster Aitzaloa se probaron 256 procesadores y en esta congelación se obtuvo el mejor tiempo con 512 procesos lanzados. Cuando se ocupan de la mejor manera posible los recursos del cluster el tiempo disminuye aún más pues la distribución de carga se realiza de una manera más adecuada, esto se ve cuando ejecutamos el simulador paralelo usando un proceso por nodo. Con estos resultados se muestra una buena escalabilidad cuando se aumenta el número de procesadores a utilizar.
Otra conclusión que se determina al utilizar el simulador paralelo, es que cuando son ocupadas redes de mayor tamaño, la simulación de los dos procesos de porosimetría de mercurio toma más tiempo, debido a la gran cantidad de datos que se manejan y las operaciones que realizan. Además en redes de tamaño muy grande el uso de memoria aumenta, debido a que las redes son cargadas en memoria para su ejecución. Cuando se utiliza el esquema de memoria compartida y distribuida el uso de memoria se distribuye en los diferentes nodos del cluster utilizado, reduciendo así el uso de memoria por nodo. En el simulador, al distribuir la red en los diferentes nodos se aprovechan más los recursos de memoria y de procesamiento distribuidos en las máquinas utilizadas, haciendo que el tiempo que toma ejecutar los procesos de intrusión y retracción disminuya.
Al comparar los tiempos obtenidos para los procesos de intrusión y retracción de mercurio con los algoritmos secuenciales mencionados en el Capítulo 3 se obtienen mejores resultados en cuanto a menor uso de memoria y procesamiento al distribuir las tareas en varias máquinas.
El simulador paralelo mejoró los tiempos secuenciales obtenidos para los procesos de intrusión y de retracción de mercurio hasta en 100 veces en redes de mayor tamaño lo que nos indica que se mejora con este simulador la ejecución de estos procesos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Simulación paralela de los procesos de
intrusión y retracción de mercurio (Hg)
en medios porosos para clusters multicore
Para obtener el grado de
MAESTRO EN CIENCIAS
(Ciencias y Tecnologías de la Información)
PRESENTA
Lic. Carlos Hiram Moreno Montiel
Asesores de la Tesis
Dra. Graciela Román Alonso
Dr. Miguel Alfonso Castro García
Sinodales
Presidente: Dra. Graciela Román Alonso
Secretario: Dr. Fernando Rojas González
Vocal: Dr. Santiago Domínguez Domínguez
México D.F
Febrero del 2010</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Prototipo de Simulador Distribuido de Eventos Discretos</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Implementación de un prototipo de DDES sobre un ambiente
distribuido dinámico.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. Revisión de la literatura en el tema.
2. Dise~no de los elementos del sistema.
3. Selección de una herramienta de programación.
4. Implementación del prototipo
5. Evaluación de un caso de estudio.
6. Comunicación de resultados.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se han cumplido todos los objetivos para la construcción de este prototipo de simulador, por un lado el objetivo general: implementación de un prototipo sobre un ambiente distribuido dinámico, por otro lado, los objetivos particulares: especificar las operaciones, plantear las entidades de una arquitectura que implemente dichas operaciones, construir e integrar los componentes y el simulador tendrá soporte para una falla de paro.
Se ha establecido una base para el funcionamiento de este prototipo que podrá  ser utilizado para estudios de sistemas que se pueden representar mediante el enfoque de eventos discretos, y también para la enseñanza y estudio de algoritmos que resuelvan problemas correspondientes a los sistemas distribuidos.
Por otro lado, para el primer conjunto de pruebas, si no se ejecutan los algoritmos para toma de estado global y el cálculo de GVT, se puede utilizar 2 o 3 trabajadores.
Eso depende de la elección del usuario. En caso que se incluya la ejecución de los algoritmos ya mencionados, es muy conveniente usar 3 trabajadores a partir de grafos desde 300 a 1000 nodos, justo como se puede apreciar en la figura 5.9.
Los resultados de las secciones 5.2.6, 5.2.7, 5.2.8 y 5.2.9, para el primer conjunto de pruebas demuestran como en una simulación en donde se hace presente una falla de paro, resulta muy costoso recuperarse, sin embargo, ese costo se justica para lograr que la simulación analice satisfactoriamente. Es preciso notar que sólo para simulaciones donde se involucran grafos con 1000 nodos es conveniente dejar que el mecanismo de restauración se lleve a cabo, para el resto de datos conviene más reiniciar la simulación.
Para el segundo conjunto de pruebas, el mejor escenario es utilizar 5 trabajadores, con cualquiera de los grafos. A pesar de que se utilicen los mecanismos de toma de estado global y cálculo de GVT, 5 trabajadores sigue siendo la mejor opción. Si se cuenta con menos trabajadores, cualquiera de las simulaciones es aceptable, con 2,3 y 4 trabajadores.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>corpus_proyectos en Ciencias y Tecnologias de la Informacion
Prototipo de Simulador Distribuido de Eventos Discretos
Idonea Comunicacion de Resultados para obtener el grado de 
Maestro en Ciencias 
PRESENTA:
Ing. Jorge Luis Ramirez Ortiz
Asesor:
Dr. Ricardo Marcelin Jimenez
18 de Marzo 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Propuesta y evaluación de un protocolo
híbrido de control de acceso al medio (MAC)
con reservación de recursos</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La principal problemática en las redes de difusión es determinar cuándo y quién puede utilizar
el medio de transmisión compartido, cuando dos o más estaciones quieren transmitir. Los protocolos
utilizados para evitar o limitar esta situación pertenecen a la subcapa inferior de la capa de enlace de
datos llamada subcapa MAC (del inglés medium access control) [1]. En las redes de difusión, el papel
de la subcapa MAC es coordinar el acceso al medio de transmisión compartido, para evitar la
interferencia entre las estaciones que forman parte de la red [2]. Las redes de difusión también se
conocen con el nombre de redes de acceso múltiple, ya que múltiples estaciones comparten el mismo
medio de transmisión. En resumen, el control de acceso al medio es la parte de una arquitectura de red
que permite compartir un único medio de transmisión entre varias estaciones.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de este proyecto es proponer y evaluar un protocolo híbrido de control de
acceso al medio con reservación de recursos. Para tal efecto, se deben identificar los parámetros
relevantes de nuestra propuesta y su efecto en el desempeño de la red. De la misma manera, se debe
encontrar la diferencia en desempeño con respecto a una variante sin reservación, la cual es el
protocolo 2C en el cual se basa nuestra propuesta.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Imagine una red de telecomunicaciones con una velocidad impresionante
donde se pueda navegar sin demoras de tráfico y poder bajar información,
enviar fotos, música, videos y miles de archivos con mucha capacidad de
información que podría tardar minutos o tal vez horas en cuestión de
segundos.

Las redes de banda ancha o bien banda amplia, son la consecuencia de esa
necesidad de manejar información a altas velocidades, en esta monografía se
presentaran las tecnologías que han surgido.
En el capítulo uno se muestra cómo entender la banda ancha y las primeras
tecnologías que fueron surgiendo con capacidades de alta velocidad en las
telecomunicaciones, las cuales fueron iniciadas con medios físicos de par de
cobre (ATM, DSL, xDSL, ADSL), siguiendo con nuevas tecnologías que ya son
guiadas por fibra óptica como son (DWDM, SDH, FAST y GIGABIT
ETHERNET, FDDI).

En el capítulo dos se hace referencia a los avances de las tecnologías de alta
velocidad usando técnicas por fibra óptica, las cuales utilizan fibras monomodo
por su mayor capacidad de información, utilizando las mismas topologías para
redes. A partir del uso de la fibra óptica surgen las tecnologías como TDMA,
WDMA, SCMA, CDMA.

También se analizan técnicas donde ADSL se involucra con el bucle de
abonado, y técnicas usando comunicaciones eléctricas PLC. Y las técnicas
hibridas donde se utilizan fibra óptica junto con el cable coaxial, llamadas
redes HFC y CATV, que abarca desde la TV digital interactiva hasta el acceso
a Internet a alta velocidad, pasando por la telefonía.

En el tercer capítulo se analizan las tecnologías de banda ancha
radioeléctricas, las cuales son inalámbricas tales como LMDS y MMDS las
cuales cuentan con plataformas bien diseñadas para dar soporte a las
comunicaciones y ancho de banda, otra tecnología que cuenta con ese tipo de
técnicas es WLAN que es de área local, así mismo se analizan tecnologías
aplicadas a futuras generaciones de comunicación como GSM y sus
aplicaciones.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo se propone y se evalúa el desempeño, principalmente por medio de simulación, de un protocolo híbrido de control de acceso al medio con reservación de recursos síncrono de detección limitada, denominado protocolo 2C-R2. También, en este trabajo se evalúa el desempeño de un protocolo de acceso aleatorio sin reservación, principalmente por medio de simulación, el protocolo 2C.
De los resultados que se obtienen en la evaluación de desempeño, se puede concluir que nuestra propuesta presenta un mejor desempeño comparado con el protocolo 2C, en cuanto a la utilización efectiva del medio de transmisión, cuando la carga ofrecida a la red es alta. Cuando la carga ofrecida a la red es muy baja, ambos protocolos presentan el mismo desempeño en ese aspecto.
En lo que se refiere al retardo de acceso al medio de transmisión, el protocolo 2C-R2 presenta menores retardos, con respecto al protocolo 2C, en las mismas condiciones de carga ofrecida a la red.
Cabe mencionar que con el protocolo 2C-R2 se pueden presentar retardos de acceso muy altos, por ejemplo, de casi cien ranuras de tiempo con veinte estaciones en la red cuando M=2, pero éstos se presentan en condiciones de estabilidad y cuando la carga ofrecida por las estaciones es alta.
En cuanto al porcentaje efectivo del tiempo total en estado de transmisión, cuando la carga ofrecida es muy baja, es mejor utilizar el protocolo 2C en lugar del protocolo 2C-R2, ya que, como no existe un alto riesgo de colisión, con una alta probabilidad el paquete de datos se transmitirá con éxito
al primer intento, y entonces no será necesario realizar algún tipo de reservación. Por el contrario, cuando la carga ofrecida es media o alta, lo mejor es realizar primero una reservación con paquetes pequeños de señalización y luego realizar la transmisión de los paquetes de datos, y entonces desperdiciar poca energía a causa de las colisiones que se presentan en la RRP.

En resumen, cuando la carga ofrecida a la red es baja, el acceso aleatorio logra un buen desempeño y probablemente no se justifique el utilizar otros métodos para compartir el medio de transmisión. Sin embargo, los resultados obtenidos muestran que, con medias o altas cargas de tráfico, como se presentan en distintos escenarios en la actualidad, un protocolo que combine acceso aleatorio y reservación puede mejorar significativamente el desempeño. El protocolo propuesto, 2C-R2, es un ejemplo de este enfoque híbrido y puede ser una buena opción para controlar el acceso a un medio de transmisión cuando la carga ofrecida a la red es media o alta y cuando se requiere un bajo consumo energético, como por ejemplo en redes inalámbricas de sensores.
Cabe mencionar que, con los resultados obtenidos durante la realización de este proyecto se presentó un poster en la 11ª Feria de Posgrados de Calidad 2010 del CONACYT, en representación de la Maestría en Ciencias y Tecnologías de la Información de la Universidad Autónoma Metropolitana.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD AUTÓNOMA METROPOLITANA
División de Ciencias Básicas e Ingeniería
Propuesta y evaluación de un protocolo
híbrido de control de acceso al medio (MAC)
con reservación de recursos
Idónea comunicación de resultados presentada por
Pablo Damián Hernández Durán
Para obtener el grado de
Maestro en Ciencias y Tecnologías de la
Información
Asesor: Dr. Miguel López Guerrero
Defendida públicamente en la UAM-Iztapalapa el 31 de Agosto del 2010 a las 10:00 hrs.
Jurado Calificador:
Presidente: Dr. Javier Gómez Castellanos
Secretario: Dr. Víctor Manuel Ramos Ramos
Vocal: Dr. Miguel López Guerrero
Derechos reservados (C) Pablo Damián Hernández Durán 2010.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Minería Sobre Grandes
Cantidades de Datos</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este caso se seleccionaron 6 clasificadores, de los cuales se obtienen sus clasificaciones individuales, estas se combinan mediante un criterio de votación ponderada, en donde el peso para cada clasificador son asignadas mediante el uso de un algoritmo genético, a diferencia de la mayoría de los trabajos que consideran las redes neuronales para obtener dicha ponderación. Una vez que se combinan las clasificaciones individuales mediante el criterio de votación ponderada se obtiene la clasificación final del conjunto de prueba.
En la comparación de nuestros resultados contra las herramientas para Minería de Datos, podemos apreciar en la Figura 5.9 que los resultados obtenidos por el WGME propuesto presentan un mayor grado de exactitud, con un 2.26% en promedio, al aplicarlo sobre la BD gemius completa, garantizando que existe una mejora con respecto a las herramientas consideradas. Por otro lado al comparar los resultados contra los resportados en el ECML/PKDD/2007 Discovery Challenge, nos encontramos solo por debajo del mejor resultado con más del 1% de exactitud.
Si bien tomando en cuenta que en este congreso se presentaron los mejores trabajos aplicados a la BD gemius completa, nuestros resultados se encuentran dentro de un rango aceptable con respecto a los resultados presentados en el congreso. No podemos garantizar que el WGME se podrá comportar bien para todas las BD que probemos, este caso se puede ver en la Tabla 5.3, en donde para la BD Mushroom, Stacking obtiene mejor exactitud que nuestra propuesta. Esto es razonable ya que como menciona el Teorema del no hay comida gratis (No free lunch), no existe un algoritmo que tenga un mejor rendimiento para todos los problemas a los que sea sometido, siempre va existir un algoritmo que sea mejor al nuestro, tal y como sucede en este caso.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Minería Sobre Grandes
Cantidades de Datos
Para obtener el grado de
MAESTRO EN CIENCIAS
(Ciencias y Tecnologías de la Información)
PRESENTA:
Lic. Benjamín Moreno Montiel
Asesor de la Tesis
Dr. René Mac Kinney Romero
Sinodales
Presidente: Dr. Eduardo Morales Manzanares
Secretario: Dr. René Mac Kinney Romero
Vocal: Dr. John Goddard Close
Vocal: M. en C. Alma Edith Martínez Licona
México D.F
Noviembre del 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Modelado de técticas de atributos de calidad para la generación de esqueletos de arquitecturas ejecutables.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Dentro del contexto descrito anteriormente, se debe señalar que actividades de diseño y construcción de la arquitectura son actividades que requieren un tiempo considerable y no existe una herramienta que facilite el desarrollo de la arquitectura ejecutable. Con el fin de facilitar el desarrollo de la arquitectura, una herramienta para este propósito debería permitir a los arquitectos y diseñadores trabajar a nivel de modelos y permitirles el generar de manera sencilla un esqueleto de la arquitectura ejecutable (de aquí en adelante se considerará esqueleto de arquitectura ejecutable como una implementación parcial de requerimientos no funcionales, específicamente atributos de calidad).</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Los objetivos generales de este proyecto de investigación son:
· Definir un proceso que utilice como base el enfoque MDA para el modelado y generación de un esqueleto de arquitectura ejecutable de una plataforma específica con soporte de un cierto número de atributos de calidad.
· Construir una herramienta que permita generar esqueletos de arquitecturas ejecutables para una plataforma en específico.
· Reducir el tiempo de diseño y construcción, así como facilitar el trabajo en estas fases de desarrollo de sistemas, utilizando dicha herramienta.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En esta sección se describe la metodología empleada en el desarrollo de este proyecto de investigación. Es decir, la manera en que se hizo la investigación; específicamente, investigación previa (antecedentes), hipótesis, variables, tipo de estudio, procesos utilizados y evaluación. El detalle de la metodología empleada para el desarrollo de este proyecto de investigación esta descrita continuación:
· Investigación previa. Se realiza la investigación sobre arquitecturas y atributos de calidad, así como también de cómo es que varios Framework de desarrollo implementan un conjunto de técticas para el soporte de atributos de calidad (el término de técticas de atributos de calidad seré definido més adelante en este trabajo).
· Hipótesis. La hipótesis de la que se parte en este proyecto es que el proceso que se define en este trabajo de tesis, permite construir un modelo específico de plataforma el cual contiene definiciones de soporte de un conjunto de atributos de calidad. El proceso definido permite la construcción de una herramienta que permita realizar la generación de código de manera automética a partir del modelo específico de plataforma, por lo que se estaría reduciendo el tiempo de desarrollo dedicado a la construcción de esqueletos de arquitecturas ejecutables (también conocidos como prototipos). En síntesis, se toma como hipótesis que el proceso definido permite la construcción de una herramienta que minimizaría el tiempo de desarrollo durante la transición entre las fases de elaboración y construcción de RUP.
· Variables. Las variables dentro de la metodología, pero més aun dentro del proceso del proyecto son el conjunto de técticas de atributos de calidad que son soportadas, así como la manera en que son implementadas en cada plataforma (Framework). Sin embargo, por cuestiones de tiempo sólo se utilizaré un conjunto de técticas sobre una única plataforma de desarrollo.
· Tipo de estudio. El tipo de estudio para este trabajo de tesis es experimental, ya que mediante la evaluación de la herramienta generada se pretende corroborar que el tiempo de desarrollo disminuye en la transición entre las fases de desarrollo de elaboración y construcción.
· Procesos utilizados. Se tomó como base parte del proceso MDA (més adelante se haré énfasis en que partes de este proceso se enfocó este trabajo) para la implementación y construcción de la herramienta que permitiré validar el trabajo realizado en esta tesis.
· Evaluación. El método de evaluación del trabajo es realizar comparativas de tiempo en el desarrollo de software empleando la herramienta generada en este proyecto.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Realizando un análisis del los objetivos generales de este proyecto de investigación, así como del trabajo realizado a lo largo del mismo y los resultados obtenidos, se llega a las siguientes conclusiones:
· Se logró definir un proceso que sigue el enfoque MDA, el cual permite modelar y generar esqueletos de arquitecturas ejecutables con soporte de tácticas de calidad sobre una plataforma específica (Spring).
· Se construyó una herramienta que permite la generación de esqueletos de arquitecturas ejecutables con soporte de un conjunto de tácticas de calidad.
· La herramienta construida fue probada como herramienta de apoyo en el desarrollo de un Caso de Uso (CU), reduciendo el tiempo en la fase de construcción, pues esta herramienta genera gran parte del código concerniente a requerimientos no funcionales, específicamente tácticas de atributos de calidad.
· Se logró adquirir una mayor visión y conocimientos de la importancia de la arquitectura de software, de cada uno de los artefactos que son necesarios para representarla y poder así comenzar la construcción de un sistema.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Modelado de técticas de atributos de calidad para la generación de esqueletos de arquitecturas ejecutables.
Que presenta:
Lic. Pedro Antonio Marcial Palafox.
Para obtener el grado de
Maestro en Ciencias
(Ciencias y Tecnologías de la Información).
Asesor
Dr. Humberto Cervantes Maceda.
Sinodales
Presidente: Dra. Perla I. Velasco Elizondo.
Secretario: Dr. Humberto Cervantes Maceda.
Vocal: M. C. Alfonso Martínez Martínez.
11 de Noviembre de 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Evaluación de Desempeño de un Sistema de
Almacenamiento Distribuido</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los servicios de almacenamiento ofrecidos de forma rápida y contable se han convertido en el factor clave de las operaciones de las empresas. Esta situacion motiva diversas ofertas para crear sistemas de almacenamiento recientes. En este contexto, las redes de almacenamiento son una alternativa interesante. No sólo ofrecen una alta capacidad, comparadas con el almacenamiento local, sino también su naturaleza distribuida puede usarse para crear sistemas tolerantes a fallas. A partir de este trabajo se puede observar que es factible emplear
tecnóloga las de red usadas comúnmente, tales como Fast Ethernet, para implementar una red de almacenamiento y aún proveer un nivel de rendimiento satisfactorio. Aunque el uso de tecnología de red dedicadas de alta velocidad (por ejemplo, Canal de Fibra) es deseable, podemos usar la naturaleza distribuida de una red de almacenamiento y un balance de carga apropiado para lograr un rendimiento aceptable. Al hacer esto podemos tener un sistema capaz de hacer frente, de forma significativa, a cargas de trabajo altas.
Los resultados obtenidos de la evaluacion de desempeño del sistema en términos de su tiempo de respuesta, muestran que para las condiciones simuladas y bajas intensidades de llegadas de las solicitudes, menores a 0.29 peticiones/s de almacenamiento y 0.05 peticiones/s para recuperación (cada petición es para almacenar o recuperar archivos de 1 MB), se puede implementar la Política de Atención Centralizada. Sin embargo, cuando las intensidades se incrementan es preferible emplear la Política de Atención Descentralizada, para obtener as un buen desempeño del sistema. Además se puede observar que el sistema es independiente de la tecnologa de red sobre la cual se implemente, con intensidades de llegadas de las solicitudes menores a 0.2 peticiones/s para almacenamiento y recuperación. Esto se debe a que el tiempo de procesamiento de los archivos tiene el mayor impacto en el tiempo de respuesta.
Por otro lado, del estudio de contabilidad muestra que el tiempo de vida media del sistema depende principalmente del número de máquinas que se agreguen a la red de almacenamiento, ya que cuando aumenta este número se forma un mayor número de comités y disminuye la vida media del sistema. Otro resultado interesante es que a partir de 2 máquinas en el sistema, se obtienen resultados marginales, es decir, no se obtiene una gran diferencia agregando más reservas por lo tanto resultara un gasto innecesario.
El resultado más importante observando los resultados del estudio de evaluación es que, mientras los resultados para la evaluación del tiempo de respuesta indican que al agregar más computadoras se obtiene un mejor desempeño, por otro lado el estudio de su contabilidad indica lo contrario, que al aumentar el número de computadoras, disminuye el tiempo de vida media del sistema. En la Política de Atención Centralizada no afecta el número de computadoras en la red de almacenamiento y esto es porque el despachador realiza las tareas de procesamiento y gestión de las solicitudes, mientras que la red de almacenamiento cumple con el _único propósito de almacenamiento de archivos. Por otra parte en la Política de Atención
Descentralizada, el despachador sólo gestiona las solicitudes y las computadoras de la red de almacenamiento son las que se encargan de procesar las solicitudes, por esta razón cuanto sea más grande el número de computadoras en este bloque del sistema, mayor capacidad se tiene para atender solicitudes. Sin embargo, con el Modelo de Contabilidad Centralizado, al aumentar el número de computadoras activas se reduce el tiempo de vida del sistema. Así, con el Modelo de Contabilidad Descentralizado se logra evitar este efecto, haciendo que  el tiempo de vida aumente considerablemente hasta llegar un punto _optimo, que en el caso de este modelo es para seis nodos activos , para el cual se tiene el mayor tiempo de vida media.

Sin embargo al aumentar a siete computadoras el tiempo de vida media del sistema disminuye debido al incremento de la tasa de fallas en el sistema, aunque se tiene un resultado mejor que en el caso del modelo descentralizado y que en el mismo modelo descentralizado con 5 nodos activos. Además, como se puede observar en la tabla 5.1, al aumentar el número de comités se tiene un mayor balance de carga en el sistema, ya que cada computadora disminuye el tiempo en el cual trabaja realizando tareas de almacenamiento, es decir, cada nodo no participa en todos los comités.

En conclusión de este trabajo se puede decir que se lograron los objetivos de la tesis,
es decir, se logró evaluar un sistema de almacenamiento distribuido en términos de dos
prámetros: tiempo de respuesta y confiabilidad. Además se concluye que es factible tener
un buen desempe~no del sistema, empleando la dispersión de información sobre redes con
tecnología de uso común, como lo es Fast Ethernet. Además, un resultado que no es tan
evidente, es el que se obtuvo al realizar la evaluación de la confiabilidad, pues en principio se
creería que aumentando el número de nodos en la red de almacenamiento, aparte de obtener
un buen desempe~no en cuanto tiempo de respuesta, se obtendría un mayor tiempo de vida del
sistema al tener un mayor número de computadoras para procesar y recuperar los contenidos
de los nodos que fallen; sin embargo al aumentar este número de computadoras se incrementa
la tasa de fallas.
Durante el desarrollo de este proyecto se identificaron algunas líneas de investigación
que pueden servir para futuros proyectos. Una de ellas es realizar la verificación formal de
cada uno de los protocolos planteados para los modelos de tiempo de respuesta, así como
de los modelos de confiabilidad. Otra línea a investigar es el estudio del impacto, de otros esquemas de almacenamiento, sobre el tiempo de vida sistema. Por último, se puede realizar
una investigación sobre el impacto de variar los parámetros del IDA, es decir, variar la
cantidad de dispersos que se generan y con cuántos se recupera el archivo original.
Con base en los resultados obtenidos en este trabajo de tesis se publicó el artículo Service
Policies for a Storage Services Dispatcher in a Distributed Fault-Tolerant Storage Network
and their Performance Evaluation" [20]. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Evaluación de Desempeño de un Sistema de
Almacenamiento Distribuido
Idónea Comunicación de Resultados para obtener el grado de
Maestro en Ciencias
(Ciencias y Tecnologías de la Información)
P R E S E N T A
Ing. Moisés Quezada Naquid
Asesores:
Dr. Miguel López Guerrero
Dr. Ricardo Marcelín Jiménez
16 de Octubre de 2007</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Método Asistente para la toma de decisiones de diseño de arquitecturas de softwre (MATDDS)</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Una fase importante en el ciclo de vida del desarrollo de la arquitectura es la de diseño y aunque para realizar
esta tarea existen métodos que permiten realizar las actividades de forma sistemática, éstos no guían en el
cómo hacer la toma de decisión para elegir los patrones y tácticas de diseño que satisfagan a los drivers
arquitectónicos.
Las decisiones de diseño que guían la arquitectura son tomadas por el Arquitecto y se hacen con base en su
experiencia adquirida a través de los años, en torno a los patrones y las tácticas de diseño. Si no se tiene la
experiencia necesaria, en particular con respecto a los patrones, esta toma de decisión es muy complicada
debido a que existe una gran cantidad de ellos que influyen de alguna manera sobre los drivers arquitectónicos.
La influencia de un patrón sobre un driver puede ser positiva, negativa o nula, esto es, que satisfaga, perjudique
o no afecten a algún driver arquitectónico en particular. Esta influencia es la que determina si se aplica un
patrón o no. A este problema se le suma que existen pocos arquitectos para hacer desarrollo de software y aún
menos con la experiencia necesaria para hacer buenas tomas de decisiones con respecto a los patrones.
La consecuencia de hacer una mala toma de decisiones de diseño es no obtener una arquitectura que satisfaga
los drivers arquitectónicos y por lo tanto no obtener un sistema que cumpla con las necesidades requeridas por
clientes y desarrolladores.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Crear un método que ayude a arquitectos a hacer toma de decisiones de diseño en la fase de Diseño en
el Ciclo de Desarrollo de la Arquitectura de Software con respecto a los patrones (MATDDS).</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El proyecto que dio como resultado la escritura de esta tesis se realizó en varias etapas, bajo ciertas condiciones
dando como resultado algunos alcances, todos estos explicados a continuación:
1.5.1. Etapas
Las etapas para el desarrollo de este proyecto fueron las siguientes:
* Investigación del Marco Teórico y del Estado del Arte. En esta etapa se llevó a cabo la revisión e
investigación de los elementos básicos para entender la realización del diseño de la arquitectura, así
como los métodos utilizados para hacer el diseño y los trabajos relacionados con el tema.
* Identificación del Problema. Se hizo un análisis tanto del marco teórico como del estado del arte, para
determinar nichos de oportunidad.
* Desarrollo del Algoritmo. Se consideraron varias opciones de algoritmos que ayudaran a resolver el
problema identificado, de tal forma que el resultado fuera aceptable.
* Desarrollo del Método y Asistente MATDDS. Analizando los elementos obtenidos, se describió un
método para auxiliar en la toma de decisiones de diseño, posteriormente creando un Asistente con base
en el método obtenido.
* Evaluación y Análisis de Resultados. Con el Método MATDDS desarrollado se realizaron pruebas sobre
dos ejemplos, para establecer los alcances del método.
1.5.2. Condiciones y Alcances del Proyecto
Este proyecto se llevó a cabo bajo ciertas condiciones para tener ciertos alcances, los cuales se mencionan a
continuación:
Condiciones
* Este proyecto sólo se enfocó en los atributos de calidad del conjunto de drivers arquitectónicos. Se toma
en cuenta sólo éstos ya que son los considerados para cuantificar la calidad del sistema, aspecto que es
muy importante tanto para el grupo de desarrollo (arquitecto y desarrolladores) como para el cliente.
* Sólo se tomaron seis categorías de atributos de calidad, los sugeridos por el SEI [8] (Software
Engineering Institute), Modificabilidad, Disponibilidad, Desempeño, Usabilidad, Seguridad y Facilidad de
Pruebas.
* De las decisiones de diseño, el enfoque fue sólo sobre los patrones, en particular aquellos descritos en
el catálogo escrito por Bushmann [17]. La razón de ésto fue porque los patrones están relacionados con
los atributos de calidad, es decir, por medio de los patrones se pueden satisfacer éstos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Idealmente la arquitectura de software es diseñada por personas con experiencia al respecto, que cubren cierto nivel se conocimiento para poder hacer toma de decisiones de diseño, tales como saber sobre qué plataforma se implementará el sistema, tecnologías que ayudan a satisfacer necesidades, así como patrones arquitectónicos, de diseño y tácticas. Ciertamente la tarea de hacer diseño de arquitectura no es trivial, al contrario es complicada y aún más complicado es el contar con personas con experiencia, en este caso arquitectos que reúnan estas características, sin contar que la demanda de arquitectos con esta experiencia es grande. Por tal motivo existen métodos que guían el desarrollo del diseño de la arquitectura, pero éstos igualmente refieren la necesidad de la experiencia de los arquitectos en la toma de decisiones de diseño. Para hacer la toma de decisiones de diseño existen algunas herramientas igualmente auxiliares de los arquitectos, como son las mencionadas en esta tesis (sección 2.5 de Estado del Arte). Algunas de ellas sugieren hacer cambios en los procesos que se siguen para hacer el diseño y así poder hacer uso de dicha herramienta, lo cual incrementa el tiempo de inversión en la construcción del diseño de la arquitectura. Por estas razones se detecto la necesidad de encontrar una solución
más factible a la problemática de toma de decisiones de diseño en el diseño de la arquitectura de software, creando un método (MATDDS) que ayude a la toma de decisiones de diseño donde se vean involucrados los patrones.
Con base en los resultados obtenidos con el presente proyecto de tesis, se concluye que se cumplió con el objetivo general de esta tesis, donde se propone la creación de un método para el diseño de arquitecturas de software que ayude a hacer toma de decisiones de diseño. Ésto se logró a través de los objetivos específicos:
1. Identificar la influencia que tiene cada patrón sobre varias categorías de atributo de calidad - Se identificó la influencia que tienen los patrones contenidos (alrededor de 100) en el libro o catálogo POSA
4 sobre los atributos de calidad.
2. Encontrar una estrategia para poder determinar el conjunto de patrones que satisfagan los atributos de calidad de un Sistema de Software dado - La estrategia encontrada para poder determinar un conjunto de patrones que satisfagan los atributos de calidad de un sistema de software dado fue, a través de la utilización de un grafo de patrones para lograr encontrar un conjunto de ellos que ayuda a satisfacer las necesidades de los involucrados, expresadas por medio de atributos de calidad. Para ésto se utilizó como
motor de búsqueda el algoritmo en profundidad, adaptado de tal forma que el valor heurístico que utiliza este algoritmo es determinado por medio de la influencia que tiene cada patrón con respecto a los atributos de calidad y también de una ponderación asignada a las necesidades (atributos de calidad).
Ésto se plasma en el capítulo III de esta tesis, específicamente la sección 3.2 de “Propuesta".
3. Desarrollar dicha estrategia para obtener un método que pueda ser aplicado - Teniendo la estrategia se logro desarrollar un método para poder ser utilizado como auxiliar en la toma de decisiones de diseño relacionadas con la arquitectura de software.
4. Crear una herramienta (Asistente) que facilite la aplicación del método obtenido y aplicarla a un ejemplo, de tal forma que se pueda hacer un análisis de resultados - Con base en el método obtenido se creó un Asistente que fue aplicado a dos ejemplos de los cuales se analizaron sus resultados, identificando con
este análisis pros y contras del mismo.
5. Evaluar los resultados obtenidos - Los resultados obtenidos se consideraron satisfactorios, tomando en
cuenta que la aplicación de éste fue hecha por un arquitecto sin experiencia en el diseño de
arquitecturas de software.
Con ésto se concluye que los objetivos, tanto el objetivo general como los específicos fueron cumplidos, a través
de los cuales se obtuvieron algunos logros como son:
* Utilizar un algoritmo sencillo de búsqueda sobre un grafo, ésto permitió que el entendimiento de la
forma de aplicar el método fuera de igual manera sencillo.
* La aplicación de MATDDS sobre el proceso de diseño ADD resulta casi de manera natural, sin necesidad
de hacer cambios drásticos a la forma en que se aplica el proceso.
* Se observó que MATDDS se puede aplicar a otros procesos de diseño, como los que se presentan en
esta tesis, ya que la intervención de éste es únicamente en la selección de patrones que satisfacen a los
atributos de calidad. Por lo tanto, no afecta al proceso que se utilice para generar el diseño.
* Se creo un programa (Asistente) que facilita la aplicación de MATDDS, conteniendo la funcionalidad de
éste, aumentado la rapidez con la cual se obtienen resultados.
* Con el Asistente se logró incluir una ayuda visual, es decir, se facilita la consulta de los patrones en la
misma aplicación, al tener una lista de opciones.
En contraparte se observó:
* MATDDS apoya en la selección de los patrones, pero aún así es necesario hacer toma de decisiones de
diseño, ya que sólo se hace la propuesta de patrones. El arquitecto es el que decide cuál patrón utilizar,
la forma de aplicarlos o instanciarlos y él es el que debe decidir qué tanto se han satisfecho los atributos
de calidad.
* La experimentación con MATDDS se vio restringida por el hecho de no haberse probado en un ambiente
laboral, siendo los resultados obtenidos totalmente académicos.
Finalmente se concluye que con la ayuda de MATDDS, ya sea como método o programa, es posible generar diseños de arquitectura de software, aún sin la experiencia en la aplicación de patrones y sin afectar el proceso que se esté utilizando o del cual se auxilie para hacer el desarrollo del diseño de la arquitectura de software. El nivel de ayuda que ofrece MATDDS es considerable, ya que va mostrando un camino para poder hacer la selección de patrones de diseño.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>"Metodo-Asistente para la toma de decisiones de diseño de arquitecturas de software (MATDDS)"
Tesis que presenta:
Lic. Sandra Mendez Luna
Para obtener el grado de 
Maestra en Ciencias
Ciencias y Tecnologias de la Informacion

Asesor: Dr. Humberto Cervantes Maceda
Jurado Calificador:
Presidente: Ing. Luis Fernando Castro Careaga
Secretario:Dr. Humberto Cervantes Maceda
Vocal: M. en C. Edith Valencia Martinez

19 de Marzo del 2012</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ESTRATEGIAS DE MANTENIMIENTO
DE LA DISPONIBILIDAD EN SISTEMAS P2P</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Identificar estrategias que al ser adoptadas por los pares permitan mantener una disponibilidad
de los recursos de acuerdo a un umbral mínimo, considerando la transitoriedad
de los pares y la demanda de los recursos en el sistema P2P.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología propuesta para el desarrollo de este proyecto consiste en las siguientes
cuatro fases: diseño de modelos, implementación, simulación y análisis y discusión de resultados.
A continuación se describe brevemente cada una de ellas.
Fase I. Dise ño de modelos
Se plantea el diseño de modelos que permitan incorporar las características relevantes
al problema de la disponibilidad en el sistema P2P considerando la transitoriedad de los
pares y la demanda de los recursos.
Fase II. Implementación
Esta fase contempla la implementación de una aplicación (en un lenguaje de programaci
ón orientado a objetos) que integre los modelos expuestos en la fase I para su simulaci
ón.
Fase III. Simulación
Esta fase consiste en la configuración y ejecución de las simulaciones para los modelos
propuestos en la fase I utilizando la aplicación implementada en la fase II.
Fase IV. Análisis y discusión de resultados
En esta fase se analizarán los resultados de las simulaciones ejecutadas en la fase III
considerando:
1. El análisis del comportamiento de la población.
2. El tiempo requerido por los pares para restaurar la disponibilidad de un recurso.
3. Patrones de comportamiento presentes en la población de estrategias evolucionadas.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Este trabajo estudió el comportamiento emergente de los pares en un sistema P2P al adoptar acciones para restaurar la disponibilidad de recursos afectados por la transitoriedad de los pares y la demanda de éstos. Bajo este contexto, el objetivo principal se enfocó en identificar estrategias que al ser adoptadas por los pares permitan mantener una disponibilidad de los recursos de acuerdo a un umbral mínimo, considerando la transitoriedad de los pares y la demanda de los recursos en el sistema P2P.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD AUTÓNOMA METROPOLITANA-IZTAPALAPA
DIVISIÓN DE CIENCIAS BÁSICAS E INGENIERÍA
ESTRATEGIAS DE MANTENIMIENTO
DE LA DISPONIBILIDAD EN SISTEMAS P2P
Presenta
Maria Elena Melgar Estrada
Para obtener el grado de
Maestro en Ciencias y Tecnologías de la Información
Asesora: DRA. ELIZABETH PÉREZ CORTÉS
Jurado Calificador:
Presidente: DRA. NARELI CRUZ CORTÉS CIC-IPN
Secretario: DRA. ELIZABETH PÉREZ CORTÉS UAM-I
Vocal: DR. RENÉ MACKINNEY ROMERO UAM-I
Vocal: DR. RICARDO MARCELÍN JIMENEZ UAM-I
México, D.F. Noviembre 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Estimación de marcas
en redes RFID</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Ya que las colisiones entre marcas son uno de los problemas más comunes y uno de los
más estudiados dentro del ámbito de las redes RFID, enfocamos nuestro trabajo a resolver el
problema de colisiones entre marcas bajo el esquema de un lector y varias marcas, asumiendo
que el número exacto de marcas es desconocido, con el fin de realizar una identificación fiable,
rápida y simultánea de varios objetos.
Debido a que existen diferentes entornos (definidos por el tipo de marca RFID utilizada)
dentro de la tecnología RFID, es necesario que exista un mecanismo estándar que permita
solucionar el problema de colisiones. Actualmente, las redes RFID ya siguen estándares que permiten resolver el problema de colisiones entre marcas, según el entorno definido por el
tipo de marca utilizado: EPC Gen 2 Class 1 [21] para ambientes RFID pasivos e ISO-18000-7
[8] para ambientes RFID activos. En secciones posteriores abordaremos el funcionamiento de
estos estándares.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Proponer y/o mejorar algún protocolo anticolisión en el contexto de redes
RFID.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. Revisión de la literatura en el tema.
2. Identificación del trabajo realizado en cada uno de los entornos RFID.
3. Identificación de los parámetros de evaluación de desempeño más importantes dentro
del contexto de redes RFID.
4. Selección del protocolo anticolisión a proponer.
5. Selección de una herramienta para llevar a cabo la simulación.
6. Implementación del protocolo propuesto, y de los protocolos a comparar.
7. Evaluación de los protocolos implementados.
8. Comunicación de resultados.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo se ha presentado una revisión de los protocolos anticolisión existentes
para redes RFID. De igual manera, se presentó el desarrollo de una propuesta basada en
el protocolo CSMA p-persistente con distribución Sift. En base al modelo de simulación
propuesto y a los resultados obtenidos, se observa que se tiene un mejor comportamiento al
del protocolo CSMA no-persistente presentado en [22], con una mejora en el desempeño de
hasta el 4.8% para el caso de comparación bajo los parámetros del estándar ISO-18000-7, y
del 1.3% para el caso de comparación bajo los parámetros del EPC “Gen2". De igual forma, se tiene una mejora de hasta un 11.8% y 50.57% para el caso del estándar ISO-18000-7 y EPC Gen2, respectivamente.
Observamos que el tiempo involucrado en un ciclo de identificación afecta directamente
el desempeño de cualquier protocolo, ya que se intercambia una mayor cantidad de mensajes
entre el lector y las marcas, por lo que al disminuir el número de ciclos de identificación se mejora el desempeño de cualquier protocolo. Por otro lado, se observó que entre mayor sea el tiempo utilizado en un ciclo de identificación (tal como la duración de los mensajes intercambiados entre los dispositivos, duración de las ranuras dentro de la ventana de contienda o trama), la mejora con respecto a otros protocolos es más evidente.
Resta trabajo por hacer, tal como medir la tasa de pérdidas y el caudal de datos de
nuestra propuesta.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>UNIVERSIDAD AUTÓNOMA METROPOLITANA
UNIDAD IZTAPALAPA División de Ciencias Básicas e Ingeniería.
Estimación de marcas
en redes RFID
Tesis que presenta:
Leonardo Daniel Sánchez Martínez
Para obtener el grado de:
Maestro en Ciencias en
Tecnologías de la Información
En el área : Redes y Telecomunicaciones
del Departamento de Ingeniería Eléctrica
Asesor:
Dr. Víctor Manuel Ramos Ramos.
Defendida públicamente en la UAM-Iztapalapa el 2 de agosto de 2011 a las 09:00 hrs
frente al jurado integrado por :
Presidente : Dr. Ricardo Marcelín Jiménez UAM-I, Redes y Telecomunicaciones Texto
Secretario : Dr. Víctor Manuel Ramos Ramos UAM-I, Redes y Telecomunicaciones T
T Vocal : Dr. Marcelo Carlos Mejía Olvera ITAM, Depto. Acad. de Computación</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DLML PARA UN AMBIENTE GRID</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo de tesis, se diseñó e implementó una nueva arquitectura para la librería
DLML sobre un ambiente Grid. El objetivo de esta propuesta es utilizar recursos de más de
un cluster, mejorando el funcionamiento de las versiones originales de DLML que solamente
funcionan en un único cluster.
La arquitectura propuesta sigue un modelo jerárquico de distribución de carga, en el cual
se favorece al balance de carga intra-cluster sobre el balance de carga inter-cluster. Es decir,
la solicitud de transferencia de carga de un cluster X a otro Y no podrá efectuarse, mientras
exista carga en el entorno local de X. En esta arquitectura, para el balance de carga inter-
cluster se usa información de carga de todos los clusters (balance global), mientras que para el
balance intra-cluster, se usa DLML con información global o DLML con información parcial.
Con la nueva arquitectura, se logró disminuir el tiempo de procesamiento al usar más de un
cluster para procesar y balancear carga en aplicaciones que demandan más recursos, que los
que están presentes en un sólo cluster.
Para crear el ambiente Grid se usó una VPN (Virtual Private Network), la cuál permite
interconectar un conjunto de clusters a través de Internet, y usar los recursos de los dos
clusters, de la misma forma que en un cluster normal. Usando la arquitectura jerárquica, se
reduce el número de comunicaciones (costo en las comunicaciones) entre diferentes clusters
(ya que las comunicaciones entre clusters se hace a través de Internet), y en consecuencia
también se reduce el tiempo de procesamiento global. Esto es importante, ya que al tener una
VPN sobre varios clusters, las versiones originales de DLML (DLML con información global y DLML con información parcial) pueden ejecutarse sobre los recursos de todos los clusters
(recursos VPN). Sin embargo, como en estas versiones no se hace una distinción entre las
comunicaciones locales (comunicaciones entre cores del mismo cluster) y las comunicaciones
que se hacen con recursos de otro cluster (a través de Internet), se provoca un aumento en
el costo de las comunicaciones y en el tiempo de procesamiento global.
Para las pruebas realizadas se usaron las dos versiones originales de DLML (a las cuales
denominamos Glo-VPN y Toro-VPN) y dos nuevas versiones de DLML, una denominada
Glo-Grid, en donde se hace balance de carga sobre un ambiente Grid usando DLML con
información global para el balance intra-cluster y otra denominada Toro-Grid, en donde
también se hace balance de carga sobre un ambiente Grid, pero usando DLML con informaci
ón parcial para el balance intra-cluster. Usando estas cuatro versiones de DLML, se
ejecutó la aplicación dinámica N-Reinas y la aplicación estática Multiplicación de Matrices
sobre una VPN formada por 2 clusters, uno en la UAM-Iztapalapa y otro en el CINVESTAV.
Los resultados para el tiempo de procesamiento y la distribución de carga son los siguientes: 
Tiempo de procesamiento
Para la aplicación dinámica de las N-Reinas, las dos nuevas versiones de DLML Glo-
Grid y Toro-Grid mostraron mejorías en el tiempo de procesamiento, con respecto a las
versiones orinales Glo-VPN y Toro-VPN al ejecutarlas sobre los recursos de la VPN.
Esta mejoría se debió a la reducción de las comunicaciones a través de Internet en las
2 nuevas versiones.
En Multiplicación de Matrices, se obtuvieron resultados similares en el tiempo de procesamiento,
Toro-Grid fue la versión más rápida de las 4 versiones, a pesar de que en esta
aplicación la carga transferida de un cluster a otro fue mucho más grande que en NReinas
(lo que implica un retraso debido a que las transferencias se hacen a través
de Internet). Por este motivo, en Multiplicación de Matrices se observó, en este caso
particular, que es aconsejable usar más de un cluster para procesar carga, siempre que
la proporción del poder de cómputo externo sea mayor al 28% del cómputo del cluster local. En caso contrario, podría ser que un sólo cluster termine de procesar la carga
más rápido que con la ayuda de otros clusters. A diferencia de N-Reinas, en donde no
se observó este comportamiento.
En estas pruebas también se observó que en Multiplicación de Matrices la diferencia de
velocidades entre Glo-VPN y Glo-Grid no fue tan evidente como en el caso de N-Reinas,
esto se debe a que la principal ventaja de la nueva arquitectura es limitar el número de
mensajes a través de Internet. Por consiguiente, si el número de peticiones a través de
Internet en las versiones Glo-VPN o Toro-VPN no es significativo (como en el caso de
Multiplicación de Matrices), la nueva arquitectura no podrá mostrar grandes ventajas.
Distribución de carga
En la distribución de carga, la aplicación dinámica de las N-Reinas mostró que usando
la versión Toro-Grid, la cantidad de carga asignada a cada core (en cada uno de los
clusters) es más equitativa que en la versión Toro-VPN, y en las otras dos versiones.
También se observó que la cantidad de carga que procesa un cluster está en relación al
poder de cómputo que este posee, con respecto al poder de cómputo total (el poder de
cómputo de la VPN).
La versión Glo-Grid también mostró buenos resultados con respecto a la versión Glo-
VPN, en la cual, se obtuvo la peor distribución de carga de las cuatro versiones. Esto
se debe a que en esta versión se usa DLML con subasta global, en donde el costo de
comunicación es muy alto, ya que cada core se debe comunicar con todos los cores de la
VPN al hacer un balance de carga, y esto incrementa el número de mensajes a través
de Internet.
En Multiplicación de Matrices se obtuvieron resultados diferentes. La versión que
mejores resultados mostró (aunque no hubo mucha diferencia como en N-Reinas) fue
Glo-Grid, seguida de Glo-VPN, Toro-VPN y Toro-Grid. Esto se debe, a que al ser una
aplicación estática no se necesita en general de muchas búsquedas y transferencias, ya
que se distribuye la misma cantidad de carga a todos los cores, los cuales tienen capacidades de procesamiento similares y por consiguiente terminan de procesar su carga
en tiempos similares. Por esta razón, la subasta parcial (del toroide) no tuvo la oportunidad
de mostrar las ventajas que tiene al presentarse un gran número búsquedas y
transferencias frente a la subasta global, como en el caso de N-Reinas.
Con el desarrollo e implementación de la nueva arquitectura, DLML puede balancear
carga en uno o varios clusters (ambiente Grid) interconectados por una VPN. Esto brinda la
posibilidad de ofrecer mayor poder de cómputo a problemas que demandan más recursos de
los que están presentes en un único cluster.
Como se pudo observar a lo largo de este trabajo, actualmente DLML proporciona un
balance de carga a nivel Grid, en donde un cluster descargado ofrece su poder de cómputo
a otros clusters (balance iniciado por el cluster receptor). Sin embargo, sería conveniente
desarrollar una versión en donde un cluster que se encuentra sobrecargado, pida ayuda a
otros clusters que tengan menos carga (balance iniciado por el cluster emisor). Ya que con
estos dos tipos de balance, probablemente se obtendrían mejores resultados en cuanto a
tiempo de procesamiento y balance de carga.
Otro aspecto importante a considerar en el balance de carga inter-cluster, es la determinaci
ón de la capacidad de un cluster para procesar una cierta cantidad de carga, al momento
de determinar el porcentaje de carga que se enviará del cluster emisor al cluster receptor.
Para evaluar dicha capacidad, en este trabajo únicamente se toma en cuenta el poder de
procesamiento del cluster en base al poder de cómputo de todos sus nodos. A partir del
poder de procesamiento de los clusters se determina el porcentaje de carga que debe transferir
el cluster emisor. Sin embargo, el poder de procesamiento no es lo único que determina
la capacidad de un cluster para procesar una carga, se deben considerar aspectos como la
velocidad de la red interna del cluster, la memoria cache de los procesadores y las capacidades
en RAM y disco duro para aplicaciones que así lo requieran. Aunado a esto, en esta tesis sólo
se toma en cuanta la cantidad de carga que reporta un cluster para designarlo como emisor.
Sin embargo, en un ambiente Grid también se debe considerar el costo en la comunicación
al hacer una transferencia, por lo que es necesario tomar en cuenta la latencia del cluster receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, seria recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más efificiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no
perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderían una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder finalizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podría estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el fin de recuperar los resultados parciales que había obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, éstos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para
interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de
recursos, como en el caso del sistema Grid Globus. Por lo cual, sería conveniente probar y
hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid
como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.
Receptor hacia los demás clusters (al momento de designar a uno de ellos como emisor). Por lo anterior, sería recomendable crear un modelo analítico que considere los aspectos mencionados para evaluar la transferencia de carga de un cluster a otro, con el objetivo de hacer más eficiente el balance de carga y en consecuencia mejorar el tiempo de procesamiento global.
Por otra parte, ahora que se hace procesamiento en más de un cluster, es importante no perder de vista la posibilidad de falla en alguno de los cluster o su conexión a Internet. Con una falla de este tipo se perderán una gran cantidad de datos, por lo cual, se necesita un mecanismo que permita recuperar los datos perdidos y los resultados parciales que se vayan obteniendo en cada uno de los clusters, para poder analizar el procesamiento global sin pérdida de datos o resultados parciales. Este mecanismo podría ser un dispositivo intermedio (que también podrá estar distribuido en varios cluster con replicación), en donde se almacenen los resultados parciales y los datos que se van enviando a los clusters antes del procesamiento, con el  de recuperar los resultados parciales que habrá obtenido el cluster (hasta antes de la falla) y de recuperar los datos que debió haber procesado un cluster que ha fallado. Al contar con un respaldo de estos datos, estos se pueden asignar a otro cluster en el momento en que haga una petición de carga al Administrador Grid.
También es importante recordar, que actualmente el ambiente Grid sobre el que se ejecuta
DLML está formado por una VPN, la cual proporciona una forma relativamente sencilla para interconectar clusters. Sin embargo, esta no cuenta con sistema para el descubrimiento de recursos, como en el caso del sistema Grid Globus. Por lo cual, será conveniente probar y hacer modificaciones a las nuevas versiones de DLML, para ejecutarlas en un sistema Grid como Globus.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>DLML PARA UN AMBIENTE GRID
Para obtener el grado de
MAESTRO EN CIENCIAS
(Ciencias y Tecnologías de la Información)
PRESENTA:
Apolo H. Hernández Santos
Asesores
Dra. Graciela Román Alonso
Dr. Miguel Alfonso Castro García
Sinodales
Presidente: Dr. Ricardo Marcelín Jiménez [UAM-I]
Secretario: Dr. Santiago Domínguez Domínguez [CINVESTAV]
Vocal: Dr. Miguel Alfonso Castro García [UAM-I]
México D.F.
Febrero 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>“Desarrollo de una Suite BPM para el modelado,
ejecución y monitoreo de los procesos de un Modelo
de Mejora de Procesos de Desarrollo de Software"</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los procesos para desarrollar productos de software “complejos",
frecuentemente lo son también; debido a esta complejidad, su modelado,
documentación y monitoreo se dificulta. En particular, estas dificultades se les
presentan a las organizaciones que trabajan de acuerdo a los Modelos de
Mejora de Procesos. También, como los procesos son continuamente
estudiados y rectificados, mantener la documentación actualizada representa
un reto adicional. En general, las dificultades a las que se enfrentan las
organizaciones con respecto a los procesos son: modelar, documentar, ejecutar y monitorear los procesos; así mismo, mantener actualizada la documentación
donde se encuentren descritos.
Existen organizaciones en México con menos de cincuenta participantes,
conocidas como PyMES, que tienen que enfrentar las dificultades antes
mencionadas con escasos recursos económicos y de personal. Dar solución a
estas dificultades con bajo costo, permitiría a las PyMES dedicadas al
desarrollo de software, implementar, sin estas dificultades específicas, los
Modelos de Mejora de Procesos. Al lograr implementar estos modelos,
trabajarían con procesos que ayudarían a producir software de calidad.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Estudiar la factibilidad de construir una Suite BPM de bajo costo que permita
soportar el ciclo de vida de los procesos de un Modelo de Mejora de Procesos
de acuerdo a las etapas del enfoque BPM de Modelado, Ejecución y Monitoreo.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Una Suite BPM puede ser adaptada para soportar el ciclo de vida de los
procesos de un Modelo de Mejora de Procesos en las etapas de Modelado,
Ejecución y Monitoreo.

En base a esta hipótesis, se plantean el objetivo general y los objetivos
específicos que guiaron la investigación.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. Investigación y redacción del Estado del arte.
2. Definición de los requerimientos de la Suite BPM
2.1 Identificación de Requerimientos Funcionales.
2.2 Identificación de Requerimientos No Funcionales.
3. Diseño y Construcción de la Suite BPM.
2.3 Diseño de la arquitectura.
2.4 Desarrollo de componentes.
4. Evaluación de los objetivos de la investigación
4.1 Selección de procesos a modelar.
4.2 Modelado de procesos seleccionados.
4.3 Revisión del cumplimiento de los Requerimientos Funcionales.
5. Análisis e interpretación de resultados y redacción de conclusiones.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se concluye que es posible construir una Suite BPM de bajo costo que permita soportar el ciclo de vida de los procesos de un Modelo de Mejora de Procesos de acuerdo a las etapas del enfoque BPM de Modelado, Ejecución y Monitoreo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>“Desarrollo de una Suite BPM para el modelado,
ejecución y monitoreo de los procesos de un Modelo
de Mejora de Procesos de Desarrollo de Software
Tesis que presenta:
C.P. y Lic. Silvia Nagheli Márquez Solís
Para obtener el grado de:
MAESTRA EN CIENCIAS
DEL
POSGRADO EN CIENCIAS Y TECNOLOGíAS
DE LA INFORMACIÓN
Asesores:
Dr. Humberto Cervantes Maceda
Dr. Carlos Montes de Oca Vázquez
Jurado calificador:
Presidente: M. en C. Alfonso Martínez Martínez
Secretario: Dr. Humberto Cervantes Maceda
Vocal: M. en C. Mery Helen Pesantes Espinoza
MEXICO, D.F. JULIO 2011</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DISEÑO DE UN NÚCLEO DE
ARQUITECTURA COMÚN PARA
CONSTRUIR PRODUCTOS DE PACS.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La creación de un PACS, tal y como se presenta en la figura 3, requiere del desarrollo de
una serie de entidades de aplicación dedicadas a aspectos particulares tales como
almacenamiento o visualización. Por otro lado, cada una de esas entidades de aplicación
tiene asociado un conformance específico. El desarrollo de estas entidades de aplicación
tiene entonces varias implicaciones:
* Un desarrollador debe concentrarse tanto en la programación de los aspectos
aplicativos, como de la adherencia o compatibilidad al conformance requerido.
* El desarrollador debe conocer aspectos específicos de implementación del
estándar DICOM.
* La modificación de una entidad de aplicación y por ende su conformance puede ser
una tarea compleja si el código aplicativo está mezclado con el código de soporte
al relacionado con el estándar DICOM.
* Para modificar código de una implementación particular del estándar DICOM, es
necesario entender primero las especificaciones del estándar (descritas en 16
tomos o partes) y en segundo término es necesario entender la lógica con la cual
fue implementada.
* Las entidades de aplicación comerciales no permiten agregar nuevos servicios
DICOM, normalmente, cada uno de éstos se vende como una nueva licencia.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de la tesis es realizar el diseño y construcción de un núcleo de arquitectura común para construir productos de PACS.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología que se siguió dentro de este proyecto fue la siguiente:
* Realización de un estudio del estado del arte, relacionado con arquitecturas de
software, líneas de producto y PACS.
* Definición de un proceso para construir los productos de línea de producto para
PACS.
* Definición de los requerimientos relativos al núcleo arquitectónico.
* Diseño e implementación del núcleo.
* Evaluación del núcleo a partir de la construcción de un prototipo.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Aplicando un proceso de desarrollo de software y representando los modelos en UML ha
sido posible obtener diferentes modelos que representan el núcleo para la arquitectura de la línea de producto a través de un conjunto de vistas estáticas y dinámicas.
La metodología propuesta en este proyecto fue adecuada para identificar los elementos
importantes del núcleo y los mecanismos de ensamblado para los productos de un PACS,
sin embargo se requiere realizar otro análisis para determinar cuáles son las herramientas
y metodologías para implementar los componentes de intersección que se plantearon a
través del modelo respectivo.
Debido a la complejidad de los PACS, fue necesario crear un proceso de desarrollo que
facilite a los desarrolladores la implementación de los productos que se deben generar a
partir de la arquitectura. La metodología propuesta, nos ayuda a identificar las
restricciones técnicas que impone la organización, ya que se contempla una etapa de
captura de requerimientos debido a que los PACS no son sistemas genéricos, es decir,
deben apegarse a las necesidades de la organización y en este sentido nuestra propuesta
tiene una gran ventaja sobre los productos comerciales, ya que no existen dependencias
sobre un sistema o una versión especifica de alguna aplicación para el funcionamiento de
los productos.
Otro punto importante a destacar es que la arquitectura esta pensada para
desarrolladores que no tengan un conocimiento detallado del estándar DICOM. Esto es un
logro muy especial ya que con conocimientos de patrones de diseño y compilación
condicional, se diseña una aplicación que cumpla con las especificaciones de un PACS,
sin tener que conocer la funcionalidad interna de la implementación del estándar DICOM.
Con solo analizar la parte 2 (DICOM conformance) del estándar el desarrollador tiene una
mecanismo para realizar la abstracción general del producto deseado y la arquitectura se
encarga de ensamblarlo. El proponer un mecanismo con diferentes niveles de abstracción
generó un conjunto de componentes con alta cohesión independientes que facilitan la
integración de librerías o nuevos componentes.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>DISEÑO DE UN NÚCLEO DE
ARQUITECTURA COMÚN PARA
CONSTRUIR PRODUCTOS DE PACS.
Tesis que presenta:
Marco Antonio Núñez Gaona.
Para obtener el grado de:
Maestro en Ciencias y Tecnologías de la
Información.
Asesores:
Dr. Humberto Cervantes Maceda.
M. en C. Alfonso Martínez Martínez.
Jurado Calificador:
Presidente: Dr. Juan Ramón Jiménez Alaníz.
Secretario: Dra. Perla Inés Velasco Elizondo.
Vocal: Dr. Humberto Cervantes Maceda.
México, D. F. Julio 2010.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Curso en Línea elementos generales en pedagogía (EGP) dirigido a alumnos de la Maestría en Desarrollo Educativo, Línea: Tecnologías de la Información, UPN-Ajusco</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La importancia de la Educación en la sociedad actual es indiscutible, por lo que los estudios con relación a este campo han sido amplios, sobre todo si se trata de educación superior. Este nivel educativo ha cobrado gran relevancia en las últimas décadas, así como sus diferentes modalidades. Es  caso de la Educación a Distancia, se le ha dado significativa importancia, porque se considera como elemento relevante y una opción para la demanda educativa en el nivel superior.
Al respecto, es adecuado subrayar que con el concepto de educación superior se alude al sistema compuesto por los subsistemas: universitario, tecnológico y de normales. A los que se agregan otras instituciones públicas de educación superior y nivel especializado, constituidas bajo el régimen de asociación civil, autorizadas y reguladas por dependencias de la Secretaría de Educación Pública, estas instituciones realizan las funciones de docencia, investigación y extensión de la cultura entre otras1. Bajo esa perspectiva, es pertinente señalar que:

"las tecnologías intelectuales cada vez más, están ampliando y transformando funciones cognitivas como la memoria (a través de las bases de datos, hiperdocumentos, archivos digitales de todo tipo); la imaginación (a través de procesos de simulación); la percepción (a través de procesos de sensores digitales, telepresencia, realidad virtual), el pensamiento (a través de la inteligencia artificial, modelado de fenómenos complejos)"2.
Tal hecho, implica que la educación necesita nuevas formas de aplicar el proceso de enseñanza 
y aprendizaje, estar más ajustada a la realidad, la actividad cognitiva tiene que permanecer en contacto con los avances tecnológicos.
De lo anterior, surge la intensión de diseñar, desarrollar y evaluar el curso en línea EGP, y 
sustentar en torno suyo todo un proceso de investigación que dé origen al presente trabajo de tesis, en el contexto de la TI de la MDE de la UPN.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Durante el desarrollo de este trabajo se presentaron los aspectos generales y particulares que se consideraron para el diseño, desarrollo y evaluación del curso en línea Elementos Generales de Pedagogía, En la puesta en marcha, se observaron problemas técnicos que imposibilitaron el adecuado desarrollo del curso, pero en términos generalizados se pudo valorar la funcionalidad del mismo.
La tecnología a lo largo de las décadas ha atravesado por diferentes momentos, como se observó en este trabajo, pero es en la actualidad que cobra relevancia, sobre todo en lo que se refiere a las nuevas modalidades de impartir educación, como es el caso de los cursos en línea. En el caso del curso, aunque se no se desarrolló como se tenía planeado, se pudo obtener la participación de los profesores participantes, contestando algunas preguntas que aparecen en el anexo 2, lo que permitió responder las interrogantes que se plantearon al inicio de este trabajo.
En principio se cuestionó
1. ¿El mapa curricular de la línea de TI como parte de la MDE contempla la utilización de los medios informáticos para la impartición de cursos en línea? Como se planteó en el inicio, siguiendo el análisis que se realiza de la MDE y de la línea TI, no se menciona alguna forma de impartir educación utilizando los medios, salvo algunas materias en donde se enfocan a la enseñanza de determinados programas como Flash, Dreamweaver o a utilizar HTML. No se plantea en la línea la utilización de otras modalidades para la impartición de cursos.
2. ¿Qué nociones elementales de Pedagogía debe conocer el aspirante a ingresar a la línea de TI de la MDE? La parte del tronco básico como se observó contempla aspectos tanto generales como especializados en el área de la Pedagogía, por lo que es importante que el alumno que ingresa a la MDE de la Línea TI, tenga nociones elementales sobre aspectos que se refieren a la Pedagogía.
3. ¿Cómo se ha trabajado el uso de la tecnología para el desarrollo de la educación a distancia? En el capítulo tres del trabajo se mostró como se ha manifestado la educación a distancia y como es que se trabaja lo que se ha denominado como nuevas tecnologías de la información. En el vínculo que se establece entre las tecnologías y la educación se encuentran las modalidades de impartir ésta, a través de medios informáticos.
4. ¿Qué elementos teóricos deben ser considerados para desarrollar un curso en línea? Para la creación de cursos en línea deben de considerar diferentes elementos y aspectos, desde el tipo de población a quien va dirigido el curso como todos los aspectos de diseño, así como el tipo de plataforma que se va a utilizar, como se mostró a lo largo de este trabajo.
5. ¿El curso en línea EGP tendrá un adecuado diseño y desarrollo que permita a los profesores realizar las actividades y evaluar el mismo? No se puede hablar de un absoluto, pero sí de una funcionalidad en el curso que se diseñó, permitió que los profesores evaluarán los aspectos generales que se contemplan en un curso a distancia.
6. ¿El curso EGP puede ayudar para que los profesores que ingresen a la MDE reconozcan y recuerden los aspectos generales de la Pedagogía? En las evaluaciones que hicieron los profesores, algunos mencionaron que el curso les permitió recordar y definir con mayor claridad algunos términos.
7. Las aportaciones que deriven del curso EGP serán de valor teórico y educativo, en lo que se ha denominado como tecnologías de información aplicadas a la educación. Las aportaciones que derivaron del curso fueron valiosas en el sentido de proporcionar una alternativa a la línea de TI para impartir sus cursos, así como una oportunidad de brindar a los estudiantes un acercamiento a conceptos que se manejan en Pedagogía, considerando la problemática inicial.
Finalmente, el curso en línea EGP se presenta como una alternativa para los profesores que ingresan a la MDE y que su formación se encuentra en otra área que no sea la educativa y para los profesores de la línea TI se muestra este curso, como una opción para impartir alguna temática del curriculum escolar y como propuesta de curso propedéutico para los alumnos que ingresan a la MDE en la línea de TI.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Curso en Línea elementos generales en pedagogía (EGP) dirigido a alumnos de la Maestría en Desarrollo Educativo, Línea: Tecnologías de la Información, UPN-Ajusco

TESIS QUE PRESENTA
MINERVA CRUZ SALDIVAR
Para obtener el grado de Maestra en Desarrollo Educativo
Linea: Tecnologias de la Información

Directora de Tesis:
Dra. Santa Soledad Rodriguez de Ita
Septiembre 2006</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Comparación de mecanismos de incentivos
en sistemas par a par</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los sistemas P2P tienen una característica fundamental: están constituidos por nodos
autónomos que deciden qué, cuándo y cuánto cooperar con recursos en el sistema. Diferentes
estudios realizados en algunos sistemas P2P, como Gnutella, Maze P2P y eDonkey [26],
indican que la mayoría de los nodos solo consumen recursos del sistema y no comparten los
propios y además que sólo un pequeño porcentaje distribuye la mayoría de los recursos solicitados.
Esta situación pone en riesgo los recursos contenidos en un sistema, ya que si sólo
unos cuantos ponen a disposición los recursos que almacenan de manera desinteresada, mientras
la gran mayoría sólo consume, entonces se pone en riesgo la disponibilidad de los mismos.
Debido a la situación antes descrita, se han realizado varios trabajos de investigación en
los cuales se proponen mecanismos que permitan incrementar la cooperación de los nodos en
los sistemas P2P. Algunos de estos trabajos, como los mencionados en la sección 3.5 utilizaron
Teoría de juegos para incentivar a los nodos a cooperar con el sistema. Si bien los resultados
que se obtuvieron eran los deseados, también es cierto que se suponía que los nodos siempre
tenían un comportamiento racional, así como en algunos casos, conocimiento total de las
decisiones tomadas por el resto de los nodos.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Observar cómo se ve afectada la evolución del comportamiento de los nodos de un sistema
P2P, cuando se encuentran bajo esquemas o mecanismos de compensación que buscan motivar
su cooperación (duplicación de documentos), y cómo este comportamiento afecta el beneficio
global del sistema.
Sin embargo, estudios científicos en Economía indican que los involucrados en una situación de toma de decisiones no se comportan completamente racionales al momento de
tomarlas [25], además de que en un sistema P2P, cuyas conexiones entre nodos es variable,
difícilmente se cumple el hecho de que cada nodo tiene conocimiento de lo que hace cada uno
de los otros nodos en el sistema [42].
De lo anterior, surge la siguiente pregunta: >cómo motivar, que los nodos de un sistema
P2P cooperen con recursos, considerando que su comportamiento in
uye tanto en el beneficio
propio como de manera global en el sistema?
Esta interrogante determina el objetivo de este trabajo de investigación, así como las
características de los mecanismos que proponemos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. Estudiar las características de los sistemas P2P.
2. Estudiar la Teoría de juegos, así como de los trabajos de investigación realizados en
sistemas P2P en los cuales se aplicó esta teoría.
3. Diseñar mecanismos de compensación que evalúen la cooperación de los nodos con
respecto a su contribución de recursos (duplicación de documentos) en un sistema P2P.
4. Diseñar un juego que permita modelar la situación estrategia (duplicar o no duplicar)
de los nodos y el sistema P2P.
5. Estudio de algoritmos genéticos.
6. Implementación del juego propuesto, aplicando los mecanismos propuestos y algoritmos
genéticos.
7. Experimentación y análisis de los resultados obtenidos.
El primer punto fue descrito en el capítulo 2, los puntos 2 y 3 en los capítulos 3 y 4. Los
puntos 4-7, se presentarán en los capítulos 5-7.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El trabajo de investigación presentado, tuvo por objetivos estudiar y analizar como es
afectada la evolución del comportamiento de los nodos de un sistema distribuido P2P, cuando se encuentran sometidos a la inuencia de algún mecanismo que busca incentivar su
cooperación (duplicar) con el sistema.
Para llevar a cabo lo antes descrito, se diseñó e implementó un juego, al que llamamos
Juego de duplicación, en donde se modeló la situación estratégica de los nodos (duplicar, no duplicar) y el sistema, tiendo como finalidad que se lleve a cabo la duplicación de documentos.
Además se consideró el hecho que la disponibilidad de los documentos está condicionada
a la disponibilidad que presentan los nodos. El juego consiste en lo siguiente: un documento
que requiere ser duplicado para alcanzar un determinado nivel de disponibilidad, solicita
espacio de almacenamiento al sistema. Los nodos interesados se proponen para duplicar, si
el documento alcanza la disponibilidad que requiere con los nodos candidatos, entonces se
lleva acabo la elección de nodos que garanticen la disponibilidad del documento, para lo cuál se consideraron tres tipos de elección: secuencial, aleatoria y turno rotatorio. Una vez que el documento es almacenado, los nodos reciben una compensación por su cooperación.
También fue necesario diseñar dos mecanismos de compensación, para aplicarlos al juego
de duplicación. El primero llamado, mecanismo de compensación económica, paga a los
nodos que realizan la duplicación. El segundo denominado, mecanismo de compensación por
reconocimiento social, los nodos son evaluados en función de la medida en la que cooperan
con el sistema, aún y cuando no sean seleccionados para duplicar. Una vez que los nodos
han decidido cuánto cooperar al sistema, podrán llevar a cabo la consulta de documentos
en función del capital con que cuentan considerando al primer mecanismo, y la calificación
obtenida en el segundo.
Diseñamos e implementamos un simulador, el cual nos permite observar la evolución del
comportamiento de los nodos que juegan el juego de la duplicación, bajos los dos mecanismos de compensación propuestos. El simulador desarrollado hace uso de algoritmos genéticos para evolucionar el comportamiento de los nodos de la población.
Por último se llevó a cabo un conjunto de simulaciones que nos permitieron observar lo siguiente:
El mecanismo de reconocimiento social, consigue más rápidamente almacenar el 100%
de los documentos, que el mecanismo de compensación económica.
La aptitud de los nodos si es afectada por el tipo de elección, ya que los nodos alcanzan
su máximo más rápido cuando se utiliza elección aleatoria.
Los dos mecanismos motivan a la población de un sistema P2P a cooperar, ya que aún
y cuando la población de nodos cuyas estrategias no son altruistas (comportamiento
programado), deciden en la mayoría de las veces cooperar (comportamiento exhibido).
Es importante mencionar que los resultados antes mencionados, consideran los escenarios
descritos en las secciones 6.3.1 y 6.3.2.
Para trabajo futuro consideramos los siguientes puntos.
Considerar el espacio de almacenamiento en los nodos para observar el impacto en los
resultados obtenidos.
Considerar distintas formas de evaluación de los nodos en el caso del mecanismo de
compensación económica, ya que actualmente no capta completamente la cooperación
de los nodos.
Utilizar otras técnicas de cruza y mutación, así como variar la probabilidad de las
mismas.
Utilizar otras técnicas evolutivas, para observar qué resultados se obtienen.
Considerar la topología del sistema P2P.
Implementar el juego y los mecanismos de compensación en sistemas P2P simulados,
para comparar si los resultados obtenidos se mantienen.
Experimentar con las poblaciones finales obtenidas, para observar si se mantiene estables.
Llevar a cabo paralelización, para disminuir tiempos de ejecución.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Comparación de mecanismos de incentivos
en sistemas par a par
Idónea comunicación de resultados presentada por
María Esther Sosa Rodríguez
Para obtener el grado de
Maestra en Ciencias
(Ciencias y Tecnologías de la Información)
Asesora:
Dra. Elizabeth Pérez Cortés
Jurado Calificador:
Presidente: Dr. Santiago Domínguez Domínguez CINVESTAV - IPN
Secretario: Dra. Elizabeth Pérez Cortés UAM-I
Vocal: Dr. Ricardo Marcelín Jiménez UAM-I
Vocal: Dr. Manuel Aguilar Cornejo UAM-I
27 de enero de 2012</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Algoritmos de Búsqueda y
Actualización de Información
para ruteadores IP</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El algoritmo UAM encuentra el prefijo BMP que es el que tiene el mayor número de bits que coinciden con los más significativos de la dirección IP.
Para clasificar un prefijo de red dentro de la circunferencia se utilizan dos formulas
para obtener el ángulo que le corresponde y el limite superior de su cobertura.
Este mapeo depende del valor y la longitud del prefijo. El mapeo sirve para convertir
la búsqueda de prefijos en una búsqueda exacta.
El algoritmo UAM tiene una implementación sencilla, las estructuras de datos
que utiliza son arreglos ordenados, uno se utiliza para guardar el valor de los ángulos
que existen en la circunferencia, para la información de ruteo y los limites superiores
de la cobertura. Se utilizan también tantos arreglos como el número de bits que
forman la dirección IP, en estos arreglos se guarda el límite superior de la cobertura
junto con la información de ruteo acorde a la longitud del prefijo de red. Así, las dos
estructuras en donde se realizan las búsquedas son arreglos ordenados, en el primer rreglo se realiza una búsqueda binaria y con el índice encontrado de está búsqueda
se realiza en seguida la búsqueda lineal sobre los arreglos de longitud de prefijos.
Los resultados de la simulación no están alejados de los resultados de los algoritmos
clásicos que implementamos. Además es posible optimizar el funcionamiento
del algoritmo UAM para tener mejores resultados en tiempo, ya que la búsqueda
lineal que se hace en los arreglos de longitud de prefijos, se puede mejorar por que
no en todas las entradas en los arreglos de longitud de prefijos se debe buscar.
Generalmente el número de entradas en el arreglo de ángulos es menor que el
número de prefijos en la tabla de ruteo. Esto se debe a que un ángulo puede tener
información de más de un prefijo.
El tiempo de Búsqueda del algoritmo UAM está relacionado directamente con la
cantidad de prefijos en la tabla de ruteo, no importando la longitud de la dirección
IP. La implementación para IPv6 donde la dirección IP es de 128 bits tiene el
rendimiento relacionado únicamente con la cantidad de prefijos en la tabla de ruteo.
La principal aportación de este trabajo es la propuesta del mecanismo que permite
convertir la búsqueda de prefijos en una búsqueda exacta dentro de un arreglo.
El tiempo de búsqueda no es dependiente de la longitud de la dirección IP, depende
del número de prefijos en la tabla de ruteo. El tiempo para actualizar la información
de los prefijos de red está normalizado con respecto al número de prefijos en la tabla
de ruteo. Las estructuras de datos utilizados son sencillas ya que se trata de arreglos
ordenados y estructurados.
Los sistemas digitales que necesitan compartir información, cada vez son más
pequeños, y se pueden utilizar en cualquier parte, esto lleva al problema de que tendremos más sistemas que necesiten enviar información a través de las redes, se
trabaja en redes de sensores que también tendrán la necesidad de atravesar redes,
probablemente de menor extensión pero de igual densidad de nodos. El número de
entradas en la tabla de ruteo será grande y se usará mayor tiempo de búsqueda. Así,
proponer un algoritmo que sea menos costoso en tiempo de búsqueda es necesario
para el futuro.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Algoritmos de Búsqueda y
Actualización de Información
para ruteadores IP
Para obtener el grado de
Maestro en Ciencias
(Ciencias y Tecnologías de la Información)
P R E S E N T A
Ing. Joel Yazbek Buendía Gómez
A S E S O R E S
Dr. Miguel Ángel Ruíz Sánchez
Dr. César Jalpa Villanueva
S I N O D A L E S
PRESIDENTE: Dr. Javier Gómez Castellanos
SECRETARIO: Dr. César Jalpa Villanueva
VOCAL: Dr.Ricardo Marcelín Jiménez
VOCAL: Dr. Miguel López Guerrero
30 de Julio de 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Adaptación de una Metodología de Desarrollo Arquitectónico al Contexto de Equipos de Desarrollo Pequeños</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los métodos que integran a la metodología de desarrollo arquitectónico propuesta por el
SEI buscan ayudar a los arquitectos a realizar diseños arquitectónicos de forma sistemática, a
documentar estos diseños y a comprender las consecuencias de las decisiones arquitectónicas
que se toman con respecto de los atributos de calidad y objetivos de negocio del sistema
[Nord2004]. Uno de los primeros métodos relacionados con el desarrollo arquitectónico es
el método de evaluación arquitectónica llamado ATAM, el cual con el paso del tiempo ha
permitido a los investigadores del SEI crear otros métodos derivados del mismo, ejemplos de
estos métodos son QAW, ADD y VaB [Kazman2004].
Los métodos arquitectónicos del SEI antes mencionados tienen 3 características que difi-
cultan su uso por equipos de desarrollo pequeños, estas características son:
1. No están integrados dentro de un proceso de desarrollo de software particular y para
obtener mayor beneficio estos métodos se deben utilizar juntos, lo cual requiere de
ajustes importantes (alguien en la organización de desarrollo debe tener mucho conocimiento
en su uso para poder ajustarlos en un proceso cohesivo) [Lattanze2005]. Está caracter
ística es esencial ya que con los ajustes necesarios los métodos arquitectónicos del
SEI se pueden usar dentro de cualquier proceso de desarrollo.
2. Los métodos arquitectónicos del SEI están pensados para que sean usados en el diseño
de sistemas complejos de alto riesgo. Está característica los hace ser pesados y costosos
en términos de documentación, recursos financieros, de calendario, capacitación, etc.
[Lattanze2005]
3. Están orientados hacia grandes equipos tanto de desarrollo como de involucrados. Está
característica se podría ver como un derivado de la anterior debido a que sólo equipos
de desarrollo grandes pueden desarrollar sistemas complejos con alto riesgo.
Adicionalmente, se ha identificado que la secuencia con la cual el SEI propone que los
métodos sean realizados tiene potenciales riesgos de re-trabajo en la documentación arquitec-
tónica, esto se debe a que una vez que el diseño ha sido documentado se evalúa y en caso
de que se necesiten correcciones la documentación se tiene que actualizar.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de este proyecto de investigación es:
Adaptar la metodología de desarrollo arquitectónico del SEI para que pueda ser empleada
por equipos de desarrollo pequeños</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El desarrollo de este proyecto se basa en las siguientes hipótesis:
Cada uno de los métodos que integran la metodología propuesta por el SEI se pueden
adaptar al contexto de equipos de desarrollo pequeños.
* Es posible agilizar los pasos que cada uno de los métodos comprende.
* Es posible reducir el número de artefactos o reducir el contenido de algunos de
estos.
* Es posible que arquitectos con poca experiencia mejoren sus diseños arquitectónicos
siguiendo las adaptaciones propuestas.
Los métodos propuestos por el SEI pueden trabajar de forma coordinada.
La metodología propuesta por el SEI se puede integrar con TSPi.
Es posible determinar los elementos arquitectónicos específicos de ASOA con la literatura
y aplicaciones disponibles.
Es posible establecer un proceso general que permita identificar los elementos arquitectonicos para patrones distintos a ASOA.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La realización de este proyecto de investigación permitirá a las organizaciones de desarrollo reducir los costos de:
Adaptación de los métodos arquitectónicos del SEI : Para realizar la adaptación de los
métodos que integran a la metodología de desarrollo arquitectónico del SEI las organizaciones de desarrollo tendrían que contratar a expertos en la misma o asignar a parte de su personal en la realización de esta tarea, lo que involucra un gasto en recursos financieros, humanos y de calendario (costos que pocas organizaciones pueden pagar).
Integración de la metodología del SEI en un proceso de desarrollo: De igual forma que
en el punto anterior, las organizaciones deberían invertir recursos para determinar en
qué etapas de su proceso de desarrollo se puede integrar a la metodología de desarrollo
arquitectónico del SEI y realizar las actividades necesarias para adoptar estos cambios
en su proceso.
Entrenamiento en la metodología: Para realizar entrenamiento se requiere que expertos
en la metodología propuesta por el SEI creen el material e impartan las sesiones de
entrenamiento.
El uso de algún método de desarrollo arquitectónico, como el propuesto por el SEI, permite
incrementar la posibilidad de éxito (entendido como entregar sistemas de calidad, a
tiempo y con los recursos acordados) en el desarrollo de sistemas de cómputo, por lo cual es necesario dotar a los equipos de desarrollo pequeños con alguno de estos métodos adaptado a sus características y necesidades propias (posibilidad de comunicación persona a persona, pocos roles involucrados, desarrollan sistemas de complejidad media a baja, se considera que desarrollan sistemas no críticos1, etc.). Lo anterior le permitirá a estos equipos realizar diseños arquitectónicos de calidad de forma repetible.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología propuesta en este proyecto de investigación consiste de las siguientes fases:
Investigación: Considera la investigación de los siguientes elementos:
* Conceptos relevantes de arquitectura de software.
* La relación de la arquitectura de software con la calidad del sistema.
* Las diferentes opciones de metodologías de desarrollo arquitectónico.
* La identificación de los niveles de aplicación de la Arquitectura Orientada a Servicios
(SOA) disponibles en la literatura.
Estudio: Comprende el estudio de las actividades que integran cada método de la
metodología de desarrollo arquitectónico propuesta por el SEI.
Adaptación: Se adaptan los métodos que comprende la metodología de desarrollo arquitect
ónico propuesta por el SEI para que se puedan aplicar en el contexto de equipos
de desarrollo pequeños.
Realización: Se selecciona un caso de estudio de la vida real que considere el uso de
ASOA. Por último, se realiza, con ayuda de un grupo de estudiantes de maestría, el
diseño arquitectónico del caso de estudio siguiendo las adaptaciones propuestas a la
metodología de desarrollo arquitectónico propuesta por el SEI. El resultado de esta
fase es un diseño validado y documentado que permita implementar al sistema.
Recopilación y reporte de resultados: Se recopilan los resultados obtenidos y se realiza
el reporte de los mismos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este proyecto se definió un conjunto de adaptaciones a los métodos de desarrollo
arquitectónico del SEI para que estos funcionen de forma coordinada y, de forma breve, se
presentó como estas adaptaciones se pueden incorporar en un proceso de desarrollo orientado a equipos de desarrollo pequeños (TSPi). En estas adaptaciones se logró mantener el espíritu de cada uno de los métodos aligerando los artefactos que produce y las actividades que cada uno comprende. Estas adaptaciones se realizaron considerando que equipos de desarrollo pequeños
realizan sistemas de complejidad media a baja y sin riesgos que involucren pérdidas financieras o de vidas humanas (en caso contrario es recomendable realizar análisis minuciosos).
Para realizar la evaluación de las adaptaciones propuestas se eligió un caso de estudio
real cuyos objetivos de negocio se satisfacían con el uso de ASOA. Como apoyo para la
evaluación de las adaptaciones se creó un conjunto de elementos arquitectónicos (catálogo
de tácticas y patrones arquitectónicos y tablas de generación de escenarios de atributos de calidad) que son necesarios para soportar la ejecución de los métodos en el contexto de la creación de aplicaciones basadas en el enfoque ASOA, además, se definió un proceso genérico que permite definir estos elementos arquitectónicos para otros contextos diferentes de ASOA.
Finalmente, se describió una evaluación preliminar con estudiantes de maestría, está evaluacion arrojó resultados que permiten pensar que es altamente factible que gente con poca experiencia se beneficie de realizar las adaptaciones propuestas en este trabajo dado que les permite crear diseños arquitectónicos de calidad aceptable desde sus primeros intentos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Maestría en Ciencias y Tecnologías de la Información
Adaptación de una Metodología de Desarrollo
Arquitectónico al Contexto de Equipos de
Desarrollo Pequeños"
Idónea Comunicación de Resultados que para obtener el grado de
MAESTRO EN CIENCIAS
(Ciencias y Tecnologías de la Información)
PRESENTA:
Lic. José Ismael Nuñez Reyna
Asesor:
Dr. Humberto Cervantes Maceda
Sinodales:
Dr. Cuauhtémoc Lemus Olalde
M. en C. Eduardo Rodríguez Flores
Dr. Humberto Cervantes Maceda
23 de octubre de 2009</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Control de Seguimiento de Rutas para un Robot Móvil de Ruedas</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Como se puede apreciar en el estado del arte, es de gran importancia el problema
de seguimiento de rutas en el campo de los RMR debido a sus variadas aplicaciones. La
estrategia comúnmente usada en aplicaciones industriales para resolver esta problemática
es diseñada en dos etapas:
1). Un controlador PID clásico se utiliza para determinar los perfiles de velocidad
deseados que deben ser alcanzados por las ruedas del RMR con el fin de cumplir
el seguimiento de ruta.
2). Los lazos internos de velocidad impulsados por controladores proporcionales lineales
de alta ganancia o proporcional-integral se utilizan para calcular los pares de
torsión para obligar a las velocidades reales de la rueda a alcanzar los perfiles de
velocidad deseados obtenidos en 1.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo del presente trabajo consiste en el diseño de una ley de control para
el seguimiento de rutas de un RMR de tipo diferencial, así como simularla y después
instrumentarla en tiempo real. De igual forma, se pretende realizar los experimentos
que den soporte a un resultado teórico de estabilidad del esquema de control arriba
mencionado que incluye las etapas i) e ii). Este resultado teórico junto con los resultados
experimentales son la aportación más importante del trabajo que se pretende realizar.
Es importante mencionar que la diferencia entre 1), 2) e i), ii) es muy pequeña y,
por lo tanto, se pretende afirmar que se trata de un resultado muy cercano a demostrar
1), 2) también.
Los objetivos particulares que se desarrollarán para lograr el objetivo general de este
trabajo, se presentan de acuerdo a las áreas que lo conforman:
Ingeniería mecánica
◃ Comprender el modelo cinemático y dinámico de un RMR de tipo diferencial.
◃ Construir la infraestructura necesaria para colocar el sistema de sensado.
Ingeniería electrónica
◃ Conocer el principio de operación de la tarjeta DS1104.
◃ Realizar la instrumentación necesaria para sintetizar el sensado de las variables de
interés en la ley de control.
Ingeniería en control
◃ Diseñar un control simple que considere el modelo dinámico del RMR de tipo
diferencial para resolver el problema de seguimiento de ruta.
◃ Simular la ley de control propuesta, a través de Matlab/Simulink, tal que permita
observar el comportamiento del sistema a nivel simulación.
Ingeniería en computación
◃ Implementar a nivel experimental la ley de control con la ayuda de la tarjeta
DS1104 junto con Matlab/Simulink y ControlDesk.
◃ Comparar los resultados obtenidos en simulación y experimentación, y realizar el
reporte correspondiente.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El diseño de esta estrategia de control se ha realizado basándose generalmente en ideas intuitivas, aunque varios controladores avanzados se han propuesto para resolver el problema de seguimiento de rutas [40], [41], [42], [43], [44], [45], [46], la estrategia en 1) y 2) es aún la más común en la industria debido a su simplicidad. Además, muchos de estos controladores citados están diseñados sobre la base del modelo cinemático, es decir, muchos de éstos no proporcionan un análisis de estabilidad donde se considere el modelo dinámico del robot. Notable es el trabajo descrito en [47], donde un regulador PID clásico está diseñado para la etapa 1), sin embargo, esto se hace al considerar un modelo cinemático simplificado del robot y el modelo dinámico no se tiene en cuenta, es decir, la etapa 2), no se considera. Por lo cual se pretende realizar una estrategia de control para resolver el problema de seguimiento de rutas de un RMR impulsado de manera diferencial. Este controlador estará diseñado sobre la base tanto de los modelos cinemática y dinámicos y tendrá dos componentes: i) un controlador lineal más un término lineal adicional de la velocidad angular del robot que se utilizará para determinar los perfiles de velocidad deseados para las velocidades de las ruedas, ii) los lazos internos de velocidad proporcionados por un controlador se utilizarán para forzar las velocidades reales de las ruedas para alcanzar los perfiles deseados calculados en i). Por lo tanto, la principal contribución de este trabajo es la introducción de una estrategia de control que sólo se diferencia de la práctica común industrial en 1), 2) por encima del uso de un término lineal en función de la velocidad angular del robot, en lugar de un término integral, es decir, sólo existe una pequeña diferencia entre las etapas 1) y i).</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El objetivo principal de este trabajo fue diseñar una ley de control, para el seguimiento
de rutas de un RMR de tipo diferencial, así como simularla y después instrumentarla
en tiempo real. También se demostró formalmente, que el error de seguimiento y el error
de la velocidad de las ruedas tienen una cota final, que aunque puede aumentar si la
velocidad del móvil aumenta se puede compensar mediante el aumento de las ganancias
del controlador. Por lo tanto el rendimiento logrado durante la tarea de seguimiento
de la trayectoria se puede mejorar seleccionando adecuadamente las ganancias de los
controladores. Por otro lado se observó que uno de los factores que afectan la velocidad
máxima de avance del RMR, está relacionado con la nitidez de curvatura de la ruta.
Una vez presentado el desarrollo de los diferentes capítulos de este trabajo y observando
los resultados obtenidos tanto en simulación como en experimentación se puede
concluir que se cumplió satisfactoriamente el objetivo general. Cabe mencionar que este
trabajo se pudo llevar a cabo gracias al prototipo que previamente se había construido
en el área de mecatrónica, el cuál se construyó con el objetivo de que se pudieran implementar
diferentes leyes de control, lo cual se sigue logrando. A continuación se enlistan
algunas de las actividades que se realizaron en este trabajo, con la finalidad de lograr el
objetivo inicialmente planteado:
◃ Se comprendió el modelo dinámico del RMR de tipo diferencial.
◃ Se construyó la infraestructura para colocar el sistema de sensado.
◃ Se comprendió el principio de operación de la tarjeta DS1104.
◃ Se construyó la instrumentación necesaria para sintetizar el sensado de las variables
de interés en la ley de control.
◃ Se diseñó un control simple que considera el modelo dinámico del RMR de tipo
diferencial, para resolver el problema de seguimiento de ruta.
◃ Se simuló la ley de control propuesta, lo cual permitió observar el comportamiento
del sistema a nivel simulación.  Se implementó a nivel experimental la ley de control con la ayuda de la tarjeta
DS1104 junto con Matlab/Simulink y ControlDesk.
◃ Se demostró, formal y experimentalmente, que se puede llevar a cabo la tarea de
seguimiento de rutas con un controlador PD en lugar de un controlador PID.
◃ Se compararon los resultados obtenidos en simulación y experimentación, y se
realizó el reporte correspondiente. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
CIDETEC
EDUCACIÓN
INNOVACIÓN
CONSERVACIÓN
IPN
CONTROL DE SEGUIMIENTO DE RUTAS PARA UN
ROBOT MÓVIL DE RUEDAS
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
P R E S E N T A:
Ing. Celso Márquez Sánchez
DIRECTORES DE TESIS:
Dr. Ramón Silva Ortigoza
Dr. Victor Manuel Hernández Guzmán
México, D.F. Junio 2014.
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Desarrollo de un Sistema de Realidad Aumentada para el Aprendizaje Utilizando Dispositivos Móviles</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
El presente trabajo tiene como objetivo el desarrollo de una aplicación interactiva
utilizando la Realidad Aumentada, que pueda ofrecer tanto al docente como al alumno la
interactividad de una clase a través de una plataforma móvil.
Objetivos Particulares
Determinar el dispositivo móvil que será utilizado.
Definir la tecnología de comunicación que será empleada (móvil-computadora).
Aplicar técnicas avanzadas para el desarrollo de un software educativo.
Realizar la simulación del ambiente interactivo desarrollado.
Evaluar el sistema implementado.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La educación actual, requiere transformar los mecanismos de transmisión del
conocimiento, por lo que el proceso de enseñanza-aprendizaje debe estar en semejanza con
los avances tecnológicos.
Los procesos de enseñanza-aprendizaje han generado el estudio de diferentes teorías y
paradigmas educativos, en los cuales se busca que el alumno adquiera competencias que
pueda aplicar en cualquier contexto de su vida, convirtiéndose en un ser autónomo y capaz
de tomar decisiones asertivas para enfrentarse a un mundo globalizado, sin embargo el gran
problema que se ha presentado en el estudiante, es que considera las estrategias, métodos y
técnicas poco eficaces, provocando altos índices de deserción y bajo rendimiento
académico en todos los niveles educativos.
Por tal situación y en la búsqueda por lograr que el estudiante se apropie de
competencias y genere el aprendizaje significativo de una manera más sencilla e
interesante, se ha pensado en incorporar las tecnologías como parte fundamental del
aprendizaje.
Partiendo que en la actualidad un número importante de personas, cuentan y además
utilizan las tecnologías de la información y comunicación, las cuales son incorporadas a su
vida cotidiana como herramientas necesarias para realizar distintas actividades, en este
sentido es necesario aplicarlas para experimentar una nueva forma de acercar al estudiante
al conocimiento.
Asimismo, cabe señalar que la Realidad Aumentada aplicada en la educación permite
conocer otra metodología de aprender aprovechando estos dispositivos que son tan
comunes en la vida diaria de las personas, convirtiéndose en una oportunidad, para la
adquisición del conocimiento de una forma distinta, intuitiva y sencilla gracias al dominio
en el manejo de estos dispositivos.
Una aplicación que posibilita mejorar este proceso, es precisamente el desarrollo de un
sistema de Realidad Aumentada para el aprendizaje, el cual utilice un dispositivo móvil que
tenga una acción recíproca, para coadyuvar en la enseñanza y adquisición del
conocimiento.
Asimismo, la incorporación de dispositivos tecnológicos en el proceso de enseñanza aprendizaje
mejora la eficiencia en la transmisión del conocimiento, permitiendo un mejor
aprovechamiento por parte del alumno, además de facilitar y hacer más eficiente las
actividades frente a grupo del profesor.
El trabajo se respalda partiendo de los aprendizajes que existen en la actualidad, ya que
todos ellos aportan un aprendizaje electrónico, presencial, a distancia y móvil. 
El sistema con Realidad Aumentada propuesto, apoya lo mencionado anteriormente con
la diferencia que el proceso educativo docente-alumno se da con el concepto de Realidad
Aumentada, es decir las clases presenciales son reales y virtuales al mismo tiempo, donde
los actores del proceso educativo participan en tiempo real aprendiendo y enseñando a
través de los dispositivos móviles, esto significa que no solo basta el video, sonido o texto
en una clase como actualmente se dan.
Asimismo se puede llevar la transmisión del conocimiento, ya que el uso de los
dispositivos móviles en los alumnos es muy importante y a la vez es una herramienta
multimedia de calidad completamente equipada.
La propuesta contempla el seguimiento de una clase en tiempo real entre el alumno y el
docente combinando lo real con lo virtual a través de un dispositivo móvil con la finalidad
de hacer la clase más cognitiva para el alumno. </Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se logró adquirir los conocimientos necesarios, en el manejo de las herramientas para el
desarrollo del sistema.
Las formas de aprendizaje van cambiando a medida que avanza el uso de la tecnología
en el salón de clase y la Realidad Aumentada aplicada en un libro de texto puede contribuir
en esta transformación.
Se implementó un sistema con Realidad Aumentada que complementa la información
que tiene el libro de texto interactivo con elementos virtuales creados por computadora para
coadyuvar en el proceso enseñanza-aprendizaje.
Se integraron varios videos en una página web que ayudan como soporte a los temas
que se explican el libro de texto interactivo. Los videos tienen una duración máxima de 5 a
10 minutos, con la finalidad de no aburrir al alumno.
Cabe destacar que en este sistema se usan marcadores codificados en la imagen, para
superponer objetos virtuales a la información real o del libro de texto interactivo.
Los objetos virtuales se posicionaron a un costado de cada marcador, ya que al verlos
en tiempo real, se pudo observar que tanto el marcador como el objeto virtual se encimaban
y no se permitía ver con claridad ambos objetos.
Asimismo, este desarrollo permite una interacción en tiempo real entre el usuario y el
escenario creado con Realidad Aumentada.
Con el avance de los dispositivos móviles es posible la implementación de este trabajo
en una plataforma móvil.
Se optó por el sistema Vuforia, Blender y Unity, por ser software libre y debido a que
después de realizar un análisis comparativo de los diferentes programas para generar la
Realidad Aumentada.
Se hicieron varias pruebas de descarga, instalación y ejecución con diferentes
dispositivos móviles y se pudo determinar las características que debe cumplir el
dispositivo para la aplicación propuesta en esta tesis.
Se simulo el proyecto terminado con la finalidad de ver si los modelados tenían la
posición, tamaño y luminosidad correcta al momento de enfocar la cámara hacia el
marcador.
Se evaluó el sistema con algunos alumnos de nivel medio superior y se hicieron una
serie de encuestas para conocer su punto de vista con respecto al proyecto.
A continuación se mencionan algunas de las actividades que se realizaron en este
trabajo, con la finalidad de lograr el objetivo inicialmente planteado.
 Se diseñó un libro de texto interactivo de la materia de arquitectura de computadoras.
 Se utilizaron 27 marcadores, mismos que fueron impresos en el libro de texto
interactivo.
 Se crearon los modelados 3D correspondientes para cada marcador.
 Se simuló con la cámara de la computadora la detección de los marcadores, lo cual
permitió observar el comportamiento de los modelados del sistema como posición,
tamaño, iluminación y distancia.
 Se construyó una página web para concentrar varios videos tutoriales donde se
expliquen los temas principales de la materia de arquitectura de computadoras.
 Se puso a prueba el sistema de aprendizaje con Realidad Aumentada a los grupos de
tercer semestre del Cecyt 3 de la carrera técnico en computación, y se observó un alto
grado de aprobación por parte de los alumnos.
 Se realizó las pruebas correspondientes en diferentes dispositivos móviles con
sistema operativo Android, observando que la mayoría cumplían con los requisitos
necesarios para el uso de la aplicación generada.
 Se integró la Realidad Aumentada dentro de las nuevas tecnologías para la enseñanza
y se implantó una aplicación educativa para la materia de arquitectura de
computadoras, en un campo donde la Realidad Aumentada aún no ha sido presente en
el centro de estudios científicos y tecnológicos No. 3 y que nos da total libertad de
aplicarla.
 Los alumnos se sienten motivados y aprenden directamente interactuando con
elementos virtuales. Es tan intuitivo para ellos que en ningún momento se les
dificultó la aplicación del uso de las Tecnologías al contrario, se les ha facilitado la
forma de aprender.
Desde un punto de vista técnico la visualización con Realidad Aumentada cada vez va a
estar más presente en el campo educativo, ya sea con soluciones comerciales o de código
abierto. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
DESARROLLO DE UN SISTEMA DE REALIDAD AUMENTADA PARA EL
APRENDIZAJE UTILIZANDO DISPOSITIVOS MÓVILES
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
PRESENTA:
CARLOS ANTONIO MADRID TREJO
DIRECTORES DE TESIS:
M. EN C. MIGUEL HERNÁNDEZ BOLAÑOS
DR. JUAN CARLOS HERRERA LOZADA
México, D.F. Diciembre 2014.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Diseño de una Interfaz para la comunicación de datos entre redes IPv6 e IPv4, basada en una intranet.
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Todo órgano como instituciones académicas, empresariales y gubernamentales han
tenido la necesitad de conexión a Internet. teniendo una creciente demanda hasta el punto de
agotar la oferta de las direcciones IPv4, por este motivo se desarrolló un protocolo con mayor
capacidad de acceso, mejorando sus características, tales como seguridad, movilidad, flexibilidad y
simplicidad en la gestión de la planificación de direcciones del protocolo, a esta nueva versión se le
denominó IPv6.
La situación es que no son compatibles IPv6 a IPv4, por lo que se necesitan medios para la
coexistencia de los protocolos para poder realizar investigación, implementación y desarrollo de
aplicaciones en base al protocolo de Internet versión 6, esto implica utilizar un método de transición para la comunicación, esta serie de mecanismos son propuestos por el IETF y sirven para
garantizar el proceso de transición ya que hay un despliegue de servicios basadas en IPv6 a
través de Internet 1.
Los tipos de transición de protocolos son tres principales a los cuales se les hará un
análisis para determinar cuál es el más conveniente a implementar en el CIDETEC, es decir,
realizar un estudio, configuración, conexión de protocolos de capa 3 y monitoreo al rendimiento de
la red para tener conexión en una Intranet con IPv6, utilizando los recursos Institucionales.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general.
Implementar y configurar el protocolo IPv6 en dispositivos de comunicación de datos y
dispositivos terminales, sobre redes adyacentes, para su comunicación en la tecnología de Internet
2, a través de la infraestructura del Instituto Politécnico Nacional, permitiendo la coexistencia entre
los protocolos de Internet.
De igual forma se pretende documentar el análisis, configuraciones, pruebas dentro de la
intranet como material técnico para el apoyo de otros Centros del IPN y difundir la transición al
protocolo IPv6.
Objetivos particulares.
Los Objetivos particulares para esta tesis son los siguientes:
I. Estudiar los métodos de implementación y configuración del protocolo IPv6.
II. Implementar métodos de transición de capa tres de los protocolos IPv4 a IPv6 y viceversa.
III. Aplicación del simulador, para la configuración de redes LAN-WAN.
IV. Configuración de dispositivos CISCO para la transferencia de datos para Linux como para
Windows.
V. Conexión de protocolo de capa tres de Internet 2 a Internet 1 para Linux y Windows en
equipos terminales.
VI. Monitoreo de protocolos enrutables por medio de un escáner de protocolos, tanto para
Linux como para Windows.
VII. Hacer un estudio de los dos protocolos y sus métodos, para determinar su pertinencia</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Ésta será dividida en pasos para que el tema de investigación logre alcanzar sus objetivos
establecidos, para ello se aplicará el Método Científico, este servirá para indicar la estrategia que
apoyará a resolver el problema detectado, el cual se mencionó anteriormente, este define el
marco teórico, hipótesis, herramientas y recursos.
El desarrollo del proyecto contará con los siguientes elementos:
Laboratorio con equipos de cómputo conteniendo los sistemas operativos de Linux y
Windows
 Equipos de comunicación de datos.
 Equipo para el manejo de aplicaciones, servidores y terminales.
 Conectividad IPv4.
 Recursos económicos.
 Recursos de software.
Se aplicarán los tres tipos de investigación para el desarrollo del tema, los cuales serán:
 Investigación documental; se realizará a los diferentes artículos y libros relacionados con el
tema.
 Investigación de campo; ésta se realizará para identificar en que centros del IPN ya se
está utilizando IPv6 y como realizaron su implementación.
 Pruebas de laboratorio; se utilizará el Laboratorio CC1 para el desarrollo, ejecución y
pruebas para la implementación en la intranet con IPv6.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Si bien el objetivo general se cumplió, cabe mencionar que como proyecto se presentaron
inconvenientes no contemplados con anterioridad en la definición. Esto sirve para obtener una
base de conocimiento de los asuntos presentados, a continuación se enlistan los problemas más
significativos que se superaron en el desarrollo del trabajo de investigación
 El simulador Packet Tracer versión 5.3 no soporta protocolos de IPv6 por lo cual
esta herramienta era de uso limitado, esto se solucionó con el GNS3 como la
herramienta para analizar los métodos de transición.
 El emulador GNS3 presentaba inestabilidad y consumía recursos considerables del
equipo local donde se instaló, sin embargo, es una herramienta capaz de virtualizar
un enrutador cisco ya que utiliza los sistemas operativos reales, se solucionó
descargando la última actualización GNS3-0.8.6-all-in-one.exe y complementos.
 Los enrutadores Cisco 1841 contenían un sistema operativo que no soportaba el
protocolo IPv6, por esta razón se tuvo un retraso considerable. Esto se solucionó
instalando por medio de un servidor FTP el sistema operativo c1841-
adventerprisek9-mz.124-23.bin que soporta la memoria flash de 32 bits y los
módulos de las tarjetas HWIC
 Después de varios formateos e instalaciones de Linux y Windows los equipos del
laboratorio CC1, no soportaban arranque dual para sistemas operativos Linux y
Windows, se solucionó solo instalando un sistema operativo a cada equipo en el
laboratorio CC1, esto sirvió para encontrar fallas en la tarjeta madre y memorias de
los equipos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITECNICO NACIONAL
CENTRO DE INNOVACION Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
Diseño de una Interfaz para la comunicación de datos entre
redes IPv6 e IPv4, basada en una intranet.
TESIS PARA OBTENER EL GRADO DE:
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
PRESENTA:
Ing. Guadalupe Cristina Balderas Cortez.
Directores de tesis:
M. en C. Marlon David González Ramírez.
M. en C. Adauto Israel Ortiz Romero.
México, D.F. Noviembre 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Simulación holográfica usando los principios de la realidad aumentada</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El poder desarrollar y desplegar hologramas tridimensionales realísticos sin necesidad
de algún equipo óptico especial es uno de los más interesantes y difíciles planteamientos
por resolver. Como se ha mencionado en secciones pasadas, hay una fuerte tendencia en
el desarrollo de sistemas que realicen proyecciones holográficas tridimensionales, pero
que hasta ahora son muy complicados de fabricar. Es necesario añadir también que en
la actualidad, es muy importante el constante flujo de información en la sociedad, y que
es importante contar con sistemas o aplicaciones que permitan a las personas contar con
la mayor cantidad de información en el menor tiempo posible, lo cual puede ser resuelto
mediante la aplicación de la realidad aumentada y así incrementar la cantidad de
información que llega a las personas.
En este sentido, se ha planteado la problemática en el tema de tesis, es decir presentar
una simulación holográfica convincente e interactiva al usuario para proveerle de
información que pueda necesitar de una manera dinámica y visualmente atractiva.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General.
Concluir con el diseño de un simulador holográfico a color, y que sea económico.
Objetivos Específicos.
 Implementar un sistema basado en realidad aumentada para simular un
holograma.
 Desarrollar un prototipo holográfico a color, basado en los principios de la
realidad aumentada.
 Realizar pruebas sobre imágenes estáticas.
 Pruebas del sistema terminado.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Existen diversas técnicas holográficas que permiten obtener efectos de profundidad, pero
la mayoría son en un solo color, y las técnicas usadas para llevar a cabo hologramas de
color son complicadas y caras. Pertenecen a empresas que lo explotan con patente y no
permiten una fuerte interacción entre el holograma y el observador.
En referencia a lo explicado en la sección de antecedentes, es fácil identificar el potencial
de los dispositivos móviles para el desarrollo de aplicaciones de realidad aumentada, ya
que los dispositivos móviles han mejorado considerablemente su potencial, en los
aspectos más relevantes para realidad aumentada. Particularmente cuentan con
procesadores más rápidos, mayor capacidad de memoria interna y expansible, mejores
interfaces de entrada, pantallas más grandes y de mayor resolución, mejor desempeño
en el hardware debido a que cuentan con más sensores y mejora en el desempeño y
resolución de las cámaras integradas.
Para aprovechar todas las capacidades de los dispositivos móviles actuales, se plantea
el desarrollo de un simulador holográfico por medio del diseño de una aplicación
utilizando las características de la realidad aumentada, para que sea portable y sea de
fácil acceso al público en general, quienes podrán tener una interacción más directa con
los objetos virtuales, a diferencia de los existentes mediante sistemas ópticos que no
permiten la interacción directa del holograma con el usuario.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para proceder con el desarrollo de la aplicación, es necesario llevar a cabo varios pasos
importantes que en su conjugación proporcionen los resultados esperados para una aplicación
móvil de realidad aumentada. Para esto, la metodología llevada a cabo se divide en los
siguientes módulos:
I. Análisis sobre el funcionamiento del proyecto muestra que proporciona la plataforma de
Vuforia que servirá como base para el desarrollo de la aplicación.
Para comenzar con el desarrollo de la aplicación, se aprovecha el proyecto muestra que
ofrece la plataforma de Vuforia, para modificar su estructura y que sea posible cargar
distintos modelos tridimensionales directamente en el código de la aplicación sin tener
que usar otro software comercial por el cual se tenga que pagar una licencia de uso.
Como el modelo 3D se va a desarrollar a través de Blender, no hay problema con la
licencia ya que ésta es gratuita. Por lo tanto se analiza la estructura del código del
proyecto muestra para conocer la manera en que debe de construirse el modelo 3D para
poderlo utilizar.
II. Implementación de un script de adquisición de datos de cada coordenada usada para la
creación de un objeto 3D en Blender.
En este módulo se trabaja directamente sobre Blender para desarrollar un script que
permita obtener los valores de las coordenadas de los puntos de los vértices, las
normales, las texturas y los índices que forman las caras del objeto, ya que son
necesarias para que pueda ser dibujado dentro de la aplicación de realidad aumentada.
Es necesario conocer acerca del tipo de programación en Python, el cual es un lenguaje
script de propósito general que tiene una interfaz especial para acceder a todas las funciones internas de Blender. Los scripts se escriben en este lenguaje para extender las
funcionalidades de Blender, sin tener que recompilar la distribución binaria.
La versión recomendada de Python esta habitualmente incluida e instalada con la
distribución de Blender, no obstante se puede descargar directamente de la página
oficial de Python.
III. Determinar la manera en la que son agregados los datos obtenidos, al programa que
ejecutara la aplicación de realidad aumentada.
Una vez obtenidos los puntos, se debe determinar la forma en la que los datos serán
agregados al programa de la aplicación para que puedan ser utilizados en el momento
que se detecte el marcador dentro de la imagen captada por la cámara del teléfono
móvil.
Debido a que se parte de un proyecto muestra, es importante conservar las
características de cada una de las clases que conforman el proyecto, siendo una de estas
clases la que contiene la información de los puntos.
La estructura del lenguaje de programación es java y se maneja a través de buffers que
envían el contenido de cada uno de los cuatro arreglos de vértices, texturas, normales
e índices a las funciones de OpenGL, que son las encargadas de realizar los cálculos
matemáticos para formar los objetos tridimensionales.
IV. Pruebas del script con distintos objetos y texturas para evaluar su desempeño dentro de
la aplicación de realidad aumentada.
Cuando puedan cargarse los datos de manera satisfactoria dentro del proyecto en
Vuforia, se procede a extraer las coordenadas de otros objetos 3D para comprobar que
está funcionando adecuadamente el programa de obtención de puntos junto con la
aplicación de realidad aumentada, sin importar la cantidad de puntos necesarios para
la construcción del modelo tridimensional. Es decir, que se realiza de forma correcta la
interpolación de las caras del objeto para su adecuada visualización junto con la textura
asignada, del mismo modo que se obtiene al renderizar el modelo dentro del software
de Blender.
V. Análisis y prueba de la aplicación para múltiples texturas.
Al haber comprobado que el script funciona para cualquier tipo de objeto con una
textura simple, se comienzan a realizar pruebas con objetos que requieran más de una
textura para ser visualizados de manera adecuada, por lo tanto de debe probar que el
programa y el script desarrollado es efectivo para colocar múltiples texturas dentro del
modelo 3D y que la aplicación es capaz de mapearlas adecuadamente cuando el modelo
sea aumentado dentro del ambiente real que captura la cámara del móvil.
VI. Migración de la plataforma donde se desarrolla la aplicación a Android Studio.
Hasta este punto, se había estado diseñando la aplicación sobre el software de desarrollo
Eclipse, pero ahora que Vuforia actualizó su SDK a la versión 5.5.9, fue modificada la
estructura de su contenido para poder trabajar mediante un nuevo software de
desarrollo. Utilizando Android Studio como nueva herramienta, es necesario realizar
unos pasos adicionales para que sean reconocidas las bibliotecas y extensiones de
Vuforia mediante cambios en la distribución del contenido del proyecto final, y asegurar
que la aplicación se ejecute sin ningún problema.
VII. Modificación del canal alfa dentro de las propiedades del objeto virtual para ser presentado
en la pantalla como una simulación holográfica dentro del mundo real.
Para que el objeto virtual que fue modelado y colocado en las imágenes capturadas por
la cámara del dispositivo tenga apariencia de un holograma, es necesario modificar los
niveles de transparencia del objeto y de esta forma se proyecte de forma que se pueda
ver a través de él. Es decir, el objeto virtual no debe aparecer como un cuerpo sólido,
de otro modo la ilusión de ser un holograma no habrá sido lograda. Por lo tanto, deben
hacerse modificaciones en el código de los valores del objeto, para que la aplicación lo
proyecte como un objeto transparente para cuando es renderizado.
VIII. Diseño y configuración de botones virtuales para permitir la interacción directa del
usuario con el objeto virtual, modificando su ángulo, a través de la rotación del objeto
sobre su propio eje.
Como paso final en el diseño de la aplicación, se configuran botones de manera virtual
para que el usuario pueda modificar el estado en el que se encuentra el objeto virtual
simulado como holograma, es decir, lo que se desea obtener de la aplicación, es que el
usuario pueda tener control sobre el objeto virtual que se le está enseñando, porque la
finalidad al ser tridimensional, es que el usuario pueda observarlo desde cualquier
ángulo. Como una manera práctica de hacerlo, se permite modificar la rotación sobre
el propio eje del objeto para que el usuario si lo desea, lo haga girar hasta 360°,
garantizando que es un objeto tridimensional y no solo un objeto plano.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Durante el desarrollo de este proyecto aparecieron distintos retos que se tuvieron que
resolver, debido a algunas limitaciones en los dispositivos móviles, particularmente el
procesamiento computacional.
Tras superar las dificultades encontradas para hacer funcionar la aplicación en los
dispositivos móviles, se realizaron pruebas básicas para comprobar su adecuado
funcionamiento. Tras este paso se comprobó que obtener los datos necesarios de los objetos
virtuales mediante un archivo de texto, presenta los diseños óptimos para la realización de
pruebas traduciendo dichos valores al formato utilizados en Vuforia, por lo que su uso no es
trivial. A lo largo del documento se expone la manera en que se realizó la exportación de los
objetos virtuales desde el modelado en Blender para hacerlos compatibles con la aplicación.
Se planteó la posibilidad de desarrollar simulaciones holográficas a bajo costo y portables
que permitan una interacción de manera dinámica con el observador, con la finalidad de
acercar a más personas al uso de tecnologías en creciente desarrollo. El presente trabajo deja
muchas áreas de oportunidad por desarrollar, ya que es una herramienta que se puede
mejorar en varios aspectos.
La plataforma de Vuforia demostró ser una buena elección para el desarrollo de aplicaciones
en realidad aumentada, pues sus algoritmos de rastreo y registro son bastante estables,
realiza un tracking continuo del target cuando éste se mueve rápidamente. Aunque esta
funcionalidad no se ha abordado en el estudio, Vuforia ofrece tracking de objetos 3D.
En la parte gráfica el estándar OpenGL ES es eficiente. Pero, también limita el grado de
complejidad de los gráficos ya que sólo dispone de funciones de bajo nivel para representar
gráficos. Para representar cualquier modelo basta con especificar las coordenadas de sus vértices, de las texturas y de las normales, así como ejecutar operaciones básicas de
transformación. Aunque resulta tedioso, existe la posibilidad de convertir un archivo creado
en Blender en un archivo txt que puede ser directamente incluido en la aplicación para que
sea utilizado en la función de render,
Aunque la prioridad de Vuforia es el desarrollo de aplicaciones mediante Unity que con la
API de Android Studio, pues la mayoría de las preguntas y la información disponible gira
en torno al diseño y aplicación sobre la plataforma de Unity.
Con este trabajo se observa que el futuro implica el desarrollo y aplicación de la holografía
en celulares, la televisión, los videojuegos, simuladores y la educación entre los principales.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
Centro de Innovación y Desarrollo
Tecnológico en Cómputo
“Simulación holográfica usando los principios de la realidad
aumentada”
Tesis que para obtener el grado de Maestría en Tecnología de Cómputo
Presenta:
Ing. Joaquín Abraham Serrano Cervantes
Directores:
M. en C. Israel Rivera Zárate
M. en C. Jesús Antonio Álvarez Cedillo
Ciudad de México, Agosto de 2016</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Modelado dinámico de un manipulador paralelo de tres grados de libertad</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En robótica está ampliamente difundido el uso de robots de tipo serial para su uso industrial y en el área de la educación, por lo cual existe una cantidad considerable de trabajos de investigación acerca de modelos cinemáticos y dinámicos para implementar leyes de control en este tipo de robots. En el caso de los robots paralelos, estos son mecanismos de cadena cinemática cerrada cuyo efector final está unido a la base por varias cadenas cinemáticas independientes, permiten una infinidad de configuraciones de acuerdo a la topología empleada en su diseño, respecto al número de cadenas cinemáticas y al número de grados de libertad con los que cuenta el efector final, de acuerdo a la aplicación en la que se emplean, además que presentan una gran relación fuerza - peso de carga, debido a una gran rigidez y estabilidad. En el área académica se han presentado una cantidad considerable de trabajos desarrollando modelos cinemáticos, pero no así, en el modelado dinámico, ya que su obtención depende en gran medida del número de grados de libertad y configuración del robot paralelo, en nuestro caso se plantea la necesidad de la obtención de un modelo dinámico de un manipulador paralelo, para diseño y evaluación de leyes de control para renderización háptica en simuladores. Actualmente en el Centro de Innovación y Desarrollo Tecnológico en Cómputo (CIDETEC), se cuenta con un prototipo experimental de robot de tipo manipulador paralelo, con una plataforma móvil de tres grados de libertad, actuada mediante tres mecanismos de tipo revoluta - revoluta (R-R) y con un mecanismo pasivo de tipo prismático - universal(P-U), para su empleo en el desarrollo de simuladores, hasta ahora su uso se ha visto limitado por la ausencia de un modelo dinámico, el cual es requisito para simular los movimientos llevados a cabo por el efector final, evaluar la estructura mecánica e implementar leyes de control, que permitan la regulación de la velocidad de movimiento y compensar las perturbaciones que puedan presentarse.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
El presente trabajo tiene como principal objetivo proponer un modelo dinámico de un manipulador
paralelo de tres grados de libertad con tres mecanismos subactuados tipo revoluta
- revoluta (R-R) y un mecanismo pasivo de tipo prismático - universal (P-U).
1.5.4 Objetivos particulares
Analizar y comparar los posibles métodos para obtener el modelo dinámico para elegir el
más adecuado para la configuración del manipulador paralelo de tres grados de libertad.
Validar mediante simulación numérica computacional el modelo dinámico. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Como se ha observado en los trabajos de investigación antes mencionados, el modelado
dinámico de robots manipuladores, en especial los que cuentan con plataformas móviles, ha
llevado al desarrollo de simuladores de movimiento para su uso como parte de ambientes
de realidad virtual, creando con ello un nicho tecnológico para su empleo en la industria
del entretenimiento y en especial en el área académica; es por ello que es necesario poner a
disposición del ámbito académico modelos para que se lleven a cabo este tipo de simuladores,
y que permitan desarrollar controladores, implementar y probar leyes de control o para el
desarrollo de ambientes de realidad virtual, para la generación de tecnologías de bajo costo
y satisfacer una demanda por el uso de estas herramientas en el desarrollo de nuevo conocimiento.
En el presente trabajo se lleva a cabo el planteamiento de un modelo dinámico de
un prototipo experimental de robot de tipo manipulador paralelo, el cual cuenta con una
plataforma móvil de tres grados de libertad como efector final.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para el cumplimiento del objetivo de este trabajo se plantea la siguiente metodología:
Analizar los casos de estudio de métodos de modelado dinámico en la literatura de
manipuladores paralelos.
Asistencia a un curso de técnicas de modelado de sistemas mecatrónicos.
Instrumentación del prototipo experimental.
Obtener y analizar datos experimentales para la identificación paramétrica.
Asistir a un curso de técnicas heurísticas para hacer la identificación paramétrica del
manipulador paralelo.
Aplicación de técnicas heurísticas para el modelado dinámico.
Validar experimentalmente el modelo dinámico con el manipulador tipo R-R-R-U paralelo
de tres grados de libertad
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En esta tesis se propone un modelo matemático para un prototipo de robot tipo manipulador
paralelo de tres grados de libertad, con base en el análisis cinemático y dinámico
del sistema de mecanismos actuadores. Para su validación se hicieron simulaciones numéricas
computacionales con el modelo propuesto y una comprobación experimental de forma cualitativa,
permitiendo validar que el sistema se comporta de la manera como se plantea en el
modelo, siendo coherente con la activación de los motores y las fuerzas que actúan sobre la
plataforma móvil.
Otro aspecto que se llevó a cabo fue el de la identificación paramétrica del resorte, en el sistema
de amortiguamiento, la cual se realizó planteando un problema de optimización numérica.
A partir de los resultados obtenidos en este trabajo de tesis, se establecen las siguientes conclusiones:
En este trabajo se tomó un enfoque de análisis diferente al que se habían planteado
anteriormente en los trabajos relacionados con el tema, ya que para la formulación del
modelo dinámico del manipulador paralelo, se consideró llevar a cabo un análisis de
las fuerzas que actúan en el efector final; una formulación similar a la que se lleva a
cabo en el análisis dinámico de aeronaves no tripuladas, en las cuales, las fuerzas que
actúan en la estructura de esta aeronave de tipo helicóptero cuadrotor, son fuerzas de
empuje debidas al cambio en la velocidad de rotación de las hélices de los rotores, los
cuales causan un desplazamiento vertical y por consiguiente un grado de inclinación de
la estructura del cuerpo del helicóptero para moverse en una dirección especifica.
Aunque esta forma de modelar plataformas móviles de los manipuladores paralelos, no
se ha considerado como una metodología de análisis de amplia difusión, satisface los
requerimientos para el desarrollo del modelo dinámico, dando la descripción del cambio
en la magnitud de la fuerza que se ejerce sobre la plataforma móvil, como función de
los torques de los mecanismos actuadores y de la derivada de la variable actuada, para
cada uno de los diferentes movimientos debidos a los grados de libertad del manipulador
paralelo.
Con base en lo anterior se concluye que si bien los resultados cuantitativamente son
buenos, existen otros factores que no se consideraron, entre los cuales se puede mencionar:
Un mejor ensamblado, ya que se notó que él funcionamiento del sistema no es el
adecuado, debido imprecisiones en el ensamble, desajustes por el mismo uso, por el tipo
de materiales que se emplearon en su fabricación, así como por el juego en las uniones
mecánicas del ensamblado del manipulador. Un caso notable, es el del sistema de amortiguamiento,
en este caso se deberá hacer un análisis exclusivamente del ensamblado
del manipulador.
Considerando que se trata de un prototipo desarrollado con un bajo presupuesto, en
las pruebas de funcionamiento se observó que su comportamiento cambiaba debido a
los desgastes propios del mecanismo, los cuales no se pudieron arreglar debido a que no
se contaba con mejores materiales. Todos estos factores fueron un factor fundamental
para no llevar a cabo una validación experimental del modelo dinámico obtenido.
Se llevó a cabo la identificación paramétrica del resorte del sistema de amortiguamiento
del soporte central de la plataforma móvil. Dicha identificación paramétrica mediante
un problema de optimización numérica. La solución a este problema se planteó mediante
la utilización del algoritmo heurístico de evolución diferencial. En los resultados
obtenidos se observa que el problema planteado es multimodal, ya que se obtiene más
de un valor mínimo, para el valor de la elasticidad del resorte. Sin embargo el resultado
está dentro de los límites de las cotas de diseño que se propusieron.
Dentro de los algoritmos o de la computación evolutiva se ha comprobado que el algoritmo
de evolución diferencial, es un algoritmo que es fácil de implementar, que produce
buenos resultados y que ha tenido muchos éxitos para el diseño en ingeniería. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
MODELADO DINÁMICO DE UN MANIPULADOR
PARALELO DE TRES GRADOS DE LIBERTAD
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
PRESENTA:
EDGAR MOYA SÁNCHEZ
DIRECTORES:
DR. GABRIEL SEPÚLVEDA CERVANTES
DR. EDGAR ALFREDO PORTILLA FLORES
MEXICO, D.F. ENERO 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>OPTIMIZACIÓN DE PROCESOS DE ATENCIÓN A FUGAS HIDRÁULICAS PARA MINIMIZAR EL DESPERDICIO DE AGUA Y TIEMPOS DE REPARACIONES
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la actualidad el problema de mejora de servicios de suministro y atención a
desperfectos, juegan un papel importante en los Sistemas Operadores de Agua Potable, ya
que planificar adecuadamente envíos de cuadrillas puede significar considerables ahorros
logísticos y de costos como desperdicios de agua y horas hombre.
Son por estas causas que surge el problema de ruteo, ya que diariamente los Organismos
Operadores de Agua se encuentran con fugas hidráulicas, mismas que generan un
desperdicio de agua potable y como consecuencia, un problema al momento de buscar
cómo atenderlas de manera eficiente y poder trasladar empleados a su reparación.
Este problema consiste en generar rutas para evitar el desperdicio de agua en atención a
la demanda ciudadana como una prioridad, por lo que se espera la solución de los
reportes de fugas a la mayor brevedad posible, así como manejar eficientemente los
recursos humanos y materiales en el organismo.
Por ello, esta investigación se centra en solucionar el problema de atención rápida a las
fugas hidráulicas que hay en una ciudad, disminuyendo las pérdidas de agua que generan
y mejorando el manejo de los recursos que en ellas intervienen, permitiendo minimizar
ciertos factores que ayuden a la empresa a obtener beneficios; estos pueden ser:
minimizar tiempos y maximizar el ahorro de agua, lo cual lleva a obtener menores costos y
por lo tanto obtener beneficios y una mejor calidad de servicio e imagen.
Este tipo de problemas, de manera intrínseca, conllevan a realizar cálculos
computacionales muy elevados para su solución, debido a su intratabilidad es que son
clasificados como problemas de clase NP, de manera específica NP-Hard ya que todo
problema NP se puede reducir a él. Si bien se pueden resolver, sus soluciones no suelen
ser exactas, sino aproximadas.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Realizar mediante análisis combinatorio una optimización de procesos en la programación
de tareas y enrutamiento de vehículos, buscando la disminución en el desperdicio de agua
generado por fugas hidráulicas, presentando un procedimiento alternativo de solución
para su control y atención, minimizando el gasto de traslado de recursos para su
reparación, utilizando como caso de estudio el OOAPAS (Organismo Operador de Agua
Potable Alcantarillado y Saneamiento de la ciudad de Morelia, Michoacán).

Objetivos específicos
 Recopilar la información necesaria para atender el mantenimiento correctivo que
realiza el organismo para identificar y estructurar los datos que se integrarán en el
sistema.
 Hacer un análisis de la información que permita realizar un programa cuyos
resultados de decisión minimicen el tiempo de atención a las fugas hidráulicas para
evitar los desperdicios excesivos de agua, utilizando el promedio de desperdicio de
agua de cada una de las fugas hidráulicas y la cantidad de horas estimadas para su
reparación.
 Estudiar la aplicabilidad, ventajas y desventajas de metaheurísticas para la solución
de los problemas de ruteo y programación de tareas, para seleccionar la adecuada
para este trabajo con base en la información recopilada y analizada.
 Utilizar algoritmos y técnicas de programación acordes a la toma de decisiones que
se requerirá como procedimiento alternativo de solución para el sistema que se
implementará.
 Seleccionar una ruta efectiva para recorridos de cuadrillas de reparación, con base
en las distancias que hay entre los puntos en donde se encuentran las fugas
hidráulicas de la ciudad y los sectores en los que se encuentra dividida y efectuar
un control de fugas hidráulicas a través de un manejo adecuado de recursos
materiales y recursos humanos, empleando los algoritmos seleccionados.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Existe una gran variedad de programas especializados tales como los sistemas SCADA
(Supervisory Control and Data Acquisition) que permiten el trabajo de las estaciones que
operan bajo el control de los organismos operadores de agua potable, sin embargo el
control se limita sólo al nivel operativo de las mismas. 

La detección de fallas en pozos y tuberías, en el caso del OOAPAS se basa en inspecciones
de rutina diarias por parte de los encargados del área de producción. Siempre que son
detectadas fugas o que se necesitan hacer mantenimientos obligados, los empleados
encargados de las inspecciones emiten notificaciones a través de un canal de radio hacia
las oficinas de producción, ahí son registradas las fallas por quien se encuentra en esa
terminal, enseguida son pasados los reportes a los encargados de mantenimiento que a su
vez asignan los problemas que necesiten arreglar. Otra forma de emisión de fallas, es la
realizada por los habitantes de la ciudad, a través de líneas telefónicas para que, una vez
que cualquier usuario detecte una falla, pueda notificar al centro de atención a clientes
con que cuenta el organismo.
El OOAPAS tiene como política atender todos los reportes de fuga en un plazo no mayor a
24 horas, cabe señalar que el proceso interno para el control de fugas resulta ineficiente,
provocando así tiempos grandes para su atención y reparación, habiendo ocasiones que
se pasan por alto las fugas hasta una segunda notificación del problema, es por ello, que
se justifica esta investigación y considera al OOAPAS como su caso de estudio. </Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los problemas planteados en este trabajo son fundamentales para cualquier empresa
involucrada con el trazado de rutas y traslado de elementos o que requiera de una
programación de actividades con el objetivo de hacer eficientes sus labores, por lo que la
solución propuesta resulta ser una herramienta viable para cualquier organización que se
encuentre inmersa en este contexto.
Se logró obtener la información necesaria para atender el mantenimiento correctivo que
realiza el OOAPAS (Organismo Operador de Agua Potable Alcantarillado y Saneamiento de
Morelia, Michoacán) y así, organizar los datos que se integraron en el sistema.
También, se realizó un estudio comparativo de los algoritmos y técnicas de programación,
orientado a problemas de análisis combinatorio, de manera particular, en instancias del
Problema del Agente Viajero (TSP) y del Problema de la Tardanza Total Ponderada en una
Sola Máquina (SMTWTP), a partir de los resultados obtenidos, se seleccionó la opción más
adecuada que fue: la Optimización basada en Colonias de Hormigas (ACO).
ACO se aplica bien en problemas de optimización combinatoria, en donde el espacio de
soluciones es exponencial, gracias a su adecuación en este trabajo es que se lograron
evitar grandes desperdicios de agua, a través de una programación de actividades
proporcionada por esta técnica y ayudar a disminuir los gastos de traslado de los
empleados a las reparaciones necesarias mediante una ruta de ejecución, demostrando
con esta investigación, que la metaheurística ACO es altamente competitiva para este tipo
de cuestiones.
Se consiguió aportar un procedimiento alternativo de solución para los problemas de
análisis combinatorio, tales como programación de tareas y enrutamiento de vehículos,
sustentado en la metaheurística ACO, técnica en la que a partir del comportamiento
colaborativo de las hormigas, busca entregar soluciones óptimas mediante procesos
iterativos, capaz de encajar como motor en los Sistemas de Toma de Decisiones.
En las evaluaciones de la solución propuesta que se realizaron a instancias ficticias y a
instancias reales siempre se obtuvieron buenos resultados, variando únicamente la
cantidad de hormigas y ciclos a utilizar; en las instancias ficticias, se observó que entre
mayor era el número de ciclos los resultados mejoraban bastante, sin embargo, al hacer
las pruebas en el entorno real no fue necesario el uso de un gran número de iteraciones
debido a que la discrepancia entre los resultados no fue notoria.
Se mostró que para las respuestas dadas por el algoritmo para instancias ficticias cumple
acertando a los resultados óptimos o en su defecto con un porcentaje de error mínimo,
mientras que con datos reales, el análisis generó ahorros en promedio de un 50 % con su
utilización.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Secretaría de Investigación y Posgrado
Centro de Investigación en Computación
OPTIMIZACIÓN DE PROCESOS DE ATENCIÓN A FUGAS
HIDRÁULICAS PARA MINIMIZAR EL DESPERDICIO DE
AGUA Y TIEMPOS DE REPARACIONES
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN
P R E S E N T A
ING. MANUEL MARTÍNEZ ÁLVAREZ
Director de Tesis: M. en C. Sandra Dinora Orantes Jiménez
México, D.F. Mayo 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Seguimiento de una ruta por un
cuadrotor mediante procesamiento
digital de imágenes
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En este trabajo se busca controlar el desplazamiento de un UAV tipo cuadrotor a través
de una ruta que será identificada utilizando únicamente el procesamiento digital de
imágenes, se emplea una plataforma comercial tipo cuadrotor.
En los trabajos del estado del arte, descritos en el capítulo 2, se observa que la mayoría
de los sistemas de navegación empleados se auxilian de una gran variedad de sensores,
entre ellos se encuentran los telémetros ultrasónicos, el sensor Kinect [1] y el GPS. Un sistema de navegación basado en la información proveniente de un GPS está limitado
a la disponibilidad de señal; el sensor Kinect provee una gran cantidad de información
útil, pero a cambio de un peso considerable, lo cual es un gran inconveniente debido a
la capacidad de carga limitada de los cuadrotores. Por su parte el uso de telémetros
ultrasónicos requiere de una gran cantidad de ellos para un desempeño aceptable.
En cambio, una cámara provee información de distintas índoles aplicable en diversas
etapas de un proceso de navegación, por ejemplo, el cálculo de velocidades de
traslación utilizando para ello el flujo óptico, la identificación y el seguimiento de
objetivos o la detección y evasión de obstáculos. Además, las aplicaciones donde se
utilizan UAVs generalmente requieren que el vehículo lleve una videocámara.
El seguimiento de una ruta por un cuadrotor, basado en el procesamiento de imágenes,
requiere en primer lugar desarrollar un programa que funcione como interfaz gráfica
entre un ordenador y el cuadrotor; y que permita, por una parte el envío de comandos
en tiempo real al cuadrotor para controlar su desplazamiento tanto de forma manual
como autónoma, y por otra, la recepción del video y datos de navegación recolectados
por los sensores.
Posteriormente se debe realizar el tratamiento de las imágenes con el fin de detectar la
línea de ruta, la ubicación de la línea en el cuadro de imagen permitirá determinar los
errores de posición y orientación existentes entre la ruta actual y la ruta a seguir. Se
requiere un módulo de detección de líneas robusto, ya que aun en un entorno con
condiciones controladas, al encontrarse el vehículo en movimiento, en la imagen
existirán cambios de iluminación. También se debe prevenir la detección de líneas
ajenas a la ruta a seguir, por ejemplo en pisos de mosaico, o pisos con divisiones de
colores. En la Figura 1 son evidentes los cambios de la iluminación durante el trayecto
a seguir, también se aprecia la existencia de líneas ajenas a la ruta trazada.
Finalmente, se requiere implementar un lazo cerrado de control que utilice los errores
encontrados con la información visual, para calcular los parámetros de desplazamiento
corregidos que permitan el seguimiento de la ruta trazada y que serán enviados al
cuadrotor mediante un comando. Se debe considerar además, la posibilidad de que la
línea salga del campo visual del cuadrotor</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>OBJETIVO GENERAL
Controlar el desplazamiento de un cuadrotor de manera autónoma, sustituyendo a un
operador por medio de la visión de la cámara inferior, utilizando para esto el
procesamiento digital de la imagen recibida de manera remota en un ordenador y
enviando comandos de dirección al vehículo, para que se desplace a través de la ruta
representada por una línea en el piso.
 OBJETIVOS PARTICULARES
 Realizar un programa que permita el envío de comandos al vehículo, así como
la recepción del video y datos de navegación; utilizando los protocolos de
comunicación contenidos en el software de desarrollo del cuadrotor, para
realizar el procesamiento de las imágenes y el control del desplazamiento.
Detectar la línea que representa la ruta a seguir por el cuadrotor, mediante la
aplicación de técnicas de procesamiento digital de imágenes, para determinar
los errores de posición y orientación del cuadrotor con respecto a la línea.
 Implementar un lazo de control cerrado para el desplazamiento del cuadrotor
en el plano xy, utilizando los errores obtenidos visualmente para el cálculo de
los valores de roll y yaw del comando de desplazamiento que se enviará al
vehículo, para regular y mantener los errores al valor mínimo que permita
seguir la ruta sin perderla.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible controlar el desplazamiento autónomo de un cuadrotor a lo largo de una ruta
marcada en el piso, descrita por una línea de color contrastante con respecto al fondo,
al utilizar información visual para determinar la ruta del UAV, dentro de un entorno
cerrado en condiciones de iluminación controlada</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Se aplica un método que permite realizar una misión de navegación autónoma con un
cuadrotor, utilizando únicamente procesamiento de imágenes para definir la ruta de
navegación. Se utiliza una plataforma cuadrotor de modelo comercial de bajo costo.
Este desarrollo permitirá la navegaión sin dependencia de señal de GPS dentro de un
entorno controlado, al mismo tiempo que se utiliza un sensor que suele estar incluido
en este tipo de vehículos, ya que es requerido para sus aplicaciones habituales.
Un sistema de navegación autónomo que funcione en interiores también es útil en el
desarrollo de trabajos de investigación, se pueden probar algoritmos de control
necesarios para una tarea de navegación autónoma provechosa, por ejemplo algoritmos
contra perturbaciones de viento, evasión de obstáculos, detectores de colisiones y vuelo
en formaciones. El desarrollo en una plataforma de bajo costo facilita su aplicación, ya
que el material es fácil de conseguir, es económico y fácilmente reemplazable.
Este es el primer trabajo práctico utilizando un UAV que se desarrolla en el Laboratorio
de Robótica y Mecatrónica del Centro de Investigación en Computación, el laboratorio
ha incursionado recientemente en esta área de investigación y se contempla la
realización de numerosos proyectos afines.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Se utiliza un cuadrotor comercial de bajo costo, que por sus características y
dimensiones, permite realizar pruebas en el mismo laboratorio y la experimentación en
el estacionamiento del centro de investigación. Esta plataforma es regularmente de uso
recreativo, y se puede controlar mediante una aplicación para celulares y tabletas que
se distribuye libremente, sin embargo, el fabricante proporciona un kit de desarrollo de
software (SDK por sus siglas en inglés) para la realización de aplicaciones, compatible
con sistemas operativos de Linux, Windows, iOS y Android.
El programa para la comunicación entre el cuadrotor y el ordenador es una interfaz
gráfica, la cual permite el control manual del vehículo, la puesta en marcha de una
misión de seguimiento, la calibración del segmentado por color y la visualización del
proceso de navegación a través de su cámara. El programa está basado en el SDK
proporcionado por el fabricante del cuadrotor.
En la etapa del procesamiento de imágenes, cuyo objetivo es la detección de la línea de
ruta, se realiza un segmentado basado en el color. Se utiliza una cinta de color
contrastante con el piso para trazar la línea, y por medio de la transformación de la
imagen desde RGB hacia el espacio de colores HSV se consigue filtrar la imagen para
visualizar únicamente el color de interés. Esta transformación brinda al módulo de
visión la robustez para funcionar aun trabajando en un entorno donde el piso tenga
líneas ajenas al trazado de la ruta.
Se utiliza el controlador clásico proporcional derivativo, debido a su robustez y agilidad
demostrada previamente en el control de vehículos aéreos tipo cuadrotor, además de la
sencillez de su implementación.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se controló el desplazamiento autónomo de un cuadrotor, a través de una ruta
representada por una línea en el piso. El alzo cerrado de control que se implementó,
utiliza los errores de posición y orientación del vehículo con respecto a la línea de ruta,
estos errores se obtienen del procesamiento digital de las imágenes adquiridas por la
cámara inferior del cuadrotor. Las imágenes se procesan en un ordenador, que las
recibe de forma remota por medio de una conexión Wi-Fi. Después, el programa
calcula los parámetros de la función de desplazamiento, y se los envía al cuadrotor en
forma de comandos de dirección.
Se programó una interfaz gráfica para la manipulación del cuadrotor desde un
ordenador, permitiendo la comunicación bidireccional entre ambos elementos, de
acuerdo a los protocolos establecidos en el SDK del cuadrotor. El programa permite
enviar comandos de control y configuración al cuadrotor a 30 veces por segundo en
promedio, con ello se puede desplazar al vehículo tanto de forma manual como
autónoma. El programa también recibe el video y los datos de navegación recolectados
por el cuadrotor, el video se muestra en la interfaz gráfica en tiempo real, la recepción de video se realiza a 20 cuadros por segundo en promedio, mientras que la telemetría
se recibe a 15 veces por segundo en promedio.
Se logró la detección de la línea representante de la ruta a seguir, y se obtuvo con ello
el cálculo de los errores de posición y orientación entre el cuadrotor y la línea. El
programa interfaz gráfica recibe las imágenes del cuadrotor, y las procesa digitalmente
con el fin de encontrar los puntos inicial y final de la línea. Se implementó un
segmentado mediante la caracterización del color de la línea en parámetros HSV, con
lo que se dotó al sistema de robustez ante cambios de iluminación.
Se implementó un controlador proporcional derivativo, que utiliza los errores obtenidos
de la información visual, para controlar el desplazamiento del cuadrotor en el plano xy
durante el seguimiento de una ruta. El controlador calcula el valor de los parámetros
roll y yaw, que corrigen los errores de posición y orientación respectivamente, mientras
que los parámetros de pitch y velocidad vertical se mantienen constantes. Después se
envía el comando de desplazamiento con los parámetros actualizados, con lo que se
consigue el seguimiento de la ruta. Durante la experimentación se obtuvo un valor
promedio de error lateral (posición) de 44.5264 píxeles, y un promedio de error de
ángulo de ruta (orientación) de 15.9816°.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Seguimiento de una ruta por un
cuadrotor mediante procesamiento
digital de imágenes
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN CIENCIAS EN INGENIERÍA DE CÓMPUTO CON
OPCIÓN EN SISTEMAS DIGITALES
P R E S E N T A:
Ing. Javier López Rivera
DIRECTORES DE TESIS: Dra. Elsa Rubio Espino
 Dr. Juan Humberto Sossa Azuela
MÉXICO, D.F. Julio de 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Análisis de Propagación de Fibras Ópticas</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Determinar la distribución de campo electromagnético en una fibra óptica unimodo de
doble revestimiento con una geometría de adelgazamiento que permita mantener las
condiciones de adiabaticidad es decir que no permita que exista conversión de modos.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Obtener la Distribución de Campo Electromagnético en fibras ópticas unimodo
adelgazadas de doble revestimiento basado en el análisis de propagación modal.

Objetivos Particulares
o Resolver la ecuación de eigenvalor correspondiente a una fibra óptica unimodo
para determinar la forma de Distribución de Campo.
o Demostrar gráficamente mediante la aproximación gaussiana de Marcuse, la
forma de distribución de campo para el modo fundamental de una fibra de índice
escalonado con frecuencia normalizada V=2.405
o Resolver la ecuación de eigenvalor correspondiente a una fibra óptica de doble
revestimiento para determinar la forma de distribución de campo con una
herramienta de simulación desarrollada en el lenguaje de programación de
Matlab.
o Determinar la distribución de campo a lo largo de la fibra adelgazada de doble
revestimiento y obtener el valor de la constante de propagación modal así como el
índice efectivo de refracción modal.
o Determinar la geometría de adelgazamiento de fibras ópticas unimodo en reportes
existentes en la literatura técnica y determinar si se cumplen las condiciones de
adiabaticidad para evitar conversión de modos.
o Determinar la distribución de campo con un taper experimental realizado en el
laboratorio de fibra óptica del Centro de Investigación e Innovación Tecnológica
(CIITEC) y determinar si se cumplen las condiciones de adiabaticidad para evitar
conversión de modos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Existe una amplia gama de dispositivos basados en fibras ópticas unimodo que
complementan los sistemas de telecomunicaciones y sensores como son acopladores de
fibras ópticas, rejillas de difracción, etc. en particular se tiene un dispositivo muy básico
al adelgazar una fibra óptica unimodo. Un “taper” de fibra óptica es esencialmente una
guía de onda cónica que puede funcionar como una lente o una forma de accesar más
fácilmente a la distribución de campo electromagnético o haz de luz que va en el núcleo
de la fibra. Al poder interactuar con este haz se puede tener una amplia gama de
aplicaciones, que se pueden agrupar en 5 áreas: Óptica de Campo Cercano y Guía de
onda, Optomecánica, Plasmónica, Óptica No Lineal y Óptica Cuántica y Atómica. </Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El objetivo de esta tesis fue el de obtener la distribución de campo electromagnético en
una fibra óptica adelgazada de doble revestimiento basado en el análisis de propagación
modal. Este se logró tomando como base la resolución de un conjunto de ecuaciones ya
existentes que se conocen como “ecuaciones de eigenvalor” para una fibra óptica de tres
niveles de índice de refracción, siendo la aportación principal el desarrollo de una
herramienta de simulación que permite diseñar la geometría de la fibra adelgazada para
un dispositivo de baja pérdida y predecir la forma y extensión del campo, lo que a su vez
permita el diseño de sensores de campo evanescente, por ejemplo.
Las principales conclusiones del trabajo se exponen a continuación:
1. En el caso de una fibra óptica no adelgazada estándar en operación unimodal (ej.
Frecuencia normalizada V= 2.25), la solución de la ecuación de eigenvalor posee
una solución única que se visualiza graficando el lado izquierdo (LI) y el lado
derecho (LD) de la ecuación de eigenvalor y se observa un solo cruce de dichas
gráficas.
2. Al graficar la distribución de campo (eléctrico o magnético) del modo fundamental
polarizado linealmente LP01 en una fibra óptica no adelgazada estándar, se
comprueba que existe continuidad del campo contenido en el núcleo y el campo
contenido en el revestimiento (campo evanescente).
3. Al resolver la ecuación de eigenvalor en una fibra óptica con un núcleo
relativamente grande (ej. 11.2 μm de diámetro, operando a una longitud de onda
de 0.633 μm), la gráfica LI and LD presenta cuatro cruces que corresponden a los
modos LP01, LP02, LP03 y LP04.
4. La distribución de campo del modo fundamental LP01 en una fibra óptica no
adelgazada estándar en el régimen multimodal, (ej. 11.2 μm) no cumple la
aproximación gaussiana.
5. En el caso de una fibra óptica de doble revestimiento, la solución de la ecuación
de eigenvalor se debe buscar en dos ecuaciones: ecuación 1) cuando la constante
de propagación es mayor a n2k0, es decir, cuando la mayor parte del campo está concentrada en el núcleo de la fibra y ecuación 2) cuando la constante de
propagación es menor a n2k0, es decir cuando la mayor parte del campo se
concentra en el primer revestimiento.
6. Al calcular la distribución de campo (eléctrico o magnético) en una fibra óptica
adelgazada de doble revestimiento, se observa en las gráficas como la
disminución del diámetro del núcleo, disminuye a su vez la cantidad de energía
que se transfiere al primer revestimiento y es limitada por el segundo
revestimiento.
7. Existe dos casos importantes de análisis para fibras adelgazadas de doble
revestimiento, 1) cuando el segundo revestimiento es un vidrio de un índice de
refracción ligeramente menor al del sílice fundido, o un líquido de un índice de
refracción ligeramente menor; 2) cuando el segundo revestimiento es el aire
(índice de refracción = 1). En el primer caso, se puede utilizar la teoría de análisis
de propagación de guiamiento débil (n1~n2~n3) lo que conduce a la aproximación
de modos polarizados linealmente (LP), en el segundo caso, la diferencia de
índices entre el primer y segundo revestimiento es muy alta por lo que se debe
utilizar la ecuación de eigenvalor sin aproximaciones, es decir, se utilizan los
modos exactos (TE, TM, TEM, EH y HE).
8. Se observó que en algunos casos, la ecuación de eigenvalor se indetermina en
algunos puntos y en lugar de “ceros”, tiene “polos” o singularidades. Es
importante que se considere en el programa de cómputo que esto puede ocurrir.
9. El criterio de adiabaticidad compara la tasa de variación de la geometría de un
taper, es decir su ángulo local, con un ángulo crítico, determinado por la diferencia
de constantes de propagación del modo fundamental LP01 y el siguiente modo
circular simétrico LP02. La herramienta de simulación desarrollada permite calcular
estas dos constantes, al resolver los dos primeros cruces o “ceros” de la ecuación
de eigenvalor y por tanto predecir si la geometría de una fibra óptica adelgazada
es adiabática o no, lo anterior es importante para el desarrollo de sensores por
campo evanescente, lo que significa que existen pocas perdidas (idealmente
atenuación de 0dB). Se encontró que una forma de detectar visualmente la
solución de la ecuación de eigenvalor es comparar la gráfica del lado izquierdo con
la gráfica del lado derecho de la ecuación de eigenvalor.
.Una aplicación inmediata de este trabajo es predecir si un taper es adiabático o
no, a partir de su geometría obtenida de su imagen en el microscopio óptico. En
esta tesis se analizó un taper adiabático reportado en la literatura y un taper no
adiabático fabricado en el CIITEC.
11. Este trabajo constituye una herramienta de simulación auxiliar en el diseño de
dispositivos basados en fibras ópticas adelgazadas o microfibras ópticas.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>I N S T I T U T O P O L I T É C N I C O
N A C I O N A L
Centro de Investigación e Innovación Tecnológica
Síntesis y Caracterización de Micro-barras de
Óxido de Zinc por Precipitación Asistida por
Calentamiento usando Nitrato de Zinc
TESIS
QUE PARA OBTENER EL TÍTULO DE INGENIERO EN
METALURGIA Y MATERIALES
PRESENTA:
JUAN MIGUEL INFANTE ORTIZ
DIRECTOR DE TESIS:
DR. SEBASTIÁN DÍAZ DE LA TORRE
MÉXICO D.F. 2011
Análisis de Propagación de Fibras Ópticas
Adelgazadas Unimodo de Doble Revestimiento
Tesis que para obtener el grado de
Maestría en Tecnología Avanzada
Presenta
Ing. Dalia Isabel Cisneros Chablé
Directores de Tesis:
Dr. Fernando Martínez Piñón
Dr. Lelio de la Cruz May
México D.F. </Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Adelgazamiento de fibra óptica monomodo de sílice y
su caracterización espectral</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Caracterizar espectralmente las fibras ópticas monomodo adelgazadas en el laboratorio,
las cuales se encuentran en un intervalo de 3 a 10 µm de diámetro de cintura, con longitudes de
hasta 110 mm. Así como caracterizar otros atributos de las mismas como es su geometría y
atenuación
Adelgazar fibra óptica de 3 a 10 µm de cintura y longitudes de 10 a 110 mm
 Controlar la longitud de estirado de la fibra con una precisión de 5 ??.
 Monitorear y medir de la potencia óptica de salida de 0 a -70 dBm (1 mW a 0.1 nW).
 Caracterizar espectralmente (potencia óptica entre longitud de onda ???
??
) las fibras ópticas
antes y después del adelgazamiento.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Existe una amplia gama de dispositivos basados en fibras ópticas unimodo que
complementan los sistemas de telecomunicaciones y sensores como son acopladores de fibras ópticas, rejillas de difracción, etc., en particular se tiene un dispositivo muy básico al adelgazar
una fibra óptica monomodo. En la Figura 1.1 se ilustra un esquema tipo “árbol” que muestra
diferentes aplicaciones de las fibras ópticas adelgazadas.
Un taper de fibra óptica es esencialmente una guía de onda cónica que puede funcionar
como una lente autoalineada o una forma de acceder más fácilmente a la distribución de campo
electromagnético o haz de luz que va en el núcleo de la fibra. Al poder interactuar con este haz
se puede tener una amplia gama de aplicaciones, que se pueden agrupar en 5 áreas: Óptica de
Campo Cercano y Guía de onda, Optomecánica, Plasmónica, Óptica No Lineal y Óptica
Cuántica y Atómica</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Las conclusiones principales de este trabajo de tesis se presentan a continuación.
1. Se demostró que es posible fabricar fibras ópticas adelgazadas monomodo adiabáticas
utilizando el método de miniflama por combustión de gases con baja atenuación en el
orden de 0.1 a 1 dB para longitudes de hasta 20 mm y cinturas de adelgazamiento entre
18 y 3 µm.
2. La temperatura de la flama para fabricación de fibras ópticas adelgazadas se determinó
experimentalmente en el rango de 600°C a 900°C para fibras ópticas adelgazadas
flexibles.
3. La velocidad de estirado utilizada en el proceso de adelgazamiento estuvo en el rango
de 30 a 60 ??
?
permite la fabricación de tapers a temperaturas de 600 a 900°C sin que las
fibras se rompan.
4. Se comprobó que la técnica de “flama viajera” o “flame brushing” [9] es adecuada para
la fabricación de tapers de longitudes largas y flexibles.
5. Del sistema de control electrónico basado en microcontrolador se puede concluir que
este fue adecuado utilizando 3 microcontroladores PIC 16F877A, uno por cada etapa de
traslación debido a que es de fácil programación (se programó en lenguaje ensamblador),
es de bajo costo, es suficientemente rápido con un oscilador de cristal de 4 MHz y existe
mucha documentación disponible.
6. El uso de un diodo superluminiscente como fuente de luz permite inyectar a la fibra
varias longitudes de onda lo que es una forma eficiente de excitar con diferentes
longitudes de onda el experimento y así obtener una respuesta espectral en forma rápida.
Además de que la potencia óptica acoplada al pequeño núcleo (8 µm) de una fibra óptica
monomodo es suficiente para realizar trabajo experimental con una buena relación señal
a ruido.
7. El uso de la empalmadora de fusión por arco eléctrico permite fibras ópticas adelgazadas
de longitud muy pequeña (menor a 1 mm) lo que puede ser útil en algunas aplicaciones.
Del método de estirado vertical por gravedad se concluye que se tiene poco control sobre
el estiramiento debido a que se trata de un proceso no lineal. Al mantener un peso
(fuerza) constante, el esfuerzo (fuerza/area) producido se incrementa exponencialmente
al disminuir el área de aplicación de la fuerza en forma inversamente proporcional al
cuadrado del área.
9. Del método de estirado horizontal por bobina de inducción se concluye que nuevamente
por ser un proceso no lineal, para evitar que la fibra se rompa se requiere un control no
lineal de la corriente aplicada a las bobinas de inducción.
10. Del método de estirado horizontal con horno eléctrico de resistencias de Niquel-Cromo
se requiere reducir el tamaño del prototipo empleado y aumentar la corriente aplicada (a
aproximadamente 100 Amperes).
11. La respuesta espectral de un taper adiabático aun siendo de baja perdida en todo el rango
de longitudes de onda, presenta atenuación diferenciada para cada una de las longitudes
de onda debido a que cada longitud de onda “ve” su propia guía de onda. Aunado a la
anterior, la guía de onda cambia en función de la distancia de acuerdo a la geometría del
taper.
12. Un taper no adiabático por definición puede presentar una atenuación muy alta, por
ejemplo 20 dB. Sin embargo, cuando se observa la respuesta espectral esta puede
presentar múltiples picos de atenuación con una tasa de extinción mayor a 10 dB. Esto
puede deberse a “batido” o interferencia del modo fundamental con el siguiente modo
de propagación lo cual también, a su vez, puede tener aplicaciones muy interesantes
como filtro o sensor.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
Centro de Investigación e Innovación Tecnológica
(CIITEC)
Tesis
Adelgazamiento de fibra óptica monomodo de sílice y
su caracterización espectral
Que para obtener el grado de
Maestro en Tecnología Avanzada
Presenta
Ing. Sigifredo Marrujo García
Director de Tesis: Dr. Fernando Martínez Piñón
México D.F. Julio 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Metodología heurística y autómatas celulares para la clasificación de patrones</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>
Actualmente el manejo de información tiende a incrementarse, y los tipos de patrones
tienden a ser de varias clases y con muchos atributos, la necesidad de tener un algoritmo que
pueda clasificar con buena precisión sin importar el tipo de atributo y una metodología que
lo haga eficiente.
En esta tesis se resuelve el siguiente problema: ¿Es posible clasificar un universo de
patrones con AC y AG de manera eficiente y eficaz, sin importar el tipo de patrón de tal
forma que el costo computacional sea un problema tratable?
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General 
Diseñar e implementar un clasificador de patrones eficiente y eficaz basado en los conceptos de autómatas celulares y algoritmos genéticos.
Objetivo Particulares Diseñar un algoritmo genético para la creación del clasificador de dos etapas DS-DV.? Diseñar algoritmos para: la creación de la población inicial, la función de evaluación, cruce y mutación de un individuo del AG para la etapa de entrenamiento del clasificador DS-DV.? Desear el clasificador MCJJ2.? Diseñar un AG que obtenga un DV para cada nodo del árbol de decisiones del clasificador MCJJ2.? Diseñar un algoritmo para la creación de un árbol de decisiones binario.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>	El diseño de clasificadores eficaces y eficientes es un problema actual y relevante, dada la gran cantidad de datos que se generan hoy en día, por ejemplo, las cámaras digitales, los sistemas de video vigilancia, redes sociales, entre otros, generan una gran cantidad de datos que es necesario clasificar de tal forma que se pueda obtener una representación compacta y permita una mejor toma de decisiones. La solución propuesta en este trabajo de tesis tiene el mérito de ser eficiente con un alto nivel de precisión y además combina diferentes herramientas del tipo heurístico tales como: AC y AG. El aporte de este trabajo de tesis es el diseño e implementación del clasificador MCJJ2 para el cual hubo necesidad de desarrollar diferentes algoritmos tanto para el AG como para el ´árbol de decisión, lo que permite o tener respuestas eficientes para su respectiva tarea. También se diseñaron todos los algoritmos necesarios para crear un clasificador DS-DV.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se diseñaron e implementaron dos clasificadores de patrones basados en los conceptos de autómatas celulares de múltiples sumideros (ACMS) y algoritmo genético más la característica de elitismo (AGT), ya que el ACMS posee la propiedad de que los patrones que pertenecen a un mismo sumidero tengan una distancia haming menor que cualquier otro AC lineal, y el algoritmo gen ético AGT mostró los mejores resultados sobre el AGS por su característica de elitismo. El clasificador DS-DV en la etapa de entrenamiento utiliza el sistema AGT-DSDV para crear una población de CSS y Davis (los cuales representan dos ACMS) que puedan realizar la clasificación, al final se selecciona al mejor individuo de esta población. Mientras que el clasificador MCJJ2 crea un DV para cada nodo del ´árbol binario con el sistema AGT-ADV. Aún queda por probar más variantes del AG, ya que no se garantiza que el AGT sea la mejor opción de todas. Se diseñó el algoritmo gen ético AGT-DSDV para la etapa de entrenamiento del clasificador de dos etapas DS-DV. Se crearon los algoritmos necesarios para el AGT del clasificador DS-DV y se diseñó un nuevo clasificador llamado MCJJ2. Se diseñó el algoritmo genético AGT-ADV que obtiene un vector de dependencia para cada nodo del árbol de decisiones del clasificador MCJJ2. Se diseñó un algoritmo CREA-MCJJ2 para la creación de un árbol de decisiones basado en vectores de dependencia.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
Centro de Investigación en Computación
Título de la tesis
Metodología heurística y autómatas celulares
para la clasificación de patrones
Que para obtener el grado de:
Maestría en Ciencias de la Computación.
P R E S E N T A:
Ing. Jaime Jonathan Martínez Chepe
Director de Tesis.
M. en C. Germán Téllez Castillo
AGOSTO 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Algoritmos de enjambre de partículas para
agrupamiento y clasificación de patrones
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El problema de agrupamiento, como ya se mencionó, es una tarea compleja
del reconocimiento de patrones debido a que no se cuenta con una muestra de
supervisión que indique los grupos que se deben formar. Actualmente esta tarea
sigue siendo un problema abierto en el estado del arte y representa un gran reto.
En general, el propósito de este trabajo de tesis consiste en desarrollar variantes
de algoritmos de enjambre de partículas adaptados para resolver problemas
de agrupamiento de patrones, así como incorporarlos a una plataforma o
entorno con el propósito de ayudar a los investigadores a experimentar con la
conVguración de estos algoritmos.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Estudiar las variantes de algoritmos de enjambre de partículas y adaptarlas para
resolver problemas de agrupamiento de patrones.
1.5.2. Objetivos particulares
Desarrollar la plataforma principal del entorno Heuristic Pattern Recognition
Studio (HPRS) para ayudar a los investigadores en el estudio del Enfoque
Heurístico en Reconocimiento de Patrones.
Resolver problemas de agrupamiento de patrones usando el enfoque
heurístico en reconocimiento de patrones.
Incorporar las variantes de algoritmos de PSO estudiados a la plataforma
HPRS.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Dado que los algoritmos de enjambre de partículas han demostrado ser una
excelente técnica para resolver problemas de optimización complejos y al
proponer los agrupamientos como un problema de optimización, se puede
proponer variantes de algoritmos de PSO que resolverán de manera satisfactoria
la estructura Vnal que tendrán los grupos.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El presente trabajo permitirá a los investigadores experimentar con algoritmos
de enjambre de partículas para resolver diferentes problemas de agrupamiento,
así como servir de referencia para la futura implementación de otras técnicas
metaheurísticas con el mismo objetivo. También la plataforma desarrollada en
la presente tesis permitirá la posibilidad de incorporar nuevas metaheurísticas y
con ello el estudio del enfoque heurístico en el reconocimiento de patrones.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>6.1. Sobre los algoritmos de enjambre de partí-
culas
Los algoritmos de enjambre de partículas se han usado satisfactoriamente
para resolver una gran cantidad de problemas dentro del estado del arte, en
especíVco, dentro del área de reconocimiento de patrones se ha usado para
resolver problemas de agrupamiento y clasiVcación de documentos empleando
una combinación con la técnica clásica de agrupamiento K-means.
Sin embargo el problema de agrupamiento puede plantearse como un
problema de optimización desde dos enfoques, uno optimizando la posición de
los centroides o representantes de clase, y optimizando la estructura Vnal del
agrupamiento usando índices de validación de lo agrupamientos para medir la
calidad de estos.
6.2. Sobre las variantes de PSO
A partir del algoritmo básico de PSO se han generado muchas variantes, algunas
de estas enfocadas a resolver ciertos problemas particulares, algunas de estas
versiones también se pueden adaptar para resolver problemas de agrupamiento
de patrones.
Es importante mencionar que los parámetros de conVguración de las
variantes de PSO, en la mayoría de casos, están sujetos al problema que se
está resolviendo. Como se mencionó en la parte experimental del trabajo, los
parámetros utilizados en los algoritmos de PSO también varían de acuerdo al
conjunto de datos que se desea agrupar y a la versión de algoritmo que se esté
utilizando.
Hoy en día surgen variantes de PSO que se combinan con otras heurísticas
con el objetivo de mejorar las soluciones que produce en cada generación y así
ayudar a no quedar atrapados en óptimos locales. También surgen versiones que
intentan mejorar el costo computacional reduciendo el tamaño de la población
y conservando la diversidad de ésta mediante nuevos procedimientos como la
reinicialización de partículas.
6.3. Sobre el PSO y los resultados al resolver
problemas de agrupamiento
Como se observó en los resultados experimentales las variantes de PSO que
se probaron resuelven los problemas de agrupamiento siguiendo el principio
fundamental de clasiVcación, es decir, patrones similares se deben agruparse
en clases similares y patrones diferentes deben agruparse en clases diferentes.
Así, en todos los agrupamientos desarrollados con las variantes de PSO se
observa este fenómeno, lo cual se considera una buena solución al problema de
agrupamiento.
También es importante mencionar que no siempre las heurísticas convergen
en la solución óptima pero si se aproximan a una buena solución en una
cantidad de tiempo razonable. En los resultados de los agrupamientos se nota
que las soluciones son muy parecidas pero existe una pequeña diferencia entre
un agrupamiento y otro, esto es un problema general de todas las heurísticas,
dependiendo de la conVguración inicial y del procedimiento que manejan puede
que lleguen a soluciones subóptimas.
El PSO como técnica para resolver problemas de agrupamiento parece ser,
por los resultados obtenidos, una herramienta potente que debe ser explotada.
Además nos permite explorar otras técnicas con diferentes enfoques a los
que existen en reconocimiento de patrones, o también, podría combinarse con
algoritmos ya existentes con el objetivo de proporcionar resultados de mayo
calidad.
En el Capítulo 5 se muestran los resultados de los agrupamientos obtenidos
con las diferentes variantes de PSO implementadas, como se puede observar, el
algoritmo de PSO global resuelve muy bien el problema de agrupamiento con
todos los conjuntos de datos (ver Sección 5.2).
Observando el caso promedio de las gráVcas de convergencia de las Figuras
5.6 y 5.21 se puede notar que la versión de PSO local genera un mejor valor de
aptitud con respecto a la versión de PSO global al resolver el mismo problema de
agrupamiento, en otros casos el valor de aptitud es similar en ambas versiones.
De igual manera la versión de PSO con factor de inercia variable en la gráVca
de convergencia mostrada en la Figura 5.30 tiene un mejor valor de aptitud que
la versión de PSO global debido a que inicialmente el factor de inercia es muy
grande y se explora más el espacio de búsqueda y al Vnal como se tiene un factor de inercia muy pequeño se explota más el espacio de búsqueda.
Tomando en cuenta los resultados del agrupamiento usando el PSO con
población reducida de la Figura 5.42 se observa que tiene un mejor desempeño
que el algoritmo de PSO global debido a que incorpora una etapa de
reinicialización y de esta manera se tiene más diversidad en la población, dando
oportunidad a explorar mejor el espacio de búsqueda. Sin embargo, mediante la
experimentación se observó que para poblaciones con 2 partículas el algoritmo
de PSO con población reducida no genera buenos resultados (ver Figura 5.43).
Los resultados obtenidos con el algoritmo híbrido de PSO y algoritmo en
escalada muestran que el algoritmo converge más rápido y muestra mejor valor
de aptitud en el caso promedio debido a que internamente el algoritmo de
escalada toma una partícula y la mejora realizando una búsqueda local (ver Figura
5.46). Por último, en los resultados del algoritmo de µP SO, el número de grupos
encontrados para el conjunto data_10_2 fue correcto (ver Figura 5.47).
6.4. Sobre los índices de validación de agrupamientos
El agrupamiento Vnal del conjunto de patrones depende estrictamente del índice
de validación que se use como función de aptitud ya que, aunque persiguen
que las clases sean compactas y estén separadas, miden de diferente manera
la separación y compactación; por ello al utilizar estos índices de validación
se espera que algunos de estos no encuentren el número óptimo de grupos o
agrupen los conjuntos de datos de una forma no esperada.
6.5. Trabajo futuro
Como se mencionó anteriormente, este trabajo de tesis apenas es el comienzo
para lograr un objetivo mayor, por ello aún quedan muchas técnicas
metaheurísticas por estudiar y adaptarlas al problema de agrupamiento. Además,
falta analizar qué sucede al crear algoritmos híbridos y si son mejores para
resolver este tipo de problemas.
Estas son algunas de las técnicas metaheurísticas que se desea implementar
como parte de la plataforma:
Algoritmos Genéticos Celulares.
Evolución Diferencial.
Algoritmos de hormigas.
Falta explorar el área de problemas de testores y cómo resolverlos mediante
cómputo evolutivo. Sería bueno plantear este tipo de problemas también como
un problema de optimización; encontrar la manera de evaluar los testores que
propone el algoritmo evolutivo y de esta manera hallar los posibles testores del
conjunto de patrones</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
Centro de Investigación en Computación
Algoritmos de enjambre de partículas para
agrupamiento y clasificación de patrones
T E S I S
Que para obtener el grado de
Maestro en Ciencias de la Computación
Presenta
Ing. Edgar Alfonso García Martínez
Directores de tesis
Dr. Salvador Godoy Calderón
Dr. Ricardo Barrón Fernández
México, D.F. enero, 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Integración semántica de datos geoespaciales utilizando una ontología de aplicación</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Como se ha mencionado anteriormente, el Internet es actualmente el medio principal por el
cual se localiza información turística alrededor del mundo. Existen diferentes sitios en la Web
ampliamente utilizados para este fin; sin embargo, se trata de sistemas comerciales
independientes entre sí, cuyas consultas se basan en diferentes fuentes de datos,
proporcionando al usuario resultados más o menos satisfactorios pero sin una visión unificada
del servicio turístico.
Para muchos usuarios del turismo electrónico, existe el ideal de contar con sistemas que
combinen en tiempo real diferentes fuentes de información para realizar planeación de viajes,
de tal manera que sea posible resolver consultas de manera óptima, completa y que realmente
responda a las necesidades planteadas en la consulta; es aquí donde las ontologías juegan un
papel preponderante, al modelar una representación y clasificación de datos que abarque una
amplia descripción semántica de la información turística, para un área geográfica determinada,
valiéndose de muchas fuentes de información complementarias a la vez.
El presente trabajo está enfocado en el desarrollo de un sistema Web orientado al sector
hotelero. En esta propuesta se hace uso de una ontología de aplicación que describe el dominio
turístico acorde a la ciudad de Campeche, basada en el diccionario de datos turísticos del
INEGI [14]. Se plantea igualmente el uso de una aplicación Web híbrida para explotar la
información de dicha ontología, trabajando con los servicios de Google Maps para la
visualización de los resultados. Como característica adicional, el modelo persistente de datos
construido a través de la ontología toma como fuentes tanto las instancias existentes en la
ontología como registros provenientes de archivos tipo shapefile, enfocando el trabajo
desarrollado hacia la integración y representación semántica de datos en la Web.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
! Proporcionar una herramienta accesible desde la Web que recupere información
geográfica y descriptiva de diversas fuentes en un contexto turístico, integre y organice
los datos a través de una ontología de aplicación, genere un modelo persistente de
datos sobre el cual se realice la búsqueda solicitada y devuelva los resultados a través
de la interfaz Web, incluyendo la localización geográfica por medio de un mash-up
(Google Maps).
Objetivos particulares
! Desarrollar una aplicación híbrida accesible desde la Web para la recuperación y
visualización de datos geográficos y descriptivos según los requerimientos del usuario
final, utilizando una ontología de aplicación para la integración y organización de los
mismos.
! Diseñar y construir una ontología de aplicación para la representación y descripción de
los datos geográficos del problema planteado con el caso de estudio, de manera que se
promueva la reutilización de las estructuras generadas para ampliaciones posteriores
del mismo o de otros sistemas.
! Generar un modelo persistente de datos a través de la ontología para la organización, la
integración, el manejo y la recuperación de los datos geográficos y descriptivos
requeridos por el usuario, provenientes de fuentes heterogéneas (instancias ontológicas
y shapefiles).
El sistema será implementado en un contexto turístico, tomando como ejemplo
funcional la Ciudad y Puerto de San Francisco de Campeche; cabe señalar que, en esta
ciudad, el turismo representa la principal fuente de ingresos para sus habitantes.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El nacimiento de la Red de Redes como tal fue el resultado de una evolución que desde
entonces no se ha detenido un solo momento. Dicha evolución corresponde a la búsqueda de
los individuos por plasmar de la mejor manera posible el rasgo más característico de la
humanidad en esta forma de comunicación que es la Web: el lenguaje.
Sabemos que el objetivo principal de una red es la de compartir recursos, y el recurso más
importante en Internet es la información. En la búsqueda de lograr un lenguaje compartido
individuo-máquina-individuo, que fuera comprendido, analizado y extendido por nuestro
indispensable intermediario, hemos visto surgir fenómenos conceptualizados como la Web 1.0
[15], que nos maravilla con la capacidad de empoderamiento que proporciona al individuo
común a través de la información al alcance de la mano; posteriormente, la Web 2.0 [15], con
enfoques colaborativos, redes sociales y representaciones más detalladas de la información
gracias al XML [16] y todos los estándares derivados. Asimismo, en este momento como el
principal objetivo de muchas líneas de investigación, la Web Semántica, que pretende
describir, organizar e integrar todos los dominios de interés humano, de manera que puedan
ser compartidos, ampliados y reutilizados, con la ventaja añadida de que la representación
semántica de los datos no sólo pueda ser comprendida por una máquina, sino que nueva
información puede ser inferida a partir de la existente sin necesidad de la intervención
humana.
Al analizar las implicaciones de este paradigma es posible apreciar las ventajas que
representan las ontologías como herramientas de organización del conocimiento en todos los
ámbitos del quehacer humano. Actualmente existen iniciativas en varios campos importantes
tales como la medicina, finanzas, matemáticas, entre otros, para la estandarización de los
conceptos descritos en cada ontología, así como de su utilidad práctica y la colaboración entre
las mismas [17].
Añadido a todo lo anterior, encontramos que una de las características más importantes de
toda ontología es la organización de la información, de tal manera que funcione como una herramienta para la integración de fuentes de datos heterogéneas para búsquedas o consultas
por campo de conocimiento.
Considerando lo anterior, en la presente tesis se propone el uso de un sistema Web capaz de
recuperar datos geográficos y descriptivos de un sitio de interés en el contexto turístico,
utilizando una ontología de aplicación para llevar a cabo el proceso de integración,
organización y recuperación de la información a través de fuentes de datos heterogéneas,
obteniendo de esta manera el máximo beneficio posible de las principales características de
una ontología.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En la presente tesis se ha descrito el desarrollo de una aplicación para la localización de entidades
turísticas a través de Internet, utilizando una ontología para la organización e integración de la
información disponible a partir de fuentes heterogéneas; dicha integración de datos es la parte
medular y la aportación más importante del presente trabajo, ya que el éxito de todo sistema de
consultas recae justamente en la información que éste sea capaz de recuperar, procesar e inferir
para el usuario, por lo cual las ontologías resultan una herramienta clave.
Dicha integración fue realizada a partir de fuentes heterogéneas de datos, a través de los
estándares más utilizados actualmente, teniendo en mente la posibilidad de incrementar en
trabajos futuros tanto sus alcances como las fuentes de datos susceptibles de ser utilizadas.
Para el desarrollo del caso de estudio presentado, se utilizó una ontología de aplicación en el
dominio turístico a partir del diccionario de datos del INEGI; siendo esta institución la que rige
los estándares de trabajo en localización espacial a nivel nacional, se espera que otras líneas de
investigación compartirán esta característica inicial con el presente trabajo, lo cual favorece la
posibilidad del trabajo cooperativo entre diferentes investigaciones e instituciones por igual.
De esta manera, a través de la integración de datos provenientes de fuentes heterogéneas, quedan
cubiertos dos de los principales objetivos planteados al principio de este trabajo, que son la
construcción de una ontología de aplicación para promover la reutilización de las estructuras
generadas de manera semántica, así como la posibilidad de ampliaciones posteriores al mismo o a través de otros sistemas. Los otros dos objetivos propuestos quedan cumplidos igualmente en esta
tesis, y por supuesto abiertos a la extensión y mejora: una aplicación híbrida accesible desde la
Web para la recuperación y visualización de datos geográficos y descriptivos, con el ejemplo
funcional de la ciudad de Campeche.
Así mismo, adicionalmente a los objetivos planteados inicialmente para esta tesis, se implementó
la inferencia de datos a partir de información existente en la ontología, a través de la utilización
de reglas de inferencia especificadas para dicho fin, lo cual presenta un área de oportunidad para
el establecimiento de líneas de investigación afines a lo realizado hasta el momento en el presente
trabajo.
No obstante haber llevado a buen término la conclusión de dichos objetivos, quedan algunos
elementos de la aplicación susceptibles de ser optimizados, los cuales se presentan más adelante
como trabajo futuro.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Computación
Integración semántica de datos
geoespaciales utilizando una ontología de
aplicación
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRA EN CIENCIAS DE LA COMPUTACIÓN
P R E S E N T A
ING. MARISOL DE JESÚS PAREDES REYES
DIRECTOR DE TESIS
DR. MIGUEL JESÚS TORRES RUIZ
México, D.F., Febrero de 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Modelo de recomendación de problemas a realizar para competir en la Olimpiada de Informática</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El problema a resolver en esta tesis es el siguiente:
Dado un conjunto de problemas ?, un conjunto de usuarios ? y una matriz ??? donde se indica
para cada usuario ?∈? que ha enviado una solución al problema ?∈?, la puntuación ?∈[0, 100].
El problema es encontrar la mejor recomendación posible a cada usuario de aquellos problemas a
resolver en un periodo de tiempo que le permitan mejorar su nivel de competencia.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Desarrollar un prototipo capaz de comunicarse con alguno de los sistemas de entrenamiento para
obtener la información de los usuarios para así analizar los datos de los problemas y poder emitir
una secuencia de problemas recomendada.
Para lo cual se evalúan diversos modelos de recomendación para identificar cuales motivan a los
estudiantes a resolverlos (incrementan el número de problemas intentados durante un período de
tiempo) y mejoran sus habilidades (incrementan la dificultad de los problemas que pueden
resolver).
Objetivos particulares
 Utilizar una herramienta de extracción, transformación y carga (ETL por sus siglas en inglés)
que se comunique con un sistema de entrenamiento.
 Generar un módulo encargado de simular el proceso de entrenamiento de varios usuarios.
 Generar un módulo de visualización de parámetros generadas por la simulación.
 Generar y programar 5 modelos de recomendación.
 Comparación de recomendaciones generadas por los diferentes modelos de
recomendación.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los sistemas de entrenamiento han incrementado la cantidad de problemas en forma constante
volviéndose cada vez más complicado tener un orden en el que se les presentan al estudiante.
Esto lleva a la necesidad de una guía para poder elegir cual es la mejor secuencia de problemas que
puede ayudar en el proceso del entrenamiento.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología para la resolución de problemas del Distrito Federal y Estados de México, se basa
en una serie de pasos que buscan asegurar acercarse a la solución del problema evitando la
aparición de errores en etapas cercanas a la conclusión del proceso, ya que estos errores tienden a
ser muy costosos en tiempo. Esto esta metodología es muy similar a los ciclos de vida del desarrollo
del software por lo cual puede adaptar diferentes conceptos de esta área.
A continuación se muestra un diagrama de la metodología utilizados por el Distrito Federal y Estado
de Mexico.
Debido a que el alumno necesita resolver 3 o 4 problemas en un examen de 5 horas es muy
importante medir la velocidad del alumno.
Esta metodología está dividida en 4 Etapas principales:
1) Análisis: Durante esta etapa se busca comprender las características del problema, así como
sus limitantes y forma de interactuar con el evaluador. Se recomienda que el tiempo
otorgado a este paso sea de alrededor 10 min, pero en ocasiones es necesario otorgar más
tiempo para no descuidar el entendimiento. Se subdivide en 3 etapas.
a. Entender: el alumno debe comprender que es lo que se le está pidiendo.
b. Explicar: Es común que se tenga la noción de entender cuando realmente no se
entendió completamente la idea, se recomienda el proceso de explicar a alguien
más (durante un examen imaginarse explicando a alguien más), esto resulta de gran
ayuda para descubrir detalles que no se hayan entendido. Se le pide al alumno que
preste atención en las siguientes características:
i. Problema.
ii. Entrada.
iii. Salida.
c. Casos de prueba: Es muy importante que antes de generar una idea de solución se
comprenda las limitaciones, es por ello, que se recomienda pensar en las siguientes
casos:
i. Mínimo.
ii. Máximo.
iii. Promedio.
iv. Excepciones.
2) Diseño: Esta es la etapa en donde el alumno demuestra sus habilidades de análisis, durante
esta etapa el alumno debe tener la capacidad de generar diferentes soluciones y al mismo
tiempo identificar si esas soluciones son capaces de satisfacer todas las necesidades del
problema.
a. Ideas: Se trata de generar un esquema general de como un problema va a ser
resuelto y una estimación de la complejidad del problema en tiempo y memoria
para determinar si es viable pasar a codificar la solución. En ocasiones la solución
no es obvia por lo tanto es necesario:
i. Buscar otra forma de plantear el problema.
ii. Subdividir los casos.
iii. Identificar similitudes con problemas resueltos previamente.
b. Algoritmos: Se tiene que describir cuales van a ser los pasos de la solución lo
suficientemente detallado para que no exista duda de la posibilidad de implementar
la solución. Se pide poner atención a lo siguiente:
i. Complejidad
1. Medir memoria y velocidad.
2. Optimizar memoria y velocidad.
ii. Seudocódigo
1. Claro.
2. Sencillo.
3. Elegante.
iii. Prueba de escritorio
c. Estructura de datos: Es común que algunos algoritmos requieran la utilización de
alguna estructura de datos que facilite el análisis de los datos es por ello que el
alumno debe identificar que característica deben tener las estructuras de datos
necesarias y para poder hacer esto se recomienda:
i. Dibujar.
ii. Identificar tipo de estructura de datos.
iii. Tener claro el tamaño de la estructura de datos.
3) Codificar: En esta etapa el alumno muestra sus habilidades técnicas para consolidar sus
ideas en código que la maquina pueda verificar, esta es una de las etapas que depende
mucho de la practica previa, ya que esta facilita la transferencia de idea a código.
a. Verificar instrucciones
b. Claridad
i. Comentarios
ii. Anidamientos
4) Pruebas: Esta etapa final sirve para verificar que el código generado cumpla con las
características que se planteó, se recomienda probar con al menos los casos que se idearon
en la sección de análisis.
a. Compilar
b. Verificar
i. Casos
ii. Tiempo
c. Generar código</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este documento se detalló el proceso para la evaluación de los 6 modelos de recomendación
propuesto, para ello se especificó una simulación y las variables que se tomaron en cuenta para
compararlos. La simulación utiliza los datos registrados por la página de Karelotitlán de tal manera
que el proceso tenga características similares a un juez en línea. A partir del análisis realizado se
puede concluir:
 El orden de los problemas puede afectar la motivación de los usuarios: bajo el modelo de
usuarios propuesto, recomendar constantemente problemas que el usuario no puede
resolver lleva a que el usuario desista de continuar y por lo tanto no invierte el tiempo
necesario para mejorar sus habilidades de igual manera proponer problemas de una gran
dificultad pero que se pueden resolver ayudan a que el usuario mejore sus habilidades.
 Se pueden utilizar filtros colaborativo para producir recomendaciones que son mejores que
el azar y una lista otorgada por un experto: Por los resultados obtenidos se puede decir que
los recomendadores basado en inversiones, basado en similitud de usuarios y basado en la
factorización de usuarios presentan un mayor número de problemas intentados y mayor
número de incrementos de nivel que los recomendadores al azar y basado en un experto.
 El recomendador basado en número de inversiones, similitud de usuarios y en la
factorización de matrices, mostraron que pueden incrementar significativamente el nivel de
los usuarios y mantenerlos motivados para que resuelvan más problemas: Estos tres
recomendadores mostraron mejores resultados que el resto de los recomendadores en el
número de problemas intentados y en el número de incrementos de nivel.
 En caso de no contar con un experto para sugerir una secuencia de problemas a resolver, es
posible utilizar otros modelos de recomendación (SVD, Inversiones, Usuarios) que hacen un
trabajo igual o superior: Estos recomendadores fueron probados utilizando el
recomendador al azar cuando no poseían suficiente información, esto muestra que pueden
ser utilizados sin la necesidad de una lista previa.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Computación
Modelo de recomendación de problemas a realizar
para competir en la Olimpiada de Informática
T E S I S
Que para obtener el grado de:
Maestría en Ciencias de la Computación.
P R E S E N T A :
Ing. Rodrigo Ruben Santiago Nieves
Directores de Tesis
Dr. Gilberto Lorenzo Martínez Luna
Dr. Adolfo Guzmán Arenas
SEPTIEMBRE 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema de visualización de la información de tópicos más importantes generados en medios sociales		

</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La dinámica de las noticias publicadas en los medios de información y los medios sociales (RSS,
Facebook, Twitter1
, LinkedIn, Google+, Flicker, YouTube etc.) es una forma de ver el
comportamiento de la sociedad, en ella los usuarios de ambos medios se informan y pueden
realizar una comunicación, mediante la cual se llegan a distintos destinatarios ya sean amigos,
conocidos o personas de interés con las que desean tener un contacto más cercano o
simplemente por accidente se conoce la publicación.
La relación de la información en los medios es utilizando Twitter como red social, dado que se
pueden hacer comentarios en ambos sentidos (a partir de las noticias o de los tweets), la
importancia de la relación puede considerarse en principio ya sea por la frecuencia de las
noticias que aparecen en los tópicos y de igual forma por los tweets. Los tópicos que se
generan de las noticias o tweets, se pueden mantener a través del tiempo, demostrando así
que ellos tienen otra forma de importancia, la cual es soportada por la cantidad de usuarios
que publican información (noticias y tweets) sobre temas (tópicos) que ellos consideran
importantes durante un periodo de tiempo. Con ello se observa que esto es una forma en la
cual los tweets están relacionados con las noticias generadas en medios de información.
Otra forma de medir tópicos importantes es que muchos tweets son publicados por uno o
más seguidores (followers) que hablan sobre el mismo tópico en cuestión, señalando la
dificultad que se tiene al momento de poder interpretar los 140 caracteres con los que cuenta
un comentario en la línea de tiempo de Twitter. 
El presente trabajo de tesis es una alternativa a la detección de los tópicos más importantes
de interés para los usuarios tanto de medios informativos como de redes sociales, aplicando
ramas de la Ciencias de la Computación como:
 Bases de datos de noticias.
 Procesos ETL de Minería de Datos [1] .
 Normalización de texto, Procesamiento del Lenguaje Natural [2].
 Visualización de la Información [3].
Lo cual se describirá en la siguiente sección.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo> Objetivo general.
Construir una herramienta que permita visualizar los tópicos más importantes que existen en
medios de información y sociales, junto con la duración en que prevalecen en estos medios.
1.3 Objetivos particulares.
 Obtener una colección de documentos (noticias) para aplicarles técnicas de
procesamiento del lenguaje natural y separar los términos significativos.
 Aplicar a los términos significativos un modelo de clasificación para generar los któpicos.
 Definir los tópicos más importantes, junto con un modelo de similitud para hallar
tópicos similares.
 Diseñar el software que permita capturar los parámetros para hallar los tópicos más
importantes durante periodos de tiempo establecidos por el usuario y un valor de
similitud de interés.
 Diseñar una visualización que permita observar el comportamiento de los tópicos más
importantes de la colección adquirida y procesada, así como, porqué ciertos tópicos
de diferentes periodos son similares.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Detectar los tópicos más importantes en medios sociales es de gran utilidad debido a su
velocidad de propagación (sin validar su verosimilitud), y que de manera casi inmediata se da
a conocer el surgimiento de problemáticas en diferentes campos de la ciencia, tecnología,
dispersión de enfermedades o algún problema existente en el país, además pueden ser
detectados en medios informativos. El conocimiento de estos tópicos puede ayudar a evitar
seguir utilizando tecnología con problemas, productos que pueden ser nocivos al ser humano,
realizar o evitar actividades que puedan afectar la economía de un país, entre otros. Por este
motivo surge la idea de localizar los tópicos importantes y mantener informadas a las
personas que estén interesadas en ellos. Basta recordar problemas tecnológicos o de otra
índole, por ejemplo:
 Fallas en automóviles con frenos dañados [4].
 iPhones que se doblan [5].
 Posibles casos de Ébola en Norteamérica [6].
 Influenza H1N1 [7].
 Delitos frecuentes en algunas áreas del Distrito Federal y Estado de México,
operativos en Tláhuac-Chalco [8].
Para dar una definición de cómo identificar los tópicos más importantes, es utilizando un nivel
de importancia definido (cantidad de noticias por tópico), una posible forma de visualizar los
tópicos y su composición (es mediante las palabras que generaron el tópico, las cuales son
escritas tanto por los usuarios de Twitter como los generadores de las noticias). La
visualización debe ayudar a determinar fácilmente lo anterior mencionado. El ejercicio iniciara
con noticias como: entretenimiento, política, ciencia, tecnología u otro tema en particular.
Utilizando Twitter como un impacto en medios sociales (periódicos, noticieros, etc.), se da
un soporte más amplio a los tópicos que son generados por las palabras en dichos medios,
por lo cual muchos de los usuarios publican comentarios con relación a dicha noticia, pero
aun así no siempre se da continuidad a las noticias y por dicha razón un tópico interesante se
llega a perder en el paso del tiempo o hasta la llegada de una noticia sobre él mismo tema de
interés.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Identificar los tópicos que pueden ser importantes en el ámbito de la ciencia, la computación,
la tecnología, los espectáculos y la nación. Haciendo hincapié en las palabras más importantes
que aparecen dentro de un tópico y con ellas determinar dentro de alguna visualización
porque un tópico logra ser más importante en un periodo de tiempo que otro gracias a la
utilización de un modelo conocido como LDA.
Las noticias son una fuente constante de información, que puede ser de un volumen difícil de
procesar por un ser humano o simplemente no se tiene la oportunidad de examinar la
totalidad de dicha información. Por lo tanto fue de utilidad realizar una herramienta que
faciliten a los lectores o usuarios de medios sociales el análisis de temas importantes o de
interés sobre sucesos que ocurren en su entorno, y mantenerse al tanto de los hechos
actuales que suceden en el país.
Las nubes de palabras muestran en un gran contexto cómo se comporta el medio social
dentro de un determinado momento del tiempo, con las palabras se puede determinar si la
existencia de un tópico contiene ciertas palabras en común más frecuentes y poder
interpretar su importancia conforme a las palabras de mayor tamaño que aparecen dentro
de la nube.
Las nubes de palabras sirven de ayuda en la importancia de tópicos, pero no sirve para
interpretar una visualización textual sobre palabras semejantes, debido a que la nube
contendrá palabras similares de distintos tamaños que no son fácilmente identificables por el
ojo humano al comparar dos nubes que pertenece a tópicos similares de periodos distintos.
La ventaja que proporciona el sistema de visualización de la información de tópicos más
importantes generados en medios sociales, no solamente es determinar la importancia de un
tópico, sino también el comportamiento que se da al encontrar tópicos semejantes en
distintos periodos de tiempo, pudiendo determinar cómo es que el tópico envejece con el
cambio de las palabras que se encuentran dentro de él.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Computación
“Sistema de visualización de la información de
tópicos más importantes generados en medios
sociales”

T E S I S
PARA OBTENER EL GRADO DE:
MAESTRÍA EN CIENCIAS DE LA COMPUTACIÓN
P R E S E N T A
EL ING. MAURICIO IVÁN GUERRERO HERNÁNDEZ


DIRECTORES DE TESIS:
Dr. Adolfo Guzmán Arenas
Dr. Gilberto Lorenzo Martínez Luna


México, D.F. Agosto 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>IMPLEMENTACIÓN DE UNA ESTRATEGIA ACTIVA
COMPLEMENTADA CON TIC PARA ENSEÑANZA DE
CIRCUITOS ELÉCTRICOS EN NIVEL BACHILLERATO</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Una vez presentado lo anterior e identificando las oportunidades de desarrollo e
investigación se propone el siguiente objetivo para este trabajo:
Diseñar, implementar y comparar dos secuencias didácticas para el aprendizaje de
circuitos eléctricos a nivel preparatoria, usando tres herramientas activas diferentes:
experimentos demostrativos, simulaciones interactivas y tutores inteligentes. Evaluar el
impacto de las secuencias didácticas tanto de manera cuantitativa como cualitativa,
utilizando las siguientes herramientas estadísticas: Ganancia Normalizada, Factor de
Concentración y Diferencial Semántico.
1.1.1 Objetivos Específicos
1. Seleccionar las actividades experimentales a utilizar en la secuencia didáctica.
2. Seleccionar las simulaciones que se usarán en la secuencia didáctica.
3. Desarrollar ambas secuencias didáctica, teniendo cuidado que las actividades
sean semejantes y solo cambien la herramienta.
4. Seleccionar y adaptar las herramientas de evaluación.
5. Aplicar las secuencia.
6. Analizar los resultados obtenidos. </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cuáles conceptos (Resistencia, Voltaje, Corriente, Circuitos conectados en
serie, en paralelo y mixtos), se comprenden mejor por los estudiantes al usar
simulaciones interactivas?
</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Cuáles conceptos (Resistencia, Voltaje, Corriente, Circuitos conectados en
serie, en paralelo y mixtos), se comprenden mejor por los estudiantes al usar
experimentos demostrativos?
</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3> ¿Qué herramienta activa (simulaciones interactivas o experimentos
demostrativos) ayuda más a una mejor comprensión conceptual de circuitos
eléctricos a nivel preparatoria? </Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4> ¿Qué herramienta activa (simulaciones interactivas o experimentos
demostrativos) facilita el entendimiento del modelado matemático del
fenómeno?
</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿De qué forma los tutores inteligentes mejoran la habilidad de los estudiantes en
la resolución de problemas de circuitos eléctricos?</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Las hipótesis de cada una de las preguntas de investigación se enuncian como sigue:
 El emplear herramientas computacionales como las simulaciones, tiene un alto
impacto para el aprendizaje de manera visual; de igual manera, las simulaciones
se han empleado para visualizar fenómenos que no son fáciles de ver a simple
vista (Esquembre, 2009). Por lo tanto, el uso de las simulaciones permitirán una
mejor comprensión de todos los conceptos involucrados dentro del tópico de
circuitos eléctricos.
 Los Experimentos Demostrativos han reportado un alto impacto en el
aprendizaje de conceptos de Física, ya que permiten que los estudiantes
interactúen con el fenómeno físico y permiten cuantificar los valores reales. La
ventaja de los experimentos demostrativos refuerzan de manera positiva el
aprendizaje de conceptos físicos (Orlaineta, 2011).
 Ambas metodologías tendrán coeficientes de ganancia conceptual normalizada
altos y semejantes, mostrando que en general ambas herramientas activas son eficaces, pero los conceptos que involucran los circuitos eléctricos como voltaje,
corriente y resistencia eléctrica no tendrán el mismo nivel de entendimiento. Las
simulaciones interactivas favorezcan la comprensión los conceptos de circuitos
eléctricos debido a que:
-Se puede observar el fenómeno desde diferentes perspectivas
-Se permite un mayor número de experiencias en menor tiempo.
-El usuario puede manipular las variables dependientes de manera más
controlada que en un experimento
-Se pueden visualizar elementos del fenómeno que no es posible a simple vista.
-Los estudiantes pueden continuar usándolo en casa, complementando la
enseñanza.
 De acuerdo a Podolefsky, et al. (2014), las simulaciones interactivas ayudan al
estudiante a entender los conceptos pero también a relacionarlos con su ecuación
correspondiente, debido a que en las simulaciones interactivas se tiene una
manipulación directa con las variables independientes y se observa un cambio en
las dependientes. A pesar de que también se pueden manipular variables en los
experimentos demostrativos y ver su consecuencias, en las simulaciones se hace
de manera más controlada y se permite una manipulación sobre los “límites” de
la ecuación, por ejemplo, tener un cable con resistencia cero, conectar una
cantidad grande de focos en paralelo, tener una fuente de voltaje muy grande,
etc. que en experimentos demostrativos puede ser caro o peligroso.
 Los comentarios típicos de los estudiantes con dificultades en la resolución de
problemas es que ellos requieren un mayor tiempo de asesoría con el docente
para la identificación de datos en el ejercicio, selección de fórmulas, despejes,
operaciones e incluso en la interpretación del resultado. Los tutores inteligentes
guían a los alumnos que lo requieren específicamente en lo que ellos necesitan,
mediante textos en la computadora al momento de resolver los ejercicios.
Además de que los tutores inteligentes están programados para seguir un
procedimiento de resolución eficaz, que ayudara a los alumnos que realizan
pasos innecesarios y comenten errores durante el proceso de resolución. Por
tanto, los tutores inteligentes no solo ayuda a alumnos que en un inicio no
podían resolver los ejercicios, sino que también ayuda a hacer eficaces a los
alumnos que saben realizar el ejercicios pero su procedimiento no era el más adecuado. Y para los alumnos que no requieren instrucción extra para la
resolución de ejercicios, no les proporcionara asesoramiento. </Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Todos los que estamos involucrados de una u otra forma dentro del área de física
educativa tenemos la motivación de mejorar el aprendizaje de conceptos científicos y el modelado matemático de los fenómenos, es decir, todos queremos que nuestros
estudiantes aprendan física. La forma en la nos acercamos a este objetivo es diferente,
debido a nuestros intereses, personalidades, vivencias e incluso herramientas
disponibles. Consideramos que la tecnología no es un lujo en nuestra sociedad actual, es
una necesidad. Creemos que las herramientas tecnologías, como la computadora y el
internet, tienen dos funciones dentro de la educación: uno es el enseñar a los
estudiantes a usarlas responsablemente y no solo como un medio para estar en contacto
y socializar o de sólo entretención, y el otro es mejorar la enseñanza, aprendizaje de
contenidos, en este caso de Física. Las herramientas tecnologías deben usarse desde
estas dos perspectivas siempre, y no solo enfocarse en una de ellas al planear una
estrategia didáctica. Por ello, la propuesta metodológica proponemos en este trabajo de
tesis involucra el uso de algunos software educativos con el fin de que los estudiantes
vean en los sistemas de cómputo un medio para aprender, y que puedan seguir
conociendo las herramientas propuestas desde sus hogares a través del internet, pues no
están limitadas para circuitos eléctricos, sino también para otros temas de física y de
otras áreas científicas como química, motivándolos al estudio y el autoaprendizaje,
aprendiendo a usar las herramientas tecnológicas para su beneficio.
Otra razón que motiva esta investigación es desde la perspectiva docente. Cada vez más
docentes cambian su dinámica tradicional de docencia por una activa. Pero en
ocasiones, al tratar de buscar la herramienta adecuada para abordar cada tema, pueden
encontrarse con un mar de información que puede llegar a confundirlos o frustrarlos.
Además, encontrar la herramienta adecuada para un tema es solo uno de los pasos de la
planeación docente, el diseño de toda la secuencia didáctica que realmente use el
potencial de la herramienta puede llegar a ser la parte más difícil del quehacer docente.
Por ello, aprovechamos este trabajo de investigación para desglosar en lo más posible la
secuencia didáctica usada, así como los fundamentos teóricos de la misma, para mostrar
que los experimentos demostrativos no son las únicas herramientas activas que existen,
ya que se tienen otras que son comparables, en cuanto a cumplimiento de objetivos y
forma de implementación. </Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los resultados de la ganancia conceptual de los cuatro grupos es semejante, con
un promedio de 0.4, valor dentro de los límites de Hake para la enseñanza
activa. Pero al analizar la eficacia de las herramientas por medio del Factor de
Concentración, se observó una ventaja para los grupos que usaron simulaciones
interactivas, pues tres de las doce preguntas se ubicaron en la región HH y ocho
de las doce se ubicaron en la región MM en las gráficas S-C del post-test,
mientras que los grupos de experimentos una mayor cantidad de preguntas (5 y
6) permanecieron en la región ML, como se tenían antes de la instrucción. Por lo
que se puede decir que ambas herramientas didácticas, simulaciones y
experimentos demostrativos, insertadas en una secuencia activa similar tiene una
eficacia similar para abordar los temas de circuitos eléctricos, pero por poca
diferencia, con el uso de simulaciones se obtienen mejores resultados.
2. En el análisis de la ganancia conceptual pregunta a pregunta de los resultados
del post-test, se observó que la dinámica y perfil de los grupos hizo que en
algunos tópicos las ganancias conceptuales obtenidas no fueran iguales, aun
usando la misma herramienta didáctica. En general el grupo EXP1 obtuvo
mejores resultados que el grupo EXP2, debido a que el primer grupo era más
rápido para realizar las actividades planteadas por la secuencia e invertía el
tiempo en indagar más en el tópico, realizando una mayor cantidad de
experimentos. En el caso de los grupos que usaron simulaciones el grupo SIM1
obtuvo mejores resultados que los grupos SIM2, ya que el grupo SIM2
realizaba preguntas extras a las planteadas en la secuencia y estas preguntas
específicas, relacionadas con el tema, hacia cambios de casos y contextos de
aplicación del concepto durante las sesiones, que si había alumnos que se
distraían brevemente durante la sesión, podían perderse en las explicaciones y
manipulaciones de la simulación. Además se observó que en este grupo, los
alumnos se sentían más intimidados al pasar frente al grupo para manipular la
simulación.
En el análisis pregunta a pregunta para definir que conceptos eran mejor
entendidos con cada una de las herramientas, se encontró que las simulaciones
son más eficaces en la mayoría de los conceptos, pero como son herramientas
usadas siguiendo una secuencia didáctica semejante, estas herramientas pueden
combinarse para obtener mejores resultados en el futuro. Por ejemplo, una
secuencia didáctica basada en simulaciones, que además haga uso de
experimentos en los tópicos de conexión de circuitos básicos y la comparación
de la intensidad luminosa de los focos conectados en forma mixta, podría
mejorar más el entendimiento de los conceptos por parte de los estudiantes, qu el
solo el usar una herramienta durante toda las instrucción.
4. En los test de retención se obtuvieron ganancias conceptuales promedio
semejantes, por lo tanto de igual forma ambas herramientas activas son
equivalentes para que los alumnos mantengan la información durante más
tiempo.
5. En los diferenciales semánticos para evaluar la actitud de los alumnos ante las
herramientas utilizadas, se observó que los alumnos que usaron experimentos
quedaron conformes con la herramienta utilizada, pues en todas las
características evaluadas se obtuvo una calificación positiva y con valor
numérico alto. El único indicador bajo fue el de ahorro del tiempo, por el tiempo
de la sesión que se invertía en preparar el material, checarlo, arreglarlo en
algunos casos, y el tiempo que algunos equipos tenían que esperar para que el
resto de sus compañeros terminaran la actividad.
6. En la opinión de los alumnos respecto al uso de las simulaciones interactivas
también se obtuvieron calificaciones positivas, pero con un valor menor que las
que recibieron los experimentos demostrativos. Por tanto, a los alumnos les
gusta un poco más trabajar con experimentos demostrativos que con
simulaciones interactivas.
7. Existen características en el diferencial semántico que fueron mejor evaluadas
por un grupo que uso simulaciones que por el otro, esto significa que la
apreciación de los grupos hacia esta herramienta es diferentes. Las
características con los cambios más notorios fueron las de Fácil-Difícil,
Incomprensible-Comprensible y Ahorra tiempo-Consume tiempo, tiene una
diferencia de casi un punto entre ambos grupos. Esto puede deberse a la
dinámica propia de cada grupo, que como ya se había dicho, el grupo SIM2 cuestionaba mucho, mientras que le grupo SIM1 se limitaba a seguir las
instrucciones dadas por el docente.
8. En la apreciación de los alumnos de los tutores inteligentes, solo la característica
de Ahorro de tiempo fue calificada negativamente. Esto se debe a que la
instrucción fue pasar los tutores de la computadora a la libreta y contestarlos a la
par, lo que los estudiantes consideraron como hacer dos veces la misma
actividad y más tardado que hacer una actividad como normalmente se hace,
pasándolo del libro a la libreta. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Secretaria de Investigación y Posgrado
Centro de Investigación en Ciencia Aplicada y Tecnología Avanzada
Unidad Legaria
Posgrado en Física Educativa
IMPLEMENTACIÓN DE UNA ESTRATEGIA ACTIVA
COMPLEMENTADA CON TIC PARA ENSEÑANZA DE
CIRCUITOS ELÉCTRICOS EN NIVEL BACHILLERATO
T E S I S
Que para obtener el grado de:
Maestra en Ciencias en Física Educativa
Presenta:
Lic. Diana Berenice López Tavares
Directores de Tesis:
Dr. Ricardo García Salcedo
Dr. Daniel Sánchez Guzmán
Enero, 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>OPTIMIZACIÓN DE MODELOS POBLACIONALES NO
LINEALES DE EFECTOS MIXTOS MEDIANTE CÓMPUTO
EVOLUTIVO</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo> Objetivo general
Desarrollar en base a un caso particular, un modelo farmacocinético poblacional de efectos
mixtos contrastando métodos estadísticos con técnicas metaheuristica como el algoritmo
genético para la selección de las mejores covariables predictoras.
1.1.1 Objetivos específicos
 Definir parámetros específicos de modelo como combinaciones lineales de los efectos
fijos y los efectos aleatorios.
 Establecer el estimador de los parámetros del modelo farmacocinético poblacional.
 Obtener la farmacocinética poblacional de Tobramicina en una población de 78
individuos utilizando el software matemático MATLAB.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Aprovechando la estructura de los métodos estadísticos para la selección de covariables en un
modelo farmacocinético poblacional, se determinará la importancia del uso del cómputo
evolutivo para la selección del mejor conjunto de covariables en un modelo farmacocinetico
poblacional para reducir el esfuerzo que requiere seleccionar las covariables, estimar los
parámetros farmacocinéticos a un menor costo al obtener las covariables realmente
significativas, y optimizar la calidad de las estimaciones de los parámetros farmacocinéticos
poblacionales. El estudio se realizará con Tobramicina el cual es un antibiótico, de estrecho
margen terapéutico, utilizado para infecciones leves (oftálmicas) y graves (neumonía). Se
selecciona este fármaco ya que previos estudios concluyen que es un fármaco que tiene poca
absorción en el tracto gastrointestinal, y por vía intravenosa su absorción es rápida y completa.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Existe pocos estudios comparativos relacionados a las estrategias utilizadas por los
investigadores del área farmacéutica para la selección de covariables que deben incluirse en el
desarrollo de un modelo farmacocinético poblacional. Los pocos estudios que comparan los
diferentes métodos de selección de covariables están relacionados con métodos estadísticos, y
en muchas ocasiones se llega a la conclusión de que la implementación de estos métodos no
representa una mejora en la precisión del modelo farmacocinético poblacional. Por lo tanto, los
analistas concluyen que ningún método es superior a otro, por lo que en ocasiones, sugieren
hacer uso del conocimiento empírico que tiene el analista en relación al fármaco en estudio; lo
que tampoco garantiza buenos resultados [1]. Estas circunstancias ponen de manifiesto la
importancia de continuar con la búsqueda de diferentes estrategias para la selección del mejor
conjunto de covariables, como por ejemplo, las técnicas de cómputo evolutivo.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En este capítulo se presenta la metodología empleada en la creación de un método de
selección de covariables para un modelo farmacocinético poblacional de efectos mixtos
utilizando un algoritmo genético, y en la de un modelo un modelo farmacocinético poblacional
de efectos mixtos utilizando una base de datos de individuos que fueron tratados con
Tobramicina, para lo cual, fue necesario elaborar un código en MATLAB. Finalmente se
presenta un panorama general de cómo se realizaron los experimentos expuestos en este
trabajo.
Se establecen 4 experimentos relacionados al efecto que tienen las covariables en un modelo
farmacocinético poblacional.
En el experimento 1 se elabora el modelo farmacocinético poblacional de efectos mixtos con
ausencia de covariables. En este primera experimento se optó por no incluir las covariables
debido a que posteriormente se va a analizar el efecto en la variabilidad del modelo
farmacocinético poblacional al momento de incorporar covariables.
Para el experimento 2 se incorporan las covariables: HT, AGE, BSA, IBW para definir el
modelo que caracteriza el parámetro Cl (Aclaramiento) dentro del modelo farmacocinético
poblacional de efectos mixtos sin utilizar métodos previos de selección de covariables, con el
propósito de tener una medida de comparación respecto a los experimentos donde se utilizan
métodos de selección de covariables.
Con respecto al experimento 3, se realiza la selección de covariables para definir el modelo que
caracteriza el parámetro Cl utilizando el método estadístico de regresión por pasos (stepwise)
dentro del modelo farmacocinético poblacional de efectos mixtos, esto con la finalidad de
evaluar y comparar el desempeño de este método respecto a los otros experimentos.
Por último, se tiene el experimento 4, donde se muestra el desempeño del algoritmo genético
para la selección de covariables en comparación con el método estadístico de regresión por
pasos, y evaluar qué tanto se optimiza la calidad de las estimaciones de los parámetros
farmacocinéticos poblacionales</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Uno de los objetivos más importantes dentro de los modelos farmacocinéticos poblacionales es
el de identificar las covariables que explican la variabilidad de los efectos producidos por la
exposición a un fármaco que es influenciado por alguna característica fisiológica de un
individuo.
El tener conocimiento previo de las relaciones que guardan ciertas características de un
individuo en relación a un parámetro farmacocinético no es suficiente cuando se trata de
desarrollar un modelo farmacocinético poblacional de efectos mixtos donde, la precisión del
modelo, es determinante. Esto se debe, a que el conocimiento previo de las características
cinéticas de un fármaco no son garantía para identificar a simple vista los problemas que
pueden surgir al utilizar indistintas covariables de manera conjunta, como por ejemplo
problemas de colinealidad, o la obtención de la sobreparametrización de los modelos
farmacocinéticos poblacionales. Por estas razones, los analistas recurren a utilizar métodos
estadísticos para la identificación de las covariables que tengan un carácter relevante dentro
del modelo farmacocinético poblacional. Sin embargo no se ha llegado a la conclusión sobre
que método estadístico para la selección de covariables es el más efectivo, y ni si quiera los
analistas pueden afirmar que se obtienen los resultados óptimos.
La principal finalidad de este trabajo constituyo en plantear, desarrollar y utilizar un algoritmo
genético en MATLAB para la selección de un conjunto de covariables que aporten un mayor
grado predicción de la variabilidad dentro de un modelo farmacocinético poblacional no lineal
de efectos mixtos. La comparación de los resultados obtenidos que se presentan en la sección
4.5 de este trabajo expone la superioridad del algoritmo genético respecto al método
estadístico de regresión por pasos.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL

MAESTRÍA EN CIENCIAS EN SISTEMAS DIGITALES
“OPTIMIZACIÓN DE MODELOS POBLACIONALES NO
LINEALES DE EFECTOS MIXTOS MEDIANTE CÓMPUTO
EVOLUTIVO”
T E S I S
QUE PARA OBTENER EL GRADO DE
M A E S T R O E N C I E N C I A S EN SISTEMAS DIGITALES
PRESENTA
ING. CARLOS ABEL SEPÚLVEDA ÁLVAREZ
BAJO LA DIRECCIÓN DE
DR. OSCAR HUMBERTO MONTIEL ROSS
DR. JOSÉ MANUEL CORNEJO BRAVO
JULIO 2014 TIJUANA, B.C., MÉXICO</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DISEÑO E IMPLEMENTACION EN UN FPGA DE MODELOS DE AMPLIFICADORES DE POTENCIA PARA RF USANDO SISTEMAS NEURODIFUSOS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la actualidad, las aplicaciones de radiocomunicaciones demandan un ancho de banda de las
señales moduladas se hace más amplio; donde los PA de RF se han convertido en una seria
limitante en el diseño de cualquier sistema de RF debido a su naturaleza no lineal. El creciente
uso en la aplicación de técnicas de linealización, y la aparición del procesamiento digital de
alta velocidad han permitido aplicar mecanismos de corrección sobre la señal de entrada del
PA, tales como la técnica de predistorsión digital (DPD). Esto representa un cambio
importante de paradigma en el diseño de estos componentes, puesto que ofrece ventajas en el
proceso de diseño enfocándose principalmente en la eficiencia y la potencia, sin tomar en
cuenta las limitaciones puestas por las estrictas especificaciones de linealidad.
Los efectos producidos por las no linealidades en los PAs repercuten de dos maneras,
por un lado causan recrecimiento espectral, dando lugar a interferencias en los canales de
transmisión adyacentes (distorsión fuera de banda de operación asignada), mientras que por
otro lado, causan distorsión dentro de la propia banda de transmisión, degradando por tanto la
Tasa de Error de Bits (TEB) [9].
Los organismos reguladores fijan a través de los diferentes estándares de
comunicaciones, los niveles máximos permitidos de emisión fuera de banda, así como de los
límites de distorsión en la banda, por ejemplo especificando porcentajes máximos de error en las constelaciones. Las sanciones por violar cualquiera de las especificaciones como son
potencia de transmisión, emisión fuera de banda entre otras son multas elevadas.
El análisis matemático de un sistema de linealización de un PA consiste en modelar con
precisión las características de no linealidad del dispositivo. La mayoría de los modelos
actuales se basan en los efectos de memoria en los PA,s reales, los cuales se manifiestan
debido a los efectos térmicos y constantes de tiempo grandes en la red de polarización. Esto
significa que las curvas características de distorsión AM-AM y AM-PM no permanecen en un
estado estático, sino que varían en función de los valores anteriores. Por lo que es necesario un
modelo que considere las perturbaciones ocasionadas por estos fenómenos. En la actualidad,
muchos tipos de modelos con memoria del PA han sido reportados, tales como las series de
Volterra, Wiener, Modelo Polinomial con Memoria, Redes Neuronales y el modelo de Sistema
de Inferencia Neurodifuso Adaptable [2-3].
La eficiencia en el consumo de energía requerida por el PA es un factor importante en
los sistemas de transmisión inalámbricos. Este componente representa una etapa esencial por
el consumo de energía que demanda en cualquier sistema de comunicación; donde la pérdida
en eficiencia se atribuye fundamentalmente con las no linealidades presentes en la etapa del
PA. De manera que, es necesario establecer un compromiso entre linealidad y eficiencia
energética durante la etapa de fabricación; la linealidad se convierte así en una consideración
importante cuando se trata del diseño de cualquier sistema de comunicación en general.
Las Series de Volterra pueden ser usadas para describir un sistema estable no lineal con
pérdida de memoria (fading memory), con un error arbitrario pequeño. Sin embargo, las
desventajas principales son el incremento dramático del número de parámetros con respecto al
orden de no linealidad y longitud de memoria lo cual causa un incremento drástico de la
complejidad de cálculo e identificación de parámetros. Esta es la razón por la que resulta
altamente impráctico el uso de Series de Volterra para representar matemáticamente el
comportamiento de sistemas con alta no linealidad y profundidad de memoria.
La implementación de este tipo de modelos es realizable en simulación por balance
armónico, pero su extracción a través de mediciones sigue siendo una labor impráctica. Lo
anterior se debe a que primeramente, es necesario conocer las relaciones de fase entre las
líneas espectrales de frecuencias diferentes, lo cual requiere una plataforma de caracterización
temporal calibrada en amplitud y en fase para obtener la medición de las curvas de distorsión
AM-AM y AM-PM [4-6]. En segunda, los modelos con memoria (por ejemplo, los basados en
Series de Volterra y Redes Neuronales) pueden implementarse en un entorno de simulación
basado en Matlab/Simulink, pero se debe programar un sistema que permita optimizar los
resultados a través de la modificación de los parámetros y del código.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de esta tesis de maestría consiste en realizar contribuciones al estado del
arte en el modelado y diseño de circuitos amplificadores de potencia, empleando modelos de
redes neuronales y sistemas neurodifusos basados en modelos de PA tanto estáticos como
dinámicos y que sean implementados en un FPGA y explotados eficientemente en una
plataforma de evaluación circuito-sistema.
Los objetivos específicos que se persiguen en este trabajo de investigación son:
 Estudio bibliográfico del estado del arte existente sobre el modelado del
comportamiento de los PAs empleando sistemas neurodifusos.
 Estudio de los sistemas neurodifusos y familiarización con el entorno de simulación
Matlab/Simulink, así como también con la herramienta DSP-Builder que se utilizará
para el desarrollo del trabajo de tesis.
 Simulación de ejemplos de Sistemas Neurodifusos usando técnicas empleadas en la
literatura con parámetros de alta precisión y bajo costo computacional.
 Realizar la implementación de una red neuronal adaptativa en FPGA y darle el
aprendizaje requerido para el modelado del comportamiento del PA.
 Implementación de un modelo neurodifuso en FPGA usando DSP-Builder.
 Análisis y comparación de los resultados obtenidos con la arquitectura construida
comparando aspectos de rapidez computacional, consumo en memoria y recursos
lógicos utilizados.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La implementación de nuevos esquemas de modulación obliga al diseñador a no dejar de
considerar el comportamiento no lineal y la profundidad de memoria del PA, cuyas
consecuencias son demasiado relevantes para ser ignoradas. La linealidad se convierte así en un requerimiento importante cuando se trata de la fabricación de cualquier sistema de
comunicación en general.
Por otro lado, los estándares de comunicación actuales (IEEE 802.11b/g/n, IEEE
802.16, ETSI HiperLAN-2, UMTS) apuestan por una alta eficiencia espectral en canales con
anchos de banda considerables, utilizando modulaciones multinivel, multiportadora o ambas
(M-QAM, π/4 DQPSK, WCDMA, OFDM). Dichas modulaciones son muy sensibles a las no
linealidades de los PAs y puesto que presentan envolventes con gran relación potencia pico a
potencia promedio requieren altos niveles óptimos de potencia para poder operar en una
región lo más lineal posible, afectando de este modo la eficiencia del PA.
La importancia de este trabajo de investigación radica en el hecho que el modelado del
PA permite al diseñador de RF implementar la señal de predistorsión adecuada para llevar a
cabo la compensación y linealizar el dispositivo. Implementar entonces en un dispositivo
FPGA un modelo que emule el comportamiento del PA se convierte en una herramienta
potente que aporta al diseñador e investigador una plataforma de evaluación circuito-sistema
que permite probar tanto el comportamiento de la eficiencia del PA, como el resultado de la
introducción de señales de predistorsión al PA buscando la linealización de éste.
Dadas las características de los FPGAs (Arreglo de compuertas lógicas programables),
el diseño de cualquier arquitectura que sea implementada en estos dispositivos representa una
solución que debe ser capaz de emular el comportamiento de un PA determinado. Dicha
emulación debe ser capaz de cumplir con ciertas especificaciones críticas como son las
restricciones temporales en función de la respuesta. Para emular físicamente la respuesta del
PA se requiere un modelo que brinde rapidez computacional, lo que descarta entonces
modelos que implican costo computacional como son Series de Volterra, Modelo Polinomial
con Memoria, Saleh, Ghorbani. Otro de los parámetros a tomar en cuenta es la precisión en los
valores de la señal de respuesta del modelo implementado correspondiente al PA.
Los sistemas neurodifusos, entre los que se encuentran las Redes Neuronales
Artificiales (RNA) y los Sistemas Adaptativos de Inferencia Neurodifusa (ANFIS), brindan
ciertas características como lo son su estructura paralela y su ventaja de ser adaptativos a
cualquier modelo a base de ejemplos de entrenamiento. Estas características proporcionan
ventajas que pueden ser aprovechadas en el diseño de una arquitectura donde se busca cumplir
con los requerimientos de tiempo de respuesta y de precisión en el diseño e implementación de
un modelo en FPGA que emule físicamente el comportamiento del PA en tiempo real y con un
nivel de precisión aceptable.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se mostraron los resultados más interesantes de la respuesta de la arquitectura ANFIS
diseñada, se compararon las respuestas de las simulaciones con las implementaciones y el
MPM. Se introdujeron señales distintas en la entrada de la arquitectura y se configuró el MPM
con distintos valores de no linealidad y profundidad de memoria. Se muestra la respuesta de la
arquitectura ante una señal modulada en AM con una frecuencia de 2 MHz en la señal Se mostraron los resultados más interesantes de la respuesta de la arquitectura ANFIS
diseñada, se compararon las respuestas de las simulaciones con las implementaciones y el
MPM. Se introdujeron señales distintas en la entrada de la arquitectura y se configuró el MPM
con distintos valores de no linealidad y profundidad de memoria. Se muestra la respuesta de la
arquitectura ante una señal modulada en AM con una frecuencia de 2 MHz en la señal </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN Y DESARROLLO DE
TECNOLOGÍA DIGITAL
“DISEÑO E IMPLEMENTACION EN UN FPGA DE MODELOS DE
AMPLIFICADORES DE POTENCIA PARA RF USANDO
SISTEMAS NEURODIFUSOS”
TESIS
QUE PARA OBTENER EL GRADO DE MAESTRO EN CIENCIAS
PRESENTA:
ING. JESÚS AARÓN SILLAS LUNA
BAJO LA DIRECCIÓN DE:
DR. JOSÉ CRUZ NÚÑEZ PÉREZ
DR. J. APOLINAR REYNOSO HERNÁNDEZ
DICIEMBRE 2014 TIJUANA B.C </Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>
Identificación de sistemas físicos mediante redes neuronales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Aun cuando una persona no conozca todas las causas detrás de un evento, es capaz de deducir que comparte ciertas características con alguna experiencia pasada, por lo que puede esperar resultados similares. Si el resultado real del evento resulta diferente a lo que la persona estimo, entonces agrega tal experiencia a su conocimiento; gradualmente el hombre refuerza sus conocimientos día a día, evitando repetir errores causados por las mismas razones. Hasta cierto punto una persona aprende a reconocer riesgos o características de ciertos fenómenos y prevenir sin entender totalmente el origen. Por ejemplo, los síntomas de ciertas enfermedades como la gripa son bien conocidos y nos ayudan a identificar el estado de salud, a pesar de no ser expertos en el tema como un médico. En el estudio de sistemas ya sea por análisis o control, es común toparse con obstáculos en la realización de modelos, como la falta de datos que nos impiden reconocer la causa de ciertas fallas o de razones por las cuales una estimación falla. Hay veces donde es posible compensar parcialmente esta falta de datos con más experimentos, sin embargo, la naturaleza es compleja e impredecible; por mucha información que se tenga, no se podrá tomar medida de todos los factores que se involucran en un sistema. Cuando se estudia un sistema como el clima regional para realizar un modelo con los datos recopilados 50 años atrás con consideraciones como la presión atmosférica, corrientes de viento, y temperatura de ese entonces, se pudiera estimar un modelo con las consideraciones que eran válidas hace cincuenta años. Sin embargo, las condiciones pueden ser totalmente diferentes después de tanto tiempo por razones como el calentamiento global o la contaminación, alterando todo el comportamiento del clima y el diseño del sistema. La naturaleza es cambiante y no es posible pensar que se puede crear una formula matemática que pueda producir todos los resultados esperados de un sistema. Hay que recordar que un modelo es solo una aproximación de la realidad, es decir, es una representación ideal de algo real que dista de ´esta por razones que salen de nuestra percepción. Siempre existen elementos no determinísticos en cualquier sistema, algunos incluso son no tan fáciles de percibir por la forma en la que se presentan; todas estas variables en general se consideran una perturbación o ruido, los cuales dificultan el análisis de un proceso o sistema. Cuando se intenta construir un modelo considerando las propiedades estáticas de un fenómeno, es decir todo aquello que no cambia, la perturbación se vuelven un factor importante en la precisión del modelo, ya que son responsables de las variaciones inesperadas del sistema real que el modelo no es capaz de imitar. Cuando se construye el modelo de un proceso, se intenta conocer lo más posible de ´este para poder comprender su funcionamiento; una aproximación común es conocer los principios físicos que participan en la forma en la que un sistema responde. Por ejemplo, saber cómo funciona una locomotora de vapor: El agua es evaporada por calderas, la cual acumula presión la cual ejerce una fuerza, ´esta se transforma en movimiento y por medio de una transmisión, transfiere este movimiento a las ruedas. Por otro lado, no se necesita saber todo lo anterior para poder operar la locomotora, puesto que solo controlando el calor de la caldera se sabe que aumentara la velocidad del vehículo. La diferencia en el tipo de acercamiento para comprender el mismo sistema depende en el conocimiento que se tiene de ´este. Saber sobre el funcionamiento detallado de la locomotora puede ser de poca relevancia para un operador; por otro lado, para un técnico reparador no es suficiente con solo conocer el cuarto de la caldera.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general 
Realizar un análisis cuantitativo y cualitativo de un método clásico basado en ecuaciones de diferencias y otro que usa redes neuronales artificiales.
Objetivo específico 
Diseñar de una plataforma de adquisición de datos para la identificación de sistemas por medio de una tarjeta de adquisición. Esto incluye la programación de la misma para que cumpla su tarea en la planta experimental al momento de la identificación. 
 Desarrollar un programa para la generación de tablas de datos para la identificación de sistemas. El programa operara por medio de una interfaz gráfica y trabajara en conjunto con la tarjeta de adquisición. 
 Diseñar de experimentos que permitan evidenciar las ventajas y desventajas de los métodos comparados. 
 Realizar un análisis cuantitativo de los modelos ARX y las RNA en base a los errores de simulación. 
Realizar un análisis cualitativo de los resultados cuantitativos por medio de tablas y graficas. 
Redactar un documento con la interpretación de los datos adquiridos en los experimentos.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>En la identificación de sistemas existen varios obstáculos que dificultan el modelado de sistemas no lineales, incluyendo perturbaciones o el mismo comportamiento no lineal de una planta. Se espera que el uso de RNR como identificador de sistemas presente un buen desempeño independientemente de las condiciones de los conjuntos de datos debido a las características de robustez de las redes neuronales; por las mismas razones, se cree que su desempeño en los peores casos sea mejor que un modelo lineal de caja negra ARX. Un análisis comparativo entre estos dos métodos podría mostrar las ventajas y desventajas de ambos en un caso práctico de identificación de una planta.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo se presentó un análisis comparativo entre el desempeño de dos métodos de identificación de sistemas: los modelos ARX y Redes neuronales. Por sí solos, estos métodos son muy efectivos si se encuentran en las condiciones adecuadas y decir que uno es mejor que otro sería una afirmación algo subjetiva. Es por esto que el objetivo principal siempre fue realizar el análisis de ambos métodos y observar sus comportamientos, para poder probar sus fortalezas y sus debilidades. Como recién se mencionó, las condiciones con las que se realiza el modelado son importantes, afectan el proceso de la identificación; desafortunadamente, en una tarea de IS, se pueden presentar problemas desde la adquisición de los datos y no siempre es posible controlar las condiciones de los experimentos para maximizar su eficacia. Se realizó una plataforma de adquisición para poder formar las tablas de datos requeridos en los cálculos para la estimación de los parámetros de los modelos o redes. Se eligieron con cuidado los datos que se utilizaron en la identificación y se tuvo mucho cuidado en la adquisición para que los datos fueran lo más descriptibles del sistema, sin errores de medición. La adquisición de los datos se realizó de manera controlada y de diferentes maneras para satisfacer las necesidades de los experimentos que se diseñaron con atención a diferentes capacidades de los modelos o redes. Como parte de las pruebas, se pudieron notar complicaciones en la identificación con algunos conjuntos de datos que presentaban perturbaciones, pero que finalmente eran parte de la dinámica que describe al sistema.
Se diseñaron e implementaron tres experimentos para poder comparar los métodos; como se utilizaron diferentes datos en cada uno, se pudo confirmar que los resultados obtenidos en cada prueba eran coherentes. Los experimentos diseñados aprovecharon las varias formas y tamaños de los conjuntos de datos para poder probar diferentes características como: la capacidad de extrapolar o generalizar datos, la robustez del modelo o red ante el ruido, comportamiento ante las estimaciones con demasiados datos y la identificación ante diferentes frecuencias de muestreo. Los experimentos muestran diferentes niveles de desempeño con niveles desde cerca del 50 Ambos métodos (ARX y RNR) de identificación utilizaron diferentes algoritmos de optimización de parámetros o de pesos sin ‘apticos, debido a las características del modelo y lo común de la combinación. Los modelos ARX comúnmente utilizan métodos de regresión de mínimos cuadrados para el ajuste de sus parámetros; es un algoritmo muy usado con estos modelos por sus buenos resultados en modelos lineales en comparación con su bajo consumo computacional. Las redes neuronales utilizaron el algoritmo LM por su rápida convergencia y también por la frecuencia de su uso en el entrenamiento de redes. Al final los algoritmos de optimización jugaron un papel importante e influyeron el desempeño de los modelos, en algunos casos fueron la razón de malos resultados, sobre todo en el caso de las redes neuronales. Los modelos ARX tienen una estructura lineal sencilla con una cantidad de parámetros finita. Calcular sus parámetros por medio de un algoritmo de mínimos cuadrados es muy común, debido a que es muy sencillo y requiere de solo un cálculo que además es relativamente rápido. Esto significa que su uso es muy apropiado para aplicaciones donde se disponga de pocos recursos computacionales como por ejemplo en sistemas con microcontroladores. El cálculo de sus parámetros por LS siempre ajusta los parámetros del modelo ARX y lo aproxima hacia el mínimo global, minimizando el error residual (e = y–ˆy). El ajuste de los parámetros mejora conforme se utilizan más datos en el proceso de optimización del modelo, pero incluso con pocos datos, el modelo puede llegar a extrapolar las respuestas siempre y cuando el sistema modelado no se aleje de un comportamiento lineal. Debido a su estructura lineal un modelo ARX tiende a decaer en su desempeño tratando de aproximar sistemas con no linealidades, aunque se puede aproximar dependiendo del tipo de no linealidades. El modelo ARX es muy vulnerable al ruido y altera la estimación de los parámetros, debido a que más datos ruidosos dentro de la regresión lineal para optimizar parámetros pueden aumentar la varianza del modelo. Los modelos ARX comparten el mismo polinomio para describir la dinámica del sistema y la del ruido, lo que limita la flexibilidad del modelo en algunos casos; si se intentara modelar un sistema con perturbaciones que tengan una dinámica independiente a las señales de entrada. Las redes neuronales pueden tener estructuras muy sencillas con pocas neuronas que hacen su computo muy ligero o pueden ser más complejas con un mayor número de neuronas y aumentar la precisión de su identificación en algunos casos, en cambio de una mayor cantidad de cálculos paralelos. Una RNR aprende mejor con una mayor cantidad de datos de entrenamiento que cubra todo el espectro de los valores de entrada posibles para poder generalizar respuestas. Las RN tienen robustez contra ruido si se entrenaron correctamente; además, con suficientes ejemplos las RN pueden modelar incluso el ruido junto con toda la dinámica del sistema. Las RN pueden generalizar respuestas incluso ante datos que no conozca pero que se parezcan a otros patrones de entrenamiento. Con pocas neuronas las RNR pueden tener un mejor desempeño que algunos modelos lineales más complejos pero que requieren más cálculos. Las desventajas de las redes neuronales es que necesita un control sobre la cantidad de neuronas necesarias para obtener un buen entrenamiento, de lo contrario puede afectar el desempeño de la identificación; muy pocas neuronas y la red no tiene suficiente flexibilidad para adaptarse, pero demasiadas no se optimizan muy bien o se complica su optimización. Se necesitan suficientes ejemplos para entrenar a una RN, de lo contrario, habrá valores que la RN no podrá reconocer y dar salidas inesperadas. Por el contrario, si existen demasiados datos, la RN puede necesitar muchas iteraciones para poder entrenarse y buscar optimizar los pesos. Con LM la búsqueda por los mínimos es local, por lo que puede fracasar en acercarse al mínimo global y nunca aproximarse en su dirección correctamente. Como resultado del análisis de la comparación entre los dos métodos, se puede decir que para casos sencillos lineales y filtrados (alta relación señal/ruido) donde se dispongan de pocos datos de entrenamiento y no se necesite una precisión muy alta, es suficiente y apropiado un ARX. Sin embargo, cuando se requieran modelar sistemas no lineales con cierta contaminación por perturbaciones y se puedan disponer de suficientes datos, una RN puede manejar el problema. Hay que considerar que los casos de la vida real no siempre son los ideales, por lo que se deben considerar las condiciones en las cuales se realizan los experimentos de adquisición, en especial cuando los modelos se usan en aplicaciones en el mundo real, fuera de las simulaciones.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional 
 Centro de Investigación y Desarrollo de ´ Tecnología Digital 

Maestría en Ciencias en Sistemas Digitales 

Identificación de sistemas físicos mediante redes neuronales Tesis 

Que para obtener el grado de Maestría en ciencias en sistemas digitales 
Presenta: 
Ing. Gula Sarasti Pérez 
Bajo la dirección de: 
Dr. Roberto Sepúlveda Cruz, 
Dr. Oscar H. Montiel Ross 
diciembre 2014 Tijuana, B.C., México</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Implementación de una red neuronal morfológica con procesamiento dendral en un FPGA</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>	La red neuronal con procesamiento central son una de las mejores opciones hoy en día
para la clasificación de patrones, al especificar sus pesos sin ‘apticos usando el algoritmo
descrito en [29]. En [29] y [32], pueden encontrarse ejemplos ilustrativos de la aplicación
de esta red neuronal junto con su algoritmo de entrenamiento en diversas tareas de clase-
fijación de patrones y análisis de imágenes.
El algoritmo reportado en [29] presenta las siguientes características:
Convergencia a una solución en un número finito de iteraciones.
Perfecta clasificación del conjunto de entrenamiento dado.
No existe un traslapado entre los hiper–cubos generados de distintas clases.
No depende del orden en el que se presenten las clases.
No hay ´ares de indecisión en la fase de prueba.
El algoritmo puede ser aplicado a la clasificación de problemas de p clases y n
atributos.
Todas estas características hacen que dicho algoritmo sea muy eficiente para resolver satisfactoriamente
diferentes problemas de clasificación. El principal inconveniente del algoritmo
propuesto en [29] es que crece exponencialmente a medida que el número de dimensiones
incrementa; en una maquina secuencial cuando el número de dimensiones llega a n = 20,
el tiempo del cálculo de los hiper–cubos deja de ser practico.
A continuación, se muestra un ejemplo de lo descrito anteriormente. Se tomó como referencia
un conjunto de datos de entrenamiento de 1000 valores generados de manera
aleatoria, se fueron agregando atributos al conjunto de datos para su entrenamiento, obteniendo
así conjuntos de datos de 1000 valores con n = 2 hasta n = 20 donde n es
el número de dimensiones o atributos. Este ejemplo fue otorgado por los autores de la
referencia citada en [29]
En la figura 1.1, se puede observar que el tiempo de entrenamiento crece de acuerdo al
número de dimensiones que tenga el conjunto de datos, además de que cuando n = 20 el
tiempo de entrenamiento del conjunto de datos tiende a crecer exponencialmente, debido
a que el algoritmo de entrenamiento es de complejidad exponencial.
Con el fin de reducir el tiempo de entrenamiento para un problema dado, en este tema de tesis se propone la implementación de una red neuronal morfológica con procesamiento en sus dendritas y su correspondiente algoritmo de entrenamiento en un FPGA, sin pérdida de generalidad para el caso de datos de una y dos dimensiones.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general Implementar una RNMD en un dispositivo FPGA, para resolver problemas de clasificación con p clases para una y dos dimensiones.
Objetivos específicos
 Implementar una RNMD para datos de una y dos dimensiones con p clases en VHDL. Implementar el algoritmo de entrenamiento para este tipo de redes en lenguaje VHDL, reportado en [29]. Probar y verificar la eficiencia y eficacia del algoritmo de entrenamiento implementado, haciendo uso de cuatro conjuntos de datos y comparar resultados con los obtenidos en MATLAB
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La red neuronal morfológica con procesamiento en sus dendritas son una alternativa a las redes neuronales clásicas. Hasta el momento este tipo de redes neuronales se han implementado (de acuerdo a la investigación realizada) en máquinas secuenciales utilizando diversos lenguajes de programación tales como: MATLAB, C, C#, entre otros. Hoy en día una de las corrientes tecnológicas fuertemente impulsadas para la resolución de problemas con alta demanda de recursos, es la utilización de los dispositivos lógicos programables. Basado en lo anterior, se propone realizar por primera vez una implementación en hardware de este tipo de redes, haciendo uso de un dispositivo FPGA.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La conclusión obtenida a partir de los objetivos planteados fueron las siguientes: 1. Se logró demostrar que es posible implementar en una arquitectura tipo FPGA una RNMD y su algoritmo de entrenamiento, compuesta de una sola neurona y K dendritas para el caso de p clases de 1 y 2 dimensiones. 2. Al verificar los resultados obtenidos en la FPGA con los obtenidos en la implementación de MATLAB se comprobó su equivalencia por simulación. 3. Con los experimentos realizados se pudo verificar la obtención de resultados plenamente satisfactorios en el entrenamiento de los mismos, con una eficacia superior al 90 % en todos ellos y con tiempos que van entre 10 y 200 µs.

En este trabajo se logró la implementación exitosa de la Red Neuronal Morfológica con Procesamiento en sus Dendritas(RNMD) hasta para dos dimensiones, además del entrenamiento de la red dentro de la misma FPGA lo que supuso una serie de problemáticas que se lograron solucionar exitosamente. Este trabajo servir ‘a no solo de base para futuras implementaciones de este tipo de red en FPGA, sino que también es la primera en su tipo. Se demostró en los resultados que es un buen camino para seguir adelante, además de la tendencia mundial del uso de las FPGA como sustitutos del cómputo paralelo, debido a que en muchas ´áreas demuestra mejores resultados y un mayor rendimiento.
Aunque a ‘un queda mucho por hacer en cuanto al desarrollo de las RNMD en dispositivos FPGA, la presente implementación podría servir de base no solo en este campo, también puede ser aplicado directamente a otros objetivos, como por ejemplo el aprendizaje de robots, ya que al tener el entrenamiento y la red en la misma implementación se puede tener una retroalimentación o recalibración continua de la información que se obtenga del ambiente, y así usarlo directamente como entrada del entrenamiento para que en tiempo real reajuste sus pesos y consiga la clasificación correcta de los objetos, tareas o situaciones, que es para lo que está destinada esa red neuronal. Finalmente, las pruebas antes mencionadas escapan al presente trabajo, pero el diseño e implementación propuestos aquí están listos para ser probados en este tipo de implementaciones. Los resultados obtenidos demuestran que la implementación de la red es eficiente y han sido los mismos que los obtenidos en las implementaciones en un entorno como MATLAB.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITECNICO NACIONAL
CENTRO DE INVESTIGACION EN ´ COMPUTACION´
Implementación de una red neuronal morfológica con procesamiento central en un FPGA
TESIS QUE PARA OBTENER EL GRADO DE:
Maestría en Ciencias en Ingeniería de Computo con opción en Sistemas Digitales
P R E S E N T A:
Tonantzin Marcaida Guerrero Velázquez
Directores de tesis:
Dr. Juan Humberto Sosa Azuela
Dr. Herón Molina Lozano
México, D.F. Julio 2015
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Implementación electrónica de sistemas de caos y su sincronización utilizando lógica difusa</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Muchos procesos naturales como lo son la relación entre las neuronas
[Fradkov and Evans, 2005] y la sincronización del corazón y los pulmones
[Imponente, 2004, Schafer et al., 1999] son ejemplos claros que exhiben
comportamientos caóticos; otros casos como lo son la seguridad de información en
servicios [Liñán and de León Morales, 2007] como lo son: los enlaces de comunicación
militar y empresas privadas, transacciones financieras, operaciones comerciales, entre
otros.
La naturaleza de los sistemas caóticos, donde su complejidad e incertidumbre son
característicos, hace posible que el uso de controladores difusos permitan sincronizar
de mejor forma estos sistemas.
La sincronización de señales caóticas ha sido estudiada utilizando diferentes leyes
de control, en particular con el uso de sistemas difusos.
En particular gracias a las ventajas de los sistemas difusos tipo 2 supone una ventaja en la sincronización de sistemas caóticos para lo cual se requiere el desarrollo de pruebas
que permitan evaluar su uso.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Realizar la sincronización de dos sistemas de caos utilizando un sistema de control
con base en lógica difusa tipo 1 y tipo 2.
1.5.1. Objetivos particulares
1. Definir circuitos analogicos que permitan la generación de señales caóticas.
2. Establecer un sistema de control difuso tipo 1 que permita la sincronización de
dos señales caóticas.
3. Analizar los resultados generados por la sincronización del sistema de control
difuso tipo 1.
4. Establecer un sistema de control difuso tipo 2 que permita la sincronización de
dos señales caóticas</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible realizar la sincronización de dos sistemas de caos utilizando un sistema
de lógica tipo 1 o tipo 2 con un error de sincronización reducido.
Tomando en cuenta las características propias de la lógica difusa y en especial de la
lógica difusa tipo-2, donde el manejo de la incertidumbre y su mejor rendimiento para
el trato de sistemas complejos, permitirá llevar a cabo una mejor sincronización de dos
sistemas caóticos en un sistema del tipo maestro-esclavo.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Generalmente cuando se habla de sincronización de sistemas se establece de la
siguiente manera: la salida de un sistema maestro es usada para el control de la
respuesta de un sistema esclavo el cual sigue la salida del sistema maestro, lo cual
recibe el nombre de sistema maestro-esclavo.
Se ha mostrado [Tanaka et al., 1998] que el uso de controladores difusos tipo 2 que
permiten manejar la incertidumbre y tener un mejor rendimiento en sistemas complejos,
permitiría lograr una mejor sincronización de dos sistemas caóticos.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Teniendo en referencia el objetivo general y los objetivos particulares planteados
previamente, se presentan las siguientes conclusiones.
1. Se estableció el sistema de control difuso tipo Takagi-Sugeno con ayuda de las
reglas establecidas por Yau que permite la sincronización de sistemas caóticos
mediante lógica difusa tipo 1, se comprobó su correcto funcionamiento en el
sistema de Lorenz.
2. El modelo de lógica difusa tipo 1 establecido, fue probado en diversas
circunstancias; se modificaron los diferentes parámetros de los modelos caóticos
así como son las condiciones iniciales diferentes en los dos modelos y se agregó una
señal de ruido , se estableció un grupo de funciones de membresía y se evaluó al
modelo con diferentes métodos numéricos. Con lo cual determino que dependiendo
de la cantidad de variables modificadas la función de membresía es clave así como
también el método numérico utilizado para la evaluación del mismo.
3. Tomando en consideración el modelo propuesto para la sicronización de caos
mediante lógica difusa tipo 1, se sustituyó por un controlador difuso tipo 2, se
comprobó el correcto funcionamiento en el sistema de Lorenz.
4. Al igual que en el modelo de lógica difusa tipo 1 el modelo de lógica difusa tipo
2 fue probado en diversas circunstancias y se estableció un grupo de funciones
de membresía tipo 2. En este caso se observa que dependiendo de las cantidad
de variables modifiadas en algunos casos la función de membresía puede ser
simplificada por otra y de igual modo que en al caso del modelo difuso tipo 1 el
método numérico utilizado para su evaluación es importante.
Se observa que el modelo de lógica difusa tipo 1 se comporto de mejor manera
en relación al modelo de lógica difusa tipo 2 en las pruebas donde solo se agrego
la señal de ruido y donde las condiciones iniciales son diferentes, esto puede ser
debido a que las funciones de membresía tipo 1 son mas simples en comparación
a las funciones de membresía tipo 2 donde es característica la incertidumbre,
por otro lado el modelo de lógica difusa tipo 2 tiene un mejor desempeño en las
otras pruebas ya que en estas circunstancias los sistemas caóticos presentan mayor
cantidad de discrepancias entre si.
6. Se establecieron una serie de circuitos analógicos capaces de crear señales de caos
y se comprobó su funcionamiento mediante la simulación de SPICE, en el caso
de el circuto de Chua este se construyó y se demostró su capacidad para generar
señales de caos.

</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN
COMPUTACIÓN
Implementación electrónica de sistemas de caos
y su sincronización utilizando lógica difusa
Tesis que presenta
Herrera Hernández Diego
Que Para Obtener el Grado de
Maestro en Ciencias en Ingeniería en Cómputo
con Opción en Sistemas Digitales
Director
Dr. Herón Molina Lozano.
México D.F.
Abril 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Visualización de indicadores y
tendencias delictivas a partir de
informes gubernamentales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Como se ha mencionado, por una parte, la mayoría de las personas no quieren
denunciar los incidentes delictivos ante las autoridades correspondientes; y por la
otra, solamente se da seguimiento a pocos delitos que acontecen. Además los reportes que se ponen a disposición de la población no están actualizados, ni
permiten seleccionar áreas de interés específicas (reportes dinámicos) para
entender el fenómeno o describirlo y tomar decisiones por parte de la población o
del mismo gobierno.
En la actualidad está aceptado que los delitos ocurren principalmente en el
medio urbano, considerado como su espacio de operaciones. Es por tanto que en
las ciudades emergen los principales problemas de seguridad y donde deben
ponerse a disposición los recursos teóricos académicos de análisis y de respuesta
institucional a las disfunciones sociales que generan los fenómenos que inciden en
la seguridad y en su percepción [Lahosa 2002].
Con base en lo anterior y en los trabajos que se conocen de sistemas de
información geográfica aplicados al registro, procesamiento y análisis de datos
delictivos; en este documento, se desarrolla un sistema de información en el cual
se registran incidentes delictivos por parte de los ciudadanos; los datos se
volverán anónimos para elaborar resúmenes e informes concentrados que se
pondrán a disposición de la población. Se espera que estos informes sean útiles a
la población para tomar decisiones de asistencia a lugares y transitar por rutas que
sean lo más confiables posible; además de proveer información útil a las personas
encargadas de la seguridad pública.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general de esta tesis es desarrollar un sistema de información
geográfica denominado VISGEDE que permita el registro de incidentes delictivos
en una base de datos. Los datos se visualizarán mediante una interfaz Web para
que los ciudadanos seleccionen áreas de interés en mapas del Distrito Federal y
puedan conocer los índices de delincuencia, así como su ocurrencia en el tiempo. 
OBJETIVOS ESPECÍFICOS
• Conocer los modelos, herramientas y sistemas de información que se han
desarrollado acerca del fenómeno de la delincuencia.
• Elaborar un esquema lógico de base de datos para el registro de incidentes
delictivos incluyendo el lugar de ocurrencia.
• Elaborar una propuesta para clasificar zonas con base en los tipos de
delitos y su frecuencia.
• Elaborar la arquitectura de procesos para la interacción con el usuario
usando mapas.
• Elaborar una propuesta de organización de datos en un cubo con las
dimensiones de tiempo, área geográfica y delitos para reducir el tiempo de
obtención de las respuestas.
• Desarrollar un módulo de consultas por tipo de delito, período de tiempo y
área geográfica cuyos resultados se presenten en mapas interactivos.
• Desarrollar un módulo generador de gráficas de la actividad delictiva en
diferentes periodos de tiempo y áreas geográficas.
• Integrar los módulos en una aplicación para su acceso mediante un
navegador Web. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los reportes que publica la Procuraduría General de Justicia del Gobierno del
Distrito Federal con información de la ocurrencia de incidentes delictivos, que se
ponen a disposición de la población, actualmente muestran únicamente cifras
generales por tipo de delito; se presentan las cifras de un semestre y en el
siguiente semestre se comparan indicando si hubo aumento o reducción sin
precisar los días o ubicaciones a nivel de colonia o delegación, solo se menciona a
nivel Distrito Federal desde 2010.
De acuerdo a las estadísticas que publica el INEGI, la situación que
presenta la Ciudad de México es similar en otros estados. Aunado a esto, la población que habita o visita nuestro país que sufre algún incidente delictivo tiene
problemas para reportarlo y darle seguimiento por el tiempo y recursos que se
requieren; por lo tanto más del 90 por ciento de los delitos que suceden en nuestro
país no se denuncian, esto implica que ni las autoridades de seguridad ni la
ciudadanía conoce la ubicación donde acontecen los delitos que día a día ocurren
en nuestra ciudad.
Actualmente, los avances en la Informática y las redes de comunicaciones
permiten proponer aplicaciones que llegan a la población a través de diferentes
dispositivos conectados a Internet como las PCs, tabletas, celulares, entre otros.
Existe la oportunidad de utilizar la tecnología actual para desarrollar un
sistema de información de registro y consulta de incidentes delictivos que sirva a
la ciudadanía y las autoridades de seguridad pública para contar con información
preferentemente en tiempo real, debido a que algunos de los incidentes deberán
pasar por un proceso de comprobación y autenticación de la veracidad de
ocurrencia; en caso que no se valide y existan personas que registren eventos
falsos por parte de personas malintencionadas podrían estarse generando cifras
equivocadas.
Esta propuesta intenta almacenar y procesar información de un problema
actual que se presenta principalmente en las ciudades del mundo, cuyas causas
se encuentran en la falta de empleo, desintegración familiar, drogadicción y otros
problemas sociales.
VISGEDE deberá considerar el nivel elemental del uso de aplicaciones que
tiene la población en general y el uso de esta información está destinada a que la
población pueda contar con recursos para implementar medidas preventivas. </Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los delitos que se tienen almacenados en la base de datos se generaron
siguiendo los reportes de la PGJDF, asignando la ubicación en colonias según la
peligrosidad publicada en medios informativos, con lo cual se obtiene una
aproximación a los datos reales.
 El desarrollo de VISGEDE se ha llevado a cabo como lo indica la Ingeniería
de Software; ya que si se lleva a cabo una alimentación correcta de los datos y por
las pruebas desarrolladas a la herramienta esta mostrará los resultados acordes a
la realidad y por tanto proporcionará la información útil, con lo cual se cumple el
objetivo del sistema de ser una herramienta informativa para las autoridades y la
sociedad.
Las conclusiones obtenidas del presente trabajo son:
• El modelo OLAP resulto útil para lograr la visualización de los acumulados
mensuales y semanales, permitiendo acceder de manera rápida a los
datos.
• La visualización de un patrón temporal se logra con los datos almacenados
y ordenados de acuerdo al tipo de delito, ubicación y fecha del delito. De
esta manera se visualizan ordenados cronológicamente en las posiciones
geográficas sobre el mapa.
• La visualización de los cambios de nivel de peligrosidad de las zonas
geográficas, se logra mostrando las frecuencias en varias unidades de
tiempo continuas.
• La visualización de zona geográfica definida por el usuario no convencional,
es útil para visualizar la movilidad de delitos en áreas contiguas.
Para unas conclusiones completas para la herramienta que se desarrolló
para el análisis delictivo, en cuanto a la utilidad total o parcial es necesario que
sea aprobada por la ciudadanía que debe de registrar y consultar los delitos en
zonas geográficas para el período de tiempo de su interés. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Computación
Visualización de indicadores y
tendencias delictivas a partir de
informes gubernamentales
TESIS
PARA OBTENER EL GRADO DE
MAESTRÍA EN CIENCIAS DE LA COMPUTACIÓN
PRESENTA
LIC. MIGUEL ANGEL CASTILLO ORTA
DIRECTORES DE TESIS:
DR. GILBERTO LORENZO MARTÍNEZ LUNA
DR. ADOLFO GUZMÁN ARENAS
 México, D.F. Julio 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SISTEMA INTELIGENTE CLASIFICADOR DE SEÑALES ENCEFALOGRAFICAS (EEG)</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Por su naturaleza estocástica, el estudio y clasificación de las señales EEG se vuelve complicado ya que existe una variación en el comportamiento de la señal encefalografía proveniente de la corteza cerebral al inducir un estímulo en un individuo.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Diseñar una arquitectura ANFIS capaz de realizar la clasificación del parpadeo y dolor
inducido por un estímulo externo.
1.4.2. Objetivos específicos
Los objetivos específicos que se contemplan en el desarrollo de esta investigación se
basan especialmente en un estudio detallado de las señales encefalografías y algunos
métodos de clasificación de patrones de datos y se presentan a continuación:
Clasificación del dolor muscular por medio de ANFIS.
Obtención de resultados utilizando diferente número de ´épocas de entrenamiento
para ANFIS.
Experimento con el sistema inteligente ANFIS.
Análisis de resultados con ANFIS.
Realizar comparaciones con otros métodos como RNA
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Con el uso de la técnica ANFIS se podrá realizar la clasificación de los estímulos cerebrales ocasionados por el dolor muscular y el parpadeo.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Existe muy poco estudio sobre la interpretación de las señales EEG mediante el sistema inteligente ANFIS para aplicaciones académicas y científicas, por lo cual en esta tesis se propone aportar resultados significativos en esta ´área. El desarrollo de esta tesis aportar ‘a resultados para la interpretación de las señales encefalografías, por un lado, para el desarrollo de experimentos y análisis de las ondas EEG mediante la técnica ANFIS; y por otro, para la comparación de los resultados obtenidos con RNA.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se conoce que el uso del ANFIS es una herramienta eficiente para la clasificación
de señales y datos provenientes de procesos estocásticos; incluso cuando no existen
suficientes trabajos de investigación enfocados a la detección eficiente del dolor producido
por agentes externos, este trabajo aporta información importante en ese aspecto. Se
presenta la metodología y arquitecturas del método ANFIS, así como diversas variantes
con la RNA de tipo perceptrón multicapa utilizadas para la clasificación del dolor
muscular y parpadeo; en combinación ofrecen una gran certeza de la existencia del
artefacto ocular. Se presenta una sección de resultados, donde se observan los datos
obtenidos por las arquitecturas ANFIS y una comparación con el sistema inteligente RNA.
Múltiples experimentos fueron desarrollados haciendo uso de diferentes algoritmos de
adaptación para una red neuronal artificial de tipo perceptrón multicapa con el objetivo
de contrastar los resultados con los obtenidos por el sistema ANFIS desarrollado en esta
tesis. Todos los experimentos hicieron uso de una base de datos la cual consistió en 60
patrones obtenidos de una lectura EEG de 20 personas con edades entre los 23 y 35 años;
30 patrones de la base de datos fueron utilizados para el entrenamiento y el resto para
su evaluación.
En la implementación del algoritmo ANFIS se hace inferencia en el entrenamiento
realizado con 50 patrones y 10,000 épocas de entrenamiento, obteniendo resultados
alentadores; mientras que la función de membresía Generalizad bel obtuvo el mejor
´índice de clasificación con un 99.97 %, no fue la mejor en cuestión de tiempo de
entrenamiento ya que alcanzó los 38232 segundos; contrario a la función trapezoidal
con un tiempo de 20783 segundos, ´esta ´ultima no realizo la mejor clasificación con
un ´índice de 98.24 %. En aspectos generales la función trapezoidal se mantuvo en el promedio de tiempo y porcentaje de clasificación con 23401 segundos y 97.56 %, respectivamente.
Cabe mencionar que la clasificación se realizó con un umbral establecido en la
norma infinita, ya que, con el umbral establecido del error cuadrático medio, las arquitecturas
entrenadas con las tres funciones de membresía obtuvieron un 100 % de clasificación.
Mientras tanto, los datos estadísticos demostraron que el algoritmo Levenberg Marquardt
fue el que mejor realizo el reconocimiento de patrones EEG en el entrenamiento de
la RNA al obtener una taza de reconocimiento de 99.7 %, y una taza de reconocimiento
para los datos de evaluación de 96.9 % lo cual es aceptable ya que son patrones de
datos desconocidos por la red neuronal. Cabe mencionar que en cuestión de tiempo de
entrenamiento, el algoritmo Gradiente Descendente fue el más rápido al alcanzar un
promedio de entrenamiento de 1167 segundos.
Además, se incluye la clasificación de las señales provenientes por el musculo ocular
al realizar un parpadeo ya que su identificación aporta información importante para
mejorar la interpretación de las señales propias de la corteza cerebral. De los resultados
estadísticos se observa que el algoritmo de adaptación Gradiente Descendente es el que
reconoce mejor los datos de entrenamiento; sin embargo, es el que necesita realizar más
´épocas de entrenamiento y posee menos generalización en la clasificación de datos de
evaluación; por otro lado, el algoritmo Resilient Backpropagation es el que tiene mayor
poder de generalización y el promedio de ´épocas de entrenamiento requeridas es bajo, con
este algoritmo se alcanza un porcentaje de reconocimiento de patrones de entrenamiento
de 99.9 % y un 98.8 % de clasificación de los datos de evaluación, por lo que se infiere que
este ´último es el más conveniente para realizar la clasificación.
Respecto a los resultados estadísticos de las pruebas con Redes Neuronales Artificiales
se observa que en promedio de clasificación de datos de entrenamiento el porcentaje
de ´éxito es del 98.5 % y 95 % para los datos de evaluación; por otro lado, los resultados
obtenidos con el clasificador ANFIS son más cercanos al 100 % de clasificación utilizando la
norma infinita y el error cuadrático medio. Con esto se puede inferir que el método ANFIS
ha cumplido con las necesidades específicas de la clasificación de señales encefalografías
generadas a partir de un estímulo inducido.
Estos resultados son suficientes para que un médico pueda conocer la condición
de un paciente sometido a dolor; además los resultados pueden mejorar positivamente
simplemente desplazando el umbral de aceptación de 0.9 a un valor mayor. Una vez clasificado, el modelo desarrollado del comportamiento de las señales
encefalografías puede llegar a infinidad de aplicaciones en el ´ámbito de la rehabilitación
física, psicológica y/o en el campo de las prótesis humanas. Se establece la posibilidad
de que un sujeto que presenta alguna discapacidad motriz como amputaciones, deficiencia muscular o lesiones físicas pueda llegar a manipular de manera mental una prótesis.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITECNICO NACIONAL 
CENTRO DE INVESTIGACION Y DESARROLLO 
DE TECNOLOGIA DIGITAL
MAESTR´IA EN CIENCIAS EN SISTEMAS DIGITALES
“SISTEMA INTELIGENTE CLASIFICADOR DE SENALES ˜
ENCEFALOGRAFICAS (EEG) ”
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRIA EN CIENCIAS EN SISTEMAS DIGITALES
PRESENTA
ING. DANIEL ABRAHAM GUTIERREZ FRANCO ´
BAJO LA DIRECCION DE:
DR. OSCAR HUMBERTO MONTIEL ROSS
DR. ROBERTO SEPULVEDA CRUZ ´
JUNIO 2014 TIJUANA, B.C., MEXICO
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Uso de Algoritmos Bio-inspirados para la Solución Optima de Problemas</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En ingeniería mecatrónica un problema de diseño concurrente de un sistema puede ser traducido
en un problema paramétrico, el cual puede ser resuelto mediante un problema numérico de
cómputo [18]. Este tipo de problemas debido a su complejidad integran en su proceso a diversas
áreas que deben trabajar de forma interdisciplinaria, ya que se requiere tanto del área mecánica,
como la eléctrica, asimismo de los componentes electrónicos y de control.
Es importante recalcar que, dentro del conjunto de soluciones posibles, para los problemas de
diseño concurrente de un sistema, encontramos aquellas que son válidas e igualmente aquellas
que son óptimas. En este sentido, en el diseño la optimización se lleva a cabo obteniendo los
valores óptimos del conjunto de parámetros que se establecieron para definir nuestro sistema y
los valores óptimos se obtienen minimizando o maximizando, según sea necesario, una o más
funciones matemáticas que describan el funcionamiento del mismo.
Este tipo de problemas puede ser resuelto por métodos tradicionales, a menos que debido a la
complejidad del mismo, el costo computacional o la obtención de los resultados no sea la
esperada (no óptimos), entonces se puede optar por la utilización de métodos no tradicionales,
mejor conocidos como heurísticas [5].
Los métodos no tradicionales ofrecen soluciones más simples para resolver los problemas de
optimización, ya que incorporan conceptos heurísticos para mejorar la búsqueda en un entorno
similar al de los problemas del mundo real [18]. En este sentido, los algoritmos de inteligencia
colectiva, por ser poblacionales, no presentan una sensibilidad al punto inicial de búsqueda, no
requieren que el problema sea modificado y ofrecen soluciones múltiples en una sola simulación.
En este trabajo se propone utilizar una de estas técnicas heurísticas de inteligencia colectiva,
específicamente el algoritmo de búsqueda cucú para demostrar su eficiencia en la resolución de
problemas de optimización de diseño concurrente en el área mecatrónica. Xin-She Yang [19]
creador del algoritmo de búsqueda cucú menciona alguna de las aplicaciones en diseño de
ingeniería donde se ha aplicado, en las que se ha demostrado que éste tiene mejor rendimiento
que otros algoritmos para una gama de problemas de optimización como son: los de diseño de
resortes y de vigas [20] [21] [22].
A diferencia de otras investigaciones, en este caso se plantea la utilización de técnicas heurísticas
(recurrentemente utilizadas para resolver problemas sin restricciones), para proponer una
solución a problemas con restricciones de diseño y funcionamiento.
Las técnicas heurísticas nos ofrecen una solución eficiente para resolver los problemas de diseño
concurrente planteados en esta investigación; debido a que presentan las siguientes ventajas: no
se requiere ser especialista en las mismas para su implementación, además de que contamos con
experiencia previa implementando este tipo de algoritmos.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo
Aplicar el algoritmo de búsqueda cucú o cuckoo search (CS) para la obtención de soluciones a
problemas de ingeniería dentro del concepto de diseño concurrente.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo se presenta la síntesis dimensional de un mecanismo de cuatro barras, para el
seguimiento de 3 diferentes trayectorias, la síntesis dimensional de un efector final tipo pinza con
la finalidad, de mantener la fuerza constante en todo el espacio de trabajo y la sintonización
óptima de ganancias de un control de regulación para un mecanismo de cuatro barras, a partir del
planteamiento de un problema de optimización numérica el cual es resuelto utilizando los
algoritmo de Búsqueda Cucú, Evolución Diferencial y Enjambre de Luciérnagas.
De los resultados presentados se concluye que los algoritmos heurísticos son una alternativa para
la solución de problemas duros de ingeniería debido a su naturaleza poblacional, al obtener más
de una solución factible, lo que permite al ingeniero de diseño evaluar diferentes aspectos como la
facilidad para la manufactura y ensamble.
Respecto al desempeño de los algoritmos se determina que ED y CS se comportan de manera
similar en tres de los cinco casos (1,2 y 5), mientras que FA no muestra resultados competitivos
desde el punto de vista computacional lo cual se observa en las estadísticas paramétricas, aunque
también logra resultados factibles desde el punto de vista de las aplicaciones duras de ingeniería.
Los tres algoritmos logran soluciones factibles para todos los problemas planteados lo cual
demuestra su eficiencia cuando se requiere encontrar soluciones a problemas prácticos de
ingeniería.
Con base a las estadísticas paramétricas se concluye que no es necesario realizar la prueba de
Wilcoxon para demostrar que el rendimiento de FA no es mejor que el de CS o ED para los
problemas presentados en este trabajo.
Con base en las estadísticas no paramétricas (Wilcoxon) se concluye que existen diferencias
significativas al comparar el desempeño de CS con ED.
Se requiere una especial atención en la sintonización de los parámetros de cada algoritmo ya que
los resultados dependen de una correcta sintonización, es decir una adecuada sintonización
permite la convergencia del algoritmo.
Teniendo en cuenta la importancia de una correcta sintonización se propone como trabajo a
futuro el desarrollo de una herramienta que permita una sintonización óptima de los parámetros
sin depender exclusivamente de la experiencia de programador, lo que permitirá incrementar la
exploración y explotación de los algoritmos de búsqueda.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
Centro de Innovación y Desarrollo Tecnológico en Cómputo
Uso de Algoritmos Bio-inspirados para la Solución Óptima de Problemas de
Ingeniería en Mecatrónica.
Tesis para obtener el grado de:
Maestría en Tecnología de Cómputo
Presenta:
Adrián Solano Palma
Directores:
Dr. Edgar Alfredo Portilla Flores
M. en C. Eduardo Vega Alvarado
México, Cd. de México Agosto de 2016</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Visión por computadora para la recolección de objetos mediante el control del robot manipulador Scorbot ER4pc	</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la época actual hay diversos sistemas que involucran robots manipuladores y
algoritmos de visión por computadora, esto se hace para realizar o llevar a cabo
tareas complejas, en las que por seguridad industrial o algún otro aspecto como en
las operaciones quirúrgicas de pacientes en los hospitales es recomendable que no
intervenga el ser humano en gran medida durante el proceso, entonces, el sistema
robótico en conjunto con el de visión es capaz de identificar patrones o
características que sirven para completar una tarea dada.
En éste trabajo se utilizará el procesamiento digital de imágenes en el dominio de
los pixeles para hacer el tratamiento adecuado de la imagen del objeto (cilindros de
color negro de 4 cm de diámetro por 4 cm de alto) y con base en la información
obtenida establecer las coordenadas “x” e “y” del centroide del objeto cilíndrico en
el plano del área de trabajo, después, a partir de dichas coordenadas establecer las
variables articulares del robot manipulador para que el efector final se posicione
sobre la pieza, la sujete y la lleve a otro punto.
Para iniciar con el desarrollo del sistema se debe aplicar un controlador PID para
cada grado de libertad (articulación) del Scorbot ER4pc, es decir, cada motor estará
controlado de forma independiente tanto en la etapa de control como en la etapa de
potencia, cada motor recibirá el ángulo al que se debe mover a través de una
comunicación serial con la computadora y el programa de Matlab®, por lo que se
puede mencionar que será un desarrollo modular.
Una vez llevado a cabo el algoritmo de visión por computadora y el algoritmo para
obtener la cinemática inversa en Matlab®, se obtienen las variables articulares y
cada una se envía por comunicación serial RS-232 al PIC que controla su respectivo
motor para posicionarlo al ángulo deseado. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un sistema de visión por computadora capaz de reconocer objetos
cilíndricos en un ambiente controlado para su recolección mediante el control del
robot manipulador Scorbot ER4pc.
Obtener las características principales del robot manipulador Scorbot
ER4pc.
 Establecer el área de trabajo del robot.
 Elaborar las tarjetas de control y de potencia para el Scorbot ER4pc.
 Obtener la cinemática directa e inversa del robot manipulador.
 Establecer el tipo de objetos que serán tratados mediante el algoritmo
de visión por computadora y elaborarlos.
 Realizar el algoritmo de visión por computadora en Matlab® para la
obtención del centro del objeto.
 Posicionar el objeto reconocido con el algoritmo de visión en otro
punto del área de trabajo mediante el Scorbot ER4pc</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Implementar un sistema con un robot manipulador y un algoritmo de visión en donde
se permita observar la relación y aplicaciones que hay entre las diferentes ramas de
la ingeniería.
El robot manipulador Scorbot ER4pc forma parte de los robots seriales que se
pueden implementar en muchos sistemas en diferentes centros de investigación y
en la industria debido a su tamaño y a su configuración tanto en el mecanismo
como en los motores y bandas, es un robot manipulador muy completo que puede
ser implementado en diferentes sistemas mecatrónicos, por ejemplo, en este trabajo
se hace uso de la visión por computadora y de parámetros mecánicos del Scorbot
ER4pc muy importantes como la cinemática inversa y la cinemática directa. Por
otra parte los algoritmos de visión por computadora son muy importantes en el
ámbito científico y tecnológico porque nos ayudan a crear sistemas cada vez más
avanzados que puedan imitar un sentido del ser humano tan importante como la
vista, es uno de los problemas que más se abordan y que tienen un gran campo de
investigación ya que los mejores algoritmos de visión pueden no ser la mejor
solución para problemas sencillos, por lo tanto, se investigan métodos que puedan
resolver problemas en específico y así obtener resultados de acuerdo a cada tipo
de necesidad.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La robótica es una rama de la ingeniería que sin duda es muy útil para la época en
la que vivimos, es decir, el hecho de adquirir un robot manipulador como el Scorbot
ER4pc nos da la pauta para entender el funcionamiento de un robot industrial
aunado a la gran importancia y utilidad en el ámbito de la investigación. Por lo tanto,
analizar los mecanismos del propio robot y aplicar leyes de control a una
arquitectura abierta para posicionar el brazo robot deja muchos experimentos y
pruebas que se concretan a un resultado único, que en este caso es la
implementación de algoritmos computacionales y electrónica, que, en efecto, juntos
pueden arrojar resultados que satisfacen las propias necesidades del prototipo y del
usuario así como de la propia arquitectura del software y hardware.
Sin embargo, además se debe hacer hincapié en la parte de visión por
computadora, ya que forma parte fundamental de la vida cotidiana del ser humano
y precisamente con el tratamiento de las imágenes se pretende imitar el
comportamiento del sentido de la vista. El algoritmo presentado en este trabajo
involucra una serie de pasos que se basan en el famoso y elemental paradigma de
González and Woods, el proceso que se da desde la adquisición de la imagen hasta
la obtención del centro del objeto es un seguimiento paso a paso del paradigma
antes mencionado, entonces, con la aplicación de un sistema de visión y uno de
robótica se ven implementadas diferentes áreas de la ingeniería. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITECNICO NACIONAL
Centro de Innovación y Desarrollo Tecnológico en Cómputo
Unidad Profesional Adolfo López Mateos
Visión por computadora para la recolección de
objetos mediante el control del robot manipulador
Scorbot ER4pc
T E S I S
PARA OBTENER EL GRADO DE:
Maestría en Tecnología de Cómputo
PRESENTA:
Ing. Geovanni Flores Caballero
Directores de Tesis:
Dr. Miguel Gabriel Villarreal Cervantes
Dr. Juan Carlos Herrera Lozada
México, D.F. Enero 2016</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Metodología para la selección de características usando algoritmos genéticos</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Dentro del área médica, la parte dedicada a la rehabilitación ha tenido gran desarrollo, pero aún
tiene marcadas necesidades. Los mayores beneficios tecnológicos en esta área médica, se han
dado específicamente gracias a la innovación de los sistemas mecatrónicos. En un inicio, se
contaba simplemente con sistemas puramente mecánicos, donde el terapeuta, con base en su
preparación interactuaba con el paciente para lograr la rehabilitación de la zona afectada. En la
actualidad, existe un creciente desarrollo en el área de robots bípedos y exoesqueletos. Sin
embargo, muchos de ellos se han enfocado únicamente en construir, instrumentar y controlar los
dispositivos para que copien el ciclo de marcha de un paciente sano. Estas condiciones no son
aplicables para el desarrollo de un sistema robótico que busque coadyuvar en el proceso de
rehabilitación de un paciente. En esta clase de sistemas, se necesita un enfoque diferente tanto en
el diseño del dispositivo como en el diseño de su algoritmo de control.
La terapia asistida para rehabilitación por sistemas robóticos permite implementar rutinas precisas
y reproducibles, que simultáneamente son monitoreadas de manera cuantitativa para evaluar el
progreso del paciente. En teoría, estos dispositivos al no estar limitados por la fatiga o por el riesgo
de lesión, parecen ser adecuados para que el fisioterapeuta reduzca su esfuerzo en la terapia y
solo supervise la sesión. Sin embargo, los resultados clínicos obtenidos a partir de dispositivos
robóticos no han mostrado mejores resultados, con relación a los obtenidos en la terapia
convencional asistida por fisioterapeutas (Wirz, 2005), (Werning, 2005). Por otro lado, el uso de
los dispositivos robóticos pueden reducir la variabilidad de la secuencia de movimientos de terapia
a terapia, así como la velocidad ejecutada en cada movimiento, lo cual implica un aumento en la
velocidad del proceso de rehabilitación.
La Tabla 4.1 muestra los resultados obtenidos de terapia asistida por sistemas robóticos, en
particular el robot Lokomat y la terapia convencional. Estos estudios se aplicaron en pacientes con
enfermedad vascular cerebral (EVC). Se tomaron como referencia las siguientes pruebas (Kim,
2013):
 Prueba de Caminata de 10 metros (10 MWT, 10 meter walk test).
 Escala de Equilibrio Berg (BBS, Berg balance scale)
Escala de Tono Ashworth (Ashworth Scale of Tone)
 Categorías de Deambulación Funcional (FAC, Functional Ambulation Categories)
A partir de las experiencias internacionales en el área de rehabilitación asistida por robots, se
puede notar que la efectividad aumenta si se combinan las dos modalidades, pero no se puede
remplazar la terapia convencional.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo del presente trabajo de tesis es:
Desarrollar una metodología para la selección de características usando
algoritmos genéticos
Como objetivos particulares se busca:
Realizar un estudio experimental para sintonizar algunos de los parámetros
presentes en la metodología
Demostrar la utilidad de esta metodología a través de pruebas con conjuntos
de datos públicos
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo de tesis se presenta en forma detallada la información necesaria para el correcto entendimiento y aplicación de la metodología DAGA, que combina de forma novedosa 2 variantes de un AG, así como diferentes clase- chadores, haciendo de esta metodología una solución exile, adaptable a diversos problemas de selección de características. Como conclusiones se obtuvo lo siguiente: Se desarrolla la metodología DAGA. Se realiza un estudio experimental para sintonizar algunos de los parámetros presentes en la metodología Se demostró la utilidad de esta metodología a través de pruebas con conjuntos de datos públicos. Al Analizar el trabajo de tesis, diversas preguntas quedan abiertas para un posterior análisis. Algunos de los puntos aconsejados para continuar el presente trabajo de tesis son los siguientes: Utilizar la metodología DAGA con diferentes algoritmos de clasificación. El uso de la metodología DAGA con algún algoritmo diferente pasa seleccionar el clasificador a usar de una forma más eficiente.
El uso de diferentes métodos para probar la acacia de clasificación e integrarlos en la metodología DAGA. La investigación del uso de memorias asociativas como algoritmos de clase- Ocasión dentro de la metodología DAGA. El realizar un estudio detallado para determinar el mejor tamaño de la población para la metodología DAGA.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>	INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
METODOLOGÍA PARA LA SELECCIÓN DE CARACTERÍSTICAS
USANDO ALGORITMOS GENÉTICOS
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
PRESENTA
JOSÉ ANTONIO ESTRADA PAVÍA
DIRECTOR DE TESIS: DR. MARIO ALDAPE PÉREZ
DIRECTOR DE TESIS: DR. OSCAR CAMACHO NIETO
MÉXICO, D.F. MAYO DE 2016

</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Diseño de un medidor de distancias usando Realidad Aumentada</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Algunos problemas que persisten y siguen siendo retos para el desarrollo de la tecnología o
los principales puntos de falla de los contendidos y ejemplos, son los siguientes: 
 Problemas de paralaje por el desfase de la cámara con respecto a los ojos.
 El seguimiento de los ojos para ampliar el campo de visión y aprovecharlo como interfaz
de interacción humano-máquina.
 La oclusión o eliminación de los objetos reales que se sustituyen con los virtuales, o
viceversa.
 La complejidad de ambientes exteriores o la extensión de una aplicación o contenidos a
ambientes no preparados.
 El retraso del cálculo de transformaciones geométricas con respecto al despliegue de
video.
 Los cálculos de localización.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar e implementar un sistema de medición de distancias para ambientes de realidad
aumentada.
Objetivos particulares
Realizar el estado del arte de las técnicas utilizadas para diseñar medidores de
distancias, enfocándose a los trabajos que se refieren al uso cámaras y en ambientes
de realidad virtual.
Diseño e implementación de un medidor de distancias con base en un sistema
embebido y un sensor ultrasónico como modelo comparativo.
Implementación de un algoritmo para medir distancias utilizando una cámara web,
considerando marcadores para realidad aumentada.
 Diseño y construcción de una plataforma para realizar pruebas experimentales.
Comparación, validación y análisis de resultados.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>De acuerdo a lo descrito antes, no existe ninguna aplicación de RA basada en visión con
marcadores que permita que los cálculos de medición entre la cámara y el patrón sean
exactos.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se logró mediante la medición por RA que esta fuera lo más exacta posible en comparación
con la medición hecha por un sensor ultrasónico y resultando ser mejor en condiciones
óptimas de luz ya que elimina el ruido que pudiera generarse al usar una medición
ultrasónica, por cual es viable la utilización de la RA para generar aplicaciones innovadoras
de este tipo obteniendo resultados aceptables a un bajo costo.
Como trabajo a futuro se propone la adaptación del sistema de medición en RA para móviles
dado que este tipo de plataformas cuenta con los elementos necesarios para poder ejecutar
aplicaciones de este tipo y están disponibles las bibliotecas para poder trabajar de manera
libre en este tipo de dispositivos.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
DISEÑO DE UN MEDIDOR DE DISTANCIAS USANDO REALIDAD
AUMENTADA
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
PRESENTA:
ERIK LÓPEZ CRUZ
DIRECTORES DE TESIS:
M. EN C. JESÚS ANTONIO ÁLVAREZ CEDILLO
DR. JUAN CARLOS HERRERA LOZADA
México, D.F. Julio 2016.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema inteligente para dispositivos móviles con sistema operativo android, orientado al apoyo de automovilistas en el D.F.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Uno de los principales problemas que aqueja a México es, que el nivel de corrupción es tan
alto, que la población desconfía de nuestras autoridades e instituciones.
La policía es una institución vulnerable a la corrupción en varios niveles. Esto se refleja en
la percepción de los ciudadanos alrededor del mundo. Existe un estudio que se realiza
cada año el Barómetro Global de la Corrupción (BGC) de Transparencia Internacional (TI)
mide, a través de encuestas cómo y en qué ámbitos de la vida de los ciudadanos se ve
afectada por la corrupción. De acuerdo a los resultados del estudio del 2013 se tiene lo
siguiente:
 Los mexicanos califican de “muy grave” el problema de la corrupción en el sector
público.
 Uno de cada tres encuestados reportó haber pagado sobornos en el último año.
 Personas que entraron en contacto con la policía en el último año, 61% declaró
haber pagado soborno.
 72% de los encuestados calificó de ineficaces las acciones que han tomado el
gobierno para combatir la corrupción en el país.
A pesar de que en todo el continente, el problema de la corrupción en el sector público es
visto como grave, México y Paraguay, son los países que califican con mayor gravedad este
problema. En una escala del 1 al 5, donde 1 significa que la corrupción no es un problema
en nuestro país y 5 significa un problema muy grave, los mexicanos encuestados
expresaron una calificación de 4.7. [3]
Al igual que el resto de los ciudadanos encuestados en el continente americano, los
mexicanos consideran que las instituciones percibidas con mayor corrupción son los
partidos políticos, las policías, el servicio público, el poder legislativo y el poder
judicial. Sin embargo, México se encuentra por encima del promedio en partidos políticos,
policías, servicio público y congresos.
Es importante que la ciudadanía conozca los lineamientos de las vialidades de DF, pero un
punto en contra de la sociedad mexicana es el bajo nivel de lectura [19], por lo que
difícilmente un ciudadano se toma el tiempo para leer la información o lineamientos que el
Gobierno del Distrito Federal dé a conocer, como por ejemplo el reglamento de tránsito.
El inconveniente de no conocer dicho reglamento puede tener consecuencias graves, ya
que “La ignorancia de las leyes no excusa su cumplimiento”, esto lo estipula el artículo 21
del Código Civil. [21]
Para verificar si los conductores del Distrito Federal conocen el reglamento de tránsito
metropolitano, se realizó una encuesta. Ésta tiene como objetivo saber si conocen los
lineamientos que rigen las vialidades de la ciudad, así como conocer cuál es el vocabulario
que emplearían para al realizar una consulta en el reglamento de tránsito vehicular.
Para cumplir con los objetivos de la encuesta, la cual consiste en catorce preguntas
abiertas, de las cuales, siete son para conocer el vocabulario que manejan al respecto, cinco
para conocer si saben o conocen algunos de los lineamientos del reglamento de tránsito y
dos son de carácter informativo sobre el encuestado, como lo es su edad y género.
Para realizar el análisis de los resultados de la encuesta, se considera una escala del uno al
nueve, para las preguntas acerca del conocimiento de los conductores sobre los
lineamientos del reglamento de tránsito. La escala se clasifica, para cada pregunta de la
manera siguiente:
Tomando en cuenta la escala anterior para decidir si el conductor tiene un conocimiento
alto sobre el reglamento de tránsito, el puntaje total es de 35 a 45 puntos; conocimiento
promedio de 20 a 34 puntos, y conocimiento bajo de 5 a 19 puntos.
En referencia a las preguntas del vocabulario empleado por los conductores, se consideró
cada una de las palabras de la encuesta para formar un diccionario de palabras y así,
generar sinónimos de cada una de ellas, con la finalidad de contar con el suficiente
vocabulario en el sistema, para brindar resultados eficientes al realizar alguna consulta en
un motor de búsqueda.
Para conocer más de la encuesta véase el anexo A. Análisis de los resultados de la
encuesta.
Por medio de esta encuesta se encontró que el 70% de los conductores desconoce los
lineamientos que rigen la vialidad del D.F., así como el vocabulario que emplean para
consultar el reglamento de tránsito vehicular. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
Diseñar, desarrollar e implementar una aplicación para teléfonos con sistema
operativo Android 4.0 o superior. En la cual se integre un algoritmo de búsqueda
inteligente, con la finalidad de brindar una orientación rápida de carácter jurídico a los
automovilistas que transiten en el Distrito Federal.
Objetivos Particulares
Desarrollar un diccionario electrónico de sinónimos de las palabras claves
empleadas en el reglamento de tránsito.
Implementación de un motor de búsqueda basado en indexación para la
recuperación de información de los artículos del reglamento de tránsito.
Implementación de una Interfaz de Programación de Aplicaciones (API) orientada
al servicio de geolocalización.
Diseño y desarrollo de una interfaz gráfica que permita al usuario acceder a cada
uno de los servicios de forma intuitiva.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Las multas y los accidentes de tránsito son una constante en la Ciudad de México ya que
existen 4,593,710 autos tan solo en el D.F. [11] registrados por el INEGI en 2013. Sin
embargo, al momento de expedir una licencia de conducir los requisitos eran mínimos y
aunque los esfuerzos por parte del Gobierno del Distrito Federal, hasta el año 2014 no
eran suficientes. Es por ello que la Secretaria de Movilidad (Semovi) ha decido tomar
medidas en este tema, en el cual para poder expedir una licencia, el conductor debe
acreditar los exámenes de conocimientos sobre los señalamientos en las vialidades,
conocer sobre el reglamento de tránsito del Distrito Federal, debe comprobar que sabe
manejar con pericia, responsabilidad y conocimiento, como la establece el artículo 65 de la
Ley de Movilidad del Distrito Federal. [20]
Aunque esta legislación entró en vigor el 15 de Julio de 2014, se comenzará a implementar
dichas medidas a principios del año 2015, por ello hasta el momento no se cuenta con
información para conocer si las nuevas medidas establecidas han ayudado a que el
conductor tenga los conocimientos necesarios para prevenir o bien actuar en caso de
cometer alguna infracción.
Otro factor que se ha tomado en cuenta son los accidentes de tránsito vehicular, de
acuerdo con los informes de accidentes viales proporcionados por el Observatorio
Metropolitano de Movilidad y Transporte, en el primer cuatrimestre del 2014 ocurrieron
3,958 accidentes de tránsito, un promedio de 44 accidentes al día, ocasionados por conducir en exceso de velocidad, en estado de ebriedad o con objetos distractores. Mucho
de estos accidentes se deben a la falta de cultura vial y desconocimiento del Reglamento de
Tránsito del DF. [16]
En México, es alarmante que la mortalidad derivada de accidentes viales proyectada para
2014 sea de 18,242 personas y para 2015 sea de 18,484 personas, de acuerdo con datos
del Consejo Nacional para la Prevención de Accidentes (Conapra). Así mismo, en la Ciudad
de México, de acuerdo con los mismos datos la mortalidad proyectada para 2014 fue de
850 personas y para el año 2015 de 815 personas. [17]
La policía es una institución vulnerable a la corrupción. Existe un estudio que se realiza
cada año, en donde se refleja la percepción de los ciudadanos alrededor del mundo y lo
realiza el Barómetro Global de la Corrupción (BGC) de Transparencia Internacional (TI)
mide, a través de encuestas cómo y en qué ámbitos de la vida de los ciudadanos se ve
afectada por la corrupción. [3] Es por ello que esta aplicación será una herramienta para el
conductor, la cual le permitirá conocer sus derechos y obligaciones, lo que evitará dar
oportunidad al oficial de tránsito a caer en actos de corrupción.
Considerando el incremento del uso de las aplicaciones móviles en la población mexicana, es
posible que esta propuesta, sea una solución que brinde al conductor una orientación rápida
de carácter jurídico en cualquier momento, al instante de cometer una infracción, o bien, que
le permita mediante consejos textuales, prevenir futuras infracciones o accidentes de
tránsito vehicular.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se logró obtener la aplicación Reglamento de Tránsito Metropolitano un sistema que apoya a
los conductores que transitan en las vialidades de la Ciudad de México.
El proyecto ha logrado cumplir todos los objetivos básicos que se habían propuesto y
respetando las especificaciones planteadas en un principio.
Se logró la implementación de:
 Módulo Buscador inteligente.
Realiza una consulta a cerca de la infracción impuesta por el oficial de tránsito. De
acuerdo a la consulta realizada le mostrará el artículo que está infringiendo así como
la sanción que amerita por dicha infracción. (Monto y si amerita corralón o algún
arresto administrativo).
 Módulo Reglamento de Tránsito
Permite al usuario acceder al capítulo que requiera leer desplegando un archivo PDF
de cada sección que el usuario desee consultar.
 Módulo Calculadora de Multas
Permite al usuario calcular el monto de alguna sanción ingresando el número de
salarios mínimos que indica dicha sanción.
 Módulo Depósitos Vehiculares
Permite al usuario ver un mapa en donde se muestra su ubicación y la ubicación de los
depósitos vehiculares que existen a su alrededor, así como visualizar los datos de cada
depósito.
 Módulo Verificación Vehicular
Permite al usuario visualizar el calendario de verificación del primer y segundo
semestre del año así como programar una alarma o notificación para que el sistema le
recuerde que es tiempo de verificar su auto, ingresando el engomado de su auto, esta
alarma se activará cada lunes aproximadamente a medio día hasta que termine el
plazo que establece el gobierno para realizar su verificación o el usuario haya
desactivado dicha alarma.
Como segunda pantalla, el usuario puede ver un mapa en donde se muestra su
ubicación y la ubicación de los verificentros que existen a su alrededor, así como
visualizar los datos de cada verificentro.
 Módulo Consejos
Permite al usuario visualizar algunos “tips” y consejos con el objetivo de prevenirlo de
futuras infracciones u accidentes así como brindar información acerca de los datos
donde puede realizar alguna queja o incomodidad que tenga sobre los agentes de
tránsito.
 Módulo Servicios de Emergencia
Permite al usuario enlazar la llamada del servicio de emergencia que requiera, como a
la policía del DF, bomberos, Cruz Roja y al CAS (Centro de Atención del Secretario)
para cualquier queja o consulta que se tenga. 
Se ha pedido a diferentes personas que prueben la aplicación para evaluar la experiencia del
usuario, es decir, que el usuario o conductor no requiera el uso de manuales o instrucción
previa.
En general el resultado fue positivo, ya que se comprobó que la aplicación es intuitiva y es más
eficiente en las últimas versiones de Android. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN CIENCIA APLICADA Y TECNOLOGÍA
AVANZADA, UNIDAD LEGARIA
SISTEMA INTELIGENTE PARA DISPOSITIVOS MÓVILES CON SISTEMA
OPERATIVO ANDROID, ORIENTANDO AL APOYO DE AUTOMOVILISTAS
EN EL D.F.
TESIS PARA OBTENER EL GRADO DE:
Maestro en Tecnología Avanzada
PRESENTA:
Ing. Nasyeli López Montes de Oca
DIRECTORES:
Dr. Antonio Gustavo Juárez Gracia
Dr. Roberto Vázquez Arreguín
México D.F. Julio 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Visualización de Emociones Basado en el Modelo de Plutchik</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La expresión facial forma una parte muy importante en la transmisión de las emociones,
muchos de los pacientes con trastornos del espectro autista se caracterizan por los
problemas para ser empáticos. Esto incluye dificultades en el reconocimiento de las
emociones en sí mismos y en otros, lo que dificulta el funcionamiento social.
Se requiere adquirir los conceptos y los conocimientos que son necesarios para
posteriormente colocarlos al modelo 3D del avatar que nos permitirá interpretar las
emociones, además de que también se requerirá poseer los conocimientos de la
plataforma Blender para poder realizar el programa de este proyecto por ello en el
Capítulo 3 se analizaran plataformas similares para comparar las funcionalidades y tomar
como base las características del software que convengan para el desarrollo de la interfaz
del sistema y formalizar un prototipo.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
El objetivo de esta tesis es “crear un modelo 3D basado en llaves de formas gráficas que
muestre el resultado de una emoción”.
Objetivos Particulares
Comprender técnicas de poses gráficas.
Comprender efectos de una emoción en un ser humano.
Generar un modelo gráfico tridimensional a partir de su información de entrada.
Correlación de las características físicas y gráficas de una emoción.
Validar resultados con base al modelo propuesto por Robert Plutchik.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El hecho de estar frente a otras persona durante el proceso de comunicación también
hace uso información de la cara, por ello la comunicación no verbal representa un
porcentaje muy alto según los estudios sobre actitudes y sentimientos realizados por el
psicólogo Albert Mehrabian, en su investigación presenta los siguientes porcentajes que
representan la información transmitida durante el lapso de la comunicación: el lenguaje
verbal (de palabras) representa el 7%, la información vocal ( la resonancia, tono,
entonación, etc. ) representa el 38% y el lenguaje corporal (gestos, posturas, movimientos
faciales, etc.) representan el 55% [6]. Es por esto que en esta tesis se enfoca en el estudio
de uno de los aspectos que envuelven la mayor información comunicada en la transmisión
de información: el lenguaje corporal; que involucra las expresiones y movimientos
faciales. El rostro humano a través de los movimientos de los músculos puede generar
expresiones de emociones o simples movimientos, estos pueden comunicarnos y brindar
información sobre el estado de ánimo en que se encuentra la persona, incluso se puede
reconocer si una persona está mintiendo por medio de las micro expresiones faciales que
se llevan a cabo en milésimas de segundo de forma involuntaria, las cuales delatan
información que no van acorde con lo que la persona está diciendo [7].
Las siguientes aplicaciones son un ejemplo de cómo se puede aprovechar el conocimiento
del estado emocional de los usuarios para tomar decisiones sobre qué acciones debe
seguir un sistema.
A) Un tutorial interactivo [11] en el que se podría adaptar la carga emocional de la
respuesta del sistema buscando motivar y captar el interés dependiendo del
estado emocional del alumno.
B) Un sistema telefónico de atención automática a clientes que provee asistencia
médica a usuarios que llaman pidiendo ayuda [12]. Dichos usuarios podrían
presentar diferentes emociones como tensión, miedo, dolor o pánico dependiendo
de la enfermedad o de la emergencia que están experimentando. El manejo de una
llamada será diferente dependiendo de la clasificación del estado emocional del
usuario, dando prioridad a las llamadas más urgentes; dirigiéndolas a la persona
indicada.
C) Otra aplicación es un Sistema de Respuesta Interactiva por Voz (IVR) que atiende
pacientes con problemas psicológicos [13] El sistema detecta si hay algún grado de
depresión basándose principalmente en características articulatorias de la calidad
de voz del paciente. El sistema alerta a un experto humano cuando detecta en el
paciente un grado de depresión alarmante.
D) Las aplicaciones del reconocimiento automático de carga emocional en la voz no
se limitan únicamente a la IHC. En la interacción humano – computadora, puede
usarse para monitorear conversaciones entre agentes y clientes en centros de
telefonía (call center) y detectar estados emocionales no deseados [17]. Por
ejemplo, un cliente enojado o frustrado o un agente con actitud altanera. De esta
manera un inspector de calidad puede tomar decisiones sobre la administración y
mejora del personal y de los servicios.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>A partir de la aplicación realizada, se ha mostrado que el avatar resulta adecuado para el
uso en interfaces hombre-máquina, ya que los resultados obtenidos para las facetas
estudiadas muestran una aceptación positiva por parte de los usuarios.
Se han conseguido los objetivos de generar un avatar que interpretara las emociones
según el modelo de Plutchik tanto en apariencia como en comportamiento y estos
aspectos se han valorado positivamente. Las expresiones básicas que genera el avatar son
fácilmente identificables y estos resultados son mejores en las animaciones, además de
que el avatar genere las expresiones mediante movimientos naturales y basados en el
comportamiento humano.
Para su desarrollo se ha creo un modelo facial en tres dimensiones a partir de dos
fotografías, evitando así el uso de un escáner 3D usado en anteriores proyectos. De esta
manera, el modelado permite mayor dinamismo a la hora de animar, ya que se podrán
situar los vértices de la forma que más conviene para su animación, reduciendo su
número y mejorando así la capacidad de deformación del modelo, lo que desembocará en
unas animaciones más realistas.
Con el objetivo de conocer el estado actual de este tipo de investigaciones, se ha realizado
un estudio de los distintos métodos de modelado y animación 3D existentes. Y dentro de
este estudio, se han diferenciado diferentes métodos de adquisición de datos, de
modelado tridimensional, técnicas de animación y de simulación de las expresiones. Una
vez creado el modelo se ha procedido a su texturizado y animación. Ésta última está
basada en el sistema de codificación de acciones faciales o Facial Action Coding Sistem
(FACS) y llevada a cabo mediante el empleo de key shapes o formas clave.
Finalmente se ha podido comprobar que la animación 3D es un trabajo interesante pero a
la vez de una complejidad considerable y para la que hay que dominar numerosas
disciplinas: artísticas, biológicas, matemáticas, robóticas, etc. Si bien es verdad que la
herramienta de modelado elegida ha permitido la abstracción de cálculos matemáticos
complejos y de programación, y la aplicación de métodos de animación tridimensional.
Además, como se ha comprobado, el estudio de la animación 3D es entendido de diversas
maneras dependiendo de la cultura, nivel de conocimientos, edad, etc.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
 Centro de Innovación y Desarrollo
 Tecnológico en Cómputo
VISUALIZACIÓN DE EMOCIONES BASADO EN EL MODELO DE
PLUTCHIK
TESIS QUE PARA OBTENER EL GRADO DE
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
PRESENTA:
Ing. Adrián Pedro González Martínez
DIRECTORES
M. en C. Jesús Antonio Álvarez Cedillo
M. en C. Israel Rivera Zárate
MÉXICO, D.F. DICIEMBRE DE 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema Monitor Detector de Intrusos usando TRIPLE-DES96</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En todas partes del mundo existen dispositivos electrónicos en los cuales se
almacena información, como lo son fotos, documentos, presentaciones, bases de
datos, sistemas informáticos, agendas, notas, calendarios en los cuales se
especifican las actividades a realizar por una persona, así como existen medios de
almacenamiento, existen también personas que desean acceder a dicha
información, ya sea a cambio de recibir alguna remuneración económica o
solamente por demostrar lo que pueden hacer.
Cuando un equipo de cómputo ha sido vulnerado y aparte éste se encuentra
situado dentro de una red de datos, esto aumenta la posibilidad de que más
equipos sean atacados, es por eso que se necesita fortalecer la seguridad, y el
acceso a los mismos, de tal manera que nuestra información esté protegida y no
pueda ser modificada. La mayoría de los IDS de libre distribución solo funcionan
en el sistema operativo Linux, siendo necesario analizar, diseñar y crear una
aplicación que funcione bajo el sistema operativo Windows y proporcione un
método de seguridad diseñado mediante el cifrado con el algoritmo simétrico
3DE96 en cada equipo de un texto que será formado por la concatenación de la
Ipv4, la Mac Adress y un texto definido por el programador, dicha cadena será un
identificador único de cada equipo conectado a la red de computadoras, esta será
enviada al servidor una vez que éste lo encuentre conectado, es decir el servidor
al momento de encontrar un equipo en la red le solicitara el envío de una cadena
cifrada al cliente, la cual será comparada con una cadena previamente obtenida
en el servidor, de esta manera se determinara si el equipo es un intruso o no.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>OBJETIVO GENERAL
El presente trabajo tiene como objetivo desarrollar software que permita detectar
un equipo no autorizado o extraño dentro de la red de datos, y con base a una
política de seguridad, determinar la autenticidad del equipo, usando el
criptosistema TRIPLEDES96.
OBJETIVOS PARTICULARES
Analizar el problema de manera algorítmica.
Intercambiar información entre cliente y servidor.
Encriptar la dirección IPV4, la dirección mac, de un equipo de cómputo, de
tal manera que la cadena obtenida nos garantice la autenticidad del equipo
dentro de la red de datos.
Integrar el análisis obtenido mediante el lenguaje de programación C#.
Evaluar la aplicación o software.
El software se ejecutara en el sistema operativo Windows. </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El problema de mantener un entorno seguro es que existe avance en el área de
seguridad, y también existe en el área contraria, es por eso que una aplicación
para detectar intrusos no garantiza que esté vigente por mucho tiempo, se dice en
el área de seguridad que no existe método de protección que sea eterno.
Debido a que por su propia definición una red es una interconexión de
dispositivos electrónicos, esto indica que si un equipo cualquiera logra tener
acceso al medio de comunicación de dicha red, también puede establecer
comunicación con los demás equipos, accediendo a su información para
obtención, modificación o borrado del mismo, debido a esto un sistema detector de
intrusos tiene como objetivo mantener un monitoreo constante a los dispositivos
interconectados y mediante un intercambio de información validar que el equipo es
un equipo admitido. 
Los sistemas detectores de intruso han resultado una solución aceptable a las
intrusiones dentro de las redes, y lo han realizado mediante distintos algoritmos o
métodos, se pretende lograr a través de la tecnología el desarrollo de una
herramienta que utilice un sistema de cifrado mediante el algoritmo 3DES96 y que
se encargue de cifrar un texto que será formado por la concatenación de la Ipv4, la
Mac Adress y un texto definido por el programador , así como la unión de distinta
áreas como son la de las redes de datos y programación y de esta manera
fortalecer el método de autenticación entre los clientes y el servidor.
La mayoría de los IDS basan su funcionamiento en capturar tráfico de red
(información, paquetes, segmentos, datagramas, etc.) y verificar que es o que
viaja en su contenido, sin embargo lo que se propone es crear un IDS que base su
esquema de detección de intrusos mediante la autenticación de clientes. </Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Proteger la información en una organización es de vital importancia, se debe
procurar preservarla, es decir tratar de que se encuentre de manera íntegra, que
se encuentre disponible y que el acceso a ella sea de manera confidencial.
Existen distintas técnicas para comprometer un sistema o la información, el
propósito del proyecto no es contrarrestar los ataques sino evitar que el atacante
se conecte al dispositivo para ejecutar métodos que corrompan el sistema.
Se necesitaría tener un experto en cada área para proteger de ataques al
sistema, es por eso recomendable tomar las precauciones necesarias para evitar
una conexión no deseada, en la práctica una persona con suficientes
conocimientos, primero necesita establecer una conexión a los sistemas, la
intrusión no es inmediata, para que tenga éxito, el intruso debe realizar trabajo de
campo, investigar medios de conexión, sistemas operativos que se usan en la
empresa, saber vulnerabilidades de los mismos, es decir construir una estrategia
adecuada para tratar de penetrar cada entorno.
Es importante diseñar un esquema de seguridad que permita tener un mayor
control sobre los accesos al entorno de red o la infraestructura, un IDS por sí solo
no garantizara la seguridad de manera completa, se deben delimitar de ser posible
el número de conexiones existentes, así como permitir solo la ejecución de
aplicaciones autorizadas dentro de la organización, otra recomendación seria solo
tener habilitados los puertos necesarios, este esquema es muy aparte del IDS que
se desarrolló.
La herramienta diseñada permite detectar intrusos en todo el segmento de red en
el que se encuentre instalada, no consume gran cantidad de recursos y permite
que se trabaje en segundo plano, gracias a que el cliente trabaja como un servicio
de Windows, la arquitectura del IDS es del tipo cliente-servidor, el cual trabaja en
base a peticiones y respuestas, éste intercambio de información solo se hace
cuando es necesario, es decir el servidor no envía mensajes de tipo broadcast, lo
cual generaría tráfico en la red o saturación, sino que el servidor espera a detectar
un equipo conectado e intenta establecer comunicación.
La rapidez de respuesta y ejecución depende de las características propias de los
equipos y de la configuración de la infraestructura de red, aunque se demostró que
equipos con pocos recursos tanto de memoria como de procesador pueden
ejecutarlos sin problema, el trabajar a nivel de sockets permite que la
comunicación sea rápida y aunque existe un riesgo también de trabajar a bajo
nivel, el proyecto garantiza que equipos no autorizados sean detectados como
intrusos. 
Con base en las pruebas realizadas se logró el objetivo de desarrollar una
aplicación que sea capaz de detectar un equipo no autorizado dentro de la red y
en base a la política de seguridad de asignación de ip’s estáticas y en conjunto
con un control criptográfico TRIPLEDES96 se obtuvo un sistema que no requiere
de equipos robustos y que no afecta al rendimiento de los equipos de cómputo.
Siendo el proyecto desarrollado en esta tesis el único IDS que utiliza el método de
autenticación para detectar intrusos en la red. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
Sistema Monitor Detector de Intrusos usando TRIPLE-DES96
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
P R E S E N T A
GILDARDO JARED DIAZ ECHEVERRIA
DIRECTORES DE TESIS:
Dr. ROLANDO FLORES CARAPIA
Dr. VÍCTOR MANUEL SILVA GARCÍA
MÉXICO, DF. Julio de 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>MODELADO DINÁMICO Y CONTROL DE UN CUADROTOR PARA EL SEGUIMIENTO DE TRAYECTORIAS</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Se cuenta con un cuadrotor que tiene una estructura basada en 4 motores, cada motor
acoplado con una propela que genera un empuje, 2 propelas giran en sentido horario y las otras
2 en sentido anti-horario. Un sistema de estas características tiene gran complejidad como en
cualquier sistema aerodinámico. Para tener la capacidad de manipular el comportamiento de
un cuadrotor es necesario formular una estrategia de control, que permita el seguimiento de
trayectorias contemplando la presencia de fuerzas externas.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar, desarrollar, simular e implementar una estrategia de control que permita el seguimiento
de trayectorias para un cuadrotor.
1.6. Objetivos Específicos
Buscar al menos 2 modelos dinámicos diferentes existentes de cuadrotores reportados en la
literatura.
Análisis y simulación del modelo dinámico del cuadrotor.
Realizar pruebas de funcionamiento con la placa de control del cuadrotor.
Buscar al menos 2 leyes de control en la literatura y hacer un análisis comparativo de
ventajas y desventajas.
Diseñar, desarrollar y simular una ley de control para el prototipo.
Implementación de una ley de control en el prototipo real.
Probar el funcionamiento de la ley de control con el cuadrotor.
Someter los resultados obtenidos en al menos un artículo científico.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los cuadrotores han sido, por excelencia en los últimos años, la plataforma aérea de investigación
y desarrollo gracias a sus prestaciones y aplicaciones a las cuales pueden someterse [30]. En
el centro de innovación y desarrollo tecnológico en cómputo (CIDETEC) no se han trabajado con
este tipo de aeronaves, por ello se pretende que este trabajo sea el inicio de la investigación aérea
dentro del centro</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La realización del objetivo general y los específicos del trabajo se desarrollarán de acuerdo a
las siguientes actividades en un tiempo establecido para que el trabajo tenga una duración de 2
años como máximo. En la figura 1.3 se aprecia de forma general el trabajo de tesis y en la figura
1.4 se muestra a detalle el tiempo estimado para la realización de cada una de las actividades.
1. Estudio del estado de arte.
2. Adquisición del cuadrotor de la empresa 3DR Robotics.
3. Ensamblar e interconectar el prototipo.
4. Reconocimiento de la placa de control del cuadrotor adquirido.
5. Estudio de herramientas y conocimientos básicos para el desarrollo del proyecto.
6. Manejo de herramientas de simulación Matlab-Simulink.
7. Estancia CINVESTAV Monterrey
8. Manejo del software de programación de la placa de control.
9. Manejo del módulo de transmisión de datos inalámbrico 3DR Radio y obtención de datos
enviados desde el cuadrotor.
10. Modelado dinámico, obtención de la matriz de tensor de inercias y simulación del prototipo.
11. Desarrollo de una técnica de control que resuelva el problema de seguimiento de trayectorias.
12. Desarrollo del programa de la técnica de control que se implementará en el cuadrotor.
13. Escritura de la tesis.
14. Examen de grado
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El trabajo de esta tesis se centro en el modelado cinemático y dinámico para el control de
un cuadrotor para resolver el seguimiento de trayectorias. Así también el diseño de una ley de
control que resuelva de forma eficiente las tareas de seguimiento de trayectoria.
Para tener una visión general del cuadrotor se partió de la investigación y entendimiento de los
modelados dinámicos presentados a lo largo de los últimos años. Para lo cual se realizó una estancia
de 3 meses de investigación en el Centro de Investigación y Estudios Avanzados (CINVESTAV)
Unidad Monterrey. Teniendo una idea del modelado de un cuadrotor se comenzó deduciendo los
movimientos de rotación y translación del cuadrotor generados por las distintas combinaciones de
los empujes de cada uno de los motores lo cual generó ecuaciones que permitieron generar un
modelado dinámico simplificado del cuadrotor. La orientación del vehículo se realizó por medio
de rotaciones sucesivas en los tres ejes lo cual diferencia los ángulos de Tait-Bryan a los ángulos
de Euler.
Se modelaron y simularon 3 distintas leyes de control, en primer lugar el controlador PID
mostró gran respuesta en simulación para las tareas de regulación y seguimiento ya que se
obtuvieron gráficas en las salidas de control suaves para los movimientos del cuadrotor este
controlador se trabajo de manera desacoplada teniendo por un lado el cálculo de la ley de control
para posición dependiente del de orientación y de otro lado el control de orientación. El controlador
de saturaciones anidadas utilizando el modelo Euler-Lagrange se comportó en regulación con
eficiencia y con salidas suaves para llegar a las posiciones deseadas, sin embargo, no ocurrió
lo mismo en tareas de seguimiento. Se propuso un nuevo controlador modificando saturaciones
anidadas + PD de la misma forma utilizando el modelo Euler-Lagrange, el cual tiene las mismas
prestaciones en regulación pero en seguimiento mejora ya que converge a las posiciones deseadas
en cada momento con ligeros retrasos a diferencia de saturaciones anidadas.
Se realizó una comparación de las estructuras de control desarrolladas en este trabajo PID,
saturaciones anidadas y saturaciones anidadas + PD, esta última se desarrolló y se propone como
una ley de control que resuelve las tareas de seguimiento de trayectoria.
Para las pruebas experimentales se trabajó con 2 plataformas diferentes.
El cuadrotor ArDrone de la empresa Parrot fue la primer plataforma en la que se llevaron acabo
trabajos de control la cual se obtuvó el control de los ángulos para generar un desplazamiento,
por lo mencionado al no tener control directo de los motores esta plataforma no es ideal para desarrollar el objetivo de este trabajo.
El cuadrotor ArduCopter de la empresa 3DRobotics fue la segunda plataforma en la que se
hicieron pruebas, la plataforma permitió un control de cada uno de los motores, lectura de sensores
esto último decanto por esta plataforma. Para obtener la posición del cuadrotor en ambientes
controlados se necesitaba de un sistema complejo de alto valor económico, esto llevó a que se
buscará una alternativa para obtener la posición del cuadrotor es por ello que el GPS se eligió y
el cual tiene una precisión de ±3 metros. El problema que se encontró es que en un ambiente
cerrado el gps no brinda ningún dato lo cual orillo a que el sistema se sometiera a pruebas en
ambientes no controlados. Esto último no se propuso en la tesis e implica una mejora al objetivo
planteado.
Por lo anterior, se eligió el controlador PID para la realización de las pruebas experimentales,
la implementación del controlador se realizó de forma satisfactoria. Este controlador se mostró
estable para tareas suaves velocidad y dirección.
La plataforma ArduCopter tiene grandes ventajas en software, hay muchos trabajos en la red
que se pueden encontrar y servir como guía para el desarrollo, por el contrario, el inconveniente
principal es la fragilidad de la plataforma a la caída durante las pruebas.
Las posibles líneas de trabajo a futuro es el modelado dinámico de un cuadrotor con álgebra
geométrica, al momento de realizar este trabajo no se han encontrado investigaciones que relacionen
la álgebra geométrica y cuadrotores. Para trabajos posteriores se trabajará en la implementación
de seguimiento de trayectorias dictadas por el usuario de forma aleatoria en línea.
Actualizar la plataforma de trabajo el ArduCopter por el nuevo cuadrotor de la empresa
3DRobotics IRIS. El cual presenta mejoras como la implementación de un procesador ARM,
nuevos motores, mejora en la arquitectura del cuadro base entre otras nuevas prestaciones.
Para simulaciones del cuadrotor se pretendrá usar el modelado dinámico de Newton-Euler que
considera los efectos aerodinámicos que se presentan en el vuelo de un cuadrotor, así también
tomando las incertidumbres paramétricas como perturbaciones externas.
Para la orientación del vehículo trabajar con cuaterniones y álgebra geométrica para realizar
una comparación del tiempo de cómputo para calcular la orientación del vehículo.
Una posible línea de trabajo es realizar un modelado dinámico para las nuevas aeronaves con
cambio de sentido de giro de los motores partiendo de la plataforma Invertix 400.
A lo largo del desarrollo de la tesis se sometió el siguiente trabajo.
A. Ponce, G. Sepúlveda. Model and Control of Quadrotor for Trajectory Tracking. In 10th
International Congress Technological Trends in Computing CTTC 2014.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
MODELADO DINÁMICO Y CONTROL DE UN CUADROTOR PARA EL
SEGUIMIENTO DE TRAYECTORIAS
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO:
PRESENTA:
GERARDO ARTURO PONCE DE LEÓN ZÁRATE
DIRECTOR DE TESIS
DR.GABRIEL SEPÚLVEDA CERVANTES
México, D.F. Diciembre 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Modelado Computacional de la Estructura del Citoesqueleto en las células Gliales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La elaboración de la experimentación para estudiar la relación que existe entre la morfología del citoesqueleto y la migración resulta compleja en términos de los recursos económicos, materiales y de tiempo utilizados, es entonces cuando resulta necesario contar con un modelo que permita simular el cambio de la estructura del citoesqueleto ante fuerzas que estimulan la migración celular y así predecir su dirección en termino de esas fuerzas</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general del trabajo
Obtener un modelo computacional tensorial del cambio de la estructura del citoesqueleto
de la línea celular C6 durante el proceso de migración.
1.4.1. Objetivos particulares
Para lograr el objetivo general se plantean los siguientes objetivos particulares.
1. Determinar las herramientas matemáticas que serán utilizadas.
2. Obtener de manera analítica el modelo tensorial de los cultivos celulares adquiridos.
3. Realizar la programación en Matlab para el modelo tensorial.
4. Determinar las técnicas de tratamiento digital de imágenes para caracterización
de las células.
5. Presentación de la distribución de la AT del citoesqueleto en 3D.
6. Presentar el cambio de la disposición de los elementos de la AT del citoesqueleto
bajo influencia de tensión externa.
7. Estimación de la dirección de la migración en términos de disposición de los elementos
de la AT del citoesqueleto. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Un modelado se concibe a partir de las distintas herramientas que serán utilizadas y
una metodología a seguir para conseguir el objetivo a modelar. La metodología a seguir
en el modelado de estructura del citoesqueleto se lista a continuación:
1.- Elección de la línea celular que será analizada. En el caso de estudio del presente
trabajo es la línea celular C6.
2.- Elección de la AT que ser a utilizada durante el modelado de la estructura del
citoesqueleto de la línea celular.
3.- Caracterizar la disposición de los elementos de la AT en un estado de estabilidad,
es decir, en ausencia de fuerzas externas, lo que sugiere un estudio de la célula en
estado aislado de otros agentes externos que pudieran influir en la disposición de
la estructura del citoesqueleto.
4.- Definir las restricciones y la disposición de los elementos de la AT de acuerdo
a la función fisiológica a modelar (migración). Es importante durante este paso
posicionar a la AT en un sistema de referencia con la finalidad de medir los desplazamientos
de cada nodo de la estructura al colocar parámetros de entrada en
el modelo construido.
5.- Identificar los elementos que conformaran a la AT, así como su caracterización
en términos de las propiedades físicas que se necesitan para calcular cada una de las matrices de rigidez que serán necesarias para la construcción de la matriz de
rigidez de estructura completa, es decir de la AT.
6.- Definir la dirección de referencia de cada elemento de la AT para considerar los
´ángulos directores dentro de la matriz de rigidez de elemento.
7.- Construir cada una de las matrices de rigidez de los elementos de la AT.
8.- A partir de las matrices de rigidez de elemento estructural se construir ‘a la matriz
de rigidez de la AT.
9.- Elección de las fuerzas que serán aplicadas a la AT para modelar la migración.
10.- Estimación de los desplazamientos después de aplicado el vector de fuerzas. Esto ´
a través del modelo computacional.
11.- Calculo de las nuevas posiciones de los nodos de la estructura.
12.- Construcción de la nueva disposición de los nodos de la AT, ´esto es el cambio de
la estructura del citoesqueleto.
13.- Actualización de las matrices de rigidez de los elementos y por consecuencia de la
matriz de rigidez de la AT.
14.- Proceso iterativo a partir del paso 9, para visualizar el cambio en la disposición
del citoesqueleto en función de las fuerzas externas que se simulan. 
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Durante el desarrollo del presente trabajo de tesis se hizo uso de las herramientas
matemáticas necesarias para la realización del modelo matemático tensorial de la
mecánica del citoesqueleto y el cambio en su morfología para la estimación de la dirección
de la migración. Una vez obtenido el modelo matemático se realizó el modelo
computacional tensegridal.
Respaldado en los resultados obtenidos durante las pruebas realizadas al modelo
computacional generado a partir de un análisis matricial de rigidez de una AT, en el
caso particular del presente trabajo del OT, se pudo verificar que es posible modelar
la mecánica del citoesqueleto a través del enfoque estructural de Temeridad. Aun más
existen distintas fuerzas externas que convergen en una célula, ya sea generadas por
otras células, por la MEC o por cualquier otro agente fisiológico propio de su función de
tal forma que en relación a su entorno externo habrá un cambio en su estructura. Para
verificar el cambio en la estructura del OT, se presentaron las gráficas correspondientes a
la disposición de sus elementos conforme fuerzas externas se aplicaban a nodos específicos
de la estructura.
Como objeto de riguroso análisis matemático, mecánico y estructural el desarrollo
de la presente tesis se enfocó en la migración celular, teniendo como resultado a lo largo
del modelado que hay una relación entre el cambio en la estructura del citoesqueleto y
la migración celular, así pues, cuando una célula entra en un proceso de migración, si
se aborda estructuralmente, los elementos que la conforman cambian de posición y la
longitud de dichos componentes no se mantiene constante para favorecer dicha función.
En términos generales y como resultado del proceso de investigación realizado, para
analizar la estructura del citoesqueleto se concluye que existe un cambio en la disposición
de los elementos del citoesqueleto cuando la célula se encuentra migrando y se puede
estimar dicha disposición en función del frente de migración ya sea que se proponga o
que por medio de experimentación en laboratorio se determine.
El hecho que la migración obtenida a través del modelo sea del tipo mesénquima da
pauta a poder aplicar el modelo a células tumorales, aquí cabe mencionar que el tipo de modelo que se ha realizado a lo largo del presente trabajo de tesis es del tipo “in
sillico”, es decir; un modelo aplicado a experimentación simulada por computadora y
puede considerarse para efectos de los resultados obtenidos como un modelo aplicable.
Para posteriores trabajos se deberá considerar la validación con pruebas en laboratorio
lo que se conoce como validación “in vitro”. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL ´
CENTRO DE INNOVACIÓN Y DESARROLLO ´
TECNOLÓGICO EN COMPUTO ´
MODELADO COMPUTACIONAL DE LA ESTRUCTURA DEL CITOESQUELETO
EN LAS CÉLULAS GLIALES
TESIS
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN TECNOLOGÍA DE COMPUTO ´
PRESENTA:
FRANCISCO RUBÉN CAMACHO MARTÍNEZ
DIRECTORES DE TESIS:
DRA. MAGDALENA MARCIANO MELCHOR
DRA. MA. EUGENIA DEL CARMEN MENDOZA GARRIDO
MÉXICO, D.F. ´ DICIEMBRE 2014.
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Redes Neuronales Artificiales aplicadas a la detección de Cáncer de Mama</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
El objetivo de este trabajo de tesis es implementar un sistema por medio de redes
neuronales que sea capaz de caracterizar la presencia de cáncer de mama a partir de
la densidad de tejido mamario, la cual puede ser es grasa, graso glandular y denso
glandular. A demás dada la presencia de un tumor, El sistema será capaz de clasificarlo
según sea benigno o maligno.
Objetivos Particulares
Investigar y comprender la fisionomía de los tumores de mama malignos y benignos.
Investigar sobre el tratamiento digital de imágenes.
Investigar y analizar los algoritmos de RNA más eficientes.
Implementar una Interfaz gráfica que permita ver resultados obtenidos.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La mamografía se considerada en la actualidad una herramienta de diagnóstico muy
´útil para la detección y diagnóstico de las lesiones mamarias. Aproximadamente el 10 %
de todas las mujeres desarrollan algún tipo de cáncer de mama y aproximadamente
25 % de todos los canceres diagnosticados en las mujeres son los canceres de mama [21].
Con las opciones de tratamiento actuales, la tasa de supervivencia a 5 años puede ser
tan alta como de un 97 %. Sin embargo, esta tasa cae a 78 % en caso de una enfermedad
avanzada, en consecuencia, la detección temprana es crucial para una terapia eficiente
[16].
La interpretación de la mamografía a menudo es difícil y depende en muchas ocasiones
de la experiencia que tenga el radiólogo. Se han realizado estudios mostrando
que aproximadamente el 9 % de una serie de canceres en una prueba de detección, eran
visibles en las mamografías obtenidas desde 2 años atrás. Por otra parte, en otro 48 %
de los casos, una señal mínima era ya visible en una mamografía previa [22].
El tejido mamario puede contener dos tipos de indicadores de cáncer comúnmente
evaluados por los sistemas de Detección Asistido por Computadora (CAD), es decir,
masas y micro calcificaciones. Las micro calcificaciones son pequeños depósitos granulares
de calcio que aparecen en la mamografía como pequeños puntos brillantes. Un
radiólogo debe examinar cuidadosamente la mamografía con una lupa para localizar
calcificaciones que pueden estar incrustadas en los tejidos muy densos.
Según las Referencias [23, 24], la probabilidad de un proceso maligno dentro de la
mama depende del número, la distribución y la morfología de las micro calcificaciones.
Cuando no está en un grupo, las micro calcificaciones son irrelevantes desde el punto de
vista diagnóstico. Si por lo menos cuatro o cinco micro calcificaciones están presentes y
forman un clúster, la probabilidad de un proceso anormal dentro de la mama es significativa.
Un ejemplo de una mamografía con uno de estos clústeres se representa en la
figura 1.2(a).
Las micro calcificaciones varían significativamente en tamaño, forma y distribución.
Por lo general, los grupos homogéneos que consta de micro calcificaciones grandes, redondas
u ovaladas indican un proceso benigno. Un ejemplo de clúster de este tipo se
representa en la figura 1.2(b).
Por otra parte, grupos heterogéneos que consisten de pequeñas micro calcificaciones,
con formas irregulares se asocian con un alto riesgo de cáncer. Un buen ejemplo de
este grupo se muestra en la figura 1.2(c). Otras anormalidades en el seno importantes
que se muestran en las mamografías son las masas. Se pueden presentar en diversas
partes de senos y tienen diferentes tamaños, formas y límites. Para evaluar el riesgo de
malignidad de la apariencia radiológica se dan en la Referencia [24].  Desde un punto de vista del diagnóstico, la característica más importante de una
masa es la morfología de su límite. En particular, un examen detallado del límite de
la masa permite evaluar la probabilidad de que la masa sea un tumor maligno, masas
con límites bien definidos, bordes afilados suelen ser benignos.
Un ejemplo se representa en la figura 1.2(d). En caso de masas con formas lobuladas,
como en la figura 1.2(e), aumenta el riesgo de malignidad. Sin embargo, los casos más
sospechosos son las masas con límites mal definidos. Un ejemplo de este tipo de masa
se representa en la figura 1.2(f). Tal apariencia de la masa, puede indicar un tumor
maligno.
En la práctica es muy difícil decidir si grupos de micro calcificación son benignos
o malignos. En un número significativo de casos, los grupos no se pueden observar
claramente. Por otra parte, es común para un grupo de micro calcificaciones revelar características
morfológicas que no pueden ser claramente clasificados como benignos o
malignos.
Esto justifica este trabajo de tesis, dadas algunas características comunes de los
tumores ya sea malignos o benignos y con ayuda de las RNA, pueden ser representadas
por patrones y ser clasificadas. 
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se analizaron las mamografías obtenidas de la base de datos MIAS, para determinar
el mejor procesamiento de dichas imágenes.
Se implementó un método automático de segmentación de mamografías, el cual
elimina tanto el ruido como son las etiquetas y pequeños puntos, así como el
musculo pectoral, para solo obtener el tejido mamario.
A partir de la literatura se revisaron técnicas de extracción de características de
una imagen y en mamografías se determinó que las mejores características son las
de tipo estadísticas, ya que proporcionan mejor información del Tejido Mamario.
Se implementó el desarrollo del perceptrón multicapa con conexiones hacia adelante
y hacia atrás usando la regla de aprendizaje backpropagation usando la
variante del Gradiente conjugado.
Se realizaron varias pruebas con diferentes conjuntos de prueba y entrenamiento,
esto con el fin de comparar resultados con las diversas referencias mostradas en
la tabla 4.26.
Se observó que el despeño del trabajo propuesto oscila entre el 85.73 % y 95.75 %
dependiendo del conjunto de aprendizaje y prueba.
También se generaron trabajos de investigación y divulgación científica de este
trabajo de tesis [61–65].
Se implementó un sistema que ayudar a disminuir los falsos positivos en los diagnósticos
de cáncer de mama, teniendo un buen rendimiento el cual ronda entre 85.73 % y 95.75 % y por lo cual ayuda a reducir las biopsias y cirugías innecesarias y apoyando al
diagnóstico de esta enfermedad, utilizando herramientas computacionales como son el
procesamiento digital de imágenes y redes neuronales. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL ´
CENTRO DE INNOVACIÓN Y DESARROLLO ´
TECNOLÓGICO EN COMPUTO ´
REDES NEURONALES ARTIFICIALES APLICADAS A LA
DETECCIÓN DE CÁNCER DE MAMA
TESIS
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN TECNOLOGÍA DE COMPUTO ´
PRESENTA:
HUGO FLORES GUTIÉRREZ
DIRECTORES DE TESIS:
DR. ROLANDO FLORES CARAPIA
DR. BENJAMÍN LUNA BENOSO
MÉXICO, D.F. ´ AGOSTO 2015.
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema de reconstrucción de un modelo tridimensional de piezas dentales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
Implementación de un sistema de generación de modelado tridimensional
para piezas dentales a partir de datos obtenidos de radiografías y fotografías.
Objetivos específicos
1. Estudio y análisis de trabajos relacionados con el tema.
2. Obtención y adecuación de las imágenes dentales (fotografías y radiografías).
3. Adquisición de datos relevantes de las radiografías y fotos (procesamiento
y extracción de puntos).
4. Generación del modelado tridimensional para las piezas dentales.
5. Despliegue y presentación del modelo tridimensional.
6. Evaluar y probar el sistema implementado. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En la actualidad se poseen dos extremos de diagnósticos para problemas
bucales; el primero de ellos, las radiografías, son económicas y no requieren
de gran tiempo para ser realizadas y evaluadas, además de que su nivel de es,
en su mayoría, acertado para lo que un dentista busca en una herramienta
de diagnóstico. A pesar de estas ventajas se sabe que las radiografías no proporcionan
ciertas características deseadas, como la profundidad, el tamaño
y la posición exacta en la cual se encuentra un diente.
Por el lado contrario, la TAC (tomografía computarizada) proporciona
una visión más completa sobre el problema a tratar, sin embargo, el equipo
y el personal que se requiere para este tipo de estudio es mucho más especializado
que el que se necesita para una radiografía. En cuanto al costo, este
tipo de estudio es bastante elevado comparado con respecto a una imagen
obtenida por rayos X.
Otro punto a considerar es el desarrollo de sistemas CAD/CAM que han
revolucionado a la industria dental, pero el costo de estos es bastante elevado
por lo que limita su adquisición para odontólogos que trabajan de manera
independiente.
Debido a los motivos antes expuestos es que se pensó en diseñar un
sistema de ayuda en el diagnóstico de patologías dentales de bajo costo y
que combinara las características de ambos métodos descritos (radiografías  y TAC); basado solo en una radiografía y máximo dos fotografías para la
generación de un modelo en 3D que apoye al dentista en la toma de decisiones;
además de una PC o laptop para cargar el sistema y generar el modelo;
lo cual reduce el costo, el tiempo y mejora la precisión en el diagnostico
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Después de haber estudiado y analizado trabajos relacionados con el tema
en la presente tesis se desarrolló un sistema de reconstrucción de un
modelo tridimensional de piezas dentales, a través de la obtención y adecuación
de imágenes digitales (fotografías y radiografías), de las cuales se
adquirieron los datos relevantes y necesarios para la generación del modelo
3D por medio de técnicas de visión artificial y extracción de puntos. Para el
diseño 3D fue empleado el lenguaje conocido como X3D, para el despliegue
y la interacción del modelo desarrollado fueron necesarios otros lenguajes de
programación tales como JAVA y HTML.
La adquisición de la imagen de la cara oclusal fue posible gracias a una
cámara intraoral, mientras que la radiografía es posible adquirirla a través
del centro radiológico que indique el odontólogo.
Una vez que se poseen las imágenes, estas fueron sometidas a un procesamiento
para extraer los datos de interés y pasar a la creación del modelo 3D
de la pieza dental. El primer paso realizado fue la segmentación de la pieza
dental a través del algoritmo conocido como “grabcut”, una vez aislado el
diente objetivo, este necesita pasar por un filtrado gaussiano para eliminar
el ruido restante, acto seguido se realiza un umbralizado para mejorar la detección
de bordes que fue realizada implementando el algoritmo de “Canny”.
Toda vez que se detectaron exitosamente los bordes, fue aplicado un algoritmo
de seguimiento de contorno que permitió conocer la ubicación espacial
de cada uno de los pixeles que componen las imágenes antes mencionadas.
Al tener la ubicación de los pixeles de cada cara, se procedí o al modelado
3D de la pieza dental tomando como base la cara oclusal, la cual se divide
en el sentido horizontal de acuerdo a la resolución indicada por el usuario a
través de un algoritmo denominado como: “línea de búsqueda” propuesto y
desarrollado en este trabajo. Acto seguido es proyectada la cara proporcionada
por la radiografía de acuerdo a la división mencionada anteriormente.
Por ´ultimo las caras proyectas son enlazadas a través de un algoritmo de
unión de caras planteado también en este documento, completado este paso,
el modelo 3D de la pieza dental queda listo.
Todo el proceso anteriormente descrito es realizado a través de una GUI
(Graphic User Interface) la cual es una interfaz amigable, practica e intuitiva
para el usuario y que facilita el proceso de supervisión e interacción entre el usuario final, el procesamiento de la imagen y la generación del modelado
3D de la pieza dental. Por lo expuesto anteriormente se comprobó de manera
satisfactoria la hipótesis inicial propuesta en este documento. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
“Sistema de reconstrucción de un modelo tridimensional
de piezas dentales”
Tesis que
para obtener el grado de:
Maestría en Tecnología de Cómputo
Presenta:
Ing. Eduardo Galicia Gómez
Directores:
M. en C. Miguel Hernández Bolaños
Dr. Mauricio Olguín Carbajal
Ciudad de México, Junio 2016</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Implementación del criptosistema AES con permutación variable para el cifrado de imágenes en hardware</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Implementar el algoritmo de cifrado Advanced Encryption Standard (AES) con permutación
variable para el cifrado de imágenes en un Arreglo de Compuertas Programables
en Campo FPGA.
1.2.2. Objetivos específicos
Aplicar el algoritmo AES de llaves de 128 bits para el cifrado de imágenes procesadas
por la tarjeta Xilinx Spartan-3AN.
Programación de la tarjeta de Xilinx Spartan-3AN para el procesamiento de imágenes en tiempo real.
Programación en paralelo del algoritmo AES de llaves de 128 bits.
Envió de la imagen cifrada y descifrada por medio de un dispositivo USB. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La investigación y desarrollo de algoritmos criptográficos es trascendental, ya que
la información se ha vuelto uno de los principales activos para nuestra sociedad. De ahí
que el aseguramiento en la transmisión de la información, la mejora de los algoritmos
existentes, y la necesidad real de proteger los datos de intrusos, sea la justificación para
implementar un algoritmo criptográfico que minimice la vulnerabilidad de la transmisión
de información que se hace en diversas frecuencias, misma que puede llegar a ser
sensible a intercepción y robo.
El desarrollo de un algoritmo más robusto garantizará la confiabilidad, disponibilidad
e integridad de la información, lo cual se traducirá en un aporte significativo al
desarrollo tecnológico. Técnicamente el desarrollo se realiza mediante la implementación
del algoritmo AES en hardware FPGA, mismo que se va a encargar del cifrado y
descifrado de la imagen, además de que es altamente adaptable, reprogramable, rápido
y flexible; características que son útiles para implementar un sistema criptográfico
tecnológicamente avanzado.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Como conclusión de esta tesis se puede decir que fue implementado el algoritmo
AES con permutación variable para el cifrado y descifrado de imágenes mediante el
dispositivo FPGA, con una conexión serial (USB-TTL), con base en histogramas y medición
de la entropía que representan las características de la imagen cifrada y descifrada.
Dicha permutación se le agregó al algoritmo AES, porque al cifrar una imagen de
una gama de colores baja esta podía ser identificada, además de que en las mediciones
de entropía medían debajo de 7, indicando que desorden de pixeles de la imagen no es
óptima, para que ésta sea óptima tiene que tener un valor aproximado a 8. Sin embargo,
al agregar la permutación a las imágenes se tuvieron resultados favorables, tanto en
la visión de la imagen, la cual ya no es visible y teniendo como resultado en la entropía
de cada imagen 7.99.
Con respecto a los tiempos se puede decir que dicha implementación es sumamente
rápida, ya que se tiene como tiempo 7.51 ns por ciclo de reloj, en esta implementación
se utilizan 23 ciclos de reloj, por lo cual se tiene 172 ns cifrando un bloque de 128
bits. En cuestión de segundos 5,782,608 bloques de 128 bits por segundo, concluyendo
que este algoritmo es óptimo implementado en hardware que en software por la velocidad
que está manejando, además de que es más robusto ante ataques.
Así mismo, se desarrolló la interfaz gráfica para que el usuario verificara que realmente
el cifrado y descifrado de las imágenes es mediante hardware.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
Implementación del criptosistema AES con
permutación variable para el cifrado de
imágenes en hardware
Tesis que para obtener el grado de Maestría en Tecnología
de Cómputo
Presenta:
Salma Janet Maldonado Cuautenco
DIRECTORES
Dr. Rolando Flores Carapia
Dr. Víctor Manuel Silva García
Ciudad de México, Agosto de 2016</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Cifrado de audio por medio del algoritmo Triple-DES-96</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Las telecomunicaciones actuales son parte primordial de la sociedad y además han
evolucionado significativamente, ya que tienen la capacidad de transportar datos en forma de
texto, imagen, voz y vídeo, lo cual pueden llegar a transmitirse en altas velocidades. Por esta
razón, dichos sistemas son “atacados” constantemente por “hackers”, por lo cual requieren ser
encriptados y protegidos evitando así fugas de información valiosa.
En esta tesis se propondrá un sistema de cifrado de audio, el cual, con algunas modificaciones
podría llegar a ser implementado (en un futuro) para cifrar llamadas de voz en tiempo real, sin
embargo, ese no es el objetivo principal en estos momentos.
Para implementar el sistema de cifrado, pudo utilizarse el algoritmo Triple DES, ya que hasta la
fecha no ha sido vulnerado por ataques de criptoanálisis, así como tampoco por ataques de
fuerza bruta. Su principal problema es que la triple implementación que requiere, que provoca
que su ejecución sea demasiado lenta. Para poder solucionar de algún modo esta situación, se
hará uso de Triple-DES-96, que es más rápido que el anterior, esto sin perder su complejidad
matemática.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo> Objetivo General.
Implementar el sistema de cifrado Triple-DES-96 para su aplicación en audio, el cual cifrará
información de forma robusta.

Objetivos Particulares.
1. Desarrollar un sistema de cifrado de audio sin compresión, el cual será un prototipo de
software.
2. Programarlo haciendo uso del lenguaje de programación C++.
3. Dotarlo de una interfaz gráfica haciendo uso del software C++ Builder.
4. El sistema será implementado con el algoritmo Triple-DES-96, se demostrará que
tendrá un desempeño más rápido que si se construyera con Triple DES.
5. El sistema será una solución eficaz, es decir, se comprobará a través de mediciones
matemáticas que el cifrado es suficientemente aleatorio.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El audio contiene información de distintos tipos y de acuerdo a su uso puede ser de gran
importancia o no. Las llamadas de voz sin duda alguna la tienen, por eso mismo, en tiempos
recientes y públicamente se han conocido casos en los cuales se han interceptado y grabado.
Algunos de los casos más celebres, han sido aquellos en los cuales se han visto involucrados
personajes de la política mexicana. A continuación, se presentarán cuatro de los más
importantes que se han suscitado, esto se hará en estricto orden cronológico.
23 de abril del 2002. El ex presidente de cuba Fidel Castro Ruz revela la grabación de una plática
telefónica que sostuvo con el entonces presidente de México, Vicente Fox Quezada. En dicha
grabación se revelaba que Fox le pidió al ex mandatario cubano que se retirara después de un
almuerzo realizado en una cumbre realizada en la ciudad de Monterrey, en dicho suceso surgió
la famosa frase “Comes y te vas” [4].
14 de febrero del 2006. A través de los principales medios de comunicación se da a conocer la
grabación de una llamada telefónica interceptada, en la cual se escuchaba una conversación
telefónica llevada a cabo entre el entonces gobernador del estado de Puebla, Mario Marín y el
empresario Kamel Nacif. En dicha llamada se revelaban las intenciones de ambos personajes de
tomar acciones en contra de la periodista Lydia Cacho [5]. 
2 de julio del 2006. El mismo día de las elecciones presidenciales de 2006 en México, se
interceptaron llamadas telefónicas entre la ex líder sindical del SNTE, Elba Esther Gordillo y el
ex gobernador del estado de Tamaulipas, Eugenio Hernández, así como de este último con el ex
secretario de Comunicaciones y Transportes, Pedro Cerisola, en las cuales se hablaba de
supuestos pactos a favor del entonces candidato a la presidencia de México, Felipe Calderón
Hinojosa. Dichas llamadas telefónicas interceptadas serían reveladas públicamente días
después [6].
8 de enero del 2016. El chapo Guzmán es capturado en Sinaloa por tercera vez tras su fuga del
penal del Altiplano en Almoloya. Llamadas telefónicas interferidas por la DEA y NSA
norteamericanas en trabajo conjunto con la Inteligencia de la Secretaría de Marina de México
permitieron ubicarlo en el denominado Triángulo Dorado y, semanas después, en un domicilio
de Los Mochis, Sinaloa, donde se organizó una operación de vigilancia permanente que derivó
en su aprehensión final [7].
Por otro lado, la práctica de interferir llamadas de voz, también ha sido adoptada por
organismos gubernamentales, justificándolo con fines de preservación de la seguridad
nacional, sin embargo, este es un tema que ha causado mucha controversia entre la población
en general, la cual teme por su privacidad, porque cualquiera puede ser espiado ante la mínima
sospecha de poder llegar a ser una amenaza, comportamiento que llevan a cabo instituciones
como la PGR [8]. Además, el espionaje se ha implementado en la nueva Ley Federal de
Telecomunicaciones y Radiodifusión en los artículos 189 y 190 de la misma [9], por lo cual
actualmente se considera una práctica legal en México.
Internacionalmente, también existen instituciones que se rigen por las mismas políticas. La más
conocida es la NSA, que es una agencia norteamericana encargada de la seguridad de los
Estados Unidos de América, la cual, como se ha revelado, realiza espionaje telefónico
indiscriminado, he aquí las pruebas que se han recolectado.
19 de mayo del 2014. En un amplio artículo publicado en el sitio web The Intercept, los
periodistas: Ryan Devereaux, Gleen Greenwald y Laura Poitras, revelaron que la NSA interfiere
llamadas telefónicas de dispositivos móviles en países como Bahamas, Filipinas, Kenya y México
[10]. Este mismo lo realizó durante tiempo en España y fue revelado en 2013 [11].
También, existen evidencias que demuestran que los hackers son capaces de interceptar y
grabar llamadas telefónicas [12], lo que implica que aún faltan muchas cosas por hacer ante
esta grave amenaza de seguridad.
Además del espionaje telefónico, existen otras problemáticas no menos importantes que se
relacionan con audio, como la piratería, que es un mal que es presente en varios países
alrededor del mundo como lo es México. Por medio de sistemas de cifrado de audio, es posible
solucionarlo.
Proteger la confidencialidad es un asunto muy serio y digno de investigación y desarrollo, lo
que permite llevar a cabo este trabajo de tesis.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>1. El primer punto de los objetivos particulares se cumplió completamente, ya que el
prototipo desarrollado ha demostrado que es capaz de cifrar y descifrar audio. Además,
el proceso es realizado sobre audio sin compresión, ya que el sistema utiliza únicamente
archivos en formato WAV.
2. Como ya se mostró en el capítulo 4 de este trabajo de tesis, la programación total del
sistema de cifrado de audio se realizó en su totalidad con C++, por lo cual el segundo
punto de los objetivos particulares se cumplió completamente.
3. Así mismo, en el capítulo 4 se mostró que la interfaz se construyó en su totalidad con el
IDE C++ Builder. Por lo cual, el tercer punto de los objetivos particulares se cumplió
completamente.
4. En el capítulo 3 de este trabajo de tesis se dio una descripción detallada de los
algoritmos DES, Triple DES y Triple-DES-96, así como sus diferencias entre estos dos
últimos. Desde ahí se estableció que Triple-DES-96 es más veloz que Triple DES, en más
del doble, se citaron fuentes que también lo comprobaban. Pero para respaldarlo, en el
capítulo 5 de este trabajo de tesis se presentaron resultados de las pruebas realizadas
al cifrar archivos idénticos con ambos algoritmos. Si Triple DES se ejecuta en un tiempo
determinado correspondiente al 100% en la mayoría de los casos, como mínimo TripleDES-96
realizó el mismo proceso en un 60% del mismo. Por lo tanto, el punto 4 de los
objetivos particulares también se cumplió completamente.
5. Por último, se demostró que el sistema construido es eficaz y que el cifrado que realiza
es aleatorio. En el capítulo 3 de este trabajo de tesis se describieron por completo las
características y las formas de calcular las magnitudes de aleatoriedad a las que éste fue
sometido, éstas son, entropía, coeficiente de correlación y X2 (Ji Cuadrado o Chi
cuadrado). Ahí mismo se mencionó que posibles resultados debían ser obtenidos para
cumplir con un cifrado aleatorio. En el capítulo 5 se mostraron los resultados obtenidos,
los cuales concuerdan en su totalidad con lo que se esperaba obtener. Por tanto, el
quinto y último punto de los objetivos particulares, se cumplió completamente.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
CIFRADO DE AUDIO POR MEDIO DEL
ALGORITMO TRIPLE-DES-96
TESIS
Que para obtener el grado de Maestría en Tecnología de Cómputo
PRESENTA:
Erick Armando Hernández Díaz
Directores de Tesis:
Dr. Víctor Manuel Silva García
Dr. Rolando Flores Carapia
Ciudad de México.
Julio, 2016.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Reconstrucción de mapas utilizando escaneo láser para la navegación de un robot móvil</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Hacer uso de un escáner láser RPLIDAR montado sobre un robot móvil tele operado como
herramienta de mapeo del ambiente, con el objetivo de mostrar de manera simultánea y en tiempo
real la localización y reconstrucción de mapas en dos dimensiones mediante el desarrollo de una
interfaz gráfica de usuario. Es decir, explorar ambientes desconocidos sin referencia previa a través
de la reconstrucción de mapas con un sensor láser acoplado a un robot móvil tele operado.

Objetivos particulares
1. Diseñar y construir un robot móvil con ruedas en configuración diferencial que sea capaz de
llevar en su estructura toda la electrónica y fuente de alimentación necesaria para
desempeñar su tarea de manera libre sobre superficies regulares en ambientes cerrados, ya
que se desea realizar la exploración por tele operación de ambientes desconocidos.
2. Hacer uso de sensores y actuadores, así como de la programación de sistemas electrónicos en
las diferentes tareas asignadas como: estimación relativa de la trayectoria y localización del
robot móvil en todo momento, habilitación, operación y comunicación eficiente de los
diferentes sistemas de medición y control en la tarea de navegación tele operada.
3. Utilizar y programar sistemas embebidos, dedicados a la transmisión y procesamiento
eficiente de los datos provenientes de las variables de medición y control de sensores y
actuadores del robot móvil explorador.
4. Desarrollar una interfaz gráfica de usuario por programación de objetos para la
reconstrucción y visualización de mapas 2D, así como de la trayectoria realizada por el robot
móvil durante la navegación tele operada.
5. Investigar y hacer uso de sensores y dispositivos de comunicación en tarea de adquisición y
transmisión de información respectivamente, así mismo de dispositivos electrónicos que
aumenten la eficiencia en el consumo de energía en la etapa de potencia y control para los
actuadores del robot móvil.
6. Proponer o adecuar algoritmos matemáticos y estadísticos sobre los datos de mapeo del
escáner para su análisis en extracción de rectas en reconstrucción y reconocimiento de
características en busca de esquinas como puntos de referencia que resuelvan posibles
errores de posicionamiento y reconstrucción de mapas en tiempo real durante la exploración. </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En la actualidad se ha hecho indispensable el uso de robots en actividades del ser humano
que se desea se realicen de manera más rápida, eficiente y segura, evitando así errores irrecuperables.
Uno de los propósitos de los robots es la búsqueda y rescate, por ello, universidades y centros de
investigación en todo el mundo se han dedicado arduamente a desarrollar robots que puedan
desenvolverse y acceder a todo tipo de zonas y ambientes para su exploración, de tal forma que se
logre recolectar información del ambiente que lo rodea con la mayor precisión y fiabilidad posible
para un análisis más fidedigno. Por esta razón se busca implementar sistemas más eficientes y
robustos haciendo uso de sensores más precisos, resistentes y de bajo costo. Este es el caso del
sensor láser telemétrico, utilizado por tener mayor velocidad de procesamiento en las mediciones,
mayor distancia y rango de operación así como una mejor precisión. Por esto y otros aspectos, el
láser como sensor es la opción más viable en la recolección de información en la exploración.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El desarrollo e implementación de este sistema se realizó a partir de una investigación
científica, llevándose a cabo un proceso metódico y sistemático sobre la problemática de saber
dónde se encuentra localizado un robot móvil y cómo es el ambiente de exploración que lo rodea.
Tres son las principales etapas que componen el proyecto:
1. En la primera etapa e inicial se diseñará e implementará un móvil con ruedas en base a las
configuraciones existentes para robots móviles con ruedas, apto para navegar en ambientes
interiores y sobre superficies uniformes. El móvil debe tener gran estabilidad con el objetivo
de realizar una navegación eficiente y rápida además de ser suficientemente robusto para
soportar el peso de las baterías que alimentarán tanto la parte de electrónica de potencia para
el control de los motores, como de la electrónica de control, comunicación y mapeo para la
navegación tele operada.
Analizar y desarrollar múltiples algoritmos que determinen: la odometría para establecer el
desplazamiento lineal y rotacional del móvil a partir de los encoders en motores con el fin de
estimar la posición del móvil en cualquier instante de tiempo. El control de lectura de
distancia y ángulo del escáner láser y el control de navegación del móvil. Todo con un
Joystick como dispositivo de mando a través de una comunicación inalámbrica con radio
frecuencia.
2. Para la segunda etapa, con ayuda de una computadora como estación central de control, se
desarrollará la configuración y protocolos para realizar una comunicación de datos de la PC
con los puestos de comunicación serial COM en la transmisión de datos inalámbricos con el
robot móvil y con el Joystick como elemento de mando para las acciones del robot.
Mediante lenguaje de programación dirigido por eventos y en un entorno de desarrollo
integrado, se realizará: el enlace entre el Joystick y el móvil como control de mando en los
movimientos y acciones de tele operación y escaneo del ambiente, además de diseñar los
controles para establecer la conexión y comunicación con el móvil. Todo esto, con el fin de
desarrollar una interfaz gráfica de usuario que controlen y monitoreen en tiempo real el
desplazamiento y trayectoria realizado por el móvil. Así mismo, analizar y aplicar los
diferentes algoritmos encontrados en la literatura en base a fundamentos matemáticos y
estadísticos en la reconstrucción de mapas por segmentación de rectas mediante los datos de
mapeo con disposición proporcional a la medida de distancia y ángulo con referencia a la
posición y orientación del móvil dentro de ambientes desconocidos.
3. En la tercera etapa, se definirán los algoritmos que procesen todos los datos de distancia
entre un sensor laser telemétrico y los objetos o superficies que detecte en toda su periferia,
mejor conocida como detección por luz y distancia LIDAR LIght Detection And Ranging.
La interfaz permitirá realizar un escaneo del medio mediante pulsos de láser en un giro de
360 grados, generando una nube de puntos en una base de datos que posteriormente serán
impresos en el plano cartesiano que vislumbra el tipo de ambiente en cualquier instante de
tiempo sin la necesidad de conocerlo previamente y así obtener una estimación más precisa
de dónde se encuentra el móvil.
La adquisición se realiza con referencia respecto a la posición del láser, que para nuestro
caso coincide con la localización del móvil debido a que el láser se encuentra montado en el
eje de rotación del móvil.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Como resultado de los objetivos planteados dentro de la justificación al inicio de este trabajo, la
necesidad de sistemas electrónicos más precisos, rápidos o robustos son indispensables dentro de la
rama de la ingeniería, en el desarrollo de proyectos que requieren de mejores resultados o sobre
nuevos avances tecnológicos. En el área de mecatrónica y robótica, el uso de sensores con calidad de
medición alta son necesarios como herramienta en el desarrollo de soluciones más eficientes.
El sensor escáner láser RPLIDAR 360 grados es idóneo como herramienta de medición de
distancia para el mapeo de ambientes en 2D al: evitar múltiples movimientos y barridos durante el
mapeo, dedicándose únicamente a la exploración al cubrir todo el entorno en un único escaneo,
entrega la medición directamente por comunicación serial, y tiene menor error de medición y mayor
alcance y velocidad de procesamiento respecto a los sensores ultrasónicos o infrarrojos. Por ello, el
uso de sensores robustos evita realizar hardware y software adicional, concentrándose solo en el
procesamiento de datos en trabajos de extracción de características y reconocimiento de patrones.
Después de analizar los algoritmos implementados en extracción de rectas, la transformada
de Hough entrega excelentes resultados debido a su robustez al encontrar rectas óptimas y no ser
afectado por puntos outliers. Sin embargo, el procesamiento computacional es elevado respecto a los
restantes, afectando tiempos de ejecución durante la navegación. Una solución alternativa es
Iterative End Point Fit, este algoritmo entrega resultados positivos al estar dedicado a puntos
ordenados y baja dispersión por mínimo error respecto a otros sensores, extrayendo rectas con
excelente aproximación y menor tiempo de ejecución sin aplicar ajuste de puntos.
El producto escalar entre dos vectores es el principio bajo el cual se calcula el ángulo dentro
del algoritmo de detección de esquinas. Sin embargo, este requiere ciertos criterios adicionales para
desarrollar un algoritmo robusto y eficiente como el límite de dispersión entre los puntos que definen
los vectores dentro de la ventana corrediza, haciendo de este algoritmo ideal para en la búsqueda de
puntos de referencia para el robot móvil durante la exploración.
De acuerdo a los cálculos de posición y orientación del robot móvil así como de los datos
obtenidos por el escáner láser, la reconstrucción del mapa obtenido por extracción de rectas coincide
con las dimensiones y formas del ambiente que rodea al robot móvil, permitiendo realizar la tele
operación sin la necesidad de un punto de referencia, un mapa almacenado previamente o de la
colocación de marcadores especiales para su localización.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
Centro de Innovación y Desarrollo Tecnológico en Cómputo
“Reconstrucción de mapas utilizando escaneo láser
para la navegación de un robot móvil”
Tesis para obtener el grado de
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
Presenta
Ing. Jorge López Ortega
Directores de Tesis
Dr. Juan Carlos Herrera Lozada
M. en C. Jesús Antonio Álvarez Cedillo
México, D.F. a Abril de 2016</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Identificación biométrica por medio de la detección de venas de la mano en imágenes infrarrojas</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
Desarrollar una metodología e implementar un sistema computacional capaz
de identificar a personas por medio del reconocimiento de la geometría
de las venas en el dorsal de la mano en imágenes infrarrojas.
Objetivos Particulares
1. Crear un banco de imágenes infrarrojas del dorsal de la mano que
permita la detección de la geometría de las venas.
2. Investigar filtros que permitan el mejoramiento de imágenes digitales.
3. Proponer una metodología para la segmentación de las venas en las
imágenes del dorsal de la mano.
4. Proponer que características se extraerán de las imágenes segmentadas
del punto anterior, para formar el patrón de características representante
de cada imagen.
5. Utilizar memorias asociativas mediante autómatas celulares para la
etapa del reconocimiento de patrones.
6. Validar el sistema mediante el método de validación cruzada (k-fold
cross validation).
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>A lo largo de la historia, los avances tecnológicos han influido en el desarrollo social y económico de la población, ´estos han tenido repercusiones tanto buenas como malas, esto se ve reflejado en la necesidad que tienen algunos bancos, bóvedas de seguridad, oficinas o residencias de recurrir a sistemas de seguridad confiables que permitan la identificación adecuada de individuos. Los sistemas biométricos aportan una solución efectiva al problema de la identificación. Uno de los problemas de interés a empresas públicas y privadas es el control de asistencia. De modo que existen sistemas para registrar la entrada y salida del personal, y hay ocasiones en que el registro a ‘un se lleva a cabo en libretas. Otro de los principales problemas en hospitales y laboratorios clínicos, por ejemplo, es el acceso a zonas restringidas, en donde solo debe de ser posible permitir el acceso a la persona indicada. Por otra parte, uno de los problemas que afecta a bancos, es la identificación de individuos adecuada para permitir a una persona el acceso por ejemplo para realizar transacciones bancarias. Hoy en día, es común encontrar en internet páginas de tiendas que ofrecen sus servicios por red, y uno fácilmente puede realizar sus pedidos de ´esta manera, de aquí la importancia del desarrollo de sistemas biométricos para efectos de comercio electrónico.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo se implementó una metodología para llevar a cabo la
identificación biométrica por medio de la detección de venas en imágenes
infrarrojas. Se desarrolló un prototipo que permite la captura de imágenes
infrarrojas del dorsal de la mano. Se creó un banco de imágenes de 10 individuos,
cada individuo aporto 10 imágenes infrarrojas del dorsal de su mano.
Una vez obtenido el banco de imágenes, se les aplico tratamiento digital a
cada una de las imágenes con la finalidad de segmentar el dorso de la mano
infrarroja, para ello se utilizó binarización con umbral igual a 50, logrando
obtener la segmentación del dorso de la mano infrarroja, de donde se obtuvo
el patrón de características por medio del histograma del dorso infrarrojo
segmentado. Como clasificador se utilizó el modelo asociativo ACA presentado
en la sección 3.4 que está basado en autómatas celulares, el modelo
fue comparado con el clasificador k-NN obteniendo los resultados mostrados
en las tablas 5.1, 5.2 y 5.3. Los experimentos fueron realizados de tal
manera que una primera instancia se aplicaron ambos modelos clasificadores
´únicamente a dos personas, posteriormente se agregó una persona más y así
sucesivamente. Las tablas 5.1 y 5.2 muestran los resultados arrojados al
aplicar el clasificador k-NN en el caso de dos personas, tres personas y así
sucesivamente hasta llegar a diez, aplicando el clasificador para el caso de
k = 1 hasta k = 10, por otra parte, la tabla 5.3 muestra los resultados de
aplicar el clasificador ACA en el caso de dos personas, tres personas y así
sucesivamente hasta llegar a diez, aplicando el clasificador para el caso en
que se aplica una erosión celular seguida de una dilatación celular y dos erosiones
celulares seguida de dos dilataciones celulares. Para el caso en que el modelo ACA y k-NN se aplica a dos individuos, se observa que se obtuvieron
rendimientos de 93.18% y 91.30% como valores máximos respectivamente,
obteniendo que en esta situación el modelo k-NN supera al modelo ACA, sin
embargo se observa que, en los siguientes experimentos, a partir de consideramos
a tres personas y hasta diez, el comportamiento del modelo ACA
supera al modelo k−NN, obteniendo mejores resultados con el modelo ACA.
Por ejemplo, se observa, en el caso de tres personas, con el modelo k-NN
se obtuvo un rendimiento máximo de 83.33% con valores de k=1, 3, 4, 5 y
10, mientras que con el modelo ACA se obtuvo un rendimiento máximo de
92.10% considerando una erosión celular seguida de una dilatación celular.
En general, con el modelo ACA considerando una erosión celular seguida de
una dilatación celular se obtuvieron mejores resultados en comparación con el
modelo k-NN.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITECNICO NACIONAL ´
Centro de Innovación y Desarrollo Tecnológico en
Computo
Identificación Biométrica por Medio de la Detección de Venas de la Mano en Imágenes Infrarrojas
TESIS
que para obtener el grado de
MAESTRIA EN TECNOLOGIA DE COMPUTO ´
Presenta
ANA LUZ BARRALES LOPEZ ´
Directores de Tesis: Dr. Oscar Camacho Nieto
Dr. Benjamín Luna Benoso
México, D. F. enero de 2015

</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Técnicas de control automático para la tarea de seguimiento de trayectorias en robots móviles de ruedas</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Como se ha podido observar en el presente trabajo, el desarrollo de la robótica comenzó
enfocado a robots manipuladores, debido a su amplia aplicación en la industria. Sin
embargo, actualmente gran parte de la investigación asociada a la robótica se encuentra
enfocada en los RMR, debido a sus múltiples aplicaciones en diversas campos. Una de las
principales problemáticas en el control de RMR, entre otras tareas, es el seguimiento de
trayectoria, debido a que con esto se dota de mayor autonomía a un robot, lo cual conlleva
a un mínimo de supervisión e intervención humana. Mucho se ha estudiado acerca
de las técnicas de control para la solución del seguimiento de trayectorias, sin embargo,
cada trabajo ha sido presentado con un enfoque único. En la practica, es muy común el diseño de controladores basados únicamente en el modelo cinemático del RMR, ya que
estos controladores proveen los perfiles de velocidad lineal y angular a ser seguidos por
el móvil. Posteriormente, se hace uso ya sea de un controlador proporcional-integral o
o de un controlador dinámico, los cuales nos entregan los perfiles de velocidad que los
actuadores deberán seguir para el seguimiento de trayectoria. Adicionalmente, la mayoría
de los estudios presentados en revistas indexadas en el Journal Citation Report,
solamente llegan a nivel de simulación. Tal y como se puede observar en la Figura 1.6, de
los artículos presentados, únicamente para la configuración diferencial el 57 % de estos
llegan a nivel simulación.
 Hasta el momento existen pocos trabajos que demuestran de manera experimental
la validación de técnicas de control para el seguimiento de trayectorias. Asimismo, la
comparación entre controladores no ha sido reportada. Por lo anterior, este trabajo pretende
realizar la validación de diversas técnicas de control, enfocadas a la problemática
de seguimiento de trayectorias en un RMR de tipo diferencial, mediante simulaciones
numéricas y su posterior implementación en tiempo real.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general y particulares
En esta sección se realiza el planteamiento de los objetivos de la tesis, los cuales se
dividen en objetivo general y objetivos específicos. Los objetivos responden a lo que se
desea lograr con el desarrollo de este trabajo.
Objetivo general
El presente trabajo tiene como objetivo general la aplicación de diversas técnicas de
control asociadas a la problemática de seguimiento de trayectoria para un RMR de tipo diferencial Dichas técnicas de control son simuladas e implementadas a nivel experimental
para su validación, y eventualmente ser comparadas.
1.5.2. Objetivos particulares
Para poder llevar acabo el objetivo general del presente trabajo, es necesario realizar
los siguientes objetivos particulares:
◦ Comprender el modelo matemático del RMR de tipo diferencial.
◦ Comprender las leyes de control a implementarse asociadas a la problemática de
seguimiento de trayectorias.
◦ Programar las leyes de control para la simulación de dichas técnicas de control a
través del uso del software Matlab-Simulink.
◦ Implementar las técnicas de control a nivel experimental, con la ayuda de MatlabSimulink,
ControlDesk y la tarjeta DS1104.
◦ Comparar los resultados obtenidos en la simulación y la experimentación para la
validación de las técnicas de control aplicadas al RMR de tipo diferencial.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El objetivo general de este trabajo fue la validación de diversas técnicas de control,
enfocadas a la problemática de seguimiento de trayectorias en un RMR de tipo
diferencial, mediante simulaciones numéricas e implementación en tiempo real. Las técnicas
implementadas fueron denominadas Control cinemático I, Control cinemático
II, Control cinemático III y Control dinámico. Se mostró de manera experimental
como la variación de las ganancias y el diseño de la trayectoria a seguir son de gran importancia
para el desempeño del control. Una vez presentado el desarrollo de cada una
de las técnicas de control así como los resultados de simulación y experimentales de cada
una de las técnicas se deduce que el objetivo de control de cada uno de los controladores
fue exitosamente alcanzado. Cabe mencionar que este trabajo se pudo llevar a cabo gracias
al prototipo previamente construido en el área de mecatrónica, el cuál se construyó
con el objetivo de que se pudieran implementar diferentes leyes de control. Está es la
primera vez que en un solo trabajo de tesis se implementan más de una técnica de control
para el seguimiento de trayectoria en RMR tipo diferencial, por lo que el objetivo con el
que el prototipo fue construido se sigue logrando. Para poder implementar los controles
mencionados anteriormente, fue necesario cumplir con las siguientes actividades:
◦ Se comprendió el modelo matemático del RMR de tipo diferencial.
◦ Se comprendieron las técnicas de control a implementarse asociadas a la problemática
de seguimiento de trayectorias.
◦ Se comprendió el principio de operación de la tarjeta DS1104.
◦ Se programaron las leyes de control para la simulación de dichas técnicas de control
a través del uso del software Matlab-Simulink.
◦ Se validaron las técnicas de control mediante la implementación en tiempo real de
cada una de ellas con la ayuda de la tarjeta DS1104 junto con Matlab/Simulink y
ControlDesk.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
CIDETEC
EDUCACIÓN
INNOVACIÓN
CONSERVACIÓN
IPN
TÉCNICAS DE CONTROL AUTOMÁTICO PARA LA
TAREA DE SEGUIMIENTO DE TRAYECTORIAS EN
ROBOTS MÓVILES DE RUEDAS
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
P R E S E N T A:
Ing. Cynthia Yolanda Sosa Cervantes
DIRECTORES DE TESIS:
Dr. Ramón Silva Ortigoza
Dr. Victor Manuel Hernández Guzmán
Ciudad de México Junio 2016.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Detección Automática de Anomalías Presentes en Mamografías Digitales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general Desarrollo de un sistema de software para la segmentación y clasificación de mamografías digitales, por medio de filtros y clasificadores para la detección de cáncer de mama. En la figura 1.2 se muestra el diagrama de funcionamiento del sistema.

Objetivos específicos
1. Procesamiento de las mamografías digitales para la obtención de mayor calidad para el proceso
de segmentación.
2. Segmentación de las mamografías digitales por medio de filtros como: por medio del histograma,
para obtener solo regiones de interés dentro de la imagen.
3. Extraer características de la mama segmentada para posteriormente ser procesadas por el clasificador.
4. Clasificación de las características por medio de la base de datos MiniMIAS, de la cual se obtendrán
las mamografías que nos servirán como base de conocimiento del sistema.
5. Mostrar los resultados de la clasificación; los resultados que se obtendrán serán: si la mama contiene
cáncer, el tipo de cáncer (benigno, maligno) en caso de que exista. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El cáncer de mama es un problema de salud a nivel mundial, así como también es el tipo de cáncer
más frecuente. Este se da por el crecimiento anormal de las células las cuales se dividen rápidamente
sin morir y duplicarse muchas veces, cuando esto ocurre, suele iniciar un crecimiento rápido que genera
la formación de un tumor.
Es importante la detección a tiempo del cáncer de mama, ya que cuando ocurre esto puede ser tratado
y en muchos casos se elimina de forma eficaz el tumor. Al contrario que pasa cuando no se detecta a
tiempo y se suele asociar con resultados negativos.
En la actualidad, el cáncer de mama es un problema donde solo el 10 % de los casos son detectados
en etapa temprana, dando un panorama poco alentador. También se sabe que el método que mejor
detecta anomalías en la mama son las mamografías en las cuales los radiólogos al analizarlas pueden
dar un panorama del estado de las mamas. Sin embargo, existen casos en los cuales los tejidos son muy
densos y pueden esconder los tumores a la vista, por ello con el uso del reconocimiento de patrones y
del tratamiento digital de imágenes se puede ayudar a mejorar la imagen.
Por este motivo se desarrollará un sistema de software el cual detectará anomalías presentes en mamografías,
lo cual apoyara a los radiólogos a tomar una decisión acerca del diagnóstico obtenido de las
mamografías. Con lo que se pretende que más casos de cáncer de mama sean diagnosticados en etapa
temprana y con esto se puedan efectuar más tratamientos, dando resultados positivos. 
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La aportación principal de este trabajo es el diseño de una nueva metodología para la segmentación y
clasificación de las mamografías digitales, esta metodología hace uso de vectores de gran tamaño por el
número de características utilizadas, que al ser procesadas por la memoria asociativa presenta un buen
nivel porcentual de clasificación.
Con la clasificación con memorias asociativas αβ tipo min, los resultados obtenidos fueron buenos
ya que, al reducir el ruido aditivo de las imágenes a la hora de clasificar, nos presenta un panorama
donde nos devuelve la mejor aproximación a una clase.
Dentro de los objetivos específicos, el primero consistió en mejorar la calidad de la imagen por lo
cual se aplicaron filtros: el de la semilla y por Histograma para reducir ruido a la imagen y mejorarla,
lo cual permite un mejor manejo de la imagen.
El segundo objetivo consistió en segmentar la región mamaria para obtener menos margen de ruidos
externos, como la supresión del musculo Pectoral y demás etiquetas que no pertenecen a la región
de la mama, esto es porque puede causar ruido a la hora de extraer características, además de que las
características adquiridas solo pertenecen a la región de interés, en esta etapa se obtuvo un porcentaje
del 91 % de extracciones correctas de la región mamaria.
Para la extracción de características se diseñaron diferentes algoritmos, los cuales obtienen datos específicos
de la imagen, que en conjunto forman el vector de clasificación.
Finalmente, a la hora de clasificar se observó que se obtuvieron buenos resultados ya que el resultado
de la clasificación obtuvo un porcentaje de 85 % exactitud, lo cual representa un buen resultado.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL ´
CENTRO DE INNOVACIÓN Y DESARROLLO TECNOLÓGICO EN COMPUTO
DETECCIÓN AUTOMÁTICA DE ANOMALÍAS PRESENTES EN MAMOGRAFÍAS DIGITALES
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRÍA EN TECNOLOGÍA DE COMPUTO ´
PRESENTA
FERNANDO CARLOS MARTÍNEZ RODRÍGUEZ
DIRECTORES:
DR. ROLANDO FLORES CARAPIA
DR. BENJAMÍN LUNA BENOSO
MÉXICO, D. F. ENERO DE 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Firma Digital Basada en Funciones HASH y un Algoritmo Criptográfico Híbrido</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Hoy en día las tecnologías de información han ido en aumento constituyendo una tecnología
innovadora de rápido crecimiento que se ha vuelo indispensable en la sociedad en el intercambio
de información y el acceso a medios digitales, las cuales convergen generalmente en internet,
haciendo uso de esta o acoplándose a ella, lo cual ha derivado cifras que se arrojan en diversos
fabricantes de antimalware.
La digitalización global de los productos, servicios y procesos tiene un profundo impacto sobre las
organizaciones ofreciendo grandes oportunidades de crecimiento y perspectivas, permitiendo
distinguirse por su competencia. Sin embargo, a pesar de todos los beneficios, las organizaciones
necesitan estar conscientes de los retos y riesgos que representan las tecnologías de información y utilizarlas a su favor para ampliar su alcance e impulsar el crecimiento rentable. Un marco de
seguridad de la información debe evaluar constantemente la función de las nuevas tecnologías y
maximizar su potencial para las organizaciones, al mismo tiempo que las mantienen a salvo.
En las estadísticas consultadas se puede observar que el problema más frecuente es el robo de
información, el cual se ha extendido entre la ciudadanía provocando la necesidad de implementar
mecanismos adecuados para brindar seguridad ante problemas relacionados con la
confidencialidad, confiabilidad e integridad en la transmisión de los datos en medios considerados
inseguros, como es el caso de internet [23], [24], [25], [26] , [27]. La criptografía es sin duda una
herramienta que puede permitir el control de la información de aquí la motivación de realizar
este trabajo, al proponer una estrategia de seguridad de información basada en funciones Hash y
un algoritmo criptográfico híbrido, tratando de lograr uno o más de los siguientes resultados:
innovación, optimización y protección.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>1 Objetivo General
Aplicar un algoritmo hibrido criptográfico conjuntamente con funciones Hash, para definir un
esquema de comunicación segura que proporcione autenticación, confidencialidad, integridad y
no repudio a través de la firma digital.

Objetivos Específicos
 Proponer un algoritmo híbrido basado en los sistemas criptográficos AES y ElGamal
 Diseñar un sistema de comunicación que integre la firma digital, basado en los algoritmos
AES y en ElGamal.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El uso masivo de internet ha crecido cada vez más, de tal manera que se ha llegado a cruzar la
barrera en donde hay más dispositivos que humanos conectados a internet y con mayor
frecuencia escuchamos noticias relativas a fraudes, robos e incidentes relativos a la información,
siendo un problema sustentado por las estadísticas presentadas en éste trabajo. Es un hecho que
la información se utiliza de mil formas, desde personal como redes sociales, consulta de seguro
social, estados de cuenta bancarios, hasta organizacional donde la información manejada dentro
de ellas se convierte en secretos industriales como ventajas competitivas, situación financiera,
entre otros, lo que llega a concluir que la información es uno de los activos más importantes de
cualquier persona u organización.
Al evolucionar las formas de comunicación también evolucionan los crímenes, dónde la
información en un medio de transmisión de datos la hace vulnerable y es posible interceptarla,
leerla e incluso alterarla (Fig. 1.4), por lo que los usuarios a distancia han limitado la capacidad de
obtener datos e interactuar con la información, debido a la desconfianza en el manejo de los
mismos. La criptografía puede ser la respuesta ante estas necesidades, evolucionando los mecanismos y
técnicas que intentan solucionar este problema. Dos aplicaciones fundamentales de la criptografía
son las firmas digitales y la encriptación, la primera consiste en probar la fuente original de los
datos (autenticación) y verificar después que éstos no han sido alterados (integridad), la segunda
proporciona la confidencialidad necesaria para la transmisión de datos y la comunicación.
En un entorno propenso a ataques como la Internet y ante un sistema, la seguridad es una
prioridad en el momento de su implementación y mantenimiento. Por esta razón es necesario
utilizar algoritmos de cifrado que ayuden a que la información transmitida no sea entendible por
un atacante, aun cuando este cuente con las herramientas necesarias para su captura, de tal
forma que sólo sea comprensible por los interlocutores autorizados [29].
Dentro de este contexto el presente proyecto denominado “Firma Digital Basada en Funciones
Hash y un Algoritmo Criptográfico Híbrido”, ofrece una alternativa de seguridad a través de un
sistema de cifrado con firma digital y una encriptación basada en un algoritmo simétrico y
asimétrico con la finalidad de optimizar la velocidad de cifrado y la transmisión de datos,
buscando que otorgue la confiabilidad, confidencialidad, integridad y no repudio en el envío de
información.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En el trabajo “Firma Digital Basada en Funciones Hash y un Algoritmo Criptográfico Híbrido” se
desarrolló como una aplicación computacional bajo el lenguaje de C#, sistematizando el proceso
de cifrado de AES, la determinación de un valor Hash de una llave y la creación de una firma
digital generada mediante el criptosistema asimétrico ElGamal, buscando mejorar y mantener un
control eficaz en la seguridad de la información que envía a través de internet.
Durante el periodo de realización de éste trabajo se llevaron a cabo una serie de actividades que
permitieron alcanzar el logro de los objetivos planteados al diseñar una propuesta de software de
firma digital basada en un algoritmo criptográfico híbrido sustentado en métodos formales de
especificación en su desarrollo y una implementación al modificar los valores iniciales del proceso
Hash y hacerlos dependientes de la llave de cifrado usada para su fortalecimiento, que fue la
principal aportación del trabajo además de hacer una variación también disminuye la probabilidad
de que se presente una colisión al determinar el valor Hash.
La utilización de métodos formales ayuda a lograr especificaciones correctas y en pocos pasos,
manteniendo los requerimientos desde principio hasta el fin y el uso de la firma valida la
información compartida a través de medios informáticos aportando un valor agregado y más
cuando se implementan cambios en la forma de usar las herramientas, incrementando aún más la
seguridad que por aplicaciones comerciales o librerías comunes a las que se tienen acceso,
personalizando la aplicación.
La criptografía desde tiempos remotos hasta la actualidad ha tenido un peso enorme y ha influido
en el curso de la historia en ámbitos de economía, finanzas, política y militar y ha ido
evolucionando con el paso del tiempo, hoy en día lo más usado es el cifrado hibrido acompañado
de la firma digital con la finalidad de incorporar elementos que nos permitan garantizar la
seguridad y al mismo tiempo alinearnos a la parte legal, variando sólo las técnicas empleadas y las
implementaciones que uno como programador realice para hacer único el criptosistema.
Otro aspecto a cuidar es la eficiencia en los criptosistemas, ya que puede ser muy seguro pero
puede consumir muchos recursos computacionales que se pueden reflejar de manera económica
Finalmente se concluye que los criptosistemas son seguros sin embargo la parte que se puede
considerar vulnerable es el factor humano, en el cual recae la responsabilidad de un mal diseño y
que en vez de proteger haga más vulnerable un sistema al desconocer las fallas del mismo, por
otro lado éste factor es considerado como el eslabón más débil dentro de un esquema de
seguridad y se debe en gran medida a la falta de capacitación en cuanto al uso de tecnología y
falta de cultura de seguridad información lo que es una amenaza que ha ido en aumento al contar
con empleados negligentes o inconscientes lo que puede provocar la pérdida de información de
manera involuntaria o bien otro problema que se presenta es el fraude que es otro punto
vulnerable porque aunque un criptosistema sea excelente y extremadamente seguro, sino se
cuenta con la discreción y lealtad de los usuarios se puede lucrar con la información para
beneficio propio.
Las aplicaciones que requieren estándares basados en seguridad han sido mejoradas, las bases de
la seguridad dependen de la fuerza criptográfica de la relación de las claves en el certificado, la
implantación adecuada de las aplicaciones de PKI y del control de la llave utilizada para firmar el
certificado por un usuario o entidad final, esto con la finalidad de establecer la infraestructura
necesaria para la seguridad del comercio electrónico y la confianza para diferentes sectores como
salud, negocios, educación y gobierno, buscando crear confianza y seguridad en el uso de
herramientas tecnológicas que tenemos a nuestro alcance, con el objeto de que nuestra vida
cotidiana se vuelva más sencilla y de mejor calidad.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Innovación y Desarrollo
Tecnológico en Cómputo
Firma Digital Basada en Funciones HASH y un
Algoritmo Criptográfico Híbrido
Tesis
Que para obtener el grado de
Maestría en Tecnología de Cómputo
Presenta:
Reyna García Belmont
Directores:
Dr. Rolando Flores Carapia
Dr. Víctor Manuel Silva García
Abril 2016</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Cifrado de imágenes utilizando advanced encryption standard (AES) con permutación variable</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los fraudes y robos de información que hay en el envío y recepción de los datos a través
de la red, a las empresas y usuarios en general cuando realizan diferentes transacciones
con la información, la cual desde que es generada, almacenada, procesada y transmitida,
existen ambientes no seguros que permiten a un agente ajeno para que la pueda
interceptar para modificar, robar, y en general hacer mal uso de la misma.
Se desea realizar el envío de información en general desde un usuario que actualmente
como emisor hacia un usuario que actúa como receptor mediante el uso de un canal
considerado como inseguro. Durante el paso del mensaje o información por el canal
inseguro, se corre el peligro de que exista una tercera persona interceptando y/o
recibiendo la información destinada al receptor que se encuentra en forma legible y
decodificada. Quizás en muchos casos los mensajes o información enviada puede que no
sea un gran secreto, sin embargo, cuando sí se requiere del envío de información
confidencial este proceso inseguro puede ser un gran problema [5].
Considerando que:
 En la actualidad un alto porcentaje de la información se encuentra digitalizada y
sus medios se encuentran conectados a una red.
 Existe información en estos medios de mucho valor llamada información sensible,
ya sea que se puede obtener una ganancia económica, política, social o
estratégica militar.
 La red de Internet carece de esquemas seguros para transmitir la información.
 Existe un creciente número de usuarios, organizaciones e inclusive instituciones
que amenazan con el manejo inadecuado de la información y que asechan la red
sistemáticamente con medios sofisticados de software y hardware.
 Gran parte de esta información utilizan imágenes las cuales contienen información
sensible y que tendría que estar al alcance sólo de sus propietarios o gestores
autorizados y protegidos por leyes del uso de la información y que en manos de
terceros no autorizados pueden tener mal uso [23].
Asimismo por la vulnerabilidad tanto de individuos como instituciones por el robo y mal
uso de su información, cada vez más frecuente en la red de Internet, como lo muestran
los resultados de de la empresa Enjoy Safer Technology (ESET), por citar un ejemplo,
cuya misión es proteger a los usuarios y empresas de América Latina contra todo tipo de
amenazas informáticas a través de tecnología multipremiada y servicios de seguridad.
 La amenaza más relevante es la pérdida de datos, identificada por el 57.62 % de
los entrevistados,
 La vulnerabilidad del software y sistemas fue seleccionada por casi el 40% de los
encuestados ratifican la relevancia de los problemas de seguridad en aplicaciones
dentro del marco general de la seguridad de la información.
El fraude informático tanto externo (37.60%) como interno (37.24%) fue
seleccionado en tercera ubicación.
 El 21,32% indicó al malware como un tema de preocupación.
 Más del 50% de los encuestados indicaron que del presupuesto de IT, menos del
5% es asignado a la seguridad de la información, mientras que sólo el 20% de los
encuestados dijeron que este número ascendía a más del 10% del total.
Y ante la creciente involucramiento de Instituciones gubernamentales como la NSA
(Agencia de Seguridad Nacional de los E.E.U.U.) en casos de espionaje como el
mencionado en la nota periodística aparecida el 23 de noviembre de 2013 en el periódico
La Jornada, cuya redacción afirma que “La NSA infecta 50 mil redes con software
maligno”, y para no quedar en desconocimiento de esta alarmante problemática mundial
se menciona la noticia completa, por citar un ejemplo:
Amsterdam. La Agencia de Seguridad Nacional (NSA) de Estados Unidos infectó a
más de 50 mil redes de computadoras en todo el mundo con software maligno
diseñado para robar información confidencial, según revelan documentos
proporcionados por el ex contratista de esa dependencia Edward Snowden, quien
se encuentra asilado en Rusia, a los que tuvo acceso el diario holandés NRC.
Una presentación de 2012 explica cómo la NSA recoge información de todo el
mundo mediante una “Red Informática Explotación” utilizada en más de 50 mil
localidades para la infiltración secreta de los sistemas informáticos obtenidos
mediante la instalación de malware. Los ataques son realizados por un
departamento especial llamado Tailored Access Operations (TAO) que cuenta con
más de un millar de piratas informáticos.
El diario The Washington Post reportó en agosto pasado que la NSA instaló unos
20 mil “implantes” en 2008 y que realizaba estas operaciones cibernéticas desde
1998, con base en un informe sobre el presupuesto secreto de los servicios de
inteligencia estadounidenses, refirió Cubadebate en su edición digital.
En septiembre de 2013, el proveedor de telecomunicaciones Belgacom descubrió
que durante varios años el servicio de inteligencia británico instalaba software
malicioso en sus redes para recolectar los datos telefónicos y de tráfico de sus
clientes. En ese caso, Gran Bretaña utilizó una página falsa en Linkedin para
atraer a los empleados de la empresa.
La NSA ha realizado estas operaciones además en Brasil y Venezuela. El software
maligno instalado en estos países puede ser controlado de forma remota y
permanecer activo durante años sin ser detectado. Los implantes actúan como
“células durmientes” que se pueden activar con la pulsación de un botón.
La NSA no quiso hacer comentarios acerca de este tema y lo remitió al gobierno
de Estados Unidos. Un vocero de la administración de Barack Obama afirmó que
la divulgación de material secreto es perjudicial para la seguridad nacional, indicó
el canal de noticias Rusia Today en su sitio de Internet.
Un grupo de organizaciones no gubernamentales entre las que se encuentran la
Electronic Frontier Foundation, Access y Aministía Internacional escribieron una  carta a los miembros de la Asamblea General de la Organización de Naciones
Unidas (ONU) para “sentar una posición común en contra de las prácticas
indiscriminadas de vigilancia masiva, intercepción y recopilación de datos, tanto en
Estados Unidos como en el extranjero”.
Previamente, Brasil y Alemania presentaron una resolución al respecto que los
países de la alianza Five Eyes (Estados Unidos, Canadá, Australia, Gran Bretaña
y Nueva Zelanda) trataron de debilitar.
Por todo lo anterior se pretende contribuir a un esquema seguro de protección de datos
basado no sólo de software sino en la implementación de estándares probados y con
parámetros medibles de seguridad para proteger la información que se refiere a imágenes
en formato BMP mediante el cifrado y descifrado y adicionando esquemas de seguridad
con algoritmos innovadores.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar e implementar un algoritmo para la encriptación de imágenes en formato sin
pérdidas empleando Advanced Encryption Standard (AES) con permutación variable.
Particulares
 Incrementar la complejidad del algoritmo de cifrado AES implementando
permutación variable.
 Diseñar el algoritmo para cifrar imágenes en formato sin pérdidas.
 Implementar el algoritmo para el cifrado de imágenes sin pérdidas en el lenguaje
de Programación Orientado a objetos C++.
 Implementar una medida para el grado de aleatoriedad que tienen la distribución
de los bits para cada uno de los colores básicos.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Como resultado de la búsqueda específica sobre la encriptación de imágenes mediante
métodos seguros, se ha observado un vacío de investigación, lo que existe plantea
soluciones a problemáticas muy específicas, por lo que sigue sin haber una metodología
para el cifrado de imágenes, que utilice algún estándar mundial de cifrado que haya sido
sometido a pruebas que para clasificarlo como seguro.
La imagen en la actualidad es un recurso cada vez más utilizado como evidencia de la
realidad y que simplifica la identidad de la misma, debido a esto, las bases de datos
incorporarán más imágenes para describir o identificar la misma. Muchas de estas
imágenes tendrán que ser cifradas con mayor frecuencia, por lo que los tiempos de la
misma deberán reducirse y por lo tanto debemos estar en la búsqueda de nuevos e
innovadores algoritmos que permitan reducir estos tiempos.
Aunque en el presente trabajo tiene un alcance sobre el formato de imágenes BMP, se
propone que el algoritmo se construya sin la dependencia del tipo de imagen, para que
con sólo algunas modificaciones pueda ser utilizado para otros formatos.
También el presente trabajo no sólo pretende el diseño y la implementación segura de un
algoritmo para el cifrado de imágenes, sino aportar en la construcción de una metodología
innovadora utilizando bloques de 128 bits en formato entero buscando tiempos aceptables
de cifrado y descifrado en ciclos más cortos en las transformaciónes de AES.
Otra innovación que justifica el presente trabajo, es la introducción de la permutación
variable al criptograma de AES, utilizando para cada bloque diferentes cadenas formadas
por los decimales del número Pi, haciendolo un esquema de cifrado propio, con el fin de
fortalecer el esquema de criptogrtafía AES.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Investigación y/o asesoría para obtener el conocimiento de las herramientas para el
desarrollo de la tesis como son:
 Investigación en el AES.
 Investigación de la Permutación variable.
 Estructura de los archivos de imágenes BMP y C++.
 Diseño de la permutación a implementar y su acoplamiento con AES.
 Diseño del Sistema que permita cifrar y descifrar una imagen.
 Realización de las pruebas y correcciones a la unidad de cifrado. </Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El método de cifrado con la combinación de Advanced Encryption Standard (AES)
y Permutación Variable resulta ser un método de encriptación que brinda
estándares de seguridad de alto nivel de acuerdo con los resultados descritos.
 Un ataque de fuerza bruta para encontrar la llave no resulta ni siquiera pensable
ya que aún con los equipos con mayor velocidad de procesamiento el tiempo que
le tomaría a equipos de este nivel resulta muy elevado y costoso.
 AES de 128 bits es suficiente para satisfacer las necesidades de por lo menos 10
años, tomando en cuenta que las computadoras más rápidas requieren tiempos
del orden de 1010 años.
 Es importante señalar que la función AddRound permite incorporar en cierta atapa
del proceso de cifrado las propiedades de los métodos de confusión y difusión,
pero con una particularidad de gran relevancia en el cifrado: la dependencia de
toda la metodología del cifrado y descifrado en una llave conocida sólo por el
usuario y que como se analizó en la sección de pruebas cualquier cambio en la
llave de descifrado por pequeño que sea, el resultado del descifrado no sólo no
será el adecuado, sino que producirá resultados más complejos para descifrar.
 Imágenes con niveles de profundidad y resolución bajos, en el cifrado de
imágenes únicamente con AES se perciben rasgos en la imagen que permiten
deducir a simple vista el contenido general de la imagen original. Para eliminar
cualquier indicio que permita identificar algún rasgo de la imagen, deberá
implementarse la función de Permutación, la cual se aplica a cada bloque de 128
bits de la imagen con n (necesarios) dígitos de Pi a partir del punto decimal a la
derecha, los cuales son utilizados en la función de Permutación.
 La función de Permutación Variable fortalece el algoritmo AES, como se puede
percibir en las imágenes de bajo nivel de profundidad. Introducir a través de los
números Pi su propiedad de aleatoriedad, permite obtener resultados a primera
vista más confusos.
Por lo anteriormente expuesto se concluye que los objetivos general y particular han
sido cumplidos y superados.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Innovación y Desarrollo
Tecnológico en Cómputo
Cifrado de Imágenes utilizando
Advanced Encryption Standard (AES)
con Permutación Variable
Tesis que para obtener el grado de
Maestría en Tecnología de Cómputo
Presenta:
Ernesto Godínez Rodríguez
Directores de tesis:
Dr. Víctor Manuel Silva García
Dr. Rolando Flores Carapia
Septiembre de 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Modelado y control de un vehículo aéreo no tripulado con validación experimental</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El problema de control en los vehículos aéreos de cuatro rotores para realizar el seguimiento
de trayectorias se puede abordar de distintas maneras, observando principalmente los siguientes
características del sistema. Primero, el sistema es subactuado, segundo, el modelo aerodinámico es
altamente no lineal por lo cual solo se tienen a disposición aproximaciones del mismo y finalmente
sus entradas son idealizadas. En la práctica, se sabe que existen perturbaciones y parámetros que
se desconocen debido a la falta de sensores y a la naturaleza no lineal del vehículo, lo que complica
la implementación de algún método de identificación paramétrica o de medición. Es por esto que
se necesitan controladores capaces de compensar estos factores para poder lograr el objetivo que
es el seguimiento de trayectoria y el vuelo estacionario. En este trabajo se propone realizar un
análisis comparativo de distintas técnicas de control cuando se realizan las tareas de regulación y
seguimiento de trayectorias empleando vehículos de cuatro rotores.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Analizar, diseñar y validar experimentalmente esquemas de control lineal y no lineales para un
vehículo aéreo no tripulado de cuatro rotores.
Objetivos específicos
Determinar la dinámica del vehículo.
Realizar simulaciones numéricas del cuadrirotor en lazo cerrado con diferentes esquemas de
control.
Analizar controladores para cuadrirotores.  Implementar y evaluar el desempeño de los controladores en el cuadrirotor.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>
La presente investigación contiene una serie de etapas que han permitido alcanzar el objetivo propuesto. Primeramente, se analizan a profundidad la dinámica de los vehículos aéreos de pequeña escala dotados con cuatro rotores. Posteriormente se procede a analizar distintos esquemas de control para observar el comportamiento de este tipo de vehículos aéreos y después se realiza la validación experimental del comportamiento de los controles propuestos. Finalmente, con los resultados obtenidos se hace un análisis del desempeño de las leyes de control propuestas para definir que controlador se comporta mejor al realizar de manera más acertada las tareas de regulación y seguimiento de trayectorias.
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Este trabajo de tesis presento un análisis comparativo entre distintas estrategias de control para
un cuadrirotor. Inicialmente se describió a detalle la dinámica del cuadrirotor representada tanto
en el marco de referencia inercial como en el marco de referencia del vehículo obtenida mediante el
formalismo de Euler-Lagrange y las ecuaciones de Kirchhoff respectivamente. Empleando la representación
de la dinámica en el marco de referencia del vehículo se realizó la simulación numérica del
sistema en Simulink de Matlab. En el presente trabajo se analizó el desempeño de tres diferentes
controladores: PID/PD, de estructura backstepping, y basado en modelo, siendo este ´ultimo un
esquema de control original, el cual fue analizado de manera teórica y que garantiza estabilidad
asintótica de manera local. En la validación experimental se empleó el cuadrirotor Qball 2 manufacturado
por Quanser y se establecieron las tareas de regulación y seguimiento de trayectorias.
En la primera de estas tareas, el cuadrirotor se mantuvo sobrevolando en un punto fijo, y que
corresponde a una tarea de vuelo que en ingles se le conoce como hovering. Por otro lado, en las
pruebas de seguimiento de trayectorias se establecieron señales de referencia que daban lugar a
una ruta circular en el espacio x − y así como también una curva lemniscata. La ruta circular es
recorrida a dos velocidades distintas en igual número de experimentos Los resultados obtenidos de
las experimentaciones sirvieron para poder realizar el análisis de los controladores a fin de establecer
las siguientes conclusiones.
Los resultados de la experimentación y las métricas de desempeño dadas por el error cuadrático
medio mostraron el comportamiento de cada controlador implementado en el cuadrirotor. De estos
datos se pudo observar que el controlador basado en modelo tuvo mejores resultados en comparación
con los controladores PID/PD y backstepping. Cabe mencionar que para implementar el controlador
basado en modelo es necesario conocer los parámetros dinámicos exactos de la planta mientras que
los otros controladores no la necesitan. Sin embargo, los resultados experimentales muestran que
con valores aproximados de estos parámetros el controlador funciona de manera adecuada.
El controlador PID mostró resultados similares al controlador basado en modelo cuando se emplea
en tareas donde el movimiento del cuadrirotor es lento. En contraste, en la tarea de seguimiento
de trayectorias que dan lugar a una ruta circular con una velocidad alta, este esquema de control
mostró mayores problemas para cumplir la tarea en comparación con el basado en modelo. Este
comportamiento se atribuye a que a altas velocidades se excita la dinámica no lineal del sistema y
el controlador PID/PD no logra compensarla. En general es posible usar el controlador para tareas
de regulación y seguimiento de trayectorias siempre y cuando no requiera de movimientos con alta velocidad.
En cuanto al controlador de estructura backstepping se obtuvieron resultados muy similares a los
otros controladores en pruebas de regulación mientras que en pruebas de seguimiento su desempeño
es inferior al mostrado por el controlador PID/PD y el basado en modelo. Una característica que es
importante resaltar de este controlador es el que en el lazo externo que determina el comportamiento
en el plano x − y se incluye una acción de control discontinua la cual se ve afectada por de las
ganancias γ
max
1 y γ
max
2
las cuales afectan de manera considerable en su rendimiento. Cuando se
incrementa la ganancia asociada con la discontinuidad el error se reduce, pero se produce mayor
vibración en el vehículo, de igual manera, se observó que es necesario modificar los valores de las
ganancias según la velocidad con que se desea se mueva el cuadrirotor. Así mismo, también se
determinó que la acción de control obtenida con este esquema demanda una mayor cantidad de
energía lo cual reduce el tiempo de vuelo.
Finalmente, del análisis realizado se puede establecer que el controlador basado en modelo
puede ser usado en tareas de regulación y seguimiento de trayectorias con certeza de que tendrá un
seguimiento muy apegado al deseado, teniendo en cuenta que se deben conocer con una precisión
importante los parámetros dinámicos del cuadrirotor a fin de que pueda ser implementado. Por
otro lado, el controlador PID con una correcta sintonización de las ganancias brinda un desempeño
adecuado siempre que la velocidad de movimiento no sea demasiado rápida con la ventaja de
que no requiere conocimiento alguno de la dinámica del vehículo y por ´ultimo el controlador de
backstepping que tiene la particularidad de ser más agresivo en sus movimientos por lo que puede
realizar desplazamientos con alta velocidad, pero con el riesgo de volver inestable el sistema, además
de que incrementa el consumo de energía.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO ´
NACIONAL
CENTRO DE INVESTIGACIÓN Y DESARROLLO ´
DE TECNOLOGÍA DIGITAL
MODELADO Y CONTROL DE UN
VEHÍCULO AÉREO NO TRIPULADO CON ´
VALIDACIÓN EXPERIMENTAL ´
T E S I S
QUE PARA OBTENER EL TÍTULO DE:
MAESTRÍA EN CIENCIAS DE SISTEMAS DIGITALES
PRESENTA:
ÓSCAR RODRIGO GARDUÑO DELGADILLO
DIRECTOR DE TESIS:
DR. EDUARDO JAVIER MORENO VALENZUELA
DR. RICARDO RAMÓN PÉREZ ALCOCER
TIJUANA, BAJA CALIFORNIA, NOVIEMBRE 2016
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Uso de redes neuronales artificiales para el rastreo de una partícula en un modelo in-vitro del intestino delgado</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Después de realizar la revisión bibliográfica, se pudo constatar que la trayectoria de
una partícula ha sido estudiada en diversas áreas de la ciencia utilizando técnicas analíticas,
numéricas y experimentales (Saxton y Jacobson, 1997; Mason et al., 1997). En
el caso de la experimentación, es común que se utilicen métodos de rastreo y visualización,
como la visualización con láser, tomografía y sus variantes, en la sección 2.2 se
explicarán algunas de estas técnicas.
En el caso particular de la posición, trayectoria y absorción de una partícula dentro
del intestino delgado investigadores como Macagno et al., (1980, 1982), han utilizado
modelos matemáticos con hipótesis simplificadoras, las cuales permiten resolver el problema
sin caer en complicaciones que de manera propia ya tiene el fenómeno. Tharakan
(2010) utilizó dinámica computacional de fluidos (métodos numéricos para resolver las
ecuaciones de Navier-Stokes); Wang et al., (2010) utilizaron el modelo de Lattice Boltzmann
para estudiar fenómenos de transporte en el tracto digestivo, Jaime-Fonseca
(2012) utilizó técnicas de visualización de una partícula dentro de un modelo dinámico
in vitro del intestino delgado, utilizando como se ha mencionado anteriormente modelos
matemáticos planteando hipótesis simplificadoras, para llegar a soluciones particulares.
Asimismo, para lograr resolver las ecuaciones de Navier-Stokes, que de acuerdo con
White (2010) son las ecuaciones fundamentales de la mecánica de fluidos, los investigadores
han utilizado métodos numéricos basados en dinámica computacional de fluidos.
Las investigaciones realizadas por Tharakan (2010) y Jaime-Fonseca (2012) tienen como
objetivo principal, estudiar los procesos de absorción en el intestino delgado debido
a los efectos de los movimientos peristálticos, basados en modelos in vitro del intestino
delgado, en virtud de que éstos contribuyen en la digestión y la absorción de nutrimentos
y fármacos.
El describir la trayectoria de una partícula dentro del modelo in vitro del intestino delgado,
se puede plantear en términos de ecuaciones diferenciales y la solución de las
mismas se puede obtener a través de métodos analíticos y/o métodos numéricos, considerando,
como se ha mencionado anteriormente, hipótesis simplificadoras (Macagno
et al.,1982; Tharakan, 2010). Con el fin que este trabajo de tesis sea pertinente, se propone
el uso de Redes Neuronales Artificiales, debido a que éstas se puedan aplicar a
problemas cuya solución es difícil de obtener, o el algoritmo de solución es extenso para
programar (Ponce, 2010).
El uso de Redes Neuronales Artificiales ha sido empleado por investigadores en
tareas de predicción (Palmer et al., 2000; Santana, 2006), clasificación (Bautista et al.,
2004) y control (Basu, 2006; Castellanos et al., 2012), éstas se han utilizado con éxito
en casi todo los campos de la ciencia (ver subsección 3.2.6), tomando mayor interés
en el campo de predicción, la literatura especializada demuestra que se han logrado
buenos resultados en este ámbito; con lo anteriormente expuesto, se puede afirmar que
las Redes Neuronales Artificiales constituyen una herramienta numérica computacional
apta para este tipo de tareas donde se ven involucradas distintas variables, y además,
la solución analítica es difícil de obtener.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Usar técnicas computacionales como son las Redes Neuronales Artificiales para hacer
el rastreo de una partícula dentro de un sistema in vitro del intestino delgado.
Objetivos particulares
Desarrollar una metodología para describir la trayectoria que sigue una partícula en
un modelo in vitro del intestino delgado, utilizando Redes Neuronales Artificiales.
Aplicar el algoritmo evolutivo de Redes Neuronales Artificiales FS-EPNet, para obtener
la arquitectura de la red que resuelva el problema de predicción.
Determinar la cantidad mínima de patrones de entrenamiento que se debe presentar
a la red para obtener un buen ajuste y predicción.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El uso de Algoritmos Evolutivos en el diseño de Redes Neuronales Artificiales para
el rastreo de una partícula, obtenida de un modelo in-vitro del intestino delgado, da
resultados precisos en cuanto a la trayectoria de una partícula con predicciones a un
paso adelante (SSP), otorgando un modelo computacional que podría ser usado en
aplicaciones médicas.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El hecho de determinar la posición de una partícula constituye un verdadero reto
para los investigadores que utilizan fármacos, puesto que desean conocer el momento
adecuado para que la sustancia se libere en el intestino delgado, de tal modo que el
organismo aproveche al máximo lo que se está suministrando.
Por todas las variables implicadas y la dificultad de hacer pruebas in vivo, el uso
de Redes Neuronales Artificiales es una alternativa para investigadores de diferentes
áreas interesados en realizar predicciones. Una ventaja notable con respecto a otras
técnicas analíticas y/o numéricas de solución de ecuaciones diferenciales es que se
puede obtener información en tiempo real, una vez que ya se realizó el entrenamiento
de la red y se descargó en una placa de circuitos.
Al hacer uso de algoritmos evolutivos en el diseño de la Red Neuronal Artificial, se
optimiza la arquitectura o topología de la red, el número de entradas, retardos y nodos
ocultos, que de otra forma, tendría que llevarse a cabo con ayuda de un proceso de
prueba y error manual (por el especialista), hecho que puede tornarse en una ardua
tarea en cuanto a tiempo y costo se refiere para el humano. Es así que la ventaja más
sobresaliente de los algoritmos evolutivos para su empleo en este trabajo es que el
trabajo de prueba y error es automatizado por éstos.
Debido a que el intestino delgado es un conducto con un camino largo y tortuoso,
además de cambiante y complejo, por los movimientos y reacciones químicas que se
llevan a cabo dentro del mismo, hace que los algoritmos computacionales denominados
como Redes Neuronales Artificiales sean adecuados para realizar este tipo de tareas de
predicción de la posición de una partícula, lo anteriormente mencionado da la oportunidad
a estas técnicas de ser utilizadas por investigadores del campo de la medicina, la
industria farmacéutica y alimenticia, así como empresarios, de estas áreas.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se observa que con el modelo de predicción SSP se obtuvieron mejores predicciones
comparado con el modelo MSP, para ambas concentraciones de Goma Guar (0.5 y
1.0 %, p/v) y para los tres ejes x, y, z.
En especial se obtuvieron las mejores predicciones para el eje x con una concentración
de 0.5 % p/v de Goma Guar, con 1500 patrones de entrenamiento y un DT=1.
Por otra parte se observó que al aumentar el número de patrones de entrenamiento
de la red y que con un paso de predicción (DT=1) se obtuvieron los mejores resultados.
Se comprobó la hipótesis experimental para el eje x: el error usando 500 patrones
de entrenamiento para la predicción de la posición, no fue significativo con respecto al
error empleando 1500 patrones, lo que significa que con 500 patrones de entrenamiento,
para una concentración de 0.5 % (p/v) goma guar, con un modelo de predicción SSP, y
un DT=1, se pueden obtener resultados en menor tiempo.
Además al utilizar un horizonte de predicción DT=100 con 500 patrones de entrenamiento,
con un modelo MSP, la métrica del NMRSE no fue tan buena como la que se
obtuvo para la predicción utilizando el modelo SSP.
Además se puede concluir que con 500 patrones de entrenamiento, utilizando un
modelo SSP se pueden obtener resultados con una correlación de 0.99 y un NMRSE de
0.014 sobre el comportamiento de la partícula dentro del modelo.
Al aumentar el número de generaciones en el algoritmo evolutivo, el error disminuyó
considerablemente, pero por otra parte la arquitectura de la red aumentó de tamaño y el
tiempo de cómputo se ve incrementado notablemente.
En general, se observó que las Redes Neuronales Artificiales son una herramienta
con un gran potencial de aplicación en las áreas farmacéuticas y de alimentos para
rastreo de partículas, en especial en el área de medicina, para el diagnóstico de enfermedades
y la desintegración de un fármaco en función del tiempo.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN CIENCIA APLICADA Y
TECNOLOGÍA AVANZADA
UNIDAD LEGARIA
USO DE REDES NEURONALES ARTIFICIALES PARA EL
RASTREO DE UNA PARTÍCULA EN UN MODELO IN-VITRO
DEL INTESTINO DELGADO
TESIS
QUE PARA OBTENER EL TÍTULO DE
MAESTRO EN TECNOLOGÍA AVANZADA
P R E S E N T A
 ING. ROMER DANIEL OYOLA GUZMÁN
DIRECTORES DE TESIS:
DRA. MÓNICA ROSALÍA JAIME FONSECA
DR. VICTOR MANUEL LANDASSURI MORENO
MÉXICO, D.F. JULIO 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Diseño e implementación de un algoritmo para detección de inyección SQL en sitios web para dispositivos móviles</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Como hemos mencionado el uso de Internet se encuentra en aumento, es
por ello que cada vez más compañías se ven en la necesidad de permitir a
sus socios y proveedores acceder más fácilmente a sus sistemas de
información. Debido a lo anterior, es fundamental conocer qué recursos de la
compañía necesitan mayor protección, con el fin de controlar tanto el acceso
al sistema, así como los derechos de los usuarios del sistema de
información.
Es necesario añadir también, que debido a la tendencia creciente hacia un
estilo de vida nómada de hoy en día, en el cual se permite a los empleados
conectarse a los sistemas de información casi desde cualquier lugar, es
necesaria una mayor protección de la información. En la actualidad, un
empleado puede llevar consigo parte de un sistema de información fuera de
la infraestructura segura de la compañía u organización, lo que puede
resultar en mayores vulnerabilidades que puedan causar un ataque que
represente pérdidas cuantiosas tanto para proveedores como usuarios.
En este sentido, se ha planteado la problemática tema de esta tesis, es decir
los ataques de inyección SQL, los cuales de acuerdo a los datos mostrados,
se convierten en fuente de mayores vulnerabilidades para las aplicaciones
web. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
Crear un algoritmo de seguridad informática que permita detectar
ataques e inyección en Bases de Datos.
Objetivos Específicos
 Concretar las técnicas de inyección de información.
 Determinar el funcionamiento del algoritmo a desarrollar.
 Analizar la viabilidad en sistema operativo.
 Evaluar procedimientos de inyección estándar conocidos.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Según los estudios realizados sobre seguridad informática, la mayor parte de
las investigaciones se centran en descubrir los incidentes en los sistemas
operativos y en la red de datos, es decir, se limitan a investigar los ataques a
los sistemas mediante investigaciones forenses informáticas. Sin embargo,
este tipo de análisis sólo permite identificar datos pre y post transacción lo
cual resulta útil para demostrar las brechas en seguridad y recolectar
evidencias falibles, no obstante es necesario contar con herramientas que
permitan hacer una detección oportuna de las vulnerabilidades del sistema
en los sitios web.
Como hemos mencionado anteriormente, el tipo de ataque de inyección SQL
ha sido muy utilizado, lo que ha representado daños considerables para las
personas que han sido víctimas de ellos. Igualmente se señala que cualquier error en el modelo “Interfaz-Proceso-Datos” puede ser causante de un
ataque de este tipo, lo que puede resultar en que el atacante sea capaz de
extraer y modificar información de la Base de Datos, leer y escribir ficheros y
ejecutar comandos.
El alcance de este tipo de ataque dependerá en gran medida de los permisos
de las cuentas de usuario implicadas, así como de los privilegios que lo
puedan colocar en una situación más ventajosa de la que se tenía en un
principio. Para que un ataque de inyección SQL se lleve a cabo es
fundamental que el atacante conozca y domine el Sistema Gestor de Base
de Datos y el entorno en el que éste opera.
Es por lo anterior, que la seguridad se ha convertido en un área importante
de investigación debido a las amenazas crecientes que día a día se
presentan en los sistemas informáticos. En este trabajo se aborda uno de los
ataques más frecuentes en la actualidad, el de inyección SQL, que se
caracteriza por su alta capacidad para evolucionar rápidamente, aplicando
cambios en su estrategia de ataque, se centran principalmente en poner en
riesgo la disponibilidad de los datos y aplicaciones, bloqueando el acceso a
los usuarios autorizados.
De la investigación realizada en el presente trabajo hemos detectado que las
medidas de seguridad existentes se centran en garantizar la confidencialidad
e integridad de los datos, presentando poca atención a la disponibilidad de
los mismos. Como respuesta a la problemática planteada se desarrolla un
algoritmo que permite detectar vulnerabilidades dentro de los sitios web,
planteado como una herramienta libre, que facilita la detección de las
mismas.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo de tesis se realizó una investigación de las diversas
técnicas de inyección de la información en específico de las que están
relacionadas con la inyección SQL. En este sentido encontramos que la
inyección SQL se realiza principalmente mediante tres técnicas
 Ataque de inyección SQL: Diseñado para amenazar el modelo “InterfazProceso-Datos”,
el dominador común en las vulnerabilidades es que en
todas ellas existe un error de programación ubicado en la capa de
“Proceso” que permite las entradas del usuario, producidas a nivel de
“Interfaz” las cuales producen que la “Gestión de Datos” pueda ser
atravesada y una directa interacción con la misma.
:
 Serialized SQL Injection: En los ataques sencillos de inyección SQL se
tenía que extraer la información poco; sin embargo, mediante esta
técnica se puede serializar los resultados de la consulta a un string
XML, con lo que es posible saltar las medidas preventivas y tener un
mejor acceso a las bases de datos de las aplicaciones. El principal
inconveniente de esta técnica es que se requiere una petición por cada
fila de la consulta inyectada.
 Blind SQL Injection o inyección SQL a ciegas: Las dos técnicas
descritas anteriormente tienen como característica común que
aprovechan un punto del programa en que se muestro algo procedente
de la Base de Datos. Sin embargo, en ocasiones es imposible obligar a
la aplicación a mostrar la información deseada, en este caso se utiliza la
técnica de inyección a ciegas, se trata de un tipo de ataque en el que el
atacante no sabe nada del resultado producido por una operación. Para
realizar la consulta se utilizan métodos de inferencia con el objetivo de
eliminar obstáculos, mediante consultas de tipo booleano.
Tal y como se ha expuesto a lo largo de este escrito, la seguridad se ha
convertido en un área importante de investigación debido a las amenazas
crecientes contra este tipo de sistemas. En este trabajo de investigación se
ha estudiado uno de los ataques más preocupantes en la actualidad: los
ataques de inyección SQL. Estos ataques se caracterizan por una alta
capacidad de evolucionar rápidamente, aplicando cambios en sus estrategias de ataque, y se centran principalmente en poner en riesgo la
disponibilidad de los datos y las aplicaciones, bloqueando el acceso a los
usuarios autorizados. La principal deficiencia observada en el estado del
arte es que las medidas de seguridad existentes se centran en garantizar la
confidencialidad e integridad de los datos, prestando poca atención a la
disponibilidad.
Una de las innovaciones de la arquitectura Phyton SQL Injection es que el
proceso de detección de intrusiones se realiza aplicando técnicas de
inyección SQL a ciegas en las diversas capas de la aplicación. La detección
de intrusiones en la capa de aplicación de los sistemas distribuidos, resulta
bastante complicada debido al enorme volumen de tráfico generado. Esta
situación posibilita la penetración de intrusos a las aplicaciones camuflando
los ataques con el tráfico legal. La cantidad de información que se requiere
procesar, resulta muchas veces inmanejable para las técnicas tradicionales
de detección de intrusiones. En este sentido, las técnicas y algoritmos del
aprendizaje manual se presentan como una alternativa ante la falta de
aplicaciones para móviles con sistemas operativos como Android.
De cara a remarcar las aportaciones de la arquitectura Phyton SQL Injection
con respecto al estado del arte se ha realizado una comparación con otras
arquitecturas existentes centradas en la detección de ataques de inyección
SQL. La comparación se realizó desde el punto de vista analítico,
principalmente por la dificultad para tener acceso a las implementaciones
de las arquitecturas existentes y, en algunos casos, las arquitecturas
existentes solo están disponibles a nivel de propuesta. Para llevar a cabo la
comparación, se realizó una revisión y evaluación minuciosa de los trabajos
considerando que en algunos casos los artículos publicados, siendo el
recurso más accesible, omiten detalles importantes por limitaciones de
espacio, requiriendo entonces un análisis profundo para entender el
funcionamiento y las generalidades del enfoque propuesto.
Además, la arquitectura ha sido comparada con las arquitecturas existentes
para la detección de ataques de inyección XML. La mayoría de las
propuestas revisadas se centran en modelos basados en cortafuegos,
configurando y aplicando distintos tipos de políticas sobre los mensajes
XML y ejecutando validaciones gramaticales para detectar mensajes
maliciosos. En conclusión, a diferencia de los enfoques existentes para la
detección de los ataques de inyección SQL, Phyton SQL Injection mejora al
resto de los enfoques en función de:
Tipo de Detección: Phyton SQL Injection se diseña para explotar las
ventajas de las técnicas más conocidas en detección de intrusos.
Emplea como un primer filtro una detección mediante firmas (misuse
detection) y como componente principal de clasificación una detección
basada en anomalía (anomaly detection).
 Capacidad Adaptativa: Phyton SQL Injection incluye tipos de agentes
inteligentes diseñados para aprender y adaptarse a los cambios en los
patrones de ataque, nuevo tipos de ataques y cambios en el
comportamiento de los usuarios.
 Escalabilidad: Phyton SQL Injection has sido diseñada para distribuir la
carga de las tareas de clasificación a través de las capas de la
arquitectura jerárquica. Adicional, dependiendo de la carga de trabajo,
Phyton SQL Injection es capaz de crecer instanciando nuevos agentes.
 Tolerancia a Fallos: Phyton SQL Injection ha sido diseñada de forma
jerárquica para limitar el impacto de errores y facilitar la recuperación de
errores, eliminando de forma sencilla los agentes comprometidos.
En la siguiente tabla se realiza un comparativo de la Arquitectura Phyton SQL
Injection con respecto a otras arquitecturas existentes centradas en la detección
de ataques de inyección SQL:
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
Centro de Innovación y Desarrollo Tecnológico
en Cómputo
DISEÑO E IMPLEMENTACIÓN DE UN ALGORITMO PARA
DETECCIÓN DE INYECCIÓN SQL EN SITIOS WEB PARA
DISPOSITIVOS MÓVILES
TESIS QUE PARA OBTENER EL GRADO DE
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
PRESENTA:
Ing. Sergio Meléndez César
DIRECTORES
M. en C. Jesús Antonio Álvarez Cedillo
M. en C. Israel Rivera Zárate
MÉXICO, D.F. DICIEMBRE DE 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Análisis de cobertura y modelado del consumo energético en una red de sensores.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Se puede definir a una red de sensores inalámbrica como una red de pequeños sistemas
informáticos embebidos colocados en el mundo físico, y capaces de interactuar con este. Las redes
de sensores tienen sus inicios en el campo militar en los Estados Unidos, durante los años de la
guerra fría en donde se instalaron una red de boyas sumergidas en el mar para detectar submarinos
empleando sensores de sonido.
La investigación en redes de sensores en los años ochenta comenzó con el proyecto redes de
sensores distribuidos (DSN, Distributed Sensor Networks), de la agencia militar de investigación
avanzada de Estados Unidos (DARPA, Defense Advanced Research Projects Agency) [3].
Considerando que cada nodo de la red debe de colocarse en la área de cobertura donde existe
restricción del suministro energético y donde solo contará con una batería, que garantice la
integridad de la comunicación y procesamiento de los parámetros a censar, existe la necesidad de
optimizar el consumo de energía para prolongar la vida útil del nodo sensor y su comunicación; la
pregunta crucial es: "cómo prolongar en el tiempo la funcionalidad de la red si se tiene que la vida
útil de la batería es finita. Por lo tanto, la maximización de la funcionalidad de la red se presenta a
través de la minimización de la energía como un reto importante en WSN; la dificultad que se
presenta es que los sensores no pueden ser reemplazados fácilmente o recargarse debido a su
despliegue si la cobertura logra abarcar áreas extensas [2].
Teniendo en cuenta que el ahorro de energía actúa como uno de los temas más candentes en las
redes de sensores inalámbricos, el objetivo central radica en obtener el consumo de nodo aplicando
una técnica eficiente para el ahorro de energía, considerando que la restricción fundamental se
presenta al emplear una red de sensores, el área de cobertura, la conservación de la energía
disponible en cada nodo sensor, y el tiempo de independencia energética. 
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
Obtener el modelo de energía de una red inalámbrica de sensores para calcular el consumo de
potencia y determinar el alcance y cobertura que proporciona.

Objetivos Específicos
• Determinar el modelo de energía de la red de sensores inalámbrica.
• Obtener el consumo de potencia de todo el sistema y el tiempo de vida de la alimentación
con baterías.
• Obtener la cobertura que tiene cada nodo de la red de sensores inalámbrica para determinar
la estimación de la cantidad de nodos necesarios de acuerdo al área de cobertura requerida
para la aplicación. </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En el análisis de cobertura y modelado del consumo energético en una red de sensores, se obtiene
información que trabajos semejantes se orientaron por predecir el rango y ubicación de cada nodo
sensor, y atendían a la optimización del recurso energético como la un algoritmo o trama de datos;
y lo misma potencia de alcance la destinaban tanto al más cercano como al mas lejano. Solo
atacaron ráfagas de datos esporádicos, y no un diagnóstico de los recursos de hardware que
emplean, por lo que el derroche energético, siempre estuvo presente. La propuesta aquí fue
identificar los parámetros energéticos que consumían cada uno de los componentes de un nodo,
así como los elementos esenciales de comunicación como sensibilidad del receptor, potencia del
transmisor, ganancias de las antenas transmisora y receptora, así como las atenuaciones que se
tienen en el sistema como conexión de antenas, línea de transmisión, atenuación por propagación
en el espacio de la energía electromagnética. Con esta información se pudo establecer el margen
de operación del sistema, con él se determinó con que fuerza llega la señal segura entre nodos y
con ello garantizar la integridad de los datos de comunicación, y el tiempo de vida de la batería.
En el cálculo del margen se empleó la ecuación de Friis para estimas las atenuaciones en la
propagación del espacio libre de obstáculos y con claridad en la línea de vista y despeje de la zona
de comunicación entre transmisor y receptor. El gasto energético que presentan la unidad de
procesamiento central, el sistema de adquisición de datos y el sistema de comunicación se
establece una cota promedio por bit transmitido, y el tiempo de duración de la comunicación
De la Tabla 3.4 se concluye que para los módulos XBee Pro serie 2 a una potencia de 63mW (nivel
de potencia 4) y con antena de alambre, el enlace más adecuado en nuestro conjunto de pruebas es
más recomendable utilizar el enlace a 300 metros. Aun cuando a 500 metros se tiene un 98% de
los paquetes recibidos, no es recomendable utilizar esta distancia debido a que se compromete la
integridad de la conexión. Se recomienda que exista una distancia máxima de 300 metros entre
cada nodo de la red de sensores para obtener la cobertura deseada. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN Y DESARROLLO DE
TECNOLOGÍA DIGITAL
MAESTRÍA EN CIENCIAS EN SISTEMAS DIGITALES
“ANÁLISIS DE COBERTURA Y MODELADO DEL CONSUMO
ENERGÉTICO EN UNA RED DE SENSORES”
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRÍA EN CIENCIAS
P R E S E N T A:
ING. ÁNGEL HUMBERTO CORRAL DOMÍNGUEZ
BAJO LA DIRECCIÓN DE:
DR. JOSÉ CRUZ NÚÑEZ PÉREZ
M.C. ANDRÉS CALVILLO TÉLLEZ
SEPTIEMBRE 2016 TIJUANA, B. C., MÉXICO</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Traductor de nombres de dominios para la comunicación entre usuarios IPV4 E IPV6</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la actualidad el agotamiento de direcciones es el principal problema para el crecimiento
de Internet y forma parte principal como un sistema de comunicación en todo el mundo.
Esta dificultad viene desde su origen del Protocolo IP, se estableció el direccionamiento con
clase, estas direcciones que se adquirían se hacía una insuficiente distribución en la
organización. Se utilizaba la distribución a partir de repetidores hasta alcanzar el máximo  que se podían en el caso de Ethernet son cuatro y se estaban desperdiciando miles de
direcciones dependiendo del bloque de direcciones que se le haya asignado [1].
Se adoptó las subredes para eliminar ese desperdicio de direcciones y poder distribuir
direcciones correctamente dentro de la organización.
Al paso del tiempo las organizaciones solicitaron diferentes bloques grandes para ser
utilizadas, pero la realidad es otra. Se han hecho estudios donde existen organizaciones
que solicitaron bloques de direcciones de la clase A o B y apenas llegan a mil host en su
organización. Si la distribución hubiera sido correcta pues existirían miles de direcciones
para su uso. Debido a esta asignación el agotamiento no se puedo evitar.
Se empleó Enrutamiento Interdominio sin Clases para asignar correctamente las
direcciones restantes en bloques de tamaño variable; si una organización solicita 2000
direcciones a este se le asigna un bloque que contenga 2048 direcciones [9].
Se llegó el momento en que las direcciones son escasas pues una solución a corto plazo
era el uso de NAT, este protocolo consiste en tener direcciones IPv4 privadas que no son
visibles en Internet y a través de un dispositivo convertir estas direcciones privadas en
públicas para ser vistas en la red de Internet. Este método es muy empleado por los ISP’s
(Proveedor de Servicios de Internet), ellos cuentan con bloques con ciertas direcciones
establecidas y debido a la escases de direcciones no pueden solicitar más bloques para
brindar el servicio de Internet a los clientes finales.
El agotamiento de direcciones fue inminente esto ocurrió en el año 2011, donde se delegó
el último bloque de direcciones. Pero con anterioridad se trabajó en una solución a largo
plazo, es emigrar de un sistema de direcciones de 32 bits a uno de 128 bits que ampliaría
las direcciones. Pero el problema es que no se pueda cambiar de un día a otro el sistema
direccionamiento ya que no todos cuentan con esa infraestructura para realizar ese cambio.
La solución es utilizar mecanismos de transición para que el cambio sea suave y no sea de
una manera muy abrupta. Estos mecanismos llevan desde tener en convivencia de ambos
protocolos IP, trasmitir paquetes de IPv6 sobre IPv4 e incluso hacer traducciones de
direcciones.
Al contar con bloques IPv6 lo normal es tener acceso a sitios IPv6, pero como aún se
encuentra en etapa de transición solo existen 2% de sitios IPv6, para solventar este
problema se utiliza una técnica de traducción para convertir sitios IPv4 a IPv6.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un DNS64 utilizando modelo cliente-servidor e implementarlo en una red para
la comunicación entre usuarios que manejan protocolos IPv4 e IPv6.
Objetivos Particulares
 Realizar un análisis de los protocolos IPv4 e IPv6 y verificar su relación con Internet
 Hacer la simulación de protocolos de IPv4 e IPv6.
 Comprender las técnicas y métodos para las comunicaciones entre protocolo IPv4
e IPv6.
 Realizar un análisis de tipos de DNS y obtener su funcionamiento en redes IPv4 e
IPv6.
 Realizar y simular una topología para evaluar el sistema.
 Realizar una página de prueba IPv6 para que interaccione con el sistema</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La escasez de direcciones de Internet IPv4 ha motivado a crear un nuevo protocolo para
seguir con el incremento de Internet. Este nuevo protocolo llamado IPv6 hizo su
lanzamiento en junio del 2012.
Una parte a este incremento se dará por los ISP’s que son los que brindan servicios de
Internet para el público en general y quienes realizaran una transición del protocolo anterior
al protocolo nuevo a través de mecanismos de transición. Esta es una nueva tecnología
para los ISP´s, y estos deberán a empezar a implementar los mecanismos de transición
para hacer el cambio de una manera suave.
Por otro lado, las investigaciones sobre IPv6 ayudaran ampliar las aplicaciones que se
encuentran actualmente en uso y desarrollar la convivencia entre ambos protocolos para
que no sea de una manera abrupta.
Muchas organizaciones han comenzado a obtener bloques de direcciones del nuevo
protocolo IPv6. El reto para estas organizaciones es hacer esa transición entre IPv4 e IPv6
que sería el gran paso para el nuevo protocolo.
Una de estas organizaciones es el Instituto Politécnico Nacional (IPN) obtuvo su bloque de
IPv6 el 18 de Julio del 2012. En el IPN se empieza a realizar el desarrollo de IPv6 en sus
instalaciones, y se encuentra en un buen momento ya que el lanzamiento oficial de IPv6
fue el 06 de Junio del 2012 y pues se pueden realizar pruebas desde mecanismos de
transiciones hasta la administración de las redes que es lo más complicado en una red. </Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El objetivo fundamental de esta tesis es abordar el problema del agotamiento de direcciones
IPv4, y la convivencia de éste el protocolo IPv6 a través del mecanismo de traducción
DNS64/NAT64. La aportación principal consiste en el desarrollo e implementación de un
software que realice la función de un DNS64, y que los módulos DNS64/NAT64 funcionen
de manera simultánea. Lo anterior debido a que no existe soporte en las aplicaciones
actualmente disponibles, mismas que presentan problemas al momento de resolver los
nombres de dominio.
Se documentó el análisis, configuración y pruebas de los diferentes tipos de DNS que
existen en los protocolos IPv4 e IPv6, para efectos de comparación. Al conocer su
funcionamiento se desarrolló un diagrama de flujo como apoyo para su implementación en
lenguaje C, cumpliendo las actividades de un DNS64.
Se implementó un escenario de pruebas para analizar el comportamiento del DNS64, y se
verificó que el escenario trabaja de manera simultánea con el NAT64. El sistema se sometió
a diferentes pruebas de conectividad entre una red IPv4 y otra IPv6 para verificar la
estructura de los mensajes DNS. Al realizar este procedimiento de pruebas se obtuvo
conectividad entre ambas redes y el intercambio de mensajes fue tal como se planeó, de
donde se desprende que este método de transición puede ayudar a realizar una convivencia
suave entre ambos esquemas de red, sirviendo como base para el desarrollo IPv6 en el
IPN. En comparación de los escenarios descritos en el estado del arte y en el RFC6147, se
observa que el escenario implementado funciona de manera simultánea y los servidores se encuentran por separado y no en un solo equipo; la parte adicional que se agregó siempre
estará la relación de nombre-dirección cargado en memoria.
Así mismo, se cumplieron todos los objetivos propuestos ya que se implementó un
escenario descrito en el RFC6147 que no se había expuesto en investigaciones anteriores,
y se le añadió almacenamiento en memoria para su rápida disponibilidad. Con el apoyo
adicional del NAT64 se pueden utilizar aplicaciones residentes en IPv4, y visualizarlas o
procesar su información en IPv6, ya que el DNS64 ayuda a realizar una búsqueda más
sencilla y eficiente de dichas aplicaciones.
Se configuró un NAT64 en una distribución Linux Centos 6, con el paquete tayga como
parte adicional para la convivencia de IPv6 e IPv4; como estrategia inicial se implementó el
paquete antes mencionado pero presentó diversas limitantes tales como el funcionamiento
exclusivamente con direcciones IP privadas, por lo cual no se pudo realizar la prueba para
un DNS recursivo, aunque este proyecto cuenta con el módulo desarrollado.
Finalmente, es importante desarrollar un NAT64 que funcione paralelamente con este
proyecto, para verificar su funcionamiento y las diversas adecuaciones que se pueden
implementar con el objetivo de una operación estable entre ambas aplicaciones.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
TRADUCTOR DE NOMBRES DE DOMINIOS
PARA LA COMUNICACIÓN ENTRE USUARIOS
IPV4 E IPV6
TESIS PARA OBTENER EL GRADO DE:
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
PRESENTA:
Ing. Danis Alexis García Martínez
Directores de tesis:
M. en C. Marlon David González Ramírez.
M. en C. Marco Antonio Valencia Reyes.
México, D.F. Noviembre 2015
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Diseño paramétrico óptimo del mecanismo de la extremidad de un robot bípedo</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En este trabajo se pretende diseñar la extremidad de un robot bípedo (ERB) para generar
una trayectoria similar al caminar humano, por medio de un problema de optimización
para encontrar las dimensiones óptimas del mecanismo. El problema formal de optimización
consiste en encontrar los parámetros estructurales (longitudes de los eslabones) p, que
minimicen un criterio de desempeño J mostrado en (1.1).
M in J (1.1)
Sujeto a las restricciones inherentes en el diseño, refiriéndose como restricciones de igualdad
(1.2) y de desigualdad (1.3), donde J es el error que existe entre la trayectoria deseada y la
obtenida con un modelo matemático que describe al sistema.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Con la finalidad de establecer el alcance de la presente tesis se formula el siguiente objetivo
general.
Diseñar paramétricamente la extremidad de un robot bípedo con base en la solución
de un problema de optimización.
 Objetivos particulares
Definir la función objetivo y las restricciones del problema de optimización en el diseño
de la extremidad de un robot bípedo.
Obtener el diseño paramétrico óptimo de la extremidad de un robot bípedo utilizando
la meta-heurística de ED.
Validar mediante simulación númerica y de forma experimental el diseño paramétrico
obtenido.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se estableció el diseño de una extremidad bípeda con base en un problema de optimización,
obteniendo resultados satisfactorios de forma práctica y en simulación.
En este trabajo se propone el diseño paramétrico óptimo del mecanismo de la extremidad inferior
de un robot bípedo que considera variables antropométricas y cinemáticas para su diseño.
El proceso propuesto puede permitir resolver la síntesis dimensional para la generación de
trayectoria para este mecanismo, tal que si se cambia la trayectoria propuesta, el proceso propuesto
encontrará las dimensiones del mecanismo para generar la nueva trayectoria.
El mecanismo de ocho eslabones se establece para realizar la marcha bípeda. Para llevar a cabo
la parametrización del mecanismo, se divide en varios sub-mecanismos del mecanismo de ocho
eslabones. Con el propósito de satisfacer el acoplamiento de los sub-mecanismos así como garantizar
la locomoción del robot bípedo a partir del mecanismo de ocho eslabones, se establecen restricciones
en su diseño.
Resultados en simulación y experimentales de la solución del problema con base en el algoritmo de
evolución diferencial muestra lo siguiente:
1. La multiplicidad de soluciones diferentes encontradas sugiere que el problema tiene un
comportamiento multimodal.
2. Si bien el utilizar métodos de optimización aproximados normalmente trae consigo un error, el
rango de dicho error es lo suficientemente bajo para considerar a las soluciones como buenas
desde el punto de vista de fabricación del prototipo del mecanismo.
3. El diseño obtenido a partir del mejor resultado reproduce de buena manera el movimiento de
marcha deseado en el robot bípedo.
Una fuente de error en el seguimiento de la trayectoria es el número de puntos a seguir. Sin
embargo, se probó que es factible el diseño de la extremidad del robot bípedo planteándolo como un
problema de optimización y resolviéndolo con meta-heurísticas.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
Diseño paramétrico óptimo del mecanismo de la extremidad de un
robot bípedo
TESIS
para obtener el grado de
Maestría en Tecnología de Cómputo
PRESENTA:
Jesús Said Pantoja García
Directores de Tesis
Dr. Miguel Gabriel Villarreal Cervantes
M. en C. Juan Carlos González Robles
México, D.F. Diciembre de 2015.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema de control de movimiento para un manipulador movil</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El control de seguimiento de trayectoria en un manipulador móvil es un problema actual
de gran trascendencia, ya que las aplicaciones de este tipo de robots en la vida real son
cada vez más frecuentes; en ese sentido, en el Laboratorio de Mecatrónica del CIDETEC
[33] diseñó y construyó un manipulador móvil con capacidad de realizar tareas definidas
por el usuario desde una estación base, mediante teleoperación. El prototipo cuenta con una
plataforma móvil de tipo diferencial y un brazo antropomórfico con tres grados de libertad
(3 GDL).
Actualmente, el movimiento del sistema se regula utilizando un control de posición en
lazo abierto, de tal forma que desde la estación base el operador indica los movimientos a
realizar por el manipulador móvil; dicha información se envía al sistema de procesamiento
digital de datos del móvil, por medio de una conexión inalámbrica, y localmente en el móvil
se realizan los cálculos para generar las leyes de control correspondientes al movimiento,
tanto del brazo como del manipulador. Ésta capacidad de realizar los cálculos localmente
proporciona una gran flexibilidad al sistema, ya que se evitan retardos correspondientes a la
comunicación.
Sin embargo, para aumentar las capacidades de operación del prototipo es necesario
desarrollar un sistema de control en lazo cerrado, que permita al sistema realizar tareas
de seguimiento de trayectoria definidas por el usuario desde la estación base, con un error
de posición mínimo. Para continuar con la misma filosofía de diseño las nuevas leyes de
control deben implementarse en el móvil; así mismo, y dado que se utiliza un sistema de
procesamiento digital de datos es recomendable que las leyes de control obtenidas sean de
tipo discreto.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un sistema de control de movimiento para el manipulador móvil desarrollado
en el CIDETEC, a partir de los modelos discretos tanto de la base como del brazo, de tal
forma que se proponga un conjunto de algoritmos que será implementado en un sistema
digital de procesamiento de datos, embarcado en el manipulador móvil.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El prototipo de manipulador móvil desarrollado en el CIDETEC tiene una arquitectura
abierta, con la finalidad de que sea utilizado como plataforma experimental para la generación
de conocimiento en diversas áreas. Dado que la funcionabilidad del robot depende de
su subsistema de control, es importante diseñar e implementar un sistema de control en lazo
cerrado para lograr una operación con un mínimo de error, utilizando para ello el sistema
embebido embarcado en el móvil.
Para lograr lo anterior, se requiere que los algoritmos desarrollados para el control en
lazo cerrado del manipulador móvil sean eficientes y rápidos de calcular. Así, el principal reto es proponer una metodología de control para solucionar el problema de seguimiento de
trayectoria de un manipulador móvil diferencial operado a distancia, con un algoritmo que
deberá ser implementado en un sistema embebido.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En el desarrollo de este trabajo de tesis, se implementan las siguientes actividades:
1. Investigación de tipo documental y bibliográfica sobre la planificación de trayectoria
en manipuladores móviles.
2. Adquirir los conocimientos para la utilización de un sistema embebido como sistema
de control.
3. Estudio y análisis de la arquitectura del manipulador móvil.
4. Estudio y análisis de la arquitectura del sistema embebido especifico, para determinar
cuáles son las mejores opciones en cuanto a programación y obtener su máximo
rendimiento.
5. Asistir a cursos de robótica móvil, control discreto, optimización y programación;
para consolidar conceptos claves para el desarrollo de la investigación.
6. Adquirir los conocimientos necesarios para desarrollar los algoritmos de control discreto,
para controlar el movimiento del sistema del manipulador móvil diferencial.
7. Realizar mediante simulación diferentes pruebas al controlador propuesto para validar
su funcionalidad y medir los valores de las variables reales generadas por las leyes de
control implementadas.
El presente trabajo de tesis esta organizado de la siguiente manera: en el Capítulo 2 se
presenta la descripción detallada del sistema, mientras que las leyes de control propuestas
para regulación y seguimiento de trayectoria se desarrollan en el Capítulo 3, incluyendo los
resultados de las simulaciones numéricas. El Capítulo 4 contiene la programación desarrollada
e implementada en el sistema, con el reporte de las pruebas realizadas. Finalmente, en
el Capítulo 5, se presentan las consideraciones finales del trabajo, contemplando tanto las
conclusiones como los trabajos a futuro.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La construcción original del prototipo de manipulador móvil se realizó con base en una
arquitectura abierta, lo cual permitió que se llevaran a efecto las modificaciones e implementaciones
que se presentan en este trabajo; por ello, y a pesar de los cambios, se mantuvo
la misma filosofía de diseño con el fin de que el sistema se pueda continuar actualizando y
adecuando para su aplicación en el trabajo académico.
Considerando lo anterior, la aportación más importante en esta tesis fue el desarrollo de
los algoritmos de control en lazo cerrado y su programación e implementación en el sistema
embebido Fit-PC3i, para el posicionamiento y seguimiento de trayectoria del manipulador
móvil. El carácter discreto de dichos algoritmos permitió simplificar los cálculos del modelo,
reduciendo el tiempo y la cantidad de recursos de cómputo requeridos para su ejecución;
así mismo, la instalación del sistema embebido otorga al prototipo la flexibilidad suficiente
para la implementación de otros algoritmos de control e incluso de nuevas funciones.
Por otra parte, la modificación del esquema de comunicaciones y el cambio del dispositivo
de comando permitieron que el robot presente una respuesta más eficiente a las
indicaciones del operario, aumentando de manera simultánea la autonomía de movimiento.
De igual forma, la implantación de un esquema inalámbrico basado en el modelo cliente–
servidor y en el protocolo 802.11 abre la posibilidad al uso de otros dispositivos para la
dirección del móvil, e incluso permiten la inserción del prototipo en redes Ad–hoc.
Finalmente, como trabajo a futuro se considera el desarrollo de otros algoritmos para
control e implementación de nuevas funciones en el prototipo. Adicionalmente se prevé la
incorporación de sensores al robot móvil, que permitan que una vez posicionado pueda realizar una gama amplia de acciones, para las cuales deberá complementarse con la instalación
de efectores finales. De igual forma, se considera la instalación de cámaras en el robot, que
permitan resolver el problema de regulación visual y cuya información también sea procesada
en el sistema embebido, por medio de módulos adicionales de programación. Otra
consideración que puede ser mejorada es adaptar un control para juego inalámbrico con un
rango mayor de alcance, o incluso la utilización de dispositivos de realidad virtual para las
funciones de comando del móvil.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO

Sistema de control de movimiento para
un manipulador móvil
Que para obtener el grado de:
Maestría en Tecnología de Cómputo
Presenta:
Noemi Hernández Oliva
Directores de tesis:
Dr. Edgar Alfredo Portilla Flores
Dra. Paola Andrea Niño Suárez
Mayo 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Diseño y Construcción de un Exoesqueleto para Rehabilitación de Lesiones de la Rama Tenar Motora del
Nervio Mediano en el Dedo Pulgar 
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El presente trabajo de tesis está enfocado a la relevancia que tiene el realizar una
adecuada terapia de rehabilitación en una persona económicamente activa que haya
sufrido una lesión que requiera de una intervención quirúrgica, ya que el impacto del
tiempo de recuperación, en caso de que la terapia sea deficiente o lleve mucho tiempo, no
solo es de carácter económico, sino también laboral y social.
Actualmente en nuestro país no se cuenta con un dispositivo para terapia que integre,
por un lado, el efecto terapéutico de la electroestimulación transcutánea y por el otro el
uso de dispositivos exoesqueléticos que entreguen una respuesta dinámica al paciente
para realizar el movimiento completo de la articulación.
Los puntos claves que se van a atender con la construcción de dicho dispositivo son,
entre otros:
1. Para producir contracción de las fibras musculares denervadas es necesario estimular
directamente la membrana de la fibra muscular con pulsos eléctricos de frecuencia,
intensidad y voltaje conocido. De ahí el empleo de la electroestimulación transcutánea
como terapia de rehabilitación.
2. La potenciación muscular se inicia cuando el recorrido articular es el adecuado, el cual
debe ser un movimiento controlado, activo y completo de la articulación, que a su vez,
gradualmente disminuye las molestias que el paciente tenga por rigidez articular, que se
presenta por ausencia de movimiento en el periodo de recuperación postoperatoria.
3. Bajo costo de producción, materiales de construcción de fácil adquisición en el
mercado nacional, que el software sea sencillo y de fácil operación por el usuario, así
como que el diseño del dispositivo sea antropomórfico. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo de este trabajo de investigación es el diseño y construcción de un dispositivo
mecatrónico de tipo exoesquelético enfocado a la rehabilitación de lesiones del nervio
mediano en el dedo pulgar, con capacidades de aplicar electroterapia para mejorar o
disminuir el tiempo de rehabilitación de dichas lesiones, así como el movimiento de la
articulación de manera pausada pero progresiva durante todo el tiempo que dure la
terapia. </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología utilizada se dividió en las siguientes etapas:
Etapa 1. Revisión documental de los métodos y soluciones propuestos para la
rehabilitación utilizando dispositivos mecatrónicos para la lesión mencionada: se revisó la
literatura médica de la aplicación de la electroestimulación y mecanoterapia, en el campo
de los exoesqueletos aplicados a la medicina. 
Etapa 2. Análisis de los diversos dispositivos mecatrónicos existentes y su respuesta en
diversas áreas: se buscó y analizó los trabajos previos a este, en donde se han empleado
exoesqueletos, tanto en la industria y la milicia como en áreas médicas y afines.
Etapa 3. Se desarrolló la Interfaz Computacional: la aplicación para la interfaz de usuario
y el sistema de comunicación de datos, envió y adquisición, vía una tarjeta de
adquisición. Esta etapa es la encargada de enviar datos de la computadora al
exoesqueleto y al electroestimulador, y de recibir datos del usuario y de la computadora
para que puedan manipularse. Se diseñó con la finalidad de crear una interfaz amigable
y/o de fácil manejo para aquel que aplicará la terapia de rehabilitación.
Etapa 4. Se desarrolló la instrumentación electrónica para la Interfaz Mioeléctrica, (EMG
y electroestimulador): el sistema de instrumentación es capaz de captar los impulsos
eléctricos conducidos a lo largo del músculo en el cual se desea trabajar para saber el
grado de amplificación con que se necesita realizar la estimulación eléctrica y a su vez
genera las formas de onda con un valor exacto de corriente y frecuencia para llevar a cabo
la electroestimulación.
Etapa 5. Diseño del exoesqueleto: una vez realizada la interfaz computacional así como la
instrumentación electrónica, se procede a diseñar el mecanismo del exoesqueleto el cual
se controla por mandos específicos enviados desde la computadora de acuerdo a la
terapia que el especialista desee aplicar.
Etapa 6. Integración del sistema: Luego de que cada uno de los subsistemas del
dispositivo se han diseñado y verificado su comportamiento por simulación y
experimentación en laboratorio, se realiza la integración del sistema para su prueba final. 
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Al inicio de este trabajo de investigación, se propusieron objetivos que se consideraron
alcanzables y comprobables, lo cual se describe a continuación:
La selección del tipo de lesión fue correcto, ya que existe una amplia gama de lesiones
traumáticas y patológicas del miembro superior, pero la lesión seleccionada en particular
es de gran importancia y relevancia su rehabilitación debido a la trascendencia del
movimiento que se ve afectado derivado de ésta.
La etapa de adquisición de datos se logro gracias al diseño del electromiógrafo por
bloques independiente, lo que significa que no se compró un amplificador de
instrumentación comercial, sino que se diseñó e implementó, lo cual dio gran libertad de
manipulación de las ganancias de adquisición, seguido de una 2 etapa de amplificación y
2 etapas de filtrado, obteniendo a la salida una señal limpia con pocas perturbaciones.
El sistema de electromiografía fue probado solamente con la autora de este trabajo con
algunos otros voluntarios sin lesión, por lo tanto la parametrización de los potenciales
mioeléctricos de pacientes con lesión del nervio medio se dejó como trabajo a futuro,
cundo el prototipo se pueda utilizar con pacientes.
La selección de las ondas de electroestimulación fue un tanto complicada, mas no el
diseño del circuito y esto debido a que existen múltiples aplicaciones de este tipo de
terapia física, así como tipos de onda, por lo que se tuvo que delimitar muy puntual el
tipo de onda para lograr la rehabilitación del nervio afectado. La amplitud, frecuencia y
voltaje de las ondas calculadas se corroboró con un osciloscopio marca Tektronic. 
El software empleado para el diseño de los mecanismos (SolidWorks 2009) en CAD para el
exoesqueleto es muy amigable y poderoso para animaciones y detección de colisiones, con
el cual se logró diseñar en capas cada elemento del sistema, por lo que el diseño final del
mecanismo puede quedar completamente simétrico y hermético de tal manera que se
pudo realizar simulaciones de movimiento y trayectorias de los mecanismos sin la
necesidad de maquinar cada prototipo diseñado; otra ventaja adicional de emplear
SolidWorks fue que permite al usuario generar planos de ensamble, así como de cada
pieza con sus respectivas mediciones en diversos formatos por lo que facilita el
maquinado empleando máquinas de prototipado o de CNC.
La interfaz con la computadora se logró realizar gracias a 2 factores relevantes, el
primero, la tarjeta de adquisición NIDAQ USB6009 que integra 2 tipos de puertos
distintos, analógico y digital, además de que permite generar una comunicación con el
CPU de forma directa gracias a su tipo de conexión USB y por el segundo factor, que
dicha tarjeta tiene comunicación directa con el software de programación escogido, o sea,
Matlab2010, el cual combina robustez con sencillez debido a su extensa cantidad de
toolboxs las cuales permiten al usuario desarrollar múltiples aplicaciones incluso de
forma visual gracias a su módulo de GUI (Graphic User Interface) con el cual se programó
la interfaz gráfica de usuario que es el centro de comunicación de todo el sistema.
Al final, el acople de todo el sistema mecatrónico se tuvo que realizar por partes debido a
que resultó ser un sistema robusto el cual, se desarrolló en etapas y de forma modular,
razón por la cual el desarrollo del prototipo llevo más tiempo del que originalmente fue
programado.
Como trabajos futuros se propone el rediseño del mecanismo propuesto, empleando
técnicas de optimización y parametrización del dedo objetivo, lo cual, al unirlo a la
interfaz y la terapia de electroestimulación, puede dar mas robustez a la presente línea de
investigación, ya que con el diseño parametrizado y las pruebas correspondientes, se
puede migrar la idea al miembro inferior para realizar terapias electromotrices.
Otra propuesta de trabajo futuro es la del diseño de la fuente de alimentación conmutada,
ya que en el presente trabajo se empleó una fuente de computadora, una fuente
conmutada permite mantener al paciente aislado de corrientes de fuga y retornos indeseados de descargas, pero al desarrollar una fuente de alimentación propia, el
sistema podría crecer y permitir agregar más rutinas de electroestimulación, así como
canales de adquisición, etc.
Otra propuesta sería agregar animación al módulo de movimiento controlado para que el
paciente pueda establecer una conexión cerebro – vista – movimiento y de esta manera no
solo se lograría una terapia electromotriz sino que ésta entraría en el campo de las
terapias psicomotrices complementadas con electroestimulación.
Por último, para que el sistema quede completamente independiente y no se necesite
invertir en hardware costoso o de difícil adquisición (tarjeta de adquisición), se puede
proponer que para la comunicación con la computadora se realice una tarjeta de
adquisición mediante un PIC que realice comunicaciones vía USB, así, el costo total del
sistema se reduciría considerablemente al generar la tecnología en lugar de adquirirla. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO TECNOLÓGICO EN
CÓMPUTO
Diseño y Construcción de un Exoesqueleto para
Rehabilitación de Lesiones de la Rama Tenar Motora del
Nervio Mediano en el Dedo Pulgar
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO
PRESENTA:
DIANA MARISOL BARAJAS VALDÉS
Directores:
Dr. Edgar Alfredo Portilla Flores
Dra. Paola Andrea Niño Suárez
México D. F. Abril 2015 </Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Implementación de un Transmisor DFTS-OFDM sobre una plataforma Universal Software Radio Peripheral.	</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Consiste en implementar un sistema de comunicaciones móviles para emular un
enlace de subida (un dispositivo móvil a una estación base celular) de acuerdo al
estándar LTE-A utilizando funciones y algoritmos en GNU Radio en conjunto con la
plataforma de hardware USRP.
Objetivos Específicos
 Estudio del estándar 3GPP-LTE Advanced, en particular lo referente al enlace de
subida (móvil a base).
 Revisión de los conceptos de transformada rápida de Fourier discreta, tanto directa
como inversa.
 Estudio de la técnica de transmisión utilizando subportadoras ortogonales (OFDM).
 Revisión de los conceptos de técnicas de transmisión utilizando antenas múltiples
(MIMO).
 Estudio del sistema de simulación y emulación GNU Radio Companion (GRC) y su
manejo en ambiente Linux.
 Desarrollo del sistema de transmisión/recepción en GRC y prueba mediante
simulación.
 Emulación del sistema de transmisión/recepción en GRC utilizando las tarjetas
USRP y recopilación de resultados con el analizador de espectros y la computadora
(BER vs SNR).
 Interpretación y análisis de resultados.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Existe bastante documentación para la simulación de sistemas utilizando las
distintas técnicas de multicanalización OFDM, sin embargo, casi no existe trabajo de
emulación utilizando tarjetas USRP empleando banda base y RF (Radio Frecuencia).
Se utiliza la plataforma GNU Radio porque nos permite trabajar con un sistema
completo de RF. MATLAB es una herramienta de simulación que nos permite trabajar
con datos en la computadora y simular sistemas de telecomunicaciones en banda base.
Si utilizamos tarjetas USRP podemos probar distintas técnicas de modulación,
codificación y multicanalización en un sistema real transmitiendo y recibiendo señales
de RF.
Nuestra plataforma experimental consta de dos tipos de tarjetas: tarjeta madre y
tarjeta hija. Las tarjetas madre contienen interfaz USB (Universal Serial Bus),
convertidores DAC (Digital Analog Converter) y ADC (Analog Digital Converter), así
como un FPGA (Field Programmable Gate Array); A cada tarjeta madre se le pueden
conectar opcionalmente tarjetas hijas, las cuales realizan la etapa final de transmisión
(montar la señal en la onda portadora de RF y transmitir) y/o recepción (eliminar la
portadora de RF). Finalmente, realizaremos mediciones de SNR y BER para obtener
una gráfica de rendimiento, esta metodología se muestra en la figura 1.3.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Estudio de la capa física de LTE-A
Simulacion y emulación del sistema en GNU Radio y USRP
Captura y análisis de resultados
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Al analizar la gráfica de la curva BER, podemos observar que tenemos bajo
desempeño a mínimos cambios de relación señal a ruido, esto puede deberse a
diferentes aspectos, entre ellos la falta de una etapa de codificación de canal más
robusta y posiblemente una mejor configuración de las tarjetas USRP.
Se puede desarrollar algunos bloques de procesamiento para mejorar la detección
de símbolo en el receptor, los bloques con los que se trabajaron se configuraron a la
necesidad del sistema emulado, sin embargo, esto abre una oportunidad para el
desarrollo de propios bloques o la creación de una completa biblioteca para la
simulación y emulación de sistemas de telecomunicaciones.
La ventaja de utilizar radio definido por software es que se puede diseñar y
probar diferentes configuraciones en el menor tiempo posible.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación y Desarrollo de Tecnología Digital
Maestría en Ciencias en Sistemas Digitales
“Implementación de un Transmisor DFTS-OFDM sobre
una plataforma Universal Software Radio Peripheral”
TESIS
Que para obtener el grado de
Maestro en ciencias en Sistemas Digitales
Presenta
Ing. José Daniel Domínguez Galaviz
Bajo la dirección de
Dr. Alfonso Ángeles Valencia Dr. Jaime Sánchez García
Tijuana B.C., México Diciembre 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Simulador de tiro con captura laser</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La adquisición de simuladores de realidad virtual, principalmente en el extranjero, requiere
de inversiones elevadas. Un sistema equivalente al desarrollado en este trabajo de tesis puede
alcanzar un costo de 70 millones de pesos, cantidad que se incrementa cuando se consideran los
costos asociados al mantenimiento, actualización del equipo y capacitación, entre otros rubros.
Además, debe tomarse en cuenta que su uso en academias de cuerpos policiales o militares tales
como en el adiestramiento de las fuerzas armadas mexicanas requiere de políticas estrictas de
seguridad, mismas que pueden verse comprometidas si se recurre a proveedores externos.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Realizar un simulador de tiro el cual capture un laser mediante una cámara web y utilice ese
punto de referencia para representar un disparo en un mundo virtual.
Objetivos Particulares
Procesamiento de imagen mediante cámara web.
Captura de laser mediante cámara web.
Segmentación de imagen por medio de umbralización.
Mapeo del espacio cámara web a espacio mundo virtual.
Desarrollo del ambiente virtual con siluetas para práctica de tiro.
Instrumentación de un arma de plástico, para interactuar con el ambiente virtual.
Desarrollo de ecuaciones de balística para determinar trayectoria de las municiones.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Considerando los costos y dependencia tecnológica de estos simuladores que no se desarrollan
en México, se propone realizar un simulador de tiro mediante la integración de diferentes
tecnologías, que permita la independencia tecnológica a un costo accesible, cuyas características
sean equivalentes o superiores a las de los desarrollos disponibles en el mercado, y que permita
entrenar diferentes habilidades tales como la velocidad de reacción y la puntería. Adicionalmente,
el sistema contará con cálculos balísticos, los cuales permitirán una simulación más cercana a la
realidad debido a que las trayectorias de las balas disparadas serán generadas considerando su
forma geométrica y diferentes combinaciones de parámetros del clima.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo de tesis se logró el objetivo de realizar un simulador de tiro mediante la
captura de un punto láser, concluyendo con lo siguiente:
Se logró la lectura de un punto láser mediante la captura de imágenes con una cámara
Optitrack V120, estas lecturas fueron enviadas hacia la computadora segmentándolas por
el método de umbralización y mapeando las coordenadas del punto láser en el mundo real
a un ambiente virtual, lo que permite generar disparos al instante en dicho ambiente.
Se desarrollaron dos ambientes virtuales, uno simulando un ambiente para cacería en
donde aparecían venados aleatoriamente. El segundo ambiente que se desarrolló fue una
casa de tiro, en donde se les dispara a patos inmóviles colocados sobre dos repisas. Estos
ambientes fueron integrados con sonidos, lo que permite una inmersión al usuario que atrae
su atención para que crea estar disparando a blancos reales.
Se instrumentó un arma de plástico a la cual se le integró un láser que emite un haz de luz
que es detectado por la cámara Optitrack e interpretado como un disparo en la aplicación,
mejorando así la interacción del usuario con la aplicación al oprimir el gatillo de dicha
arma.
Por ultimo se integró el modelo matemático que describe la trayectoria de un proyectil,
generando así disparos con trayectorias reales dentro del ambiente virtual. Comprobando
resultados con disparos reales, los cuales fueron analizados en el capitulo 1 en base al
libro Trayectoria y efectos de los proyectiles en armas cortas [13], mostrándose en las
figuras 6.3 y 6.4 dando como resultado final que el modelo genera una trayectoria muy
aproximada a la real. En caso de que el modelo no calculara valores cercanos a la realidad
se debería obtener una nueva solución numérica, revalidar el modelo y modificarlo. Así
también, al integrar las ecuaciones matemáticas a Unity 3D y realizar las simulaciones en
modo edición diferenciaron del software matemático por menos de 4 metros, mientras que
las simulaciones realizadas en modo PLAY, diferenciaron por 10 metros debido a consumo
de recursos en la aplicación.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN CÓMPUTO
SIMULADOR DE TIRO CON CAPTURA LASER
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN TECNOLOGÍA DE CÓMPUTO:
PRESENTA:
Carlos Alarcón Villarejo
DIRECTORES DE TESIS
Dr.Gabriel Sepúlveda Cervantes
M. en C. Juan Carlos González Robles
México, DF Enero 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Teleoperación háptica de un manipulador móvil omnidireccional</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la actualidad se han realizado diversos estudios de estrategias de control en manipuladores móviles, además se han propuesto diversas formas de modelar dinámicamente tal sistema. La mayoría de las investigaciones relacionadas con el control de manipuladores móviles se muestran en simulación considerando un desacoplamiento en la dinámica del manipulador y el móvil. Tomando en cuenta que la Tele operación aptica nos brinda la posibilidad de transmitir fuerzas y pares que acto una en el hacia un usuario final de manera remota, en este trabajo se planteó un sistema de tele operación aptica en un manipulador móvil.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un sistema de tele operación aptica con control de fuerza en un manipulador móvil.

1. Analizar la cinemática y dinámica del manipulador móvil. 
2. Implementar alguna estrategia de control en el manipulador móvil para la tele operación aptica en simulación. 
3. Desarrollar un sistema de tele operación aptica.
4. Realizar pruebas experimentales de la estrategia de control de fuerza. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Para entender las contribuciones de esta tesis conviene recordar el objetivo principal de la misma, el cual, es el desarrollo de un sistema de tele operación aptica de un manipulador paralelo móvil omnidireccional. Las principales contribuciones de esta tesis son:
Para el caso del robot móvil omnidireccional se desarrolló un sistema de tele operación que permite aprovechar todas las capacidades de movimiento de la plataforma, es decir, se puede mover en cualquier dirección, en cualquier instante de tiempo y con cualquier orientación, por lo tanto, se valida el modelo cinemático obtenido para el robot móvil. La tele operación del robot manipulador paralelo con retroalimentación de fuerza, nos permite realizar la manipulación de objetos con una interacción más real con el ambiente remoto al hacer posible graduar la fuerza con la que el manipulador paralelo realiza una tarea sin necesidad de sensores externos, lo cual hace posible al modificar el movimiento que se realiza en el manipulador paralelo para lograr evitar obstáculos en un ambiente remoto. El sistema de tele operación aptica se puede implementar en cualquier manipulador sin necesidad de modificar la estructura mecánica del mismo y sin montar sensores externos. Para este caso bastara con conocer la posición real del sistema robótico a tele operar.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INNOVACIÓN Y DESARROLLO
TECNOLÓGICO EN COMPUTO
TELEOPERACIÓN APTICA DE UN MANIPULADOR MÓVIL OMNIDIRECCIONAL
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN TECNOLOGÍA DE COMPUTO
PRESENTA
DIEGO ARMANDO ALVARADO JUÁREZ
DIRECTORES DE TESIS:
DR. MIGUEL GABRIEL VILLARREAL CERVANTES
DR. GABRIEL SEPÚLVEDA CERVANTES ´
MÉXICO, D.F. 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema de cifrado de voz para equipos de telefonía móvil</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>México sufre de 12 ataques cibernéticos por segundo, de los que 60% son al gobierno para
tratar de extraer información y provienen principalmente de complejas redes piratas de Rusia y
Estados Unidos, reveló la corporación alemana GData[5]. Esta información pone en vulnerabilidad
la seguridad de cualquier gobierno, empresa nacional, trasnacional o de los particulares en el país.
 Ahora bien el espionaje telefónico considerado un ataque a la privacidad es un delito en
México de acuerdo con el artículo 5to. de la Ley de Seguridad Nacional[6] y en nuestro país, como
ocurre en otros; se ha utilizado como herramienta de ataque político al infiltrarse en
conversaciones entre personajes públicos o autoridades, dando así a conocer esa información al
divulgarla.
 A lo largo del siglo XX y XXI las tecnologías de la comunicación han tenido cambios de una
forma exponencial, una de estas tecnologías son las de comunicación móvil. Algunas han
permitiendo agilizar la comunicación, pero estas a su vez han sufrido de innumerables ataques a la
privacidad al realizarse alguna llamada telefónica, con ello muchas personas se han dado a la tarea
de desarrollar diferentes formas de proteger nuestra privacidad ya sea por medio de un
dispositivo físico, una aplicación o programa que garantice una comunicación segura entre el
transmisor y receptor.
 Finalmente el uso de sniffer o analizador de paquetes es una vulnerabilidad a la que los
dispositivos móviles están expuestos; si no se tienen las herramientas adecuadas para localizarlos
es posible que estos intercepten el mensaje enviado entre un receptor y el transmisor tomando
gran cantidad de información que viaja sin algún tipo de cifrado. Estos espías se pueden encontrar
en la red y cualquier usuario puede hacer uso de ellos. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo> Objetivo General
Diseñar un sistema de comunicación segura en dispositivos de telefonía móvil cifrando la voz.
Objetivos Particulares
· Analizar la vulnerabilidad de los dispositivos móviles en el uso de llamadas telefónicas.
· Diseño de sistema de cifrado y descifrado de voz en base a los protocolos de seguridad.
· Diseño de sistema de cifrado en software o framework según la pertinencia del proyecto.
· El algoritmo de cifrado será de fuente disponible.
· Implementación del sistema en un dispositivo de comunicación móvil con un sistema
operativo Android utilizando su máquina virtual. </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>A continuación se describe la metodología seguida para desarrollar este proyecto y el
cumplimiento del objetivo general y particulares descritos anteriormente.
 Se planteó la implementación del sistema operativo Android como plataforma de
desarrollo, el cual está montado sobre una máquina virtual Java (Dalvik), por ello se requirió el
conocimiento de la programación orientada a objetos con el uso del lenguaje de programación
Java, también es requerido el lenguaje de programación xml para el desarrollo de las vistas o
actividades en Android. El IDE (Integred Development Enviroment) utilizado para la programación
es Eclipse versión Indigo o superior.
 En cuanto al tratamiento digital de la voz, se requirió el conocimiento del estándar de
audio 3gpp, establecido por The production of Technical Specifications for a 3rd Generation Mobile
System (3GPP)basado en redes de comunicación GSM. Los fundamentos principales tomados en
cuenta fueron, la creación y reproducción de archivos de audio con este formato, además de su
contenido a nivel bit para la manipulación de la información contenida en dichos archivos de
audio.
 Respecto a las comunicaciones móviles, GSM es el estándar más utilizado del mundo, por
ello y para realizar la innovación respecto a las aplicaciones existentes, se planteó el uso de esta
red; donde se requirió el conocimiento de los protocolos de comunicación sobre GSM, además del
manejo del estándar y la viabilidad de su implementación sobre el sistema operativo Android,
además de los permisos necesarios para la manipulación de los protocolos de la red.
 Por otra parte, en el uso de los algoritmos de cifrado, se optó por el manejo de Triple-DES
96, ya que ofrece una alta protección de información clasificada, además de ser un algoritmo de
poco uso ya que es poco conocido y su implementación sobre audio es escaza. Para su
implementación en el proyecto, se requirió el conocimiento de los algoritmos antecesores DES y
Triple-DES, siendo estos la base de Triple-DES 96, además del conocimiento de permutaciones
variables con el teorema JV. El algoritmo implementado fue programado sobre el lenguaje Java y
se necesitó el uso de manejo de ficheros.
 Finalmente, la aplicación hace uso de un candado físico montado sobre un
microcontrolador, para ello se requirió del conocimiento de la programación de los
microcontroladores particularmente el manejo información a través del puerto USB, tanto para el
dispositivo de telefonía móvil con Android como en el microcontrolador, ya que se requiere una
sincronización entre ambos dispositivos para el compartimiento de las llaves de cifrado utilizados
por la aplicación. </Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El sistema operativo Android ofrece una máquina virtual con la información suficiente para
darle al programador las herramientas necesarias para crear o desarrollar casi cualquier
aplicación, ya que este sistema operativo se encuentra montado en dispositivos cada vez con
mejores recursos de procesamiento, ya que los procesadores utilizados por los dispositivos
móviles continúan elevándose.
 El objetivo general de este trabajo tuvo el inconveniente de la aplicación sobre la red de
telefonía GSM, ya que por razones de hardware principalmente, no es posible su manipulación con
el sistema operativo Android, aunque existen clases que ofrecen cierta información para la
comunicación con el módulo GSM que es interpretado con los comandos AT; sin embargo, fue
posible el desarrollo de la aplicación con la pertinencia de cifrar y descifrar grabaciones de voz,
utilizando el protocolo Triple-DES 96.
 En cuanto a la vulnerabilidad de la red, se concluyó que existen dispositivos capaces tanto
de interceptar llamadas y mensajes, como el bloqueo total de las señales en aéreas específicas,
estos dispositivos son de uso exclusivo por parte de las autoridades pero son posibles de conseguir
en el mercado negro, a un costo elevado, sin el alcance para realizar pruebas completas de
vulnerabilidad.
 Con respecto al sistema de cifrado y descifrado de voz con base a los protocolos de
seguridad, se realizó la aplicación con la cual es posible integrar el algoritmo Tiple-DES 96 en el
sistema operativo Android montado en la máquina virtual Dalvik en grabaciones creadas a partir
de un dispositivo de telefonía móvil, con un funcionamiento adecuado.
 Android al ser un sistema operativo instalado en llámese una computadora de bolsillo, que
cuenta con puertos de entrada y salida, es posible agregar otros dispositivos que interactúen con
él, ya sea el caso de dispositivos de almacenamiento o algún otro que ofrezca una respuesta por
medio de una entrada enviada a través de un puerto USB, ya que el dispositivo móvil puede
convertirse en un anfitrión o host capaz de controlar otros dispositivos, con las configuraciones
adecuadas en ambos dispositivos, otorgando al usuario una herramienta de gran alcance para
múltiples aplicaciones.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
Centro de Innovación y Desarrollo Tecnológico en Cómputo
SISTEMA DE CIFRADO DE VOZ PARA EQUIPOS DE
TELEFONÍA MÓVIL.
TESIS QUE PARA OBTENER EL GRADO DE MAESTRÍA EN
TECNOLOGÍA DE CÓMPUTO
PRESENTA
Ing. Gabriel Eduardo García Rojas
DIRECTORES
M. en C. Eduardo Rodríguez Escobar
M. en C. Marlon David González Ramírez.
 Diciembre 2015 </Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>CONTROL INTELIGENTE APLICADO A ROBOTICA COLABORATIVA
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la actualidad, los robots móviles del tipo diferencial tales como el Pioneer P3-DX,
han ganado una considerable atención de investigadores ya que son de gran utilidad para
implementar y evaluar algoritmos de control [27].
Sin embargo, en ocasiones un solo robot móvil tiene ciertas limitaciones que le impiden
realizar una determinada tarea, estas limitaciones pueden ser: su capacidad de carga,
la cantidad de sensores que puede utilizar y la capacidad individual de cómputo para
procesar información. Para sobrellevar estas limitaciones se puede sugerir un robot más
complejo que cubra todas (o algunas) de ellas o hacer grupos de robots más sencillos con
respectivas limitaciones individuales que trabajen de forma colaborativa para completar
tareas complejas que, a un solo robot le es imposible realizar.
Dicho lo anterior, el presente trabajo pretende aportar una solución basada en lógica
difusa al problema de seguimiento de trayectoria de un robot móvil diferencial con el fin
de ser utilizado en un sistema de múltiples robots móviles autónomos con la capacidad
de mantener y reconfigurar una formación durante la ejecución de tareas.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El presente trabajo aborda el problema de seguimiento de trayectoria de múltiples
robots móviles simultáneamente de forma segura y eficiente sin asistencia de operadores
humanos en la navegación. Con el fin de lograr lo anterior, se presentan los siguientes
objetivos de investigación.
Objetivos específicos
Los objetivos específicos que persigue el presente trabajo son:
Analizar los modelos matemáticos que describen el comportamiento cinemático de
un robot diferencial no-holonómico.
Diseñar un controlador difuso que proporcione al robot la capacidad de seguir
asintóticamente trayectorias predefinidas en el plano XY.
Diseñar un mecanismo de formación que permita a dos o más robots seguir una
trayectoria predefinida simultáneamente.
Demostrar el seguimiento de trayectoria individual y simultanea de los robots mediante
una simulación en el ambiente de Matlab.
Proponer un prototipo experimental para una futura implementación real de un
sistema multa-robot. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>El control de seguimiento de trayectoria de un robot móvil diseñado mediante la síntesis
difusa de Lyapunov, asegura la formación y navegación factible de un sistema de múltiples
robots en tareas del tipo ir-a-meta utilizando solamente la posición y orientación de uno
de ellos como referencia. 
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Gracias a la colaboración de sistemas de control y sistemas inteligentes, los sistemas
de múltiples robots prometen ser de gran utilidad en un futuro cercano para asistir al
operador humano en tareas peligrosas, de precisión o de alto costo económico.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo de tesis se presentó una alternativa para realizar el seguimiento de
trayectoria de múltiples robots móviles autónomos. Mediante el enfoque propuesto por
Margaliot et. al para diseñar sistemas de inferencia difusa en base a una función de
Lyapunov, se diseñó un controlador difuso del tipo P D para solucionar el problema de
seguimiento de trayectoria de un robot móvil. Posteriormente, se presentó una alternativa
para implementar la navegación de múltiples robots simultáneamente bajo el esquema
líder-seguidor y se realiza una simulación que arroja resultados satisfactorios e indispensables
para continuar con una implementación física en el futuro de la investigación. Al
respecto, el presente trabajo de tesis aporto un sistema efectivo para la implementación
del controlador propuesto y futuras investigaciones.
Los experimentos de seguimiento de trayectoria, formación y navegación presentados
en este trabajo arrojan resultados satisfactorios en relación a las características cinemáticas
de operación del robot. Sin embargo, se apreció que el procedimiento para obtener
la posición de cada robot individual relativo al marco de referencia (X, Y) resulta ser
poco conveniente para aquellas aplicaciones que requieran alto grado de precisión al
realizar una tarea. Esto se debe al efecto acumulativo del error de posición conforme se
avanza en una trayectoria, la baja resolución de los codificadores, la fricción presente
en los actuadores del robot y la falta de retroalimentación del sistema. Esta afirmación
es basada en la comparación de resultados obtenidos en experimentos simulados contra
experimentos reales realizados durante el desarrollo de este trabajo.
Algunas de las limitaciones de la plataforma experimental usada en los experimentos
también afectaron al desempeño de los robots. Una de ´estas fue la tasa de muestreo de los
datos de posición y la resolución numérica de las operaciones ya que la taza de muestreo
del microcontrolador utilizado en los robots se encuentra por debajo de la misma en
la simulación por computadora y en consecuencia la aportación de cada operación al
acumulamiento del error es considerable, sobre todo en trayectorias de varios metros.
Para disminuir el error de posición se propuso mantener una velocidad lineal constante
en el robot líder. Esto debido a que el cambio de velocidades tanto lineal como angular en
conjunto con las características físicas del robot tales como el backslash en los motores,
el ruido eléctrico, entre otras, ocasionan mediciones ruidosas en los codificadores y
consecuentemente, en la posición.
Ya que la posición y la orientación del robot son sujetas al error y ruido de medición,
el resultado del cálculo y control de la orientación del robot fue comprometido. De tal
manera que, si la medición de posición y orientación se realizara con hardware más
sofisticado, el error generado por la implementación de los controladores disminuiría.
Por tanto, es evidente que el estudio y desarrollo de nuevas técnicas para compensar los
efectos dinámicos tal como la fricción y otros efectos asociados al robot es indispensable
para esta ´área de investigación.
Finalmente, y a manera de conclusión, se ha demostrado que a pesar de las no linealidades
del modelo, es factible utilizar sistemas de inferencia difusa basados en funciones de
Lyapunov para sistemas de navegación que requieren la colaboración de múltiples robots
móviles utilizando solo la de posición de un robot como referencia. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL  
CENTRO DE INVESTIGACIÓN Y DESARROLLO  
DE TECNOLOGÍA DIGITAL
MAESTRÍA EN CIENCIAS EN SISTEMAS DIGITALES
“CONTROL INTELIGENTE APLICADO A ROBÓTICA  
COLABORATIVA”
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS EN SISTEMAS DIGITALES
PRESENTA
ING. CARLOS ALBERTO VILLAR LEÓN
BAJO LA DIRECCIÓN DE  
DR. OSCAR HUMBERTO MONTIEL ROSS
DR. ROBERTO SEPULVEDA CRUZ  
MARZO 2016 TIJUANA, B.C., MÉXICO
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>ESTRATEGIAS DE LOGÍSTICA ORIENTADAS A LA EXPORTACIÓN
PARA PYMES COMERCIALIZADORAS, COMO FACTOR DE
COMPETITIVIDAD GLOBAL</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>A través de la presente investigación se determinaron las estrategias de logística orientadas a
la exportación, las cuales les permitirán a las pequeñas y medianas empresas mexicanas
dedicadas a la comercialización de bienes hacerle frente a la competitividad global y
permanecer en el mercado internacional. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo> Estrategias de logística orientadas a la exportación.
Determinar las estrategias de logística orientadas a la exportación que requieren las PYMES
comercializadoras para enfrentar la competitividad a nivel global
Específicos
 Explicar el concepto y las características de las estrategias de logística orientadas a la
exportación de bienes
 Definir el concepto y las características de la competitividad global
 Presentar el concepto y las características de una PYME mexicana comercializadora de
exportación
 Determinar las estrategias de logística que requieren las PYMES comercializadoras de
bienes de exportación para ser competitivas a nivel global.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cuáles son las estrategias de logística orientadas a la exportación que requieren las PYMES
comercializadoras para enfrentar la competitividad global?
</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿Qué es una estrategia de logística orientada a la exportación de bienes?
</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3> ¿Qué es la competitividad global y cómo se ha desarrollado?
</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4> ¿Cuál es el contexto de las PYMES dedicadas a la comercialización de bienes de
exportación?
</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Qué estrategias de logística orientadas a la exportación requieren las PYMES
comercializadoras para ser competitivas a nivel global? </Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El crecimiento del comercio mundial cayó al 2,0% en 2012 —frente al 5,2% de 2011— y está
previsto que siga registrando un nivel moderado de alrededor del 3,3% en 2013 habida cuenta
de que la desaceleración económica de Europa sigue frenando el aumento de la demanda
mundial de importaciones, según informaron los economistas de la OMC. En este estudio
México ocupa el lugar No. 11 entre los principales países exportadores de mercancías del
mundo (excluido el comercio intra-UE(27)), siendo que en el 2012 ocupó el lugar 14
(Organización Mundial de Comercio, 2013).
En la Tabla 1 se muestra como México ha disminuido sus exportaciones en los últimos tres
años, considerando que ha sido en la misma proporción promedio a las importaciones, por lo
que el gobierno federal ha impulsado a través de diferentes programas la exportación de
mercancías.
Las pequeñas y medianas empresas en México comenzaron siendo empresas familiares, por
lo que su estructura administrativa se fue adaptando conforme a las necesidades del mercado,
experimentando y arriesgando todo tipo de recursos para cumplir con la satisfacción del
cliente y permanecer en los sectores productores.
En el entorno de la Globalización requiere que las PYMES formen parte de las actividades de
comercio exterior, ya que representan más del 90% de las unidades económicas de México,
que generan el 35% del PIB, así como 7 de cada 10 empleos, sin embargo, de acuerdo a
estudios de la Red Global de Exportación el 64% de las PYMES mexicanas no tienen un
departamento dedicado a operaciones de comercio exterior, y llevan a cabo las operaciones
basados en hábitos tradicionales por lo que no tienen un procedimiento formal en las
estrategias a utilizar en las operaciones de exportación (Red Global de Exportación, 2008).
En gran medida el dueño de la empresa es quien realiza las gestiones y el trato comercial de
las operaciones de comercio exterior, y va aprendiendo sobre la práctica, lo cual no planea ni
diseña las estrategias de logística para hacerle frente a la competencia en el mercado global.
Si bien es cierto que la experiencia hace al maestro, en ésta área cada procedimiento está
regulado por el gobierno y se involucran diferentes factores específicos por lo que es necesario
conocer y aplicar aquellos instrumentos y conocimientos de comercio exterior que permiten
optimizar las operaciones de exportación.
El gobierno de México, a través de diferentes organismos ha publicado instrumentos de
financiamiento y administración para motivar a las PYMES a ser parte del comercio exterior
mexicano, pensando que este tema es el principal motivo de que no suceda. Sin embargo,
revistas especializadas de comercio exterior como los son Estrategia Aduanera, Énfasis
Logística, publicaciones en revistas de difusión y autores como Nicola Minervini y Joaquin
Rodriguez Valencia (Minervini, 2004) han señalado que las PYMES mexicanas no mantienen
un plan de acción en las operaciones con el exterior, trayendo como consecuencia el no ser
una empresa estable y fuerte para la competitividad global, específicamente el área logística,
la cual es un factor decisivo para la competitividad de las empresas, ya que organiza los flujos
entre los mercados y acerca a productores y clientes. 
La importancia estratégica de la gestión logística debe ser un hecho si se quiere alcanzar la
excelencia empresarial. Ser “el mejor de la clase” en el aspecto logístico conlleva una ventaja
competitiva respecto al resto de las compañías, no solo por la eficiencia sobre la gestión sino
además por el valor añadido generado en el producto final. (Dirección General de Política de
la Pequeña y Mediana Empresa, 2007)
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La apertura comercial de las economías ha permitido tener una visión de competencia
constante en la labor diaria de las empresas en México, esto implica tener mejoras continuas
en los procesos operativos, administrativos y de toma de decisiones gerenciales sin importar
el tipo de producto que se comercialice.
La logística en las operaciones de exportación implica una planeación de recursos y
coordinación entre los sujetos involucrados que permita cumplir con el objetivo de llevar a
cabo una entrega en tiempo y forma, exportar no sólo es vender.
México es evaluado y comparado cada dos años con el desempeño logístico de todos los
países del mundo que participan en el intercambio comercial, esto refiere al trabajo de las
empresas que lideran las operaciones de comercio exterior. A través de los resultados en las
áreas de estudio del LPI se muestra que las PYMES comercializadoras de exportación en
México se conforman solo con vender su producto, ya que no se informan o capacitan en áreas
de oportunidad que le permitan culminar el proceso de venta a su cliente en el exterior.
Las regulaciones y restricciones no arancelarias en el comercio internacional es un tema que
constantemente sale a relucir como principal barrera para que los exportadores lleven a cabo
sus operaciones en México, en la presente investigación se muestra que a pesar de los avances
en tecnologías de la información, recursos que otorga el gobierno y diferentes organismos
independientes, sigan siendo uno de los motivos por los cuales las empresas no son
competitivas a nivel global. Precisamente esto funciona como un área de oportunidad para
otras PYMES, e incluso temas de especialización para las personas que están por involucrarse
en el área.
Las estrategias de logística son adaptables a cada tipo de empresa no importando el producto
y país destino, pero de acuerdo a la investigación presentada es necesario contar con personal
capacitado en el área de negociación con los operadores logísticos para establecer tarifas
justas y tener pleno dominio de los procesos en Aduanas y cumplimiento de regulaciones del
producto no solo a la exportación si no como se cumplen en el país destino. Uno de los factores
más importantes es elegir estratégicamente al operador logístico con quien se va a trabajar,
ya que algunos son estrictamente necesarios en los procesos aduanales, tal es el caso del
Agente Aduanal; y otros brindan un servicio integral puerta a puerta que aunque eximen a las
empresas de solucionar algunos problemas, elevan las tarifas sustancialmente. Esta situación es otra área de oportunidad para que las PYMES mexicanas se involucren, capaciten y
aprendan a coordinar sus operaciones de exportación.
Para concluir, la Infraestructura logística del país sigue siendo una barrera en las operaciones
de exportación, sin embargo, los resultados demuestran que las PYMES han logrado generar
estrategias de manera empírica y no documental, restando así la posibilidad de considerar
nuevas rutas de comercio exterior.
El éxito o fracaso de las operaciones de exportación de las PYMES impacta de manera directa
a nivel nacional, principalmente en la proyección que se pretende tener como país para
establecer negociaciones con el resto del mundo, por lo tanto, se debe seguir trabajando con
la capacitación de las pequeñas y medianas empresas para que logren elevar la competitividad
logística de la nación. La preparación y actualización constante es la principal estrategia para
lograr sobresalir en el ámbito comercial. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE COMERCIO Y ADMINISTRACIÓN
UNIDAD TEPEPAN
SECCIÓN DE ESTUDIOS DE POSGRADO E INVESTIGACIÓN
“ESTRATEGIAS DE LOGÍSTICA ORIENTADAS A LA EXPORTACIÓN
PARA PYMES COMERCIALIZADORAS, COMO FACTOR DE
COMPETITIVIDAD GLOBAL”
T E S I S
QUE PARA OBTENER EL GRADO DE MAESTRIA EN CIENCIAS EN ADMINISTRACIÓN DE
NEGOCIOS
PRESENTA:
ARELI CAMACHO HERNÁNDEZ
DIRECTORES:
DR. FABIÁN MARTINEZ VILLEGAS
M.EN C. HÉCTOR MANUEL LEAL PÉREZ
MÉXICO, DF MARZO, 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>LA REINGENIERÍA DE PROCESOS
COMO HERRAMIENTA
ORGANIZACIONAL PARA LA
FACTURACIÓN ELECTRÓNICA EN
MÉXICO. CASO: H., S.A. DE C.V</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La administración actual de H., S.A. de C.V. está encabezada por una persona
dedicada al área administrativa y otra a la función productiva. La parte
administrativa se compone por los departamentos de Contabilidad, Finanzas y
Recursos Humanos. El lado productivo se integra con los departamentos de
Ingeniería, Proyectos, Residentes de obra y con las áreas de Producción de
madera por un lado, y de metal por otro.
Después de analizar el organigrama, es claro notar que no existen áreas
definidas para las compras y ventas que ponga especial atención a la
normatividad hacendaria vigente hasta el 31 de marzo de 2014 en cuanto a la
obligación de emitir y recibir facturas electrónicas ó comprobantes fiscales
digitales por internet (CFDI´s), pues quienes realizan estas actividades
dependen del departamento de Proyectos.
El impacto en lo referente a las compras por recibir comprobantes de los
proveedores, que generalmente son personas físicas y que facturan con
facilidades administrativas (modalidades del CBB y CFD vigentes al 31 de
marzo de 2014), es debido a que no hay entrega de documentos a tiempo, se
manejan documentos ya vencidos o no se han incorporado a la modalidad
electrónica en su momento a pesar de que sean sujetos obligados a ello. Por
otra parte, en cuanto a la facturación de ventas, el responsable se enfrenta a
dificultades porque no cubre, en ocasiones, los requisitos administrativo y en
acuerdo con los clientes, los cuales son adicionales a los que señala el SAT.
En la ilustración 2 se puede observar, en síntesis, los problemas que enfrenta
H., S.A. de C.V. por cumplir la normatividad fiscal con inconsistencias en sus
sistemas y procesos:
Las consecuencias en la empresa impactan principalmente al departamento
contable en que:
 Los documentos no cancelados oportunamente se registran como
documentos reales.
 Se enteran impuestos de manera inadecuada por documentos
duplicados.
 Los documentos que los proveedores entregan y que son incorrectos
retrasan el registro de las operaciones.
 Se entregan reportes fuera de tiempo y con cifras incorrectas.
Desde finales del año 2010, H., S.A. de C.V. se cambió al esquema de
comprobantes fiscales digitales (optando por la facilidad de realizar facturas por
medios propios) respondiendo a la obligación impuesta en el paquete de
reformas al Código Fiscal de la Federación publicado el 21 de diciembre de
2009 en el que se incluye el deber inaplazable para empresas que superaran
los $4´000,000. Los esquemas aprobados en ese entonces fueron: (Ver
ilustración 3)
una capacitación del personal muy básica, además de una actualización del
programa de cómputo contable para la vinculación con el adquirido y una
impresora para color. Con las modificaciones mencionadas se cumplió con la
obligación, pero generó problemas dentro de sus procesos de operación. El
hecho de que a partir del 1 de abril de 2014 todas las empresas están obligadas
a la facturación digital por internet, orilló a la empresa estudiada a modificar
nuevamente su esquema previo de CFD al CFDI, generándose principalmente
el desperdicio de recursos y tiempo por no contar para este cambio con una
adecuada preparación.
Lo anterior es lo que motiva el desarrollo de este trabajo utilizando la
reingeniería de procesos como herramienta organizacional para la identificación
de las deficiencias de la compañía y el rediseño los procesos de compras y
facturación de ventas para su manejo con mayor eficiencia y calidad y dar
cumplimiento a los requisitos impuestos por el SAT en fechas recientes.

</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Proponer el rediseño en los procesos administrativos de compras y facturación
de ventas de H, S.A. de C.V. para aprovechar la disposición hacendaria de
facturar electrónicamente con mayor eficiencia y calidad.
Específicos
1. Explicar en qué consiste la reingeniería de procesos dirigida al
mejoramiento de su eficiencia y su calidad.
2. Describir la disposición oficial en México de la obligación de la
facturación electrónica y su funcionamiento.
3. Identificar las características de los procesos administrativos actuales de
H, S.A. de C.V. en las áreas de compras y ventas vinculados con la facturación
electrónica.
4. Determinar las ventajas y desventajas económicas, organizacionales y
ambientales de la implementación de la facturación electrónica en las áreas de
compras y ventas en H, S.A. de C.V.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_1</td>
				<td width='1000'>
					<Pregunta_1>¿Cuál es la propuesta de rediseño en los procesos administrativos de compras
y facturación de ventas de H, S.A. de C.V. para aprovechar la disposición
hacendaria de facturar electrónicamente con mayor eficiencia y calidad?
</Pregunta_1>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_2</td>
				<td width='1000'>
					<Pregunta_2>¿En qué consiste la reingeniería de procesos dirigida al mejoramiento de
su eficiencia y su calidad?</Pregunta_2>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_3</td>
				<td width='1000'>
					<Pregunta_3>¿Cuáles son las características de los procesos administrativos actuales
de H, S.A. de C.V. en las áreas de compras y ventas vinculados con la
facturación electrónica?</Pregunta_3>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_4</td>
				<td width='1000'>
					<Pregunta_4>¿En qué consiste la facturación electrónica y cómo se incorporó su
obligatoriedad en México?</Pregunta_4>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Pregunta_5</td>
				<td width='1000'>
					<Pregunta_5>¿Cuáles son las ventajas y desventajas económicas, organizacionales y
ambientales de la implementación de la facturación electrónica en las áreas de
compras y ventas en H, S.A. de C.V.?
</Pregunta_5>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Desde el año 2004 en México se comenzó a hablar de la facturación
electrónica, sin embargo la planeación de las autoridades señalaba que sería
hasta 2010 cuando esta obligación abarcaría a más contribuyentes. A finales de
2009 se señala que la incorporación de la facturación electrónica comenzaría a
ser obligatoria a partir de 2010, lo que llevó a las empresas a modificar sus
operaciones para cumplir los nuevos requerimientos fiscales, la mayoría con
apuro y sin una planeación adecuada. A pesar de que las autoridades
difundieron información para que los contribuyentes se prepararan, prevaleció el
desconocimiento de los procedimientos a seguir, por lo que, en esta primera
etapa, se generaron más costos en lugar de ahorrar y los reportes de
información fueran erróneos, contrario a lo que se esperaba.
Ante el bajo número de contribuyentes obligados a facturar electrónicamente, el
Instituto Nacional de Estadística y Geografía (INEGI) señaló que en el año de
2009 había alrededor de 3.7 millones de unidades económicas, y que sólo el
20% estaban registradas en el Sistema de Información Empresarial (SIEM)
como usuarios de esta modalidad, dato que en ese momento no era indicador
de problemática alguna, pero que derivada de la disposición que operaría a
partir de 2010 donde se obligaría a más contribuyentes a participar en este
esquema, la cifra revelaría la contrariedad que estaría por surgir.
Un análisis realizado por el Centro de Estudios Económicos de El Colegio
México en el 2010, indicó que la evasión fiscal por el empleo de comprobantes
apócrifos, en el periodo de 2007 a 2009, representó alrededor de 16 mil
millones de pesos entre el Impuesto sobre la Renta (ISR) y el Impuesto al Valor
Agregado (IVA). Combatiendo esta problemática, las autoridades decidieron
implementar la facturación electrónica como una obligación a partir del 2010,
además de cumplir el plan que en un inicio fue fijado.
Por lo anterior, muchos contribuyentes, como H., S.A. de C.V. invirtieron en
programas de cómputo para apegarse a la reglamentación desde 2010.
Con la obligatoriedad para 2011 publicada en la Tercera Resolución de
Modificaciones a la Resolución Miscelánea Fiscal del 28 de diciembre (2010),
todas las adversidades ya mencionadas se hacen latentes, ya que más
empresas deben adoptar la modalidad de hacer facturas en medios electrónicos
y sigue habiendo huecos de información y falta de preparación para enfrentar el
cambio.
En 2011, el escritor Eduardo Barrera ("Factura electrónica, entre transparencia
y confusión", 2011), en entrevista con la C.P. Gabriela Rodríguez, consultora
fiscal independiente, publicó que las personas que debieron adoptar la
modalidad de facturar digitalmente no estaban preparadas, pues existía
confusión en cómo se debía cumplir con la obligación fiscal, que no era
adecuado el momento en que se recibirían las facturas por correo electrónico, y
que una vez que ya lo tuvieran, la revisión del archivo XML (Extensible Markup
Languaje) sería otra tarea en la que los contribuyentes no estarían adiestrados.
En enero del 2011 René Fragoso, Director General de ContPAQ i (empresa
dedicada a la venta de software contable y administrativo), dijo que las
empresas estaban interesadas en saber cómo cumplir las obligaciones que la
Secretaría de Hacienda y Crédito Público (SHCP) impuso, pues no tenían el
conocimiento de los requisitos y las especificaciones que sus computadoras
deben poseer ("Sólo 4% de contribuyentes factura electrónicamente", 2011).
Además, que sólo 221 mil 815 contribuyentes a enero de 2011 realizaban
facturas electrónicamente, según datos del SAT y que sólo representaban el 4%
de los contribuyentes que deberían hacerlo, pero que ya a finales del año de
2011 esa cifra debía cambiar con la inquietud de los contribuyentes de
actualizarse y estar acorde a las necesidades.
El consultor de impuestos Augusto Lagner en entrevista con el periodista José
Arteaga ("Factura digital traerá problemas a contribuyentes: AMECE", 2011),
afirmó de la facturación electrónica lo siguiente: “no es un esquema sencillo;
hay miles de contribuyentes que la usan. Tiene su grado de complejidad porque no solo es la facturación, ya que se tiene que hacer el registro contable de la
venta”.
El C.P.C. Lauro Arias, integrante de la Comisión Representativa del Instituto
Mexicano de Contadores Públicos ante las Autoridades de Fiscalización del
Servicio de Administración Tributaria, en el año 2011 mencionó que existen
problemas al adoptar el sistema de facturas electrónicas por internet, pues hay
dificultades en los procesos de regeneración de facturas ya emitidas por
descontrol en la cancelación, por la corrupción de las bases de datos en cuanto
a la transmisión que ocasionan la pérdida de comprobantes, la inconsistencia
de la información del reporte mensual de uso de folios, la desviación de la
estructura del archivo XML que generan los programas informáticos y mantener
un control adecuado de folios y entrega de facturas al SAT para su sello digital
(Arias, 2012).
El Instituto Mexicano de Contadores Públicos (IMCP) en el artículo llamado
Alertan problemas por e-factura se menciona que los problemas más
significativos se darían en el momento de la emisión de la factura electrónica
por desconocimiento de los requisitos que afectarían la deducibilidad por no
haber un formato estándar que haga fácil dicha revisión, y a la tardanza que
haya en la adaptación a la nueva modalidad que traería como consecuencias
multas constantes y costosas (Soto & Díaz, 2012).
René Torres Director de ContPAQ i, aseguró que para finales del año 2012 el
50% de empresas obligadas a facturar electrónicamente utilizando CFDI (las
cuales tuvieran ingresos superiores a $4´000,000), aún estaban fuera de
cumplimiento del objetivo primario, al utilizar cualquiera de las dos facilidades.
Del 50% de empresas rezagadas, el 39% fueron microempresas, 5% pequeñas
empresas, 4% empresas medianas y 5% de empresas grandes. Por las cifras
anteriores Gerardo López, integrante de la Comisión de Apoyo al ejercicio
independiente del Colegio de Contadores Públicos de México (CCPM) señaló
que las pérdidas para las empresas se derivarían principalmente por la decisión
de los clientes de cambiar de proveedor al no emitir CFDI´s presumiendo que se encuentran fuera de reglamentación. ("Sin migrar a la e-factura, 50% de las
pymes", 2013),
La consejera de la Administración Pública y Hacienda, Concepción Arruga,
afirmó en el año 2013: “el sistema de facturación electrónica permitirá agilizar
los pagos a proveedores y reforzar la transparencia” (Logroño, 2013), por lo que
se resalta que no sólo se agiliza y se destacan las mejoras en los sistemas de
facturación por ventas, sino también en las funciones de compras de insumos
para la producción.
Ante toda la serie de modificaciones que la facturación electrónica implica, la
administración de H, S.A. de C.V., manifestó más desventajas al adquirir un
software únicamente con la capacitación básica del personal, omitiendo
adiestramiento adicional requerido para realizar las demás tareas, dedicando
así más tiempo que cuando se hacían los documentos en papel.
Se han estudiado modificaciones administrativas de empresas encaminadas a
mejoras continuas, se han hecho estudios con base en la reingeniería de
procesos en búsqueda de la calidad total, pero ambas temáticas no se ha
abordado desde el punto de vista de la facturación electrónica tanto en compras
como en la facturación de ventas, por esa razón esta investigación se enfoca a
brindar una propuesta que ayude a H, S.A. de C.V. y a empresas similares, a
mejorar sus procesos administrativos y aprovechar los beneficios que las
autoridades pretendieron, que son ahorros en tiempo, costos y la reducción de
los impactos ambientales; a su vez, aprovechar la inversión inicial que la
compañía hizo para el mejoramiento de la productividad empresarial y
principalmente, quedar bajo la regulación para el año 2014.
Por tanto, al realizar esta investigación se brindará una orientación a los
usuarios para realizar sus operaciones de forma adecuada y acorde a los
lineamientos fiscales vigentes; de no llevarse a cabo seguirán existiendo
inconsistencias en las empresas, tales como sanciones fiscales legales,
problemas financieros y la pérdida de cartera de clientes que exigen una
administración estructurada y actualizada, principalmente. Los errores que se
han generado y la falta de información generan un área de oportunidad para realizar el estudio a fondo y resaltar la importancia de la facturación electrónica
dentro de una organización. Para H., S.A. de C.V. será un aporte considerable
pues la propuesta hará que cada departamento funcione organizada y
sistemáticamente con los demás, mejorará sin duda la eficiencia en sus
procesos y alcanzará la calidad, además en un tiempo considerable, se verá
reflejado el beneficio en sus números ante los ahorros de actividades
innecesarias. No realizar este trabajo llevaría a la empresa al desgaste excesivo
de sus recursos, al pago de multas por la presentación de información a las
autoridades fuera de tiempo o de manera incorrecta, crearía caos en el personal
de la compañía, y seguiría frenando sus oportunidades de crecimiento.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Primero se realizó una investigación documental en libros, revistas, leyes y
bases de datos de la información relativa a la reingeniería de procesos y la
facturación electrónica, así como de eficiencia y calidad para comprobar que el
tema no ha sido abordado con el mismo enfoque que se pretende.
Luego se seleccionó la información más representativa para la construcción del
marco teórico y de referencia de la investigación.
A su vez, se realizó una entrevista preliminar con la empresa para el
conocimiento general de la administración, con principal enfoque en las
funciones de compras y de la facturación de ventas.
Entonces con base en la información seleccionada, se diseñaron los
instrumentos necesarios para desarrollar la investigación. Las técnicas fueron
entrevistas por medio de cuestionarios semi-estructurados para las áreas de
compras y ventas (dirigidos a identificar los procesos de la facturación), los
cuales fueron relacionados con los resultados de las guías de observación para
determinar su validez y confiabilidad. Lo que se buscó con lo anterior fue
identificar plenamente los procesos actuales de las áreas mencionadas para
detectar sus deficiencias y a partir de ahí plantear la propuesta de reestructura.
El cuestionario para el área de facturación de ventas (ver anexo 1) y para el
área de compras (ver anexo 2), se aplicaron a las personas que ejecutan
directamente las actividades; la credibilidad se logró mediante el cotejo del
mismo con las observaciones llevadas a cabo para equiparar actos; la
transferencia y consistencia se obtuvieron mediante el piloteo, pues ambas
(compras y facturación de ventas) son actividades básicas en cualquier
organización, y otras personas que se dediquen a dichas actividades en
cualquier empresa tienen el panorama de lo que conlleva cada acción en el
respectivo departamento; la dependencia fue arrojada al entender el enfoque
sistémico de la organización, y que al tratarse de procesos se requiere de
elementos de entrada, que son requisitos de otros departamentos, y elementos
de salida, los cuales son informes para otras áreas de la empresa. Las guías de observación para las áreas de facturación de ventas (ver anexo 3) y para el
área de compras (ver anexo 4) estuvieron enfocadas a determinar si la
información obtenida mediante los cuestionarios fue correcta y estuvo completa.
Surgen de la temática de los cuestionarios, pero quedaron abiertas a recabar
mayor información al visualizar directamente las actividades de cada
departamento. Las características de las guías de observación se asemejaron a
las de los cuestionarios en cuanto a credibilidad, confirmabilidad, transferencia,
dependencia y consistencia. Lo anterior se derivó del diseño de esta
investigación, que fue mixto con enfoque predominante cualitativo.
También se hizo un estudio estadístico de las cifras de los ejercicios: 2010 en el
que se adquirió el software de facturación electrónica para pertenecer a la
modalidad de CFD´s y donde se efectuaba la facturación en papel, 2011 en el
que el manejo de la facturación para el área de ventas comenzó a utilizar de
lleno los CFD´s, 2012 cuando en su totalidad se dio el uso de los documentos
electrónicos y 2013 en el que ya se tiene cierta experiencia en las facturas
electrónicas y se hizo el cambio a CFDI. Con base en esas cifras se
propusieron las modificaciones que se requirieron para estar en regla y
conforme a la legislación en vigor, y donde se pretendió incorporar un nuevo
modelo en el área de facturación de ventas de la compañía atendiendo a las
disposiciones fiscales para 2014 y cambiar la modalidad de CFD´s a FCDI´s
con una capacitación mayor para quienes se encargan del área de compras. La
técnica para recopilar esta información fue recolección de documentos (ver
anexo 5) y se estudió la eficiencia y la efectividad mediante el tiempo invertido
en cada actividad, identificando si fue el adecuado, si hubieron horas
extraordinarias de trabajo, o si se presentaron lapsos de inactividad
(desperdicio). Las cifras se estudiaron con base en reportes solicitados al área
de Contabilidad, en estados financieros básicos y en auxiliares contables, y se
compararon con la información recabada mediante las observaciones. Otras
cifras que se estudiaron fueron el número de facturas emitidas y las que se
hayan cancelado por diversas razones, lo cual indicó que hay errores que
corregir, o capacitación que proponer. También se midió mediante cifras elimpacto de reportes erróneos enterados a las autoridades fiscales por
desconocimiento o fallas en el proceso mismo de la facturación de ventas, y en
la deducibilidad de facturas del área de compras de los ejercicios 2012 y 2013.
Adicionalmente se compararon los gastos de operación por facturar para
identificar el ahorro que representa que las facturas ya no deban ser en papel,
sino de forma electrónica. Esos gastos fueron desde la inversión en software,
hasta la capacitación del personal, el resguardo, el traslado de documentos, y
de la impresión de las facturas emitidas (ventas).
Posterior al diseño de los instrumentos mencionados en el párrafo precedente,
se realizó el piloteo (ver anexos 6 y 7), el cual consistió en entregar una copia
de los cuestionarios y guías de observación al personal que conoce las áreas
de estudio para que realizaran una revisión del lenguaje utilizado, la amplitud de
las preguntas y el orden en la información que se pretendía obtener.
A su vez, se proporcionó a expertos los instrumentos para su validación por
jueceo (ver anexos 8 y 9), en los cuales se compara con la matriz de
congruencia y metodológica para reconocer que cumplen con los puntos
requeridos para el logro de los objetivos establecidos.
Una vez teniendo la última versión de los instrumentos, se aplicaron los
cuestionarios de manera auto-administrada a todas las personas que llevan a
cabo las actividades, siendo para el área de facturación de ventas dos, y para el
área de compras cuatro. Las guías de observación se llenaron asistiendo a la
empresa en diversas fechas, considerando dos días a la semana, un día por la
mañana y otro por la tarde, durante 5 semanas, con el objeto de obtener diez
observaciones de cada área de estudio.
Después de obtener la información, se comenzó el análisis de los resultados
para identificar los pasos del proceso que agregan valor y los que no lo hacen
(desperdicio), definir las características de las áreas de compras y de
facturación de ventas de H, S.A. de C.V. y visualizar el flujo global y específico
de las actividades que se realizaron para encaminarlo a la creación de una
propuesta.Posteriormente, se planteó la propuesta de reingeniería de procesos para los
departamentos de compras y de facturación de ventas, partiendo de los
objetivos planteados, identificando los puntos débiles a atacar en cada área
para así conseguir el aumento de la calidad mediante el incremento de la
eficiencia.
Finalmente, se determinaron las conclusiones pertinentes y se generó la
redacción final de la tesis</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En respuesta a las preguntas que se elaboraron para el diseño de la investigación
se obtuvo lo siguiente:
La reingeniería de procesos es una herramienta importante para aplicar a las
empresas en cierto punto de su vida, pues con ella se replantea el objetivo de un
negocio y las herramientas que tiene para su operación. No se trata de una simple
mejora, sino una “mejora espectacular” debido al nivel de impacto de los cambios
resultantes.
También, a través de la aplicación de una reingeniería de procesos, se da el
cambio de mentalidad en los miembros de una organización, sobre todo cuando
se hace conciencia del impacto de los problemas que se presentan ante los
cambios constantes en el entorno social y económico y que hacen que sus
actividades sean tediosas y/o difíciles de llevar a cabo. En conjunto con esta
condición de constante variabilidad de situaciones, la empresa H., S.A. de C.V.
permitió que se evaluara su nivel de eficiencia en busca de mejoras considerables
en las áreas de compras y facturación de ventas.
En este caso se tomó como pretexto la facturación electrónica que se introdujo
paulatinamente pero que en esta fecha (2014) ya es una realidad operante y
obligatoria en México. En este sentido, el decir “factura electrónica” no sólo se
refiere a un documento fiscal que avala una transacción comercial ya que implica
una serie de actividades para que ese comprobante sea realizado, emitido y
controlado de manera adecuada, así como adquiera la validez. Asimismo, el
proceso de facturación involucra a toda la empresa con sus diversos subsistemas,
de tal suerte que esa actividad tiene impacto en cada parte de la administración en
mayor o menor medida. Por lo tanto, ejercer controles en las empresas también es
cosa básica, pues de ello depende que no se desperdicien recursos en tareas
innecesarias y en duplicidad de funciones, las cuales son las consideradas como
principales causas de fracaso de las empresas.
Por otro lado, documentar los procesos administrativos en las compañías es otra
herramienta importante que, en conjunto con la reingeniería de procesos, permite ejercer un control interno adecuado. Algunas empresas deciden llevar a cabo sus
funciones de forma repetitiva y sin cambios, y el nuevo personal que ingresa
aprende de esas mismas formas y funciones, y que al no tener un documento
guía, crea dispersión y confusión de información, descuido en las actividades, falta
de comunicación y desorganización en general.
Plantear la reingeniería de procesos en las áreas administrativas permite rescatar
los puntos base de las actividades de la empresa, eliminar los puntos muertos, y
proponer la forma de hacer eficientes los procesos necesarios para el
funcionamiento conveniente.
Por lo anterior, el propósito amplio de este estudio fue contribuir en dos áreas
administrativas con apego a una obligación fiscal tomando como base las
condiciones del país a nivel global e internas de una organización, para
reestructurar procesos y enfrentar los cambios que se van presentando.
Elegir a la compañía H., S.A. de C.V. para la realización de la presente
investigación se dio por dos factores principalmente: 1) La empresa es líder en el
sector de fabricación de muebles de exhibición de madera y metal y a pesar de
contar con esa ventaja, administrativamente presenta problemas que no le ha
permitido expandirse más y ampliar su cartera de clientes y 2) La experiencia
personal en dicha empresa permite identificar la raíz de sus adversidades y
generar una propuesta viable acorde a sus necesidades.
Para la elaboración de la propuesta fue necesario esquematizar la problemática
vigente de la empresa en su entorno, recabar datos para su análisis a través de
hojas de cálculo y gráficos del comportamiento de sus cifras, diseñar diagramas
de flujo sustentados con sus respectivos procedimientos, y atacar en específico
las fallas detectadas.
H., S.A. de C.V., los miembros de su organización y en general cualquier empresa
que enfrente desconocimiento en el área de facturación electrónica, puede
basarse en el presente trabajo como guía para la modificación de sus procesos
administrativos en la búsqueda de una operación más fluida y continua.
Con respecto al supuesto de investigación, la reingeniería de procesos da las
bases para hacer más eficientes las operaciones relacionadas a la facturación electrónica como lo son la emisión, recepción, control y resguardo de los
documentos, y así dar como resultado un aumento en la calidad de los procesos
de compras y facturación de ventas de H., S.A. de C.V. que se vería reflejado en
las futuras cifras de la compañía, pero que, al no ser objeto del presente estudio,
se recomendaría vigilar su comportamiento posterior a la aplicación de la
propuesta para determinar su efectividad.
Cabe destacar que como resultado del estudio de las cantidades en cuanto al
gasto por compra papel y el ahorro que el SAT planteó como motivador, en este
caso no es real, pues al cambiar de esquemas de facturación, el consumo de
papel se vio incrementado principalmente porque se tiene la libertad de imprimir
cuantas copias sean deseadas. Además, los controles en general se siguen
llevando en carpetas. Por otra parte, en cuanto al ahorro en mensajería, sí existió
una disminución total en ese rubro de gasto, pues el traslado físico se hizo
inoperante.
Si bien hay cosas que no dependen de la empresa como lo son los documentos
que los proveedores entregan, mejorar los procesos internos conducirá a que el
impacto de esos factores externos sea identificado, corregido y solventado de una
manera más adecuada. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
Escuela Superior de Comercio y Administración
Unidad Tepepan
Sección de Estudios de Posgrado e Investigación

“LA REINGENIERÍA DE PROCESOS
COMO HERRAMIENTA
ORGANIZACIONAL PARA LA
FACTURACIÓN ELECTRÓNICA EN
MÉXICO. CASO: H., S.A. DE C.V.”
TESIS:
que para obtener el grado de
Maestría en Ciencias en Administración Negocios
PRESENTA:
C.P. ALEJANDRA CONCEPCIÓN AGUIRRE ZAVALETA
DIRIGIDA POR:
M. EN C. BERTHA PALOMINO VILLAVICENCIO
DR. ALBERTO RIVERA JIMÉNEZ
MÉXICO, D.F. JUNIO, 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>CONTROL INTELIGENTE DE UN PENDULO INVERTIDO MOVIL
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los robots son sistemas mecánicos y para conseguir controlar sus movimientos es
necesario encontrar un modelo maten ático que describa las dinámicas del sistema. Los
sistemas convencionales de control son diseñados utilizando modelos matemáticos de
sistemas físicos, por lo tanto, es necesario obtener las dinámicas del sistema utilizando las
leyes de la física que lo gobiernan.
La teoría de control es un campo que trata con disciplinas y métodos que nos guían a
la obtención del control de un sistema. En las ´ultimas décadas, se han empleado diferentes
metodologías para el diseño de controladores y en el análisis de sistemas complejos que
involucran múltiples variables, entre las que destacan las técnicas de computo suave
como la Lógica Difusa y las técnicas de optimización como lo son las Redes Neuronales
y los Algoritmos Gen éticos. Estas metodologías han facilitado el diseño de controladores
con capacidades de adaptación y el aprendizaje, capaces de controlar un sistema del tipo
no lineal.
En la actualidad los principales logros realizados corresponden al estudio de sistemas
Rob óticos subastados, los cuales han dado lugar a distintas aplicaciones prácticas como
los vehículos de transporte. Estos requieren de una total comprensión del sistema para ´
garantizar la seguridad del usuario y debido a su complejidad las técnicas emergentes del
computo inteligente son una propuesta conveniente para atacar el problema. 
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo principal de este trabajo de tesis se basa en mantener un péndulo invertido
móvil (sobre dos ruedas) en su punto de equilibrio, sin la asistencia de un operador
humano, mediante el diseño de un controlador adaptativo implementando algoritmos con
base a los conceptos de inteligencia artificial. Para lograr lo anterior se presentan los
siguientes objetivos de investigación.

Objetivo general
Resolver el problema de estabilización de un péndulo invertido sobre dos ruedas mediante
control adaptativo, desarrollado con técnicas de control inteligente.
Objetivos específicos
Diseñar y construir un prototipo de robot tipo péndulo invertido móvil.
Verificar compatibilidad de software con el hardware, para realizar la implementación
del algoritmo de control.
Desarrollar algoritmo de control adaptativo, implementando técnicas de control inteligente.
Aplicar el algoritmo de control adaptativo: Sistema simulado.
Aplicar el algoritmo de control adaptativo: Prototipo didáctico de péndulo invertido
móvil.
Evaluar del algoritmo de control adaptativo: Sistema simulado.
Evaluar del algoritmo de control adaptativo: Prototipo didáctico de péndulo invertido
móvil.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Se asegura la estabilidad alrededor del punto de equilibrio de un péndulo invertido
móvil al emplear la síntesis difusa de Lyapunov en el diseño de un controlador difuso para
el control de estabilidad y mediante la sintonización de las funciones de membresía por
medio de un algoritmo gen ético se optimiza el desempeño del controlador.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En esta investigación se busca la implementación de un Sistema genético Difuso, con el fin de probar su eficiencia y desempeño en sistemas aplicados en la vida real, en este caso un péndulo invertido móvil, que asemeja los vehículos sobre dos ruedas que se utilizan como medio de transporte y necesitan garantizar la seguridad del usuario.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo de tesis se demostró la eficiencia del método propuesto como alternativa
al diseño tradicional de los controladores difusos, que permite la estabilización del
sistema de Péndulo Invertido Móvil y además se consigue utilizando el menor número de
reglas difusas mediante la Síntesis Difusa de Lyapunov, la cual garantiza la estabilidad
del sistema al establecer el conjunto de reglas que conforman al controlador difuso. Esto
se ve reflejado en las pruebas realizadas en el prototipo didáctico utilizado durante este
trabajo de tesis.
Por otra parte, la integración de los Algoritmos Genéticos permite sintonizar las
funciones de membresía con la finalidad de mejorar la respuesta del controlador difuso.
Esta propuesta denominada Sistema Genético Difuso utiliza los paradigmas del computo
evolutivo, dotando al sistema con herramientas que le proporcionan un comportamiento
semejante al de la Inteligencia Artificial.
Mediante la técnica propuesta se logró reducir adecuadamente el tiempo de asentamiento
del ´Angulo de inclinación θ y asimismo la amplitud en las oscilaciones en la
corrección del error angular. Otro aspecto notable es la implementación del Algoritmo de
Ajuste Simple para la creación de población, el cual demostró ser bastante versátil, debido
a que logra el mayor número de individuos aptos para formar parte de la solución sin la
necesidad de evaluar a cada uno de ´ellos, descartar individuos y generar nueva población en el peor de los casos, de esta manera se logró disminuir el tiempo computacional e
incrementar la diversidad de los cromosomas.
Como resultado de la integración de las herramientas utilizadas a lo largo de este
trabajo de tesis, se consiguió exitosamente concretar los objetivos propuestos para resolver
el problema de estabilización de un Péndulo Invertido Móvil. Además, se culminó el diseño
y construcción de un prototipo didáctico de TWIP, el cual puede ser utilizado en la
realización de pruebas de algoritmos de computo inteligente. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL  
CENTRO DE INVESTIGACIÓN Y DESARROLLO  
DE TECNOLOGÍA DIGITAL
MAESTRÍA EN CIENCIAS EN SISTEMAS DIGITALES
“CONTROL INTELIGENTE DE UN PÉNDULO INVERTIDO  
MÓVIL”  
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS EN SISTEMAS DIGITALES
PRESENTA
ING. CARLOS MARTÍN MAGAÑA GONZÁLEZ
BAJO LA DIRECCIÓN DE  
DR. ROBERTO SEPÚLVEDA CRUZ DR. ÓSCAR H. MONTIEL ROSS  
MARZO 2016 TIJUANA, B.C., MÉXICO
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Mapeo de entornos acotados con un robot móvil semi-autónomo utilizando lógica difusa</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La robótica móvil tiene la necesidad de saber en dónde se encuentra el robot para la
toma de decisiones, por lo que se busca una representación del entorno o mapa que nos
dé información fiable de este entorno. Otra necesidad básica en la robótica móvil es el
control de dichos robots en su entorno para obtener movimientos que faciliten el
recorrido dentro del entorno. La cantidad de información necesaria para la realización de
un entorno es enorme por lo que la cantidad de incertidumbre que se genera es cada vez
mayor y se presenta en: la longitud de los espacios, la capacidad de cómputo necesaria, el
número de sensores, la cantidad de movimientos para cubrir la mayor parte de entorno.
Especialmente al hacer mapeo de entornos con sensores ultrasónicos se tiene la
desventaja de no poder conocer en una sola medición las dimensiones del entorno. Con lo
anterior se buscan métodos que ayuden a hacer un mapa a pesar de estas dificultades.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Realizar un mapa del entorno acotado con un robot semi-autónomo de LEGO
mindstorm utilizando el teorema de Bayes y lógica difusa.
Objetivos particulares
Configurar el armado de un robot LEGO Mindstorm para la tarea de la
obtención de entornos.
Realizar la caracterización del sensor ultrasónico de un robot móvil
Mindstorm para la programación del algoritmo sobre el modelo del sensor.
Programar el algoritmo de Bayes y los diferentes operadores de lógica
difusa para el mapeo de entornos con el robot LEGO Mindstorm. Y decidir cuál
tiene el mejor desempeño para obtener ventajas y desventajas de ambos.
Combinar los resultados del algoritmo de Bayes y los operadores de lógica
difusa para obtener un mapa con mayor exactitud.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En los últimos años la robótica móvil ha ascendido en importancia para la vida
cotidiana del ser humano, los robots de servicio se vuelven esenciales para labores
cotidianas en donde deben interactuar en un entorno acotado donde se vuelve
indispensable que el robot conozca su posición actual y hacia dónde debe dirigirse, por lo
que el mapeo es primordial para el correcto funcionamiento del robot en sus movimiento
ya que al obtener una representación del entorno él tiene la seguridad de andar en áreas
seguras y el ser humano puede observar el entorno sin necesidad de exponerse. Combinar
la robótica móvil con lógica difusa se hace necesario por la cantidad de incertidumbre que
se tiene, sobre todo si se tienen pocos sensores y/o sensores con poca precisión.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se realizaron mapas del entorno acotado con diferentes técnicas de mapeo de
rejillas ocupacional como el teorema de Bayes y la lógica difusa; pueden ser una
herramienta muy útil como robots de servicio, sin el mapa el robot no podría llegar a ser
completamente autónomo al no tener idea de su entorno.
Existen robots con mayor números de sensores, pero al trabajar con un robot como
LEGO mindstorm se limitan sus capacidades por el número de sensores y su calidad, sin
embargo esto no impide que se logren hacer experimentos de interés como lo es el
mapeo probabilístico o difuso. Entre mayor investigación se obtenga con este tipo de
elementos, se podría tener una mejor educación práctica en el área de robots en escuelas
con menores recursos.
Al elegir el modelo del sensor ultrasónico que se utilizará se debe tomar atención a
la apertura del lóbulo principal donde el sensor regresa mediciones certeras, ya que así se
podrá tener mayor o menor certidumbre en las mediciones.
Al tomar en cuenta los métodos de mapeo de entornos es importante definir el tipo
de autonomía que tendrá el robot, ya que entonces su control se vuelve primordial para
mejores resultados en la gráfica del entorno. También se debe tener cuidado con las
ganancias que cada método utiliza ya que una pequeña modificación puede cambiar un
mapa de sobreconfiado a conservador, omitiendo o agregando información del entorno.
Con lo anterior se llegó a la conclusión que dependiendo el uso que tendrá el mapa
se definirá qué algoritmo es el indicado a utilizar, ya que el mapeo ocupacional con el
método de Bayes nos da un mapa más conservador al ocupar mayor cantidad de celdas
ocupadas por lo que nos serviría para definir casillas dónde el robot está muy seguro al
tener una certeza alta para las pocas celdas vacías. Sin embargo si deseamos un mapa con
mayor detalle definitivamente nos inclinamos para el uso de un operador difuso al
obtener mapas con menor incertidumbre en las mediciones y así obtener una imagen más
nítida.
Al utilizar los operadores difusos se encontró con la característica de que funciona
como un operador morfológico al obtener esquinas cóncavas en esquinas cerradas de 90°
y obtiene esquinas rectas en esquinas abiertas de 90°.
Al obtener un mapa con las características de la teoría de Bayes y la lógica difusa se
obtiene un mapa con límites del entorno más claros para observar áreas seguras del
ambiente.
Aunque la ventaja de LEGO es diferentes tipos de configuraciones se debe poner
especial atención al peso del robot por cada pieza integrada.
Finalmente al juntar la teoría del mapeo de entornos con la localización se vuelve
indispensable que se comprendan ambas áreas ya que siempre van de la mano, al ser
necesario una para la realización de la otra.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Computación
Mapeo de entornos acotados con un robot
móvil semi-autónomo utilizando lógica difusa
Tesis que presenta:
Morales Vázquez Brenda Verónica
Que para obtener el grado de:
Maestro en Ciencias en Ingeniería de Cómputo con Opción en
Sistemas Digitales
Directores:
Dr. Herón Molina Lozano
Dr. Victor Hugo Ponce Ponce
MÉXICO, DISTRITO FEDERAL Julio 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Aplicación de técnicas de optimización para la
generación de planes de ejecución de consultas
hacia bases de datos remotas</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los sistemas mediadores buscan que su funcionamiento pase prácticamente desapercibido para el
usuario, brindando la ilusión de contar con un almacén de datos ubicado de manera local,
liberando al usuario de tener que conocer la ubicación de los datos y los esquemas a los que
pertenecen. Como consecuencia, el mediador es responsable de resolver los aspectos anteriores,
por ello en tiempo de consulta se lleva a cabo la creación del plan de ejecución, lo que significa
que para que el usuario pueda obtener la respuesta a su necesidad de información, tienen que
realizarse las siguientes tareas (VéaseTabla 1.1).
Estas tareas se llevan a cabo de manera sucesiva, implicando un tiempo de procesamiento; por
consiguiente, el tiempo de espera del usuario para obtener la respuesta a su consulta es igual a la
sumatoria de estos tiempos. De esto puede deducirse que resulta imperativo contar con una
aproximación que de manera ágil genere planes de ejecución lo suficientemente precisos para
resolver las consultas del usuario, lo más eficientemente posible.
Por ello se busca desarrollar una solución a esta problemática, que incorpore los mejores aspectos
de los métodos que son usualmente utilizados para la optimización de consultas por los SABD:
basados en reglas, basados en costos, heurísticas y estadísticas.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo> Objetivo General.
 Desarrollar una aproximación que permita generar planes de ejecución, que pueda ser
incorporada por un sistema de integración virtual de datos para bases de datos
relacionales, para definir un orden de ejecución que busque disminuir el tiempo de
respuesta del sistema, mediante la incorporación de algunos aspectos de los modelos de
optimización de consultas basadas en reglas, costos, heurísticas y estadísticas.

Objetivos Particulares.
 Desarrollar un componente que elabore catálogos, donde se almacenen los metadatos y
valores estadísticos (selectividad, cardinalidad y tamaño de tablas) mediante el acceso a
las fuentes de datos implicadas.
 Adaptar e implementar procedimientos de los modelos de optimización de consultas
basados en reglas, costos, heurísticas y estadísticas para la generación de planes de
ejecución en un sistema de integración virtual de datos.
 Diseñar e implementar una propuesta de solución que incorpore los aspectos más
relevantes de los enfoques basados en: reglas, costos, reglas, heurísticas y estadísticas,
para la generación de planes de ejecución, buscando que su realización beneficie al
rendimiento del sistema mediador durante el proceso de integración.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Los sistemas mediadores permiten llevar a cabo la integración de datos provenientes de fuentes
de datos autónomas, de tal manera que este proceso pasa prácticamente desapercibido por el
usuario; no obstante, esta practicidad aunada con las tareas de extracción y procesamiento que se
realizan después de que el usuario ha expresado su consulta, implica trabajo que tiene que ser
realizado por el mediador, lo que afecta directamente al tiempo que el usuario tendrá que esperar
para recibir la respuesta a su consulta.
Por consiguiente el presente trabajo busca brindar una solución eficiente para la generación de
planes de ejecución en los que se especifiquen las operaciones y el orden para llevarlas a cabo; sin
embargo, establecer el orden de ejecución puede resultar complejo, debido a que en ciertas
ocasiones el número de posibles alternativas puede ser demasiado elevado y analizar todas ellas
podría resultar tardado, por lo que es necesario que el principio de exploración del espacio de
posibles soluciones se desarrolle de manera ágil.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo se buscó desarrollar una alternativa para la generación de planes de ejecución en
un sistema mediador (sistema de integración virtual de datos) partiendo de los enfoques utilizados
por los sistemas de administración de bases de datos (heurísticas, costos y reglas).
Inicialmente se planteó tomar en consideración los factores referentes a la transmisión de datos
en ambientes distribuidos, debido a que el sistema mediador depende completamente de los
accesos remotos que realiza a las fuentes de datos operativas (sitios que contienen físicamente los
datos a los que accede el sistema mediador), sin embargo por la autonomía de dichos sitios, en
estos no se tiene la capacidad de procesar datos provenientes de otras fuentes, por lo que no es
posible distribuir la carga de trabajo y recae en el sistema mediador la responsabilidad de realizar
las operaciones que involucran datos provenientes de múltiples fuentes.
En la aproximación desarrollada no se consideró explícitamente el tiempo de transmisión de los
datos hacia el mediador, porque el tiempo de transmisión es proporcional al tamaño de la tabla,
es decir, si el tiempo que tarda en realizarse una reunión es t, entonces el tiempo que tomará
realizar la reunión y transmitir sus resultados hasta su punto de destino es kt, donde k es una
constante mayor a uno y al realizar la comparación de tiempos solo por la relación menor,
multiplicar los tiempos por una constante no altera los resultados de las comparaciones, claro esta
afirmación supone que todas las bases de datos tienen la misma constante k, lo que no suele ser
cierto, no obstante como se menciona en [22] , esta suele ser una suposición común por los
optimizadores en ambientes distribuidos.
Supóngase que se calendarizo mal la reunión de los datos que se tienen con los datos de una base
de datos en un punto distante, y esto demora 6 veces más de lo estimado (por no haber
considerado el tiempo de transmisión). Se tarda 6t en vez de t, Sin embargo, como de todas
maneras se tienen que esperar a que lleguen los datos, no importa el momento en que se solicitó
estos datos, el tiempo 6t se suma de cualquier modo al tiempo total de ejecución, sin importar la
hora en que se piden. Entonces, tomar muy exactamente los tiempos de transmisión no es muy
importante.
Se tomó como enfoque para la generación de planes de ejecución para sistemas mediadores, la
reestructuración de la consulta mediada, la generación de consultas bajo los esquemas locales, la
transmisión de información por la red de comunicaciones y la combinación de datos. Se optó por
desarrollar un método ágil mediante el cual fuera posible reestructurar una consulta mediada en
un conjunto de consultas objetivo y el establecimiento de un orden de ejecución mediante la
estimación de resultados intermedios.
Para lograr esto se trabajó en aspectos como lo son: la creación del esquema mediado, la
localización de correspondencias entre los esquemas locales, la reestructuración de la consulta así
como el desarrollo de estimaciones de resultados intermedios.
La generación de planes de ejecución eficientes o cercanos al óptimo resulta ser una tarea difícil,
debido a que existe una gran cantidad de variables que pueden afectar la eficiencia de estos, la
mayor dificultad es conocer de manera detallada composición y distribución de los datos.
Debido a que, como lo demostraron las pruebas, las estimaciones pueden diferir enormemente a
los valores reales; esto podría facilitarse mediante la creación de catálogos más extensos, sin
embargo esto podría resultar complicado debido a que las estadísticas especializadas difieren
entre un manejador y otro.
Cabe aclarar que a pesar de las limitaciones de las estimaciones con respecto a diferir del valor
real, estas proveen de algunos indicios que promueven el desarrollo de planes más eficientes
como lo ha indicado hasta ahora los planes de ejecución desarrollados en la etapa de pruebas.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Computación
“Aplicación de técnicas de optimización para la
generación de planes de ejecución de consultas
hacia bases de datos remotas”
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN
P R E S E N T A
ING. RODOLFO NAVARRO ZAYAS
DIRECTOR DE TESIS: M. en C. ALEJANDRO BOTELLO CASTILLO
MÉXICO, D.F. AGOSTO 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Implementación de algoritmos de
procesamiento de imágenes en FPGA
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar una arquitectura para procesar imágenes digitales de iris humano, segmentarlos
y normalizarlos en un dispositivo FPGA.
1.4.2. Objetivos particulares
Los objetivos particulares son los siguientes:
• Desarrollar una técnica de segmentación del iris.
• Evaluar la técnica de segmentación de iris desarrollada.
• Desarrollar una arquitectura base para lectura, escritura y despliegue de
imágenes, así como para su manipulación y procesamiento en el FPGA.
• Implementar la técnica desarrollada en un dispositivo FPGA con los módulos
necesarios para el manejo de datos de la imagen y su despliegue en un
visualizador (display) por medio del puerto del Adaptador Gráfico de Video (VGAVideo
Graphics Array).
• Evaluar los resultados obtenidos con la implementación de la técnica desarrollada
en el FPGA. </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible crear un sistema específico para procesar imágenes con dispositivos FPGA, que
brinde ventajas con respecto a sistemas similares desarrollados mediante software en
computadoras personales de propósito general en relación con el costo y rapidez. 
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La implementación de algoritmos de alto nivel sobre FPGA tiene ventajas frente a otras
arquitecturas. A continuación se exponen algunas de ellas:
• El hecho de poder ejecutar algoritmos de forma concurrente en un dispositivo
reconfigurable logra acelerar los tiempos de ejecución con respecto a los
ejecutados de forma secuencial.
• Debido a la característica de reconfigurabilidad de los FPGA, ésta permite un
alto grado de flexibilidad, lo que conlleva a que cualquier variación del diseño
no tenga que llevar implícito cambio alguno en función de la plataforma
empleada.
• El reducido precio de la plataforma donde se desarrollará la aplicación, pues el
costo de una plataforma basada en un FPGA es notablemente inferior al que
puede tener una computadora personal (PC) o un DSP.
Todas estas razones nos conducen al intento de adaptar algoritmos de alto nivel sobre
dispositivos reconfigurables. 
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los algoritmos propuestos y más utilizados hallados en la literatura abierta y detallados en
el estado del arte, hacen uso en su gran mayoría de técnicas de procesamiento de
imágenes robustas, pero con un alto consumo computacional, ya que se basan en hacer
circunferencias de diferentes dimensiones hasta encontrar la circunferencia que más
coincide con los bordes que rodean al iris; sin embargo, en nuestro trabajo hemos logrado
una técnica y unos algoritmos de procesamiento de imágenes de iris para su segmentación
y su normalización sencillos que están a la altura de los ya desarrollados hasta este
momento.
Se desarrolló una arquitectura de 3 etapas para la segmentación y normalización del iris
humano de imágenes de la base de datos CASIA V1 en FPGA, dicho desarrollo lleva a
cabo la tarea de segmentación y normalización del iris 10.27 veces más rápido que la mejor
ejecución obtenida en una PC en lenguaje C/C++. Obteniendo como salida las imágenes
de iris normalizados con una diferencia de menos del 10% de pixeles que difieren en cuanto
a los resultados obtenidos en la PC y el FPGA.
Se desarrolló la técnica de segmentación y normalización del iris, donde se propone una
técnica de segmentación y aproximación del borde exterior de iris. La evaluación de dicha
técnica da como resultado un porcentaje de imágenes bien segmentadas de 95.6%, que
compite de buena manera con los resultados obtenidos en diversos trabajos del ámbito,
con la ventaja de que es un algoritmo sencillo de implementar y rápido en cuanto a tiempo
de ejecución.
El desarrollo de la arquitectura base es esencial para la implementación del algoritmo en
alguna plataforma FPGA, porque nos permite controlar el flujo de los datos de entrada y
salida, el control y manejo de los periféricos de la implementación, así como también la
comunicación entre el usuario y el sistema. En este trabajo se desarrolló la arquitectura
base en un procesador NIOS II para la lectura y escritura de imágenes en formato BMP de
una tarjeta de memoria SD, para el manejo y control de una memoria o buffer de video para
la visualización de las imágenes en un display VGA y para el manejo y control de datos
para la realización del procesamiento en módulos de hardware. Esta arquitectura base tiene
como principal característica que está basada en un procesador NIOS II y las tareas
mencionadas son programadas en lenguaje C/C++, lo que también posibilita la
implementación de algunas otras tareas de procesamiento en ella.
Se implementó en FPGA la técnica desarrollada para la segmentación y normalización
de iris humano. Para la implementación de la técnica o algoritmo desarrollado, existen
diversas maneras de implementarlo en FPGA. En este trabajo se implementaron tres de las
formas más utilizadas que son: la implementación de módulos de hardware que ejecutan
completamente el algoritmo; la implantación del algoritmo en un procesador suave haciendo
uso del lenguaje C/C++ en dicha implementación; y la combinación de software (C/C++) y
hardware, es decir la implementación del algoritmo en software ejecutándose en el
procesador suave, auxiliándose de módulos de hardware que realizan al mismo tiempo
cálculos matemáticos o tareas específicas, liberando así al procesador de carga
computacional. Los resultados obtenidos en cuanto a tiempo muestran que la mejor de
estas tres implementaciones es la implementación de la tarea completamente en hardware, 
mientras que la que mostró ser la más lenta es la implementación del algoritmo
completamente en el procesador suave (NIOS II).
Se evaluaron las imágenes resultantes del desarrollo e implementación de la tecnicaq
en FPGA obteniendo las imágenes y los resultados esperados, es decir, dadas las
imágenes de entrada, el algoritmo al igual que en una PC, ejecuta la tarea de procesamiento
y como salida da la imagen del iris normalizado, con las mismas características (formato
BMP, 8 bits por color) que las obtenidas en la PC.
Se compararon las imágenes resultantes de la PC y de desarrollo en FPGA de la
implementación de la técnica desarrollada. En promedio hay un error cuadrático medio de
4.53 pixeles entre el histograma de la imagen resultado obtenida en la PC y la obtenida en
el FPGA. Este valor de error se debe a representación numérica en ambas plataformas,
Punto fijo y enteros para el caso de la plataforma FPGA y punto flotante y enteros en el
caso de la PC; a pesar de este valor de error, al obtenerse la plantilla del iris o matriz binaria
de identificación, hay un porcentaje de diferencia menor al 10% (0.0822, que equivale al
8.22%) entre las mismas.
Con la implementación de procesamiento de imágenes en hardware, se gana en
velocidad.
Este trabajo se realizó teniendo como hipótesis e idea base que cualquier algoritmo
puede alcanzar su máxima velocidad si es implementado en hardware [12]; los resultados
obtenidos en este trabajo así lo demuestran.
La implementación completa de una tarea en hardware reconfigurable mediante algún
lenguaje de descripción de hardware, se ejecuta por completo en un menor tiempo y en una
menor cantidad de ciclos de reloj. Como se observa de los resultados de la primera etapa
y la tercera etapa, la implementación de ambas etapas fue implementada en hardware,
siendo desarrollada la arquitectura de los módulos en el lenguaje de descripción de
hardware VERILOG. Como se mostró en el capítulo 5, la tarea de esta etapa se ejecuta
175 veces más rápido que el mejor resultado obtenido en una PC (primera etapa); esto
corrobora la idea e hipótesis base de este trabajo, “…cualquier algoritmo puede alcanzar
su máxima velocidad si es implementado en hardware [12]”.
La implementación realizada de la segunda etapa (software) y tercera etapa (softwarehardware)
en el FPGA, a pesar de haberse ejecutado en un tiempo mayor al de la mejor
implementación en la PC, hay que hacer notar que la PC es alimentada por una señal de
reloj 50 veces mayor que la frecuencia de reloj que alimenta al FPGA y aún con esta gran
diferencia, el mejor tiempo obtenido en la PC es tan solo 8.7 veces mayor, ya que al ser la
PC un equipo de propósito general, está gobernado por un sistema operativo que ejecuta
diversas tareas aparte de la de procesamiento. Para hacer más equitativa la comparación,
se compara la cantidad de ciclos de reloj que requiere la ejecución de las tareas y ahí es
donde el FPGA muestra grandes ventajas, ya que requiere de mucho menos ciclos de reloj
para la ejecución del algoritmo (del orden de aproximadamente 20 veces menos).
Una característica importante que se obtiene de la implementación de tareas o
algoritmos en FPGA, es que como todo se diseña con circuitos digitales, la cantidad de
ciclos de reloj y por lo tanto el tiempo de ejecución es el mismo en todas las ejecuciones,
porque está dedicado a hacer solo esa tarea siempre de la misma manera; esto no sucede
en una PC, donde al ejecutarse un ciclo, una iteración puede ejecutarse en un tiempo X,
pero la siguiente iteración que realiza la tarea puede ejecutarse en un tiempo Y, es decir,
no siempre se ejecutan dos tareas iguales en el mismo tiempo. 
La implementación de algoritmos en FPGA tiene como gran ventaja que el algoritmo se
ejecuta y da los mismos resultados que su implementación en la PC, pero con un consumo
mucho menor de energía que la PC, ya que son sistemas dedicados compuestos de
circuitos integrados de bajo consumo en comparación con lo que consume una PC, además
de que un circuito integrado FPGA y sus periféricos, requieren mucho menos espacio y
pueden ser utilizados en aplicaciones donde las restricciones de espacio y tiempo son
especiales.
Los algoritmos de procesamiento de imágenes tienen características ideales para su
implementación en FPGA, ya que los procesamientos se realizan de manera puntual en
cada píxel; esto permite segmentar las imágenes y realizar varios procesamientos idénticos
al mismo tiempo (paralelizar el procesamiento), lo que permite obtener los resultados en
muy poco tiempo, con menos consumo de energía (en comparación con una PC) y a un
menor costo. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Secretaría de Investigación y Posgrado
Implementación de algoritmos de
procesamiento de imágenes en FPGA
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS EN INGENIERÍA DE CÓMPUTO
CON OPCIÓN EN SISTEMAS DIGITALES
PRESENTA:
Ing. Germán Oswaldo López Verástegui
DIRECTORES DE TESIS:
Dr. Edgardo Manuel Felipe Riverón
Dr. Sergio Suárez Guerra
México, D.F.
Junio de 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>CONTROL NUMÉRICO COMPUTARIZADO DE
ARQUITECTURA ABIERTA PARA SERVO-MOTORES DE CORRIENTE ALTERNA USANDO MODBUS
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la actualidad, la industria existe en un ambiente de grandes expectativas e incertidumbre
motivada por los cambios de la tecnología actual que por su velocidad de
desarrollo, no permite ser asimilada en forma adecuada de modo que es muy difícil
aprovecharla completamente. También surgen cambios rápidos en el orden económico
y político los cuales en sociedades como la nuestra (países en desarrollo) inhiben el
surgimiento de soluciones autóctonas o propias para dichos problemas.
Entre todos estos cambios es sin duda el desarrollo de las nuevas políticas mundiales
de mercados abiertos y globalizados, encarando la libre competencia, surge la
necesidad de adecuar nuestras industrias con la finalidad de que puedan satisfacer el
reto de los próximos años. Una alternativa para afrontar estos retos reside en la automatización
de las industria y dentro de este resurgir de la automatización se debe
considerar a las Máquinas de CNC, las cuales, frente a las dificultades industriales actuales
(la exigencia en la precisión, los diseños complejos, la diversidad de productos,
la reducción de costos de fabricación, la exigencia en la minimización de errores y el
tiempo de entrega cada vez más reducido) presentan mejores soluciones:
• Mayor precisión y mejor calidad de productos.
• Mayor uniformidad en los productos producidos.
• Un operario puede operar varias máquinas a la vez.
• Fácil procesamiento de productos de apariencia complicada.
• Flexibilidad para el cambio en el diseño y en modelos en un tiempo corto.
• Fácil control de calidad.
• Reducción en costos de inventario, traslado y de fabricación.
• Es posible satisfacer pedidos urgentes.
• Se reduce la fatiga del operador.
• Mayor seguridad en las labores.
 Aumento del tiempo de trabajo en corte por maquinaria.
• Fácil control de acuerdo con el programa de producción.
• Fácil administración de la producción e inventario.
• Permite simular el proceso a fin de verificar que este sea correcto.
Sin embargo, se pueden considerar algunas desventajas que existen en la utilización
de los sistemas de CNC, dentro de las cuales, conviene citar:
• Alto costo de la maquinaria.
• Falta de opciones o alternativas en caso de fallas.
• Es necesario programar en forma correcta la selección de las herramientas de
maquinado y la secuencia de operación para un eficiente funcionamiento.
• Los costos de mantenimiento aumenta, ya que el sistema de control es más complicado
y surge la necesidad de entrenar al personal de servicio y operación.
• Es necesario mantener un gran volumen de producción a fin de lograr una mayor
eficiencia de la capacidad instalada.
De las limitantes anteriores, la que más representa una barrera para la introducción
de dichas tecnologías en la industria, es la elevada inversión inicial que debe ser realizada
en la adquisición de la maquinaria, aunado al costo del mantenimiento que deberá
ser realizado por personal técnico especialmente calificado. Las máquinas herramienta
CNC cuentan con una multitud de funciones pre-cargadas y distintos grados de libertad
que en algunas ocasiones pueden llegar a ser excesivos (para cierta aplicación específi-
ca); el comprador se ve obligado a adquirir maquinaria sobrada para su requerimiento
realizando una inversión superior a la necesaria y donde además se terminan desperdiciando
funciones incluidas en la maquina. A lo anterior, podemos agregar la creciente
tendencia al desarrollo de Sistemas de Arquitectura Cerrada presentada por los distintos
proveedores de esta tecnología, la cual limita e incluso prohíbe legalmente la
adecuación o modificación de sus productos necesarias para satisfacer las necesidades
específicas de cada usuario y sólo permiten trabajar con una parte de sus desarrollos.

</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar, construir e integrar elementos para la fabricación un CNC de arquitectura
abierta que permita la fácil re-programación y acoplamiento del mismo a problemas
específicos en la automatización de dispositivos mecatrónicos mediante la utilización
de códigos ISO para máquinas-herramienta.

Objetivos Específicos
1. Desarrollo de la metodología para el cálculo de un sistema servo - controlado
que actúe en tres ejes de trabajo en maquinado de sólidos con velocidades de
corte convencionales; los primeros dos serán los actuadores de movimiento de
una herramienta de corte y el tercero activará el movimiento rotatorio de la pieza
de trabajo.
2. Desarrollo del software de control de código abierto de lazo cerrado por medio
del uso de una PC, que interprete y utilice instrucciones por códigos de uso
generalizado (ISO) en las máquinas de CNC de hasta 4 ejes.
3. Implementar una Interfaz Gráfica de Usuario que permita al operador utilizar
una PC para el control del prototipo.
4. Verificación experimental del sistema.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Se considera que la ingeniería en México y en general, la tecnología ha alcanzado un
nivel de desarrollo que puede permitir el diseño y la construcción de un control numé-
rico (CN) de arquitectura abierta a partir de la integración de elementos disponibles en
el mercado y el diseño y construcción de otros mediante las herramientas disponibles
en los centros de investigación del Instituto Politécnico Nacional, y que en un futuro,
con base a este y otros trabajos, en mediano plazo, podrán ser desarrollados y fabricados
en México.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Según cifras de la Secretaría de Economía el sector metal-mecánico representa aproximadamente
2,9 % del Producto Interno Bruto (PIB) nacional y 14 % del PIB manufacturero.
Las empresas del sector metal-mecánico están dedicadas a la fabricación,
reparación, ensamble y transformación de metales; aunque también existen empresas
de inyección de plástico y forja, de servicio galvanizado, trabajo de lámina, estampado,
ensamblado, mecanizado, troquelado y fundición. Dentro de las empresas registradas
aproximadamente 2000 son compañías grandes (entre ellas las automotrices), mientras
que el resto son pequeñas y microempresas. La mayor parte de las empresas se
encuentran concentradas en Nuevo León, Jalisco, Chihuahua, San Luis Potosí, estado
de México, Querétaro y Puebla.
De acuerdo con el Directorio Estadístico de Unidades Económicas del Instituto Nacional
de Estadística y Geografía (INEGI), el número de Establecimientos Económicos
del sector de Industrias Manufactureras es de un total de 104,416; según la Secretaría
de Economía, México es el décimo mercado mundial en consumo de máquinasherramienta,
con un estimado de 1,309,000 Millones de dólares (USD) (MDD). De
igual manera, México es el séptimo importador, mientras que ocupa el lugar 26 en
cuanto a exportaciones, es decir, más del 90 % de la demanda se cubre con las importaciones.
De estas, aproximadamente 28 % provienen de Estados Unidos, siguiendo en
orden de importancia Alemania, Japón, Italia, China y España. La dependencia señala
que los principales sectores consumidores de máquinas-herramienta son el automotor
y sus componentes, aeronáutico y electrodomésticos.
La ciudad de Querétaro, es en la actualidad una de las ciudades mexicanas con mayor
crecimiento industrial; ya que a la fecha de publicación de este trabajo, cuenta con
más de 10 parques industriales y es sede de empresas del ramo automotriz y aeronáutico.
Según cifras del INEGI, del personal ocupado en 2011 (más de 731,000 trabajadores),
un porcentaje considerable labora en la industria manufacturera (2010); más
del 36 % del PIB del estado proviene de actividades secundarias en la que se encuentra dicha industria con 57,847 Millones de pesos (MXP) (MDP) aportados; de la inversión
extranjera directa, Querétaro recibió 310.5 MDD en 2011 y particularmente la industria
manufacturera fue el principal sector beneficiado con este recurso. Así también,
según cifras del Consejo Nacional de Ciencia y Tecnología (CONACyT) la ciudad
cuenta con el 2.5 % de los investigadores a nivel nacional, la mayoría de ellos en las
ramas de ingeniería [INEGI 2011]. Oscar Peralta, presidente de la Cámara Nacional
de la Industria de la Transformación (CANACINTRA) en Querétaro, pronostica para
años subsecuentes un crecimiento superior o por lo menos similar al de 2011 (que fue
de 8 % aproximado).
En México, la industria formal que se dedique al desarrollo de máquinas-herramienta
con CNC no ha madurado lo suficiente como para ofrecer al mercado una opción de
bajo costo que además permita ser adaptada a necesidades específicas del cliente. Las
opciones de CNC de código abierto disponibles en la red no se encuentran actualizadas
para aprovechar los recursos computacionales, interfaces o ambientes gráficos de
las computadoras actuales además de que estas opciones se enfocan en el control de
motores a pasos, es decir, no existe en el mercado una opción de CNC de bajo coste
que utilice servo-motores de corriente alterna como sus actuadores.
En base a las cifras y conceptos expresadas en los párrafos anteriores y ante la llegada
continua de inversiones, consideramos correcto afirmar que es fundamental trabajar
en el desarrollo de la investigación industrial generando un círculo donde investigadores
locales trabajen para el desarrollo de tecnología para la industria queretana y en
general para la industria nacional evitando así la continua importación de tecnología.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Los resultados obtenidos tras la realización del presente trabajo de tesis permiten
concluir que la aplicación del proyecto es factible en maquinas-herramienta que fabriquen
piezas con tolerancias correspondientes a calidades dimensionales ISO 8. El CNC
por sí mismo es un prototipo didáctico utilizable con propósitos de formar recursos
humanos con habilidades en operación y mantenimiento de sistemas que incorporen
servo-motores y sus accesorios de control. Al ser el sistema una máquina-herramienta
de propósito específico, es posible adaptar la misma a necesidades propias.
El desarrollo de la Interfaz Gráfica de Usuario permite al operador controlar los dispositivos
de manera remota y segura con un conocimiento mínimo de los códigos ISO
para CNC, la disponibilidad del código fuente permite al usuario final la reconfiguración
el sistema de manera sencilla y segura, editando una base de datos que contiene
la información pertinente a las acciones que el código debe realizar al encontrar coincidencias.
El sistema de CNC desarrollado es capaz de interpretar de manera correcta los có-
digos ISO para CNC ingresados en formato de texto plano y los diversos modos de
operación de éste, permiten que el usuario utilice el software desarrollado de manera
didáctica para familiarizarse con los códigos ISO para CNC sin necesidad de conectar
los actuadores o de maquinar una pieza.
El software desarrollado para ambiente Windows ha probado ser confiable, ligero
y de fácil utilización, es capaz de correr en computadoras personales de características
comunes utilizando puertos de comunicación y librerías estándar, la interpretación del
código se realiza en la computadora, mientras que el control preciso de los actuadores
se delega a los manejadores individuales que controlan cada uno de los efectores finales,
asegurando así la ejecución en tiempo real del sistema, la arquitectura del sistema
propuesto es similar a un sistema SCADA en el cual la computadora principal es meramente
un supervisor.
Los dispositivos utilizados para la construcción del prototipo presentado son capaces
de realizar operaciones de maquinado metal-mecánico en velocidades convencionales
de manera similar a maquinaria disponible comercialmente, es posible adaptar el sistema propuesto a necesidades específicas modificando el hardware aquí propuesto,
las características finales del a máquina-herramienta serán dadas por las necesidades
de las operaciones de maquinado que se requieran realizar.
Finalmente, es posible que un usuario con conocimientos básicos de los temas
abordados y siguiendo como guía este trabajo de tesis replique el trabajo realizado
para la construcción de un prototipo de máquina-herramienta CNC de propósito específico,
y con un estudio básico del mismo, éste será capaz de realizar las adaptaciones
necesarias para poder aplicar el sistema a necesidades específicas.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN
CIENCIA APLICADA Y TECNOLOGÍA AVANZADA
UNIDAD QUERÉTARO
PROGRAMA DE MAESTRÍA EN TECNOLOGÍA AVANZADA
CONTROL NUMÉRICO COMPUTARIZADO DE
ARQUITECTURA ABIERTA PARA SERVO-MOTORES DE
CORRIENTE ALTERNA USANDO MODBUS
TESIS
Que para obtener el grado de:
MAESTRÍA EN TECNOLOGÍA AVANZADA
Presenta:
ING. CARLOS FRANCISCO MACIEL CASTELLANOS
Director de tesis:
DR. REYDEZEL TORRES MARTÍNEZ
QUERÉTARO, QRO. 18 DE DICIEMBRE 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>SISTEMA AUTOMATIZADO DE POSICIÓN
PARA UN HELIÓSTATO DE REFLEXIÓN
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Un helióstato tiene como objetivo reflejar la luz del Sol que incide en sus espejos hacia el
concentrador parabólico. Esto quiere decir, aunque el Sol se mueve por todo el horizonte, que el
helióstato debe siempre de reflejar la luz solar hacia el concentrador (una posición fija). Para lograr
esto se requiere solucionar los problemas siguientes:
a) Determinar la posición del Sol en un punto sobre la superficie de la Tierra con determinada
latitud y longitud:
1. Posición del Sol con respecto al centro de la Tierra
Para determinar el vector de posición del Sol S con respecto a un punto Q en la superficie de la
Tierra, se ha establecido un sistema de coordenadas esféricas, donde el Origen C es el Centro de la
Tierra (Figura 1.16). El vector de posición del Sol se define con dos ángulos de posición, ángulo de
elevación δ y ángulo de rotación en términos horarios ω referenciado a un meridiano M. La posición
del punto Q es determinada con el ángulo de elevación Φ, donde el ángulo horario ω es 0 (Chen, y
otros, 2001).
Para describir la posición del Sol con respecto al helióstato (observador) se puede establecer un
sistema de coordenadas esféricas con respecto a la orientación geográfica del mismo, determinando
de esta manera dos ángulos, ángulo de elevación αs y ángulo de rotación ϒs y una magnitud ρ del
vector (Figura 1.17) (Wei, y otros, 2011).
En base a la Ley de reflexión de Snell, se establecen dos vectores, el primer vector de incidencia de la
luz sobre el helióstato SO, y el segundo vector de reflexión del helióstato al concentrador OT. Sin
embargo, el vector OT se mantiene constante y el vector SO cambia a lo largo del día. Así mismo,
existe un tercer vector auxiliar normal al plano formado por el espejo central del helióstato ON que a
su vez, es una bisectriz de los vectores SO y OT, como lo muestra la Figura 1.18 (Chen, y otros,
2001).
A lo largo del día la posición del Sol cambia con respecto a la posición del plano formado por el espejo
central del helióstato, por lo tanto, la posición de éste debe ser ajustada utilizando su eje de elevación
FF’ y su eje de rotación TT’ para mantener direccionada la luz del Sol al concentrador (Figura 1.19). 
Se requiere diseñar el sistema automatizado que posicione al helióstato según la posición del Sol,
manteniendo la reflexión de la luz solar sobre el concentrador parabólico.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseño e implementación del sistema automatizado de posición para un helióstato de
reflexión.
Objetivos específicos
 Obtención de ángulos de posición del Sol.
 Obtención de ecuaciones de los movimientos de seguimiento del helióstato (f1 y f2)
 Diseño eléctrico-mecánico de actuadores para movimientos primarios y secundarios del
helióstato.
 Instalación e instrumentación de los actuadores para los movimientos de seguimiento.
 Implementación de algoritmo de contro</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Justificación social y energética
La utilización de combustibles fósiles, la deforestación, la quema de biomasa y la ganadería son
actividades humanas que liberan Gases de Efecto Invernadero (GEI), que absorbe y refleja gran parte
de la radiación infrarroja que emite el Sol a la superficie del globo terráqueo, sin embargo, aquella
radiación del Sol que atraviesa la capa de GEI incide sobre la Tierra, siendo reemitida en el espectro
infrarrojo, pero la capa de GEI no permite que salga al exterior elevando la temperatura de la Tierra.
Existe una gran cantidad de estudios sobre el calentamiento global y se estima que la temperatura
aumentará entre 1.4 y 5.8 °C durante los siguientes 100 años (Fernández, y otros, 2009).
Los GEI, como el CO2, han contribuido alrededor de dos terceras partes en el crecimiento del efecto
invernadero producido por el hombre en los últimos 100 años. Sin embargo, actualmente el CO2
contribuye al 50% de las emisiones anuales de GEI (Fernández, y otros, 2009).
En 1990, se produjeron alrededor de 6 y 8 billones de toneladas de CO2, las fuentes principales fueron
la quema de combustibles fósiles, fabricación de cemento y deforestación. Se estima que
aproximadamente se emiten 5.6 billones de toneladas de CO2 al año por la utilización de petróleo, gas
natural y carbono; y solamente los países industrializados contribuyen con el 80% de la producción
total de CO2. Sin embargo, los países en vías de desarrollo tienen un incremento mayor de producción
de CO2 que los países industrializados, por lo que se estima que más del 50% de la producción de
este gas será producido por éstos después del 2020 (Fernández, y otros, 2009).
Existen diversas aplicaciones en las que se puede utilizar la energía calorífica del Sol, como lo es en
el cocimiento de ladrillos, secado de semillas y frutas, ejemplos limitados a climas calurosos y
humedades relativamente bajas (40-60%) con vientos fuertes (Gómez, 2009). En México la
producción de ladrillos en hornos, se lleva a cabo de forma tecnificada y de forma artesanal; del total
de hornos (reportadas como unidades de proceso), el 70% son tradicionales (12,264/16,953) y
representan entre 30 al 50% de la producción total (Cárdenas, 2012).
Los hornos tradicionales (cámara construida con tabiques cocidos) pueden producir entre 9000 a
13000 tabiques funcionando una vez al mes. Como fuente de calor, utilizan la quema de combustibles
fósiles como aceite quemado, desechos domésticos o industriales, basura, llantas, diésel y
combustóleo; produciendo una gran cantidad de GEI que contribuyen al calentamiento global. Los
hornos trabajan durante 12 horas requiriendo aproximadamente 0.8 m3
de aceite quemado para
hornear 10000 tabiques, como lo muestra la Figura 1.15 (Villeda Muñoz, 2010). 

Sin embargo, mediante el uso de energías alternativas, como es el caso de la utilización de radiación
solar como fuente de energía calorífica, es posible remplazar el uso de la quema de combustibles
fósiles como fuente térmica implementando un sistema con la capacidad de concentrar la energía
solar.
El sistema de concentración de energía debe tener capacidad de concentrar la luz del Sol en un punto
a lo largo de todo el día, este punto debe mantenerse constante, por lo tanto, se seleccionó el sistema
de helióstato con concentrador parabólico fuera de eje, donde el helióstato puede captar y reflejar la
radiación solar de un área mayor en un área menor de un concentrador parabólico que mantendrá la
concentración en una área determinado (posición concentrador parabólico); el Sol, al cambiar de
posición a lo largo del día y cambiar su trayectoria a lo largo del año, obliga al helióstato a modificar su
posición para mantener constante la mancha de concentración de la luz solar en el concentrador
parabólico, por lo que es necesario que el helióstato cuente con un sistema automático de
seguimiento solar para lograr una mayor eficiencia al realizar la concentración.
Justificación técnica
El trabajo que se propone es la automatización del helióstato construido en el CICATA-QRO que tiene
de 2 movimientos primarios y 4 secundarios, un concentrador parabólico que es fijo y su relación con
la posición del Sol.
Se requiere automatizarlo con actuadores que soporten las variaciones del clima, sensores de
posición angular que retroalimenten la posición de los ejes de seguimiento, etapa de potencia y un
controlador, todo en un sistema de automatización para controlar la posición del helióstato construido.
Además, se requiere resolver la relación que existe entre la posición del Sol, la posición del
concentrador parabólico y la geometría y los movimientos mecánicos del helióstato involucrando
leyes físicas, problema que se resuelve mediante el uso de algebra vectorial por la relación
geométrica que se mantiene entre ellos.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>1. Definición del problema:
 Propuesta de automatización del helióstato
2. Diseño a bloques de la propuesta de automatización
 Selección de actuadores:
 6 actuadores
 2 Movimiento primario
 4 Movimiento secundario
 Instrumentación de actuadores
 Fuentes
 Sensores
 Etapa de potencia
 Montaje y ensamblaje de actuadores
 Diseño de control de posición de los actuadores
 Algoritmo de identificación de vector de posición del Sol a partir de una fecha y un
lugar: 2 ángulos relativos de la posición del Sol
 Algoritmo para determinar la posición de los movimientos del helióstato para
reflejar la luz del Sol al concentrador: entrega 6 posiciones angulares
 Posicionar los movimientos de seguimiento del helióstato con los ángulos
obtenidos
 Ciclo continuo de posición de los actuadores</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En el estado del arte existen algoritmos y ecuaciones que pueden solucionar el problema de la
posición de los movimientos primarios del helióstato, sin embargo, estos algoritmos desprecian
algunas distancias físicas del helióstato debido a que las ecuaciones se obtienen por matrices de
rotación que no incluyen desplazamientos ocasionando error en la reflexión y posteriormente se
pretende minimizar ese error con uso de variables de ajuste de la reflexión; por lo que la formulación
de las ecuaciones en base a la formación de los vectores de incidencia y reflexión basándose en
matrices de transformación homogéneas puede resolver el problema sin variables de ajuste y
relacionando todas las distancias físicas existente entre el helióstato y el objetivo.
El modelado matemático se resume en la formación de dos ecuaciones no lineales igualadas a cero,
las cuales deben resolverse con métodos numéricos, siendo indispensable el uso de un controlador
con la capacidad de uso de operaciones matemáticas. En el Capítulo 4 se plantea una metodología de
formulación de ecuaciones para un helióstato con los movimientos de seguimiento Rotación –
Elevación, sin embargo, esta metodología puede ser implementada en helióstatos con otro par de
movimientos, como lo son el Azimut – Elevación, la diferencia radicaría en el planteamiento del eje de
acción de cada uno de los movimientos sobre las matrices de trasformación homogéneas.
Tanto la longitud y la latitud geográfica se deben considerar como parámetros de diseño de los
helióstatos, debido a que los ángulos solares son afectados por éstos. La longitud geográfica
mantienen una relación directa con la hora solar y el ángulo azimutal, afectando la duración del día y a
la velocidad de los movimientos de seguimiento, por otro lado, la latitud geográfica afecta a la amplitud
del movimiento de elevación debido a que mantiene una relación con el ángulo de elevación solar y
éste a su vez cambia su amplitud máxima diaria en relación al día del año.
Debido a que la velocidad del movimiento de rotación y elevación, 4 min/° y 19 min/° respectivamente,
son considerablemente lentas, un controlador On-Off puede producir una curva de control
sobreamortiguada evitando sobreimpulsos en la posición debido a las bajas velocidades a las que
estarán sometidos los movimientos de seguimiento del sistema.
El helióstato automatizado tiene una repetitividad de 1.43 m y una precisión de 0.67 m. Estos valores
son relativamente grandes debido a que la parte mecánica del helióstato tiene un juego mecánico muy
grande que evita que tenga mayor precisión.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN CIENCIA APLICADA
Y TECNOLOGÍA AVANZADA
UNIDAD QUERÉTARO
POSGRADO EN TECNOLOGÍA AVANZADA
“SISTEMA AUTOMATIZADO DE POSICIÓN
PARA UN HELIÓSTATO DE REFLEXIÓN”
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN TECNOLOGÍA AVANZADA
PRESENTA:
CÉSAR DANIEL SÁNCHEZ SEGURA
DIRECTORES DE TESIS:
DR. JORGE PINEDA PIÑÓN
DR. EDUARDO MORALES SÁNCHEZ
QUERÉTARO, QRO. NOVIEMBRE DE 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema colaborativo para el monitoreo de tráfico vehicular</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Hoy en día la mayoría de las personas que cuentan con un vehículo propio, no
necesariamente cuentan con un dispositivo GPS por sí sólo y mucho menos con una
herramienta que ayude a determinar el estado de congestión de las zonas que se desea
viajar. La mayoría de las personas que viaja en su vehículo la mayor parte del día,
quisieran tomar una buena decisión basada en el comportamiento de la ruta en la que se
desea transitar y una manera de tener esa información es cuando la persona ya se
encuentra en el caos vehicular, pero si esta persona compartiera esa información con otros
conductores que, de otra forma tomarían esa ruta; estos podrían buscar alternativas para
evitar los atascos de tránsito.
El problema es contar con esa herramienta intermedio entre el conductor que está en el
congestionamiento y el sistema que ayude a tomar la decisión del comportamiento de la
ruta deseada. El ayudar a otras personas comunicándoles el estado de tránsito en una zona
determinada sería útil para otros conductores para disminuir sus tiempos de recorrido, al
saber un usuario que fue beneficiado por otra persona, lo menos que se hace es colaborar
con la misma información y así poner en marcha el dicho ganar – ganar.
Actualmente existen sistemas para teléfonos móviles que brindan ese tipo de servicios.
Algunos de ellos tienen la capacidad de detectar el tránsito haciendo uso de un
SmartPhone, creando una red social la cual los interesados puedan consultar, la
desventaja de este tipo de sistemas es que el usuario que va en su automóvil tiene que
hacer el reporte de la congestión publicándolo en su red social, el cual es un peligro para
la persona que va conduciendo, ya que puede causar algún accidente u otra cosa de mayor
circunstancia. Otros sistemas sólo dan la mejor ruta basados en distancias cortas, pero
eso no significa que sea la mejor, una buena ruta no es aquella de menor distancia, si no
aquella en la que el tiempo sea el mínimo, el sistema dará un mejor resultado
dependiendo de la ayuda que el usuario proporcione.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar e Implementar un sistema colaborativo móvil para la detección del tránsito
vehicular, de fácil uso para cualquier usuario, que le ayude a tomar una decisión
evitando congestionamientos mediante un análisis histórico de un conjunto de
datos proporcionados por los mismos usuarios.

OBJETIVOS ESPECÍFICOS
 Obtener información a través de un monitoreo móvil-colaborativo.
 Almacenar la información en una base de datos geoespacial, desde el dispositivo
móvil de forma transparente para el usuario.
 Brindar al conductor un entorno gráfico de fácil uso, para que tome la mejor
decisión del viaje a donde se desea dirigir.
 Obtener información sobre tránsito en diferentes lugares, fechas y temporadas.
 Determinar de manera automática los requerimientos del usuario con base en su
ubicación y el tiempo en el que se utiliza el sistema.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Después de analizar la descripción del problema con detalle, la hipótesis planteada en
esta investigación es que a través de esta propuesta se puede solucionar la problemática.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Las razones por las que se propone esta solución es debido a la cantidad de vehículos [2],
la problemática del congestionamiento diario y la pérdida de tiempo por el tránsito
vehicular que existen en la Ciudad de México, ya que sería deseable contar con un sistema
que ayude a determinar en qué zonas existe comportamiento de congestión.
Existen sistemas que determinan el tránsito, pero no existe ningún sistema que le dé
seguimiento a toda esa información reunida, que es lo que se pretende en el sistema móvil
[3] que se desea desarrollar y del que se habla en este documento.
Un sistema como el que se presenta en este trabajo de tesis sería de mucha ayuda para
aquellas personas que transitan en las calles de mayor aglomeración, con el fin de elegir
la ruta más óptima y la que tenga mejor vialidad.
Cabe destacar que el principal objetivo del sistema es sólo mostrar el comportamiento del
congestionamiento, no de calcular una ruta específica, ya que no es objetivo del trabajo
por lo que esa decisión la tomará personalmente el usuario o conductor apoyado en la
información que le brindará el sistema.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En el presente trabajo de tesis hemos presentado una metodología para desarrollar un
sistema colaborativo móvil para la detección de tránsito vehicular, que sirve de apoyo en
la toma de decisiones evitando congestionamientos, mediante un análisis histórico de un
conjunto de datos proporcionados por los mismos usuarios. Dicha metodología consta de
tres módulos: Recolección de datos, Análisis de datos y Resultados.
Como parte del caso de estudio, se proporcionó el sistema a varios compañeros del
laboratorio para realizar las pruebas y el valor que se determinó para el monitoreo de
puntos fué de 3 segundos, porque se obtuvieron mejores resultados al realizar el análisis
y el valor del buffer que se determinó fue de 60 metros de radio, de igual manera que se
obtuvieron mejores resultados.
De los objetivos específicos planteados para la presente tesis, todos los objetivos fueron
cumplidos satisfactoriamente.
 Se realizó una aplicación móvil que contenía la comunicación entre el dispositivo y el
servidor y poder monitorear su ubicación cada cierto tiempo, y almacenada
directamente a la base de datos espacial.
 Se propuso un proceso en backgroud dentro del sistema para que el usuario pueda
hacer cualquier actividad en su teléfono móvil es decir, el usuario no interrumpirá
ninguna actividad externa y el sistema seguirá monitoreando los puntos de forma
transparente.
 Se propuso utilizar tecnologías que ya existían para que la visualización del usuario
fuera de fácil uso, por ejemplo el servidor de Google Maps para la parte de la
visualización del mapa en conjunto con una API propia de Google Maps para dibujar
las líneas y algunas herramientas para crear menús en el sistema operativo android.
Con el fin de que el usuario que utilice el sistema, le sea fácil comprender su
funcionamiento.
 Se definieron varios tipos de consultas para un mejor análisis temporal dentro del
sistema, como por rangos de horarios, días de la semana y por temporadas, este último
fue definido en el sistema como Automático, el cual le brindará al usuario el tránsito
de la zona que este a sus alrededores y el horario en que se realice la consulta por
ejemplo en la mañana, en la tarde o en la noche, cual sea el caso.
La metodología presentada está basada en los conceptos de VGI y de MCS que en conjunto
tienen la ventaja para el enriquecimiento de información geográfica con ayuda de los
teléfonos móviles que hoy en día sirven de sensores para la recolección de información.
Con estos dos conceptos referenciados hacia un modelo de colaboración pueden tener
muchas ventajas para el análisis de algún tipo de interés, extendiendo la forma antigua
en que se hacían lo sistemas, es decir, que cualquier persona pueda aportar con apoyo de
sistemas geográficos móviles información útil para la ayuda de diferentes casos de estudio,
en este trabajo los usuarios comparten su posición ayudados con el sistema colaborativo,
que al final los usuarios que compartan su información serán beneficiados en los resultados
finales, colaborando de forma voluntaria.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Sistema colaborativo para el monitoreo de tráfico
vehicular
TESIS
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN CIENCIAS DE LA COMPUTACIÓN
PRESENTA:
ING.XÓCHITL OLVERA BERNARDINO
DIRECTORES DE TESIS:
DR. ROLANDO QUINTERO TÉLLEZ
DR. JOSÉ GIOVANNI GUZMÁN LUGO
JULIO 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Modelado de las propiedades estilizadas y fractales que se dan en un mercado de valores</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Las crisis financieras de los últimos años ha motivado la necesidad de estudiar e investigar lo
origines, desarrollo y desenlace de las dinámicas del mercado de valores con el objetivo de
poder diseñar mejores estrategias de inversión.
Las series de tiempo financieras empíricas presentan propiedades inherentes que se
manifiestan en la actividad bursátil del mercado. Determinar el mejor momento para
comprar, vender o abstenerse en la transacción de activos en el mercado financiero es
fundamental y es el mayor reto para tener una estrategia de inversión exitosa. Una forma de
obtener conocimiento y de esta forma tomar mejores decisiones es la modelación matemática
del problema, dado que nos permite crear escenarios virtuales del mercado de valores y que a
través de la simulación del modelo podemos ver desenlaces de dichos escenarios.
En el presente trabajo se modela la dinámica de una acción, obteniéndose propiedades
estilizadas y fractales presentes en las series de tiempo financieras empíricas..
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar un algoritmo que basado en autómatas celulares sea capaz de reproducir propiedades
estilizadas y fractales presentes en un mercado de valores, a partir del comportamiento de
agentes inversionistas y tomando en cuenta la oferta, demanda, el ajuste de precio y factores
externos.
1.5.2 Objetivo particular
 Diseñar un algoritmo tal que basado en autómata celular modele la dinámica de
una acción.
 Diseñar un algoritmo que modele la dinámica de tres tipos de inversores:
Fundamentalista, imitador y oportunista.
 Diseñar un algoritmo que reproduzca las propiedades estilizadas y fractales de la
dinámica de un mercado de valores con los 3 tipos de inversores mencionados en
el punto anterior.
 Validar los resultados obtenidos en nuestro modelo con los datos empíricos
publicados a la fecha</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La obtención de propiedades estilizadas y fractales de las series de tiempo financieras pueden
ser obtenidas a partir de la modelación de las interacciones locales entre diferentes tipos de
inversionistas e incorporando factores externos.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Una de las ventajas que ofrece la globalización es el crecimiento de los capitales en las bolsas
de valores que operan en el mundo. Sin embargo, entender la dinámica del mercado es
complejo por el número de factores que intervienen, la interrelación entre ellos, además de
eventos fortuitos que influyen en el precio de las acciones. Actualmente, existen esfuerzos en
modelar y simular el comportamiento del mercado de valores para obtener conocimiento de
su dinámica.
Un análisis sobre las series de tiempo financieras ha mostrado la existencia de propiedades
estilizadas y fractales presentes en los instrumentos financieros y mercados. La mayoría de
estas propiedades estilizadas son contra intuitivos y contrarios a las expectativas de las
teorías financieras tradicionales.
Un sistema complejo tal como un mercado de valores es ideal para ser modelado mediante un
autómata celular, ya que se obtiene una relación directa entre los agentes que operan en el mercado y las células del AC, sus interacciones como reglas de vecindad y la toma de
decisiones como regla de evolución.
El presente trabajo obtiene a través de la modelación microscópica, las propiedades
estilizadas y fractales presentes en los mercados de valores para así, tener una mejor
comprensión sobre la dinámica del mercado.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología seguida en este trabajo de tesis se presenta a continuación, se presentan las
partes principales de la misma así como los elementos que la componen.
 Revisión del estado del arte: modelos no basados en autómatas celulares y modelos
basados en AC, fractales, dimensión fractal
 Diseño del modelo: determinación del tipo de agentes a considerar en la dinámica del
mercado
 Diseño de algoritmos: determinación del comportamiento de cada uno de los agentes
que forman parte de la dinámica del mercado de valores
 Implementación del modelo: implementación de los algoritmos diseñados en el
lenguaje de programación Java
 Validación de resultados: comparación de los resultados obtenidos por nuestro modelo
contra las series de tiempo financieras empíricas del índice Standard&Poor’s500</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Este modelo basado en un autómata celular 2-dimensional puede confirmar los principales
hechos estilizados y fractales observados en series de tiempo financieras empíricas. Las
interacciones de los agentes de largo alcance, las cuales son responsables de grandes
variaciones del precio, pueden formarse a partir de interacciones locales. La agrupación de
volatilidad está asociada con la variación de la actividad de operación de los agentes, un
proceso más lento comparado con la variación en la influencia de las noticias. Las
distribuciones de colas pesadas de retorno están relacionadas tanto a grandes variaciones en
el precio como la agrupación de volatilidad. Finalmente, estas distribuciones non-Gaussianas
son producidas por el comportamiento de los agentes en respuesta a la llegada de noticias,
incluso cuando se asume que la llegada de noticias sigue un proceso aleatorio Gaussiano.
También es importante mencionar que el modelo permite experimentar bajo diferentes
escenarios del mercado mediante la variación de sus parámetros considerando, por ejemplo,
el porcentaje de agentes de cada tipo, la sensibilidad al ajuste del precio, etc., esto siempre
acompañado de medidas que muestran el comportamiento macro: distribución de retornos,
medidas de curtosis y exponente de Hurst. Además, permite observar el comportamiento
global que presentan los agentes a lo largo del tiempo en la dinámica del mercado.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Computación
Modelado de las propiedades estilizadas y fractales que se dan en un mercado de valores 
TESIS
Que para obtener el grado de maestría en ciencias de la computación

Presenta
 Manuel Alejandro Paredes Hernández
Directores de tesis
M. en C. Germán Téllez Castillo
Dr. Mordejai Zvi Retchkiman Konigsberg

México D.F Diciembre de 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Modelo computacional para el diagnóstico de fallas incipientes en un tren de engranes tipo aerogenerador
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>No existe suficiente información sobre los sistemas de monitoreo en el tren de engranes
en aerogeneradores.
• Es complicado obtener mediciones de fallos en un aerogenerador en funcionamiento, por
el tiempo necesario para que se produzcan y el difícil acceso a los mismos.
• Pocos trabajos en la literatura emplean perfiles de velocidad y carga variable.
• La mayoría de los trabajos de investigación realizan perfiles de los principales valores
de velocidad y carga, sin considerar los efectos que existen en las vibraciones mecánicas
para todos los rangos posibles.
• Reducir el tiempo de procesamiento que presentan métodos reportados en la literatura,
lo cual es relevante si la arquitectura del sistema de reconocimiento de patrones utiliza
procesamientos en paralelo o en serie.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un modelo computacional para la identificación de fallos del engranaje paralelo
de un banco de pruebas con la configuración de un aerogenerador comercial.
1.5.2 Objetivos particulares
1. Conocer las características del banco de pruebas, así como los sistemas de medición y
control utilizado.
2. Obtener un perfil de señales de vibraciones con velocidad y carga variables.
3. Realizar el análisis de vibraciones.
4. Obtener los rasgos de las condiciones mecánicas, utilizando técnicas de procesamiento
de señales como son los LPC y el Cepstrum.
5. Comparar el procesamiento de señales empleado contra un método de la literatura que
utiliza la desviación estándar de los coeficientes de un paquete wavelet.
6. Identificar los estados de fallo mediante el uso de un modelo neuronal.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible desarrollar un modelo computacional para identificar las diferentes condiciones mecánicas simuladas en el engranaje paralelo de un banco de pruebas con la configuración de un aerogenerador comercial. Con resultados que igualan a los reportados en la literatura, empleando a diferencia de trabajos previos condiciones de funcionamiento similares a las presentes en un aerogenerador real y comparando el tiempo de procesamiento con respecto a la desviación estándar de los coeficientes de un paquete wavelet.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Para poder obtener rasgos característicos de las vibraciones de un tren de engranes es necesario considerar los efectos que tienen la velocidad y la carga variable en la maquinaria rotatoria.
Muchos métodos de procesamiento de señales han caracterizado eficientemente las vibraciones
mecánicas de máquinas que trabajan bajo estas condiciones. No obstante, la necesidad de
remuestrear la señal, la complejidad de algunos métodos, el poco trabajo que existe en la
literatura con este tipo de perfiles y el tiempo de procesamiento que requieren algunas de
las metodologías reportadas en la literatura. Generan la posibilidad de explorar distintos
métodos que puedan encontrar rasgos de las vibraciones en un menor tiempo y con resultados
de clasificación que puedan igualar o superar a las metodologías reportadas.
El uso de técnicas de procesamiento de señales como son los modelos auto regresivos y
el Cepstrum para obtener rasgos de vibraciones mecánicas han demostrado tener buenos
resultados en trabajos de investigación previos [5, 6, 7]. No obstante, no se han utilizado en
perfiles de velocidad y carga variable como el utilizado en el trabajo de investigación.
Debido al incremento de la complejidad que presenta la multiplicadora de un aerogenerador,
ha surgido la necesidad de utilizar herramientas de diagnóstico no convencionales para poder
realizar una clasificación efectiva de fallos incipientes o estados de deterioro en sus engranes.
Los modelos computacionales son uno de los métodos no convencionales más utilizados por su
capacidad de trabajar con problemas que tienen soluciones no lineales; como ejemplos de ellos
se encuentran las redes neuronales y la lógica difusa [8], las cuales han tenido muy buenos
resultados en la clasificación de este tipo de problemas.
El análisis de las vibraciones y el procesamiento de señales se realiza con el uso de MATLAB,
por ser un programa que cuenta con un entorno poderoso para el cálculo numérico, la
visualización y la programación. Entre otras de las funcionalidades es posible analizar datos y
desarrollar algoritmos utilizando las herramientas y las funciones matemáticas incorporadas.
El modelo computacional propuesto se desarrolla principalmente en la plataforma y entorno
de desarrollo LabVIEW, el cual trabaja con programación visual gráfica, siendo un programa
enfocado hacia la instrumentación virtual que además permite una fácil integración con
tarjetas de adquisición de datos, técnicas de procesamiento de señales y puede trabajar
simultáneamente con MATLAB. 
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología utilizada para solucionar la problemática de la investigación consiste en:
• Documentación del análisis de vibraciones en un tren de engranes que opera bajo
condiciones de velocidad y carga variable
• Conocer los sistemas de medición y de control utilizados en el banco de pruebas.
• Generar un perfil de datos con todas las condiciones de par y velocidad a partir de una
base de datos que se realizó en un trabajo previo en el banco de pruebas, el cual evaluó la
calidad de las mediciones bajo las condiciones en la que la simulación fue desarrollada.
• Utilizar métodos de procesamiento digital de señales para el análisis de vibraciones.
• Obtener rasgos característicos de las distintas condiciones mecánicas simuladas en el
modelo físico mediante el uso del Cepstrum, los LPC y la desviación estándar de los
coeficientes de un paquete wavelet.
• Diagnosticar las condiciones mecánicas simuladas mediante redes neuronales artificiales
con la mejor arquitectura posible.
• Evaluar los resultados de las fases anteriores y desarrollar un instrumento virtual que
realice el procesamiento y clasificación de las diferentes condiciones mecánicas. 
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se desarrolló un modelo computacional para la identificación de fallos del engranaje paralelo
de un banco de pruebas con la configuración de un aerogenerador comercial.
Las señales empleadas en el presente trabajo fueron obtenidas de una plataforma de pruebas
adaptada en el centro tecnológico CARTIF, de la cual se simularon diferentes niveles de
desbalance y des alineamiento, bajo condiciones de velocidad y carga variable similares a las
condiciones que aparecen en un tren de engranes de un aerogenerador.
Para poder trabajar con las vibraciones mecánicas fue necesario conocer las características del
banco de pruebas, así como los sistemas de medición y control utilizados, para posteriormente
realizar un análisis de vibraciones y aplicar los métodos para la extracción de rasgos.
Para caracterizar a las diferentes señales se emplearon dos métodos que han tenido buenos
resultados en reconocimiento de fallas mecánicas en otros trabajos, pero que no han utilizado
un perfil de condiciones como el estudiado. Se demostró la eficacia de los métodos de
procesamiento utilizado con un pequeño número de coeficientes (20 cepstrales y 15 LPC)
en comparación con un método que emplea la desviación estándar de 16 coeficientes del
´ultimo nivel de un paquete wavelet de cuatro niveles.
Los patrones obtenidos para los LPC y el Cepstrum se localizan en los armónicos de la señal
con mayor energía, característica que les permite con un pequeño número de coeficientes
obtener excelentes resultados de clasificación comparándolos con los del WPT.
Para comprobar el rendimiento computacional resultante, se realizó un estudio de los tiempos
de procesamiento para las técnicas estudiadas. Este estudio demostró la capacidad de reducir
el tiempo de procesamiento de un método reportado en la literatura que hace uso de la
desviación estándar de los coeficientes de un WPT, incluso si no se considera el tiempo que se
requiere para realizar la interpolación de Hermite; necesaria para el procesamiento del paquete
wavelet.
Con el uso de redes neuronales artificiales MLP y RBF para los LPC y los coeficientes
cepstrales respectivamente, fue posible identificar las diferentes condiciones mecánicas
simuladas, con resultados que igualan a los reportados en la literatura
El modelo propuesto surge como una herramienta alternativa en la detección de fallas incipientes en un tren de engranes.
Se verifico la hipótesis, se cumplió el objetivo general y los objetivos específicos planteados en
la tesis. El modelo computacional propuesto será empleado como base para trabajos futuros
en el estudio de diferentes fallas mecánicas, así como alternativas del mismo para la mejora del
rendimiento en el procesamiento y la clasificación con el uso de diferentes modelos neuronales. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL ´
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
MODELO COMPUTACIONAL PARA EL DIAGNÓSTICO DE FALLAS INCIPIENTES EN UN TREN DE
ENGRANES TIPO AEROGENERADOR
TESIS
PARA OBTENER EL GRADO DE:
MAESTRÍA EN CIENCIAS EN INGENIERÍA DE COMPUTO CON ´
OPCIÓN EN SISTEMAS DIGITALES ´
PRESENTA:
ING. EDGAR ALAN VALDÉS IGLESIAS
DIRECTORES DE TESIS:
DR. LUIS P. SÁNCHEZ FERNÁNDEZ
DR. JOSÉ J. CARBAJAL HERNÁNDEZ
MÉXICO, D.F. DICIEMBRE DE 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Modelado de criptosistemas usando autómatas celulares y transformaciones no afines</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>La seguridad de la información desde la antigüedad ha sido un problema a resolver, los
primeros intentos de ocultar la información fueron hechos por Julio Cesar, ´el uso una
sustitución de letras en sus cartas que enviaba a sus legiones. A mediados del siglo
veinte hubo un gran avance cuando este problema atrajo la atención de los matemáticos
y computólogos quienes hicieron una teoría formal de los sistemas criptográficos. Hoy
en día, debido al incremento en tecnología; las redes de computadoras juegan un papel
importante en la comunicación, por tal razón el problema de seguridad de la información
sigue vigente. Las grandes industrias necesitan mantener su información segura, los
bancos nacionales e internacionales deben asegurar que los canales de comunicación al
efectuar transacciones sean seguros de terceros que intentan robar o alterar la información,
la milicia debe asegurar la integridad de la información, y la comunicación entre
dos personas a través de dispersivos móviles o a través de redes sociales debe ser segura.
En computación existe la clase de complejidad P y NP. Los problemas que son tratables
o solubles eficientemente; es decir, los problemas que se pueden resolver en tiempo polinomial,
pertenecen a la clase P, mientras que los problemas que no se pueden resolver
eficientemente en tiempo polinomial pertenecen a la clase NP.
El modelado y simulación bajo el enfoque de los Autómatas Celulares es una de las
metodologías usadas para el estudio de sistemas complejos. Para nuestro problema de
seguridad de la información vamos a modelar y simular un sistema criptográfico usando
el enfoque de autómatas celulares, añadiendo operaciones sobre estructuras algebraicas,
además de incorporar la instancia de un problema que pertenece a la clase NP. 
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar un criptosistema de llave privada con cifrado por bloques de 128 bits, usando
Autómatas Celulares de una dimensión y dos dimensiones, incorporando cuatro niveles
de transformación a cada bloque.

Objetivos particulares
  Diseñar un autómata celular de una dimensión grupo sobre GF (28
).
  Diseñar un autómata celular de dos dimensiones reversible.
  Diseñar los cuatro niveles de transformación.
  Diseñar un isomorfimo entre elementos del autómata celular de una dimensión y elementos
del autómata celular reversible de dos dimensiones.
  Diseñar los algoritmos para realizar el proceso de cifrado y descifrado de texto.
  Diseñar una interfaz gráfica que permita al usuario la manipulación del sistema.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Una combinación adecuada de herramientas matemáticas permitirá diseñar un criptosistema
robusto para cifrar y descifrar texto, donde a pesar de que el texto plano a cifrar
presente periodicidad, el resultado ser ‘a un texto cifrado donde sus caracteres presentan
una función de distribución casi uniforme.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Si utilizamos un modelo para simular un sistema criptográfico basado en autómatas celulares, podremos ver a cada símbolo contenido en la información a cifrar como una célula que perene al autómata celular. Los autómatas celulares evolucionan en etapas de tiempo discreto, permitiendo en cada evolución incrementar el nivel de seguridad de la información. Si, además incorporamos la estructura algebraica de campo finito y distintos niveles de transformación para que los autómatas trabajen sobre ellos, esto implicara que el modelo basado en autómatas celulares, aumentará la seguridad de la información para evitar que sea alterada o leída. Invertir un autómata celular reversible de dos dimensiones es un problema que se encuentra en la clase de problemas NP [17], por lo tanto si nosotros integramos en alguno de los niveles de transformación un autómata celular reversible de dos dimensiones, entonces en orden de desvelar la información, sin conocimiento alguno del funcionamiento del sistema criptográfico, se deber ‘a resolver la instancia de este problema, por lo cual el tiempo requerido para romper el criptosistema ser ‘a exponencial, incrementando así la seguridad de la información.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En base a los objetivos planteados al inicio de este trabajo, llegamos a las siguientes
conclusiones:
Utilizar AC implica una gran cantidad de computo debido a la iteración de cada una
de las células que contiene, ya que se necesita aplicar la función de transición célula
a célula junto con sus vecinos respectivos, multiplicado por el número de iteraciones.
Por lo tanto, si nosotros incrementamos el número de células, el radio de la vecindad y
además, el número de iteraciones, entonces el tiempo de cálculo también se incrementará.
Para efectos de cifrar información de manera más rápida, usando nuestro modelo, es
recomendable especificar un número pequeño de evoluciones a los AC.
Los cuatro niveles de transformación que se aplican a cada uno de los bloques del texto
plano agregan una mayor seguridad al sistema. Es posible incrementar más niveles de
cifrado, pero esto perjudicaría en el tiempo requerido por el sistema para cifrar, debido
a la gran cantidad de cálculos. Por otro lado, si eliminamos uno o más niveles de cifrado
seguimos obtenidos resultados satisfactorios, pero el sistema contiene más vulnerabilidades.
Al analizar las frecuencias de aparición de los caracteres que conforman a los textos
planos y los caracteres de sus respectivos textos cifrados, especialmente los textos que
contienen un alto ´índice de periodicidad en su contenido, nos permite concluir que el
modelo propuesto elimina la periodicidad en el texto cifrado, incorporando una distribución
casi uniforme en la frecuencia de aparición de los caracteres.
Los autómatas celulares tienen la propiedad de que soportan un paralelismo inherente.
Por tal razón, un trabajo futuro, explotando esta propiedad, es implementar los AC del
modelo propuesto usando técnicas de programación paralela y aumentar el rendimiento
del criptosistema. Por otro lado, un trabajo parecido en ´esta dirección, es implementar
el criptosistema en su versión hardware usando dispositivos lógicos programables. 
El resultado más importante de este trabajo es el software que permite cifrar y descifrar
textos. Debido a que este criptosistema trabaja a nivel de bits, podemos incorporar la
opción de cifrar imágenes para mejorar este trabajo.
Amoroso en [5] investigo posibles reglas reversibles para un AC de una dimensión. Por
lo cual, una posibilidad para seguir este trabajo de investigación es usar los resultados
obtenidos en [24] sobre la caracterización de AC sobre GF(2p) 1-dimensional y encontrar
reglas reversibles sobre ´esta clase de autómatas.
En [10] ampliaron los resultados de la caracterización de AC sobre GF (2) de una dimensión
a dos dimensiones. Si tomamos un enfoque más teórico, entonces extender la
caracterización para AC de dos dimensiones sobre GF (2p) será una buena línea de investigación.
En modelo del presente trabajo, usamos la extensión de campo GF (28). Se puede
trabajar con una extensión de campo para p mayor que ocho y analizar los resultados
del comportamiento del criptosistema.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional  
Centro de Investigación en  
Computación 
Modelado de criptosistemas usando autómatas
celulares y transformaciones no afines
Tesis que presenta:
Joel Noyola Bautista
Que para obtener el Grado de:
Maestría en Ciencias de la Computación
Director:
M. en C. German Téllez Castillo
México, D.F. diciembre de 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Implementación de algoritmos Criptográficos sobre un GPU en grandes cantidades de información
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Hoy en día el tratamiento de información crítica es cada vez más importante, por la
expansión de los sistemas informáticos.
La información necesita ser asegurada contra amenazas potenciales.
Se requieren sistemas que procesen los algoritmos criptográficos en un tiempo más rápido
sin generar un costo en hardware elevado.
Se conoce en [7] que la tendencia en los próximos 20 años es desarrollar sistemas tanto
hardware como software paralelo que ayuden a optimizar el procesamiento de la información.
Las GPUs son un ´área de oportunidad en el desarrollo de sistemas que puedan implementar
paralelismo.
Está demostrado en [6, 17–19], por citar solo algunos trabajos de investigación, que las
GPUs ayudan a reducir el tiempo de procesamiento en varias ´ordenes de magnitud comparado
con su implementación en CPUs cuando trabajan con grandes cantidades de información.
La criptografía es uno de los campos en que se pueden trabajar con las GPUs.
Aunque existen ya implementaciones criptográficas realizadas sobre GPU, ninguna ha
considerado aplicar la implementación sobre las vulnerabilidades encontradas en [1].
Se genera una nueva implementación de algoritmos de cifrado con un rendimiento mucho
mayor. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Implementar un algoritmo criptográfico estandarizado modificado, utilizando una tarjeta
procesadora de gráficos, lo que permitirá realizar comparativas de rendimiento con otros algoritmos
criptográficos estandarizados.

Objetivos específicos.
1. Implementar el método de un algoritmo secuencial modificado para criptografía simétrica
utilizando la herramienta CUDA y aplicándolo en una GPU.
2. Generar una nueva comparativa entre implementaciones convencionales e implementaciones
paralelas.
3. Contar con un criptosistema alternativo que cifre de manera paralela y forma eficiente
para el cifrado en grandes cantidades de datos.
4. Mostrar el estudio formal realizado en la implementación en las GPUs. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Desde siempre el ser humano ha tenido la necesidad de proteger la información confidencial
utilizando diferentes técnicas de ocultamiento, de ahí que cifrar signifique ocultar.
Poco a poco, estas técnicas fueron robusteciéndose buscando que la vulnerabilidad fuera
nula o casi nula, hasta llegar hoy en día a sistemas de cifrados complejos como lo son el
AES (2001) y Salsa20 (2008).
Las aplicaciones en investigación que utilizan GPUs, están teniendo cada vez un mayor
auge debido a la velocidad para realizar operaciones aritméticas y lógicas.
Aunque los estándares de cifrado simétrico por bloques AES y DES son seguros y eficientes
en términos de tiempo y requerimientos de memoria, la cantidad de información que
debe ser cifrada simultáneamente en ocasiones, nos da tiempos de respuesta que inadecuados.
Dado que los cifrados por bloques conllevan en el mismo paso el mismo proceso, aplicado
a diferentes bloques de texto plano, la aceleración usando una GPU es el siguiente paso
lógico para información que se conoce de antemano, o que se procesa en tiempo real en
grandes cantidades.
Uno de los retos en este documento es el de generar el proceso de paralelización de un
algoritmo diseñado inicialmente para trabajar en secuencial, esto nos permitirá tomar
en cuenta algunas consideraciones a observar cuando en trabajos futuros se paralelicen
paradigmas computacionales de diversas  áreas de investigación.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>La historia nos ha demostrado constantemente que la seguridad es una ilusión, que cuando
creemos que un sistema es inviolable y teóricamente seguro, aparecen nuevas herramientas y
metodologías que nos hacen dar cuenta que esto no es así. Esto genera un nivel de desconfianza
en general ya que con la información que estos  últimos meses ha salido a la luz sobre el espionaje
masivo, no solo a compañías y gobiernos, sino a nivel global general, no se puede confiar en
nada y nadie con algún nivel de certidumbre.
Lo anterior nos ha obligado a volvernos más desconfiados, y tratar de crear sistemas que
permitan disminuir esa sensación de incertidumbre y de paranoia, o, nos obligan a investigar a
fondo los sistemas actuales planteados como seguros y corregir aquellas secciones que pudieran
volverse debilidades frente a un ataque específico.
En el presente trabajo se implementó una metodología de paralelización de los algoritmos
criptográficos DES y Triple-DES con una modificación en la función de generación de las llaves
ronda o llaves programadas que soluciona una vulnerabilidad encontrada con el criptoanálisis
del teorema LR, dando como resultado una implementación más eficiente que sus implementaciones
convencionales seriales, en tiempo de procesamiento, pero conservando la complejidad
del algoritmo cuando se enfrente a ataques de tipo fuerza bruta.
Al ocupar la unidad procesadora de gráficos de los equipos de cómputo se corrobora que se
pueden realizar diversas optimizaciones de implementaciones seriales, sin generar un coste de
hardware elevado para su puesta en marcha, ocupando elementos integrados en la mayoría de
los equipos de cómputo personales. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Computación
Laboratorio de Simulación y Modelado
T ITULO:
“Implementación de algoritmos Criptográficos sobre un
GPU en grandes cantidades de información.”
“Implementation of cryptographic algorithms on a GPU in large amounts of
information.”
TESIS.
QUE PARA OBTENER EL GRADO DE
MAESTRIA EN CIENCIAS DE LA COMPUTACION.  
PRESENTA EL INGENIERO
V ICTOR ANTONIO RUIZ IBAÑEZ. 
Directores:
Dr. Juan Luis Díaz de León Santiago.
Dr. Juan Carlos Chimal Eguía.
Junio 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Modelado de tránsito y optimización del flujo vehicular en paralelo
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Las sociedades modernas requieren contar con sistemas eficientes de transporte que permitan llegar
a los destinos deseados a tiempo y de manera segura. Desafortunadamente la infraestructura vial
existente cuenta con recursos limitados, algunos de ellos con capacidad reducida, y además el
incremento de sus usuarios es una constate diaria; por lo tanto, es necesario encontrar soluciones y
contar con estrategias que permitan la toma de decisiones para prevenir y minimizar la congestión
de tránsito vehicular.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Proponer un modelo de tránsito vehicular que permita modelar y optimizar el comportamiento de
flujo de tránsito vehicular a partir de una representación de ecuaciones de flujo dentro de un grafo
dirigido.
Objetivos Particulares
 Analizar la teoría concerniente al modelado de flujo de tránsito vehicular.
 Estudiar los modelos continuos de flujo vehicular.
 Proponer un modelo de flujo de tránsito vehicular que modele el flujo dentro de un grafo
dirigido con capacidades máximas y mínimas.
 Proponer un método de optimización multiobjetivo para el flujo de tránsito vehicular del
modelo propuesto.
 Validar los resultados del modelo propuesto.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El problema del transporte es un problema que ha concernido al hombre mucho antes, incluso, de la
llegada del automóvil; pero ha sido en años recientes que el tema de los congestionamientos viales
ha adquirido una mayor importancia. Hoy en día, el constante aumento de vehículos en la
infraestructura vial y la mala planificación de las vías de transporte han propiciado
congestionamientos en varios puntos de las grandes urbes, y los embotellamientos de vehículos se
han convertido en uno de los grandes problemas que hay que afrontar a diario; aunado a esto se
tienen otras repercusiones tanto en la calidad de vida de las personas como en el medio ambiente,
por mencionar algunos encontramos el aumento en los costos de operación de los vehículos
(gasolina, mantenimiento, etc.), incremento en el número de accidentes, aumento de la emisiones de
agentes contaminantes, presencia de estrés en los conductores y agravamiento de la calidad del aire.
A pesar que los problemas de movilidad en las ciudades son claros, las posibles soluciones a estos
son contradictorias y se encuentran en etapas prematuras, es aquí donde destaca el importante papel de la investigación. Construir simplemente más caminos no es la solución, y como lo enuncia [63]
en su ley fundamental del congestionamiento en autopistas: «la demanda latente se expande para
ocupar el espacio creado siempre que se mejora la capacidad de las carreteras». Un ejemplo de esto
se puede encontrar en la paradoja de Braess para el tránsito vehicular [6], la cual menciona:
«ampliar una red vial causará una redistribución del tránsito vehicular que resultará en mayores
tiempos individuales de viaje», dicho de otra forma, conductores individuales no coordinados
siguiendo sus estrategias óptimas personales no siempre logran el estado más benéfico para el
tráfico en su conjunto.
Es por ello que surge la necesidad de estudiar y desarrollar modelos concernientes al flujo de
tránsito vehicular que permitan entender las causas y consecuencias de los fenómenos que en éste se
presentan, y a su vez, permitan contar con estrategias para la toma de decisiones y así prevenir y/o
minimizar los congestionamientos vehiculares.
La optimización de la infraestructura vial requiere reproducir situaciones de tránsito que permitan
realizar pruebas de investigación, lo cual es complicado, pues produciría situaciones difíciles como
congestionamientos, accidentes viales, etc. Ha sido a través de las investigaciones y estudios
realizados por varios científicos que se han obtenido propiedades del tránsito y modelos que
resultan útiles para realizar simulaciones, por medio de las cuales, es posible establecer y manipular
variables y condiciones del entorno y conocer el efecto que esto tendría en el comportamiento del
tránsito vehicular en una determinada región; debido a lo anterior es que actualmente se le da gran
importancia al modelado y simulación de flujo de tránsito de vehículos, ya que, permite obtener
datos sobre su dinámica sin necesidad de interferir en el lugar mismo.
En el presente trabajo propone un modelo de flujo de tránsito vehicular que permita modelar el
comportamiento del tránsito vehicular a partir de una representación en un sistema de ecuaciones de
flujo dentro de un grafo direccionado.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con base en los experimentos realizados se comprobó que el modelo propuesto permite
optimizar el comportamiento del tránsito vehicular. Asimismo, permite la predicción del
comportamiento del tránsito ante determinadas situaciones.
 La construcción del grafo dirigido a partir de una red vehicular permitió definir direcciones
de tránsito, asignar capacidades de flujo y pesos a las vialidades.
 El uso de las ecuaciones de flujo dentro del grafo ayudó a definir el comportamiento del
tránsito vehicular entre un par de nodos.
 Se mejoraron los modelos estudiados en el estado del arte al proponer un modelo que
optimiza el flujo vehicular considerando la existencia de contraflujos en determinadas
vialidades.
 La interpretación de los resultados obtenidos de la aplicación del modelo permitió detectar
puntos o secciones donde se presentan congestionamientos viales.
 La aplicación del modelo permite generar estrategias adecuadas para la optimización del
flujo de tránsito, la detección temprana de embotellamientos y la predicción de tráfico a
corto plazo en los sistemas dinámicos de navegación asistida.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros> INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
“Modelado de tránsito y optimización del
flujo vehicular en paralelo”
TESI S
QUE PARA OBTENER EL GRADO DE :
MAESTRÍA EN CIENCIAS DE LA COMPUTACIÓ N
PRESENTA :
ING. JONATAN ÁLVAREZ MÉNDE Z
DIRECTOR DE TESIS :
DR. RENÉ LUNA GARCÍ A
MÉXICO. D.F. DICIEMBRE 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Motor de juegos 3D para una plataforma móvil</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Como se ha descrito en párrafos anteriores, el desarrollo de un motor de juegos permite a los
programadores concentrarse en la lógica del juego, dejando a un lado detalles de
implementación de bajo nivel. El problema principal se centra en la elaboración de dicho
módulo de software para el sistema operativo iOS.
Actualmente, existen varios productos destinados a esta tarea, la mayoría de los cuales son de
tipo propietario, por lo cual es necesario la adquisición de derechos de las aplicaciones para
poder realizar desarrollos sobre de ellos.
Por cuestión de practicidad (en la obtención y generación de los recursos), los motores existentes
centran su atención en el desarrollo de videojuegos en 2D, imposibilitando el uso de modelos 3D
dentro de los proyectos de software. Finalmente, los motores de juego actuales de mayor uso
corren y trabajan sobre frameworks que se encuentran sobre la capa de lenguaje nativo de las
plataformas sobre las que operan, restringiendo al desarrollador la depuración de los errores.
Aunado a estos problemas, el desarrollo de motores de juegos en tres dimensiones sobre
plataformas móviles, es un campo poco explorado actualmente. Existen motores 3D complejos
como Unity[13], los cuales son costosos, y cuentan con IDEs independientes de la plataforma, lo
que implica al desarrollador el uso de herramientas nuevas para el desarrollo. Existen también
frameworks 3D como Cocos3D[14], el cual es software de código abierto, pero no se encuentran
bien documentado. El principal problema de desarrollar un videojuego 3D actualmente, es
escoger un motor de juegos 3D que resulte conveniente para el desarrollo del mismo.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar e implementar un motor de juegos 3D, que permita el desarrollo de aplicaciones sobre
el sistema operativo iOS (versión 7. x).
1.5.1 Objetivos Particulares
1. Implementar la arquitectura de un motor de juegos 3D reducido y estable††.
2. Diseñar e implementar un mecanismo de renderizado en base a modelos tridimensionales
independiente del motor que pueda ser escalable y optimizado.
3. Definir, crear y documentar una serie de funciones y módulos de software que permitan
el mantenimiento y puesta en marcha de la máquina de estados de OpenGL, definidos
sobre el lenguaje Objective-C.
4. Crear una capa Middleware, que permita la exposición de funciones de renderizado de
gráficos, basadas en primitivas de OpenGL, hacia el lenguaje Objective-C.
5. Crear una aplicación sencilla utilizando el motor de juegos desarrollado, con el fin de
mostrar las funcionalidades del mismo.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Actualmente el desarrollo de videojuegos ha entrado en un auge global. El fácil acceso a las
computadoras personales, y la reducción de costos en las consola de videojuegos, ha fomentado
que esta industria crezca de manera exponencial. Hace apenas un par de décadas, era poco
común poseer una consola de última generación, hoy en día es un sistema de entretenimiento
que pocas veces falta en cualquier hogar. A la par de este crecimiento, el surgimiento y rápida
evolución de los teléfonos inteligentes acompañó a esta tendencia.
Los tiempos donde sólo altos ejecutivos tenían acceso a telefonía móvil ha quedado atrás. Hoy en
día, es común que hasta niños pequeños cuenten con estos dispositivos, y sean capaces de
utilizarlos de manera adecuada. Este tema ha resultado de interés comercial, pero también
dentro de la comunidad científica. Por ejemplo, la Universidad de Denver[15] ha lanzado
programas de licenciatura relacionados totalmente con videojuegos, particularmente los
programas Bachelor of Science in Animation and Game Development y Bachelor of Art in Animation and
Game Development.
Poco a poco la comunidad científica cuenta el desarrollo de videojuegos como un campo de las
ciencias de la computación altamente importante, dado el campo de proyección que tienen las
habilidades que implica este conocimiento.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>El trabajo presenta como objetivo el motor de juegos, y, en base a los resultados obtenidos y la
implementación descrita, se concluye que el desarrollo del mismo se logró de manera adecuada.
En las secciones anteriores se han descrito las debilidades y fortalezas del trabajo desarrollado,
sin embargo, no presentan fallas estructurales fuertes, que comprometan el correcto
funcionamiento del motor como tal. Dentro del proceso de desarrollo se pueden presentar las
siguientes conclusiones:
1. Los mecanismos de renderizado de un motor de juegos deben contar con un alto nivel de
interacción con las capas superiores. Al analizar los elementos presentes en OpenGL,
observamos que se provee al desarrollador con un número elevado de directivas y
comandos para renderizar modelos tridimensionales, todo ello en base a una máquina
de estados (contenida en el bucle de actualización-renderizado). En un motor de juegos,
observamos la necesidad de crear una arquitectura a base de objetos, que permita
traducir estas directivas a mecanismos más simples y de más fácil acceso a los
desarrolladores de videojuegos.
2. Los motores de juegos son orientados a modelos tridimensionales específicos. El
presente trabajo demostró su validez haciendo uso de un formato de objeto
tridimensional no utilizado en motores de juegos actuales (el objeto obj). Con esto, se
demuestra que los mecanismos de renderizado son específicos a cada formato de archivo
en el cual se genera el modelo tridimensional.
3. El desarrollo sobre la tecnología OpenGLES se realiza de manera transparente en el
sistema operativo iOS. Al iniciar este desarrollo, se plantea como sistema operativo
objetivo el sistema iOS 7.x. Esta elección se basa en la posibilidad de compilar código c++
(sobre lo cual está basado OpenGL) utilizando el mismo compilador utilizado por el IDE
de desarrollo XCode (gcc). El apéndice 1 muestra la simplicidad con que se logra inclusión
de los archivos de configuración necesarios para desarrollar un juego utilizando el
motor, así como la ejecución de un proyecto prueba. Como contraparte, desarrollar el
presente trabajo en el sistema operativo Android, exige el conocimiento de las
herramientas de compilación necesariar para generar los archivos binarios del motor, y
la ejecución de los mismos en base al NDK (Native Development Kit) presente en la
plataforma Android.
4. Las superficies OpenGL manejan un sistema de referencia diferente a las vistas presentes
en Cocoa Touch. En el desarrollo del trabajo, se encontraron inconsistencias referentes a
los sistemas coordenados presentes en las superficies OpenGL, en relación a los señalados
en el framework Cocoa Touch.
Estas consideraciones deben ser tomadas en cuenta para interactuar con mecanismos de
nivel sistema operativo (por ejemplo, la salida del acelerómetro debe ser convertida a un
sistema coordenado inverso en los 3 ejes coordenados, de manera que los incrementos
presentes en CocoaTouch, son decrementos en la superficie OpenGL).
En las secciones posteriores, se describirán los logros alcanzados, las contribuciones del presente
trabajo, así como los trabajos a futuro y mejoras al sistema.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN
COMPUTACIÓN
Motor de juegos 3D para una plataforma móvil
Tesis
que para obtener el grado de
Maestría en Ciencias de la Computación
presenta:
Ing. Raymundo Pescador Piedra
Directores de tesis:
M. en C. Sergio Sandoval Reyes
y
Dr. José Giovanni Guzmán Lugo
Laboratorio de Comunicaciones y Redes de Computadoras
 México D.F. Octubre de 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Esquema de cifrado para la transmisión de video en sistema operativo LINUX</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Las aplicaciones para transmitir video requieren que su flujo sea continuo siempre
que el receptor lo solicite. Por lo tanto, no hay conceptos de playback de la
secuencia de vídeo (es decir, que se ejecuta en directo). Un cifrado convencional
está diseñado para datos genéricos, los cuales no soportan requerimientos específicos
para aplicaciones de video, por lo tanto hay muchos desafíos para la seguridad en
multimedia, tales como:
El tamaño natural de los datos multimedia después de la compresión suele ser muy grande, incluso si se utilizan las mejores técnicas de compresión disponibles. Por ejemplo, el tamaño de un vídeo con compresión MPEG-1 de dos horas es de aproximadamente 1 GB. Por lo cual no se puede cifrar en su totalidad debido a que afectaría la percepción del usuario. Las futuras aplicaciones de multimedia deben ser ejecutadas en directo en los procesos, como el videostreaming. El rendimiento del procesamiento de flujos multimedia debe ser aceptable (es decir, limitado por un determinado valor de retardo). Las técnicas de cifrado deben tomar un tiempo corto y que solo presenten pequeña sobrecarga en comparación con las técnicas de compresión. Además, el cifrado para aplicaciones multimedia en los dispositivos móviles cuenta con recursos limitados los cuales requieren de algoritmos eficientes con un mínimo procesamiento que permita el cifrado y la transmisión de video sin afectar la inteligibilidad del mismo. Actualmente no hay un esquema de cifrado para la transmisión de video en directo que cumpla con las características anteriormente mencionadas y que se acople con los recursos de un dispositivo móvil.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar un esquema para la transmisión de video con servicio de confidencialidad
para el S.O. LINUX, utilizando algoritmos de cifrado simétrico, para la protección
contra la posible escucha de información por terceros.
Objetivos Particulares
Analizar las estructuras de codificación de video para identificar una estrategia
para la protección de la confidencialidad de éste.
Analizar y proponer una plataforma de desarrollo que brinde similares
prestaciones que un dispositivo móvil.
Diseñar un esquema de cifrado que opere bajo el algoritmo AES para cifrar y
descifrar información de video.
Construir una plataforma el esquema de cifrado que eficientice el uso de los
recursos de procesamiento disponibles en la plataforma seleccionada.
Evaluar el esquema de cifrado en términos de desempeño y seguridad. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El uso de transmisión de video posibilita reuniones y trabajo cooperativo entre especialistas en diferentes áreas, a pesar de las posibles restricciones geográficas. Aunado a esto, algunas agencias gubernamentales, organismos de defensa nacional utilizan servicios de transmisión de video. Estas transmisiones están relacionadas con el uso de información privada, por lo que la confidencialidad de los datos que se transmite a través de la red puede verse comprometida, debido a que los datos se propagan en Internet y es posible el robo de datos. Sí un tercero tiene éxito en el robo de dicha información, entonces esa persona puede supervisar todas las actividades que puedan incluir información importante en ella. De ahí que se hace necesaria la implementación de sistemas criptográficos en video y la confidencialidad de los datos debe tenerse en cuenta. El constante surgimiento de nuevas tecnologías de procesamiento de datos, como son los dispositivos móviles, impone también de manera continua, rigurosos requisitos hacia los procesos e infraestructura de seguridad. Debido a que la capacidad en el nivel de procesamiento es limitada, queda disponible la creación de esquemas cuya única función sea asegurar los datos del video transmitido. En razón de que actualmente no existen esquemas de cifrado para transmisión de video que se acoplen a dispositivos móviles, es necesario realizar la investigación en este tema.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El método de investigación del presente trabajo se estableció de la siguiente
forma:
Establecimiento de los problemas a resolver.
Definición del objetivo principal y particulares.
Investigación sobre los esquemas de cifrado de video que se han realizado
anteriormente al igual que aplicaciones con estos.
Diseño del esquema de cifrado a utilizar.
Construcción de la cama de pruebas para el esquema.
Análisis de los resultados obtenidos.
Reporte de los resultados obtenidos del esquema de cifrado. 
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Teniendo en cuenta el objetivo general y los objetivos particulares que se
plantearon con anterioridad se concluye que:
Se presentó el proceso de diseño de un esquema de cifrado, para la
transmisión de video utilizando sistemas embebidos BeagleBoard-xM como
nuestro dispositivo móvil. Para ello se hizo uso de los diferentes periféricos
y procesadores que esta plataforma de desarrollo que posee y la interfaz de
programación de Ubuntu.
Se analizó la estructura del video H.263+, para no cifrar todos los frames
completos del video. Se decidió cifrar solamente los macrobloques del video,
esto permitió que el procesamiento de cifrado fuera más rápido y se aseguro
la confidencialidad de la información del video, debido a que los macrobloques
contienen la información más importante del video.
Se demostró que el algoritmo AES en modo contador funciona en aplicaciones
para transmisión de video en vivo. Permitiendo que la transmisión fuera segura
y sin tanto tiempo en retardo.
Se comprobó que al hacer uso del DSP de la tarjeta Beagleboard-xM
disminuiría el consumo del uso de su CPU, por lo que demostramos que es
eficiente el esquema de cifrado propuesto.
Se demostró que el esquema de cifrado para video propuesto es competitivo
con esquemas presentados en el capítulo 2 y al funcionar en el sistema
Beagleaboard-xm el cual presenta características similares a un dispositivo
móvil, se comprobó que puede funcionar para dispositivos móviles que se
encuentran en el mercado.
A continuación se presenta como trabajo futuro los siguientes puntos que tengan
como parte de partida el esquema de cifrado en la transmisión de video:
Poder implementar el esquema de cifrado en un dispositivo móvil, como son
los teléfonos celulares, y así utilizar el DSP de estos.  Probar el esquema en condiciones diferentes, es decir, otra resolución,
utilizando otros códec's y disminuir el tamaño de datos cifrados.
Como producto final de la investigación se entrega un esquema de cifrado que se
podrá utilizar en transmisiones de video, siendo que este funcionara en un sistema
con las mismas características que se presentaron en el desarrollo de la investigación. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO
NACIONAL
CENTRO DE INVESTIGACIÓN EN
COMPUTACIÓN
Esquema de cifrado para la
transmisión de video en sistema operativo LINUX
Tesis que presenta
Ing. Mónica García Cortés
Que para obtener el grado de
Maestro en Ciencias en Ingeniería de Cómputo
con Opción en Sistemas Digitales
Directores de Tesis
Dr. Moisés Salinas Rosales.
Dr. Raúl Acosta Bermejo.
México D.F. Julio 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Confidencialidad en VoIP para el sistema operativo Android.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>A pesar de las ventajas que ofrece la posibilidad de utilizar aplicaciones de VoIP en dispositivos móviles, existe un principal inconveniente del uso de esta tecnología y son los problemas de seguridad que la afectan, el motivo principal es que esta tecnología se conecta a través de la gran red de Internet y tanto el software como el hardware que la componen son similares a las redes utilizadas hoy en día. Por tal motivo VoIP se vuelve vulnerable a ataques intermediarios, en inglés (Man in the Middle Attack MITM). La información que se transmite a través de la red pública de datos durante una llamada puede ser victima de software capaz de capturar, reconstruir y/o modificar este tipo de conversaciones. Las implementaciones estándar de VoIP ofrecen numerosas oportunidades para los hackers entre ellas: Escuchas ilegales y grabado de llamadas. Robo de información confidencial. Modificación de llamadas telefónicas. Al ser VoIP una tecnología que se apoya en otras capas y protocolos ya existentes, hereda ciertos problemas de las capas y protocolos sobre los que está construido, siendo estos algunas de las amenazas más importantes de VoIP. El cifrado que implementan aplicaciones existentes a fin de resolver estas problemáticas, no se apoyan en los demás componentes de hardware que componen al dispositivo, sino que basan sus esquemas en la ejecución de procesos sobre el mismo componente, el procesador de propósito general, lo que resulta en la generación de retardos en la transmisión de la voz.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar un esquema de cifrado eficiente, capaz de proveer confidencialidad a
comunicaciones de VoIP para el S.O. Android, utilizando algoritmos de criptografía
simétrica para la protección contra posibles escuchas de terceros de información.
Objetivos Particulares
Seleccionar una aplicación de VoIP que opere bajo el S.O. Android y posibilite la
modificación de los protocolos de transmisión.
Analizar los principales codecs utilizados en la transmisión de VoIP para seleccionar un
caso de estudio.
Diseñar un esquema de cifrado para glujos de VoIP con base en el algoritmo simétrico
AES.
Evaluar la implementación del esquema de cifrado de VoIP en términos de seguridad y
rendimiento. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Las vulnerabilidades de los sistemas de información, en particular las heredadas por los
sistemas de comunicaciones de voz IP y la necesidad de diseñar esquemas de cifrado eficientes,
que permitan que el cifrado de la voz no impacte en la inteligibilidad de la llamada y que
ademas permitan brindar servicios de confidencialidad sin inyectar tiempos de retardo mayores
a los ya propuestos, justifican la realización de este proyecto en el área de la de computación.
En el mundo actual, la movilidad es omnipresente, sobre todo para aquellos sectores
empresariales y gubernamentales que requieren que día a día se mejore la productividad
de la organización. En la actualidad, aprovechar la facilidad con que se tiene acceso a un
dispositivo móvil y, sobre todo, aquellos que cuentan con el S.O. Android permiten la adopción
de tecnologías de la información como aplicaciones avanzadas de voz y video, sin menoscabo
de la seguridad, que contribuyan al mejoramiento de la eficiencia de las organizaciones.
Actualmente Android es el sistema operativo mas utilizado en los dispositivos móviles,
con un mercado del 56 % y alrededor de 400,000 aplicaciones disponibles para su descarga en
Google Play [18]. La aceptación de estos dispositivos ha sido sumamente alta y al contar con
capacidades de cómputo no muy lejanas a las de una computadora de escritorio permiten el
despliegue de aplicaciones de VoIP en las cuales se pudiera manejar información importante.
Es por ello que el presente trabajo busca como finalidad atender las vulnerabilidades que existen en los sistemas de información, particularmente en las comunicaciones de voz que
se despliegan sobre las redes de datos y en sistemas móviles con S.O. Android, con la finalidad
de optimizar los procesos de cada organización brindando comunicaciones seguras y de bajo
costo computacional.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>El trabajo que se presenta se basa en el método hipotético-deductivo, se realiza un análisis
detallado de la problemática a fin de presentar la mejor solución que resuelva dicho problema.
Así pues, se presentan las contribuciones esperadas y se realiza un análisis de todas las variables
que pudieran afectar dicho proceso, se lleva acabo un diseño en base a ciertos requerimientos y
se realiza la construcción de un prototipo que permita evaluar el diseño del esquema de cifrado.
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En el presente trabajo fue realizado un esquema de cifrado capaz de proveer confidencialidad
a los flujos de datos que se transmiten durante una llamada de VoIP. El esquema fue diseñado
para operar sobre una aplicación que funciona bajo el sistema operativo Android haciendo
uso del codificador de voz G.711. El esquema de cifrado opera en la plataforma de desarrollo
Beagleboard-XM Rev C, y a fin de hacer que el uso de los recursos con los que se cuenta sea
más eficiente, el algoritmo de cifrado simétrico AES se ejecuta en el procesador digital de señales.
Las pruebas de seguridad realizadas para analizar el comportamiento del esquema de cifrado
y el nivel de confidencialidad que éste provee constataron que la manera en que se diseñó el
esquema de cifrado no genera vulnerabilidades en el sistema ni fugas de información, dado que
la fuente de información cuando se cifra, se comporta como una fuente de información aleatoria.
Fue obtenido un esquema de cifrado eficiente, que minimiza el retardo de tiempo inyectado
aproximadamente en 2 ms menos que si se cifrará en el procesador de propósito general. El
esquema de cifrado mejora los rendimientos de consumo de CPU en comparación a esquemas
de cifrado propuestos por las investigaciones realizadas con anterioridad, indicadas en el
capítulo dos.
Las principales aportaciones que otorga el presente trabajo son las siguientes:
Un esquema de cifrado seguro y eficiente que hace uso del DSP con el que se cuenta en la
plataforma de desarrollo, esto demuestra que el uso de los recursos con los que se cuenta
mejora el esquema de cifrado considerablemente sin afectar la seguridad del mismo.
Un esquema de comunicaciones que provee una solución a los problemas de memoria
compartida que existen entre dos programas donde uno de ellos hace uso del DSP,
esta solución permite que dos programas independientes puedan comunicarse de manera
transparente. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN
COMPUTACIÓN
Confidencialidad en VoIP
para el sistema operativo Android.
Tesis que presenta
Ing. Joel Martínez Ortuño
Para Obtener el Grado de
Maestría en Ciencias en Ingeniería de Cómputo
con opción en sistemas digitales
Directores
Dr. Moisés Salinas Rosales.
Dr. Raúl Acosta Bermejo.
México D.F. Julio de 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Análisis de opinión en redes sociales utilizando conceptos antónimos en grafos
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Dentro del área de Análisis de Opinión, existen trabajos que clasifican distintos documentos o textos procedentes de diferentes fuentes, en donde la gran mayoría utiliza técnicas de clasificación y algún corpus previamente etiquetado. Estos trabajos sólo se enfocan en encontrar su polaridad y no en clasificar otros aspectos, además de que aquellos que utilizan opiniones dentro de redes sociales, lo hacen en Twitter y relativamente muy pocos en alguna otra. Ningún otro trabajo, hasta donde sabemos, hace uso de semejanza o distancias entre palabras para realizar una clasificación de textos. En este trabajo, proponemos un método que, tomando opiniones obtenidas de alguna red social, no sólo entregue su polaridad, sino que se consideren otros aspectos para cuantificar con qué magnitud están relacionadas y así clasificarlas. Todo esto, mediante el uso de la base de datos léxica WordNet y sin necesidad de utilizar algún corpus. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Identificar la relación de una opinión en inglés escrita en alguna red social, con cada uno de los pares de antónimos de una lista, utilizando la base de datos léxica WordNet. 
Objetivos particulares 
• Encontrar opiniones en redes sociales acerca de diferentes productos, personas o servicios, que reflejen características propias. 
• Identificar algunos de los aspectos que reflejan las opiniones y determinar las listas de pares de antónimos para realizar pruebas posteriores. 
• Realizar diferentes experimentos con las distintas medidas de semejanza de las palabras propuestas. 
• Determinar el mejor método que refleje con certeza la relación de las opiniones con los pares de antónimos propuestos para cada tema.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>	Cuando la gente escribe opiniones acerca de un determinado producto, servicio o persona, lo hace tomando en cuenta diferentes características que lo describen, como tamaño, precio, forma de entretenimiento, velocidad de movimiento o procesamiento, etc. Todo esto lo realiza dependiendo de qué sea sobre lo que esté dando su opinión, por lo que en el trabajo que proponemos, nos gustaría ser capaces de poder representar cómo la gente expresa sus opiniones de productos, servicios o personas tomando en consideración los atributos que los describen de manera individual. En este documento, proponemos la comparación entre diferentes conceptos organizados en pares de antónimos y textos. Con esto obtenemos más información de dichos textos, permitiéndonos cuantificar los atributos que los describen y así clasificarlo en el grupo representado por el atributo que tenga mayor relación.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>A pesar de que WordNet es un recurso léxico que se utiliza en una amplia variedad de aplicaciones y trabajos, éste tiene algunas limitaciones. Una de ellas es que muchas veces no es posible aplicar las definiciones de sentidos que se enlistan para cada una de las palabras que contiene, ya que son muy detalladas y la mayoría no se utilizan con mucha frecuencia en el mundo real. Esto provoca que muchas palabras contengan una amplia y extensa aplicabilidad en su uso, haciendo que cada uno de sus sentidos se relacionen a su vez con una gran cantidad de conceptos, y éstos, a formar más uniones entre los nodos del grafo principal, por lo que palabras con significados totalmente opuestos estén muy cercanas dentro del grafo. Por ejemplo, para las palabras good y bad, la trayectoria más corta tiene una distancia de cuatro aristas. Para el método propuesto en el presente trabajo, la limitación principal en el uso de WordNet fue que no todas las palabras están unidas al grafo principal. Esto es un gran inconveniente, ya que muchas de ellas pueden aportar un gran valor a los experimentos realizados, pero no es posible aplicar el proceso para obtener la distancia entre dos palabras. Por ejemplo, la palabra beautiful no tiene ninguna relación de sinonimia con otra palabra al contar solamente con dos sentidos, a pesar de que para un criterio humano, puede ser sinónimo de pretty o por lo menos, tener una trayectoria relativamente cercana una con otra. Esto quiere decir que hay que ser muy cuidadosos al momento de elegir los conceptos organizados en pares de antónimos, para medir la distancia con cada una de las palabras de los textos a probar. El conjunto de textos seleccionados para el sistema debe de reflejar información subjetiva, es decir, implicar una opinión de quien lo escribe, y no información objetiva, ya que no tendría caso obtener la relación entre conceptos o clasificar dichos textos. Las redes sociales son utilizadas diariamente por millones de personas en todo el mundo, por lo que sería relativamente sencillo obtener opiniones de los productos o servicios que ofrecen las empresas. Sin embargo, cuando se realiza una determinada publicación, la mayoría de la gente no expresa una opinión útil al producto, pero cuando lo hace, el número de palabras que utiliza es muy corto, incluyendo una o dos oraciones muy sencillas. Esto provoca que sea complicado obtener opiniones que aporten retroalimentación clara al producto. Las opiniones que funcionan mejor para aplicar el proceso, son aquellas que realmente reflejen aspectos o características del producto, persona o servicio. En muchas opiniones, utilizan una gran cantidad de palabras que no reflejan o aportan alguna relación con los conceptos a medir. Es por ello que el método implementado trabaja de una mejor manera cuando no contiene demasiadas palabras, ya que todas se consideran para obtener su distancia y por lo tanto influyen en el resultado mejorándolo o perjudicándolo. Cuando se utilizan opiniones subjetivas, utilizando palabras que sí reflejen atributos de los objetos y no una gran cantidad de palabras, se obtienen los mejores resultados. Con respecto a las medidas JCN y RES, al no admitir éstas adjetivos en la comparación de semejanza, se utilizaron los sustantivos derivados de los adjetivos en los experimentos para comparar las opiniones de los usuarios. Desafortunadamente, al hacer esto puede perderse cierta fidelidad con respecto a la comparación original, pues ciertos sustantivos no pueden formarse directamente a partir del adjetivo, como en el caso de costly y expensiveness. El aplicar nuestro método utilizando solamente sustantivos y comparar con estas dos medidas, no fue posible de manera directa porque algunos de dichos sustantivos no estaban unidos al grafo principal de WordNet. Sin embargo, para mostrar que los resultados no son lo suficientemente buenos utilizando solamente sustantivos, incluimos aquí los cálculos de ambas medidas de semejanza, además como referencia para lo que tendría que hacerse si se deseara realizar un trabajo similar utilizando las medidas existentes definidas en WordNet. Ya que estas medidas no admiten adjetivos y nuestro método sí, podemos concluir que es muy importante tomarlos en cuenta por lo que obtenemos mejores resultados que con las medidas tradicionales existentes. Para el caso de la medida HSO, es posible realizar los experimentos utilizando las cuatro categorías gramaticales; sin embargo, podemos observar en la tabla 19, que su desempeño es menor comparando con nuestro método que incluye la medida de semejanza propuesta. Al realizar los experimentos utilizando solamente adjetivos, se pudo comprobar que es mejor considerar todas las palabras. Esto es porque muchas opiniones contienen relativamente una cantidad pequeña de adjetivos por lo que no es muy conveniente probar el método con unas cuantas palabras. Se obtuvieron buenos resultados, incluso mejores que utilizando todas las palabras, cuando los adjetivos que se utilizan en las opiniones son los mismos que se encuentran en la lista de conceptos pares, por lo que los resultados numéricos que representan la relación entre pares de conceptos y texto, se separa, demostrando una mayor relación del texto con uno de ellos. También podemos observar que en la mayoría de las variaciones, la relación numérica es muy cercana. Esto se debe a que muchos conceptos antónimos, están muy cercanos dentro del grafo principal por lo que sus trayectorias con cada una de las palabras del texto, contienen casi el mismo número de aristas. La variación Orientación Semántica Individual fue el mejor método de las cinco variaciones, ya que fue la que obtuvo mejores resultados que las demás. La ventaja de esta variación se debe a que se normaliza por la distancia entre pares de conceptos antónimos, por lo que considera solamente a las palabras más cercanas, es decir, aquellas que contienen una distancia no mayor a la de sus conceptos a medir, y descartando aquellas más lejanas o con mayor distancia. Es por esto que podemos observar que es mejor si las opiniones son más cortas y con mayor número de palabras que reflejan sus atributos, ya que las demás variaciones al considerar muchas y con trayectorias muy grandes, perjudicaban algunos de sus resultados. Dado que la mayoría de las variaciones arrojaron valores numéricos no muy alejados entre sí, podemos concluir que el método no es muy conveniente para asignar una relación entre cada concepto de los pares de antónimos y el texto, ya que siempre existirá una trayectoria con una distancia entre conceptos y en algunos casos muy similar. Sin embargo, sí podemos decir que el uso de distancias en el grafo de WordNet, creado a partir de relaciones de sinonimia, es un método bueno para clasificar textos en una gran variedad de grupos que demuestren sus atributos, y no sólo clasificar de acuerdo a su polaridad.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL 
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN 
ANÁLISIS DE OPINIÓN EN REDES SOCIALES UTILIZADO CONCEPTOS ANTÓNIMOS EN GRAFOS 
TESIS QUE PARA OBTENER EL GRADO DE MAESTRO EN CIENCIAS DE LA COMPUTACIÓN 
PRESENTA: ING. MARCO ANTONIO MARTÍNEZ BEDOLLA 
DIRECTOR DE TESIS: DR. FRANCISCO HIRAM CALVO CASTRO 
“LA TÉCNICA AL SERVICIO DE LA PATRIA” 
MEXICO D.F. MAYO 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Reconocimiento de fallas en motores de inducción mediante patrones orbitales de vibraciones
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Aunque hoy en día existen diversas herramientas para el análisis de vibraciones aplicadas a los
motores de inducción, la mayoría se basan en métodos matemáticos muy complejos y muy pocas usan
métodos de inteligencia artificial para reconocer fallas en máquinas rotatorias, lo que hace necesaria
la presencia de un analista de vibraciones que interprete y emita un diagnóstico adecuado para su
posterior mantenimiento. Claro está que este diagnóstico no está exento de errores humanos. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un modelo computacional que permita el diagnóstico de fallas mecánicas de
desbalance, desalineamiento y torbellino de aceite en motores de inducción, mediante el análisis
de vibraciones y el reconocimiento de patrones.

Objetivos Específicos
i. Definir las fallas de los motores de inducción empleando el análisis de órbitas.
ii. Diseñar e implementar un sistema de adquisición de datos.
iii. Preprocesar las señales adquiridas.
iv. Elaborar las órbitas de las señales adquiridas.
v. Simular y elaborar un conjunto de pruebas y validación.
vi. Diseñar el modelo computacional de reconocimiento de fallas
vii. Realizar pruebas experimentales.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible modelar fallas de remolino de aceite, desbalance y desalineamiento en motores de
inducción eléctrica mediante el uso de señales provenientes de acelerómetros piezoeléctricos,
empleando el análisis de órbitas. Asimismo, es posible identificar dichas fallas mediante el uso de un
clasificador neuronal empleando las órbitas características del remolino de aceite.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Se investigó acerca de las
principales fallas en
motores eléctricos.

Se analizaron las
frecuencias que generan
las fallas en los motores.

Se construyó un software
simulador de fallas en los
motores eléctricos.

Todas las fallas son
simuladas mediante sus
órbitas generada.

Los valores que se
obtienen son guardados
en un archivo con
extensión .txt

Se realizó una base de datos
que se utilizara como
conjunto de entrenamiento
para la red neuronal. 

A partir de señales reales
tomadas de motores en
funcionamiento se crea una
base de datos de prueba.

Mediante el programa
LABView se crea una red
neuronal que consta de 256
neuronas en la entrada, 50
neuronas en la capa
intermedia y 3 neuronas en
la capa de salida

Se obtienen los resultados
y las fallas son clasificadas
segun con el objetivo que
se haya identificado.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>1. Se cumplieron los objetivos, tanto general como en los específicos, planteados y
mencionados en el capítulo 1, a su vez quedo demostrada la hipótesis, ya que no solo se
pudieron modelar las fallas mediante las señales que capturan los acelerómetros
piezoeléctricos, sino que también se pudieron modelar las órbitas que generan cada una
de las fallas y mediante un clasificador (red neuronal) se pudieron identificar dichas
fallas.
2. Mediante el hardware que se diseñó en este trabajo se brinda una opción más a la
industria de poder realizar un programa de mantenimiento a sus motores con un costo
bajo y sin la necesidad de contar con un experto en vibraciones.
3. En la actualidad los sistemas de detección de fallas utilizan preprocesamiento de señales
difíciles de realizar o los algoritmos de detección de fallas que utilizan suelen ser
complicados para el usuario, a comparación del algoritmo de preprocesamiento y el
algoritmo de detección de fallas que se utilizó en este trabajo.
4. El análisis de fallas de señales reales, mediante el análisis de órbitas, nos permitió
conocer cómo es que se forman las órbitas y por lo tanto, crear un software que es capaz
de simular las fallas mediante puntos de coordenadas y que estos últimos se guarden en
un archivo de texto para poder elaborar bases de datos de prueba o de experimentación
con diversos niveles de ruido.
5. Mediante el diseño de una red neuronal como clasificador se observó que el porcentaje
de error en las pruebas experimentales es mínimo además de que sin importar el número
de fallas que se quieran identificar lo único que se deberá hacer es aumentar o disminuir
el número de neuronas de la capa intermedia.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
“Reconocimiento de fallas en motores de inducción mediante
patrones orbitales de vibraciones"
T E S I S

QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN CIENCIAS EN INGENIERÍA
DE CÓMPUTO CON OPCIÓN
EN SISTEMAS DÍGITALES.
P R E S E N T A
ING. GABRIEL LONGORIA CORDERO

DIRECTOR DE TESIS:
DR. LUIS PASTOR SÁNCHEZ FERNÁNDEZ
DR. JOSÉ JUAN CARBAJAL HERNÁNDEZ
MÉXICO D.F., 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Especificación de un marco de trabajo para la implementación de aplicaciones geoespaciales en la WEB	</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los SIG se componen de una integración organizada de software, hardware y
datos geográficos, diseñados para capturar, almacenar, manipular, analizar y
desplegar en todas sus formas la información geográficamente referenciada. Para
la integración de todos estos componentes en la web, se requiere de distintas
herramientas que diversas compañías se han dedicado a desarrollar, tales como
API y Frameworks para ayudar en la elaboración de SIG, pero esto mismo ha
conducido a que hoy en día existan una gran cantidad de estas herramientas,
haciendo que al desarrollador se le dificulte la selección, la integración y
configuración de las mismas.
Por otra parte al trabajar con SIG se encuentran dificultades, como pueden ser las
configuraciones de los servicios, de base de datos, el acceso a mapas y servidores
para la visualización de los mismos. Demasiados conceptos, pocas guías orientadas
a usuarios sin experiencia, hacen que el desarrollo de estos sistemas sea complejo y
requieran de una extensa investigación.
La idea en este trabajo es permitir que los usuarios puedan desarrollar sistemas
geográficos de forma sencilla, pudiendo hacerlo sin tener experiencia previa en el
tema y sin un estudio profundo del mismo, facilitando de forma considerable el
desarrollo de prototipos funcionales completos.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Generar una herramienta que agilice y facilite el desarrollo de sistemas de
información geográfica basados en la web, evitando que el desarrollador tenga que
profundizar en los conceptos entorno a la información geoespacial, así como en las
configuraciones de las distintas plataformas necesarias; que además sea de alta
interoperabilidad con otros frameworks para la web.

Objetivos Particulares
 Realizar el desarrollo de este framework integrando múltiples plataformas de
desarrollo web existentes en el mercado de software libre o de código
abierto.
 Proponer una Arquitectura Genérica para las aplicaciones web SIG creadas
con la herramienta propuesta cumpliendo con los estándares de la Open
Geospatial Consortium (OGC) para lograr interoperabilidad y
comunicación entre las distintas tecnologías que trabajan con información
geográfica.
 Desarrollo de una interfaz web (front end2) para la creación de las
aplicaciones Web SIG y así minimizar el esfuerzo en cuanto a escritura de
código de programación.
 Implementar una aplicación, como caso de estudio, con datos geográficos de
vialidades y transporte de la Ciudad de México utilizando el framework
desarrollado en esta tesis con la finalidad de verificar el desarrollo de una
aplicación Web SIG.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Actualmente cuando se desarrolla un SIG basado en la web, el desarrollador
normalmente gasta una gran parte de tiempo en la instalación, configuración e
interconexión de las diferentes API, plataformas, BDE (Bases de Datos Espaciales)
necesarias para lograr el completo funcionamiento de un SIG. Por ese motivo, se
necesita de un entorno completo de trabajo que no requiera de muchas
configuraciones ni instalaciones y que el tiempo usado se centre sólo en el
desarrollo del propio SIG.
Por otra parte, al elaborar un SIG es esencial el trabajo con datos geográficos, o
comúnmente llamados datos geoespaciales, con lo cual el desarrollador se
encuentra con distintas áreas de estudio que a menudo son extensas, pero son
imprescindibles para el análisis, manipulación, captura y despliegue de dichos
datos geoespaciales. Dicho lo anterior, el problema aparece cuando se necesita
destinar bastante tiempo al estudio y comprensión de esas áreas para el correcto
desarrollo de un SIG. Es por ello que se plantea el desarrollo de la tesis aquí
presentada con la finalidad de obtener desarrollos de SIG sobre la web de una
manera más ágil y fácil. </Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En este capítulo se describe la metodología propuesta para llevar acabo la
implementación del framework para desarrollo de aplicaciones Web SIG.
Retomando los conceptos descritos en capítulos anteriores, partiremos de que las
aplicaciones Web SIG que generará el framework serán de una arquitectura de 3
capas (presentación, lógica de negocios y datos). En esta metodología se describe a
su vez como se integrarán las distintas herramientas, ya existentes, y además que
son OpenSource tales como el servidor de mapas GeoServer, la biblioteca de
despliegue de mapas OpenLayers, la base de datos Postgres con su extensión para
el manejo de datos espaciales PostGIS, entre otras.
En el siguiente capítulo se muestran las conclusiones del trabajo haciendo énfasis
en las aportaciones del trabajo, los alcances y limitaciones del mismo; así como los
trabajos a futuro, tomando como base el trabajo de esta tesis.
La información geográfica se está convirtiendo poco a poco en un elemento
importante en los sistemas informáticos.
Muchas aplicaciones se están desarrollando para tareas industriales,
administrativas y de investigación en los que la información geográfica es el
componente central.
Sin embargo, la información geográfica es un tipo especial de información que no
puede ser representada, manipulada y visualizada usando los métodos que se
utilizan tradicionalmente con otro tipo de información. La información geográfica
requiere métodos especiales de análisis y modelado.
La primera contribución de este trabajo es un análisis de las características
especiales que hacen que los sistemas de información geográfica difieran de los
sistemas de información tradicionales.
Además, a pesar de que la OGC y la ISO han trabajado en las especificaciones de
las normas relativas a la información geográfica y han sentado las bases para la
generación de aplicaciones SIG y herramientas de desarrollo que son capaces de
apoyar en los distintos niveles del desarrollo, actualmente no existe una
arquitectura genérica para los Web SIG que tenga en cuenta la especial naturaleza
y características de la información geográfica y, al mismo tiempo los requisitos
bien conocidos para los sistemas de información de propósito general. </Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En el siguiente capítulo se muestran las conclusiones del trabajo haciendo énfasis
en las aportaciones del trabajo, los alcances y limitaciones del mismo; así como los
trabajos a futuro, tomando como base el trabajo de esta tesis.
La información geográfica se está convirtiendo poco a poco en un elemento
importante en los sistemas informáticos.
Muchas aplicaciones se están desarrollando para tareas industriales,
administrativas y de investigación en los que la información geográfica es el
componente central.
Sin embargo, la información geográfica es un tipo especial de información que no
puede ser representada, manipulada y visualizada usando los métodos que se
utilizan tradicionalmente con otro tipo de información. La información geográfica
requiere métodos especiales de análisis y modelado.
La primera contribución de este trabajo es un análisis de las características
especiales que hacen que los sistemas de información geográfica difieran de los
sistemas de información tradicionales.
Además, a pesar de que la OGC y la ISO han trabajado en las especificaciones de
las normas relativas a la información geográfica y han sentado las bases para la
generación de aplicaciones SIG y herramientas de desarrollo que son capaces de
apoyar en los distintos niveles del desarrollo, actualmente no existe una
arquitectura genérica para los Web SIG que tenga en cuenta la especial naturaleza
y características de la información geográfica y, al mismo tiempo los requisitos
bien conocidos para los sistemas de información de propósito general. 
La principal contribución de este trabajo es una propuesta de una arquitectura
genérica de 3 capas cuyo diseño está basado en el análisis de las características
especiales de la información geográfica. Esta arquitectura cumple con las
especificaciones y estándares con los que las distintas organizaciones han
establecido.
Otro principal aporte ha consistido en la creación de una interfaz web desde donde
el desarrollador puede crear las aplicaciones Web SIG con la arquitectura ya
mencionada y sin tener que escribir una sola línea de código, logrando así un
desarrollo ágil de aplicaciones Web SIG completas y funcionales.
Como caso de estudio se ha presentado el proceso del desarrollo completo de un
Web SIG utilizando la herramienta de este trabajo y con información geográfica de
vialidades y transporte del Distrito Federal, y también se ha desarrollado la misma
aplicación con algunos de los frameworks comerciales, y se han comparado y
analizado los resultados en cuanto a tiempos y facilidad de desarrollo,
comprobando que la herramienta ayuda a agilizar la creación de aplicaciones Web
SIG.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
Especificación de un marco de trabajo para la implementación de
aplicaciones geoespaciales en la WEB
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS DE LA COMPUTACIÓN
PRESENTA:
VALDEZ HERNÁNDEZ FRANCISCO JAVIER
DIRECTORES DE TESIS:
Dr. Miguel Jesús Torres Ruiz
Dr. Rolando Quintero Téllez
Junio 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Diseño y simulación de una microbomba MEMS para flujo de sangre en un sistema de microcanales</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Dentro de la tecnología basada en MEMS encontramos dispositivos como los
microcanales que son utilizados para el transporte, filtro o dosificación de fluidos; por
ejemplo para llevar a cabo una filtración; en los microcanales se pueden construir
microfiltros, con los que se puede separar estructuras solidas del fluido, debido a la
diferencia de tamaños.
Con los microcanales se pueden construir sistemas para el análisis o filtrado de la
sangre ya sea para detectar alguna célula o alguna proteína en específico [3]. Este tipo
de dispositivos están diseñados para separar las células del plasma de la sangre; y utilizan microcanales para la transportación de la misma. Existen diversos métodos de
separación de los elementos formes o células de la sangre como glóbulos rojos o
glóbulos blancos, del plasma; uno de ellos esta bioinspirado en el sistema de filtración
glomerular de la sangre llevada a cabo por los riñones del ser humano, en el cual es en
la nefrona donde se generan grandes cargas eléctricas negativas para la atracción de los
elementos glomerulares, aunque también ductos como las fenestraciones por el tamaño
de sus diámetros filtra algunos solutos [4].
La mayoría de estos dispositivos llevan a cabo esta función de filtración sin el uso de
algún elemento externo, como por ejemplo una bomba, la cual podría usarse para
promover el movimiento del fluido. En este sentido, para estas dimensiones, en los
fluidos se consideran para su análisis y comportamiento dinámico, las fuerzas capilares
y las propiedades físicas tanto del fluido (por ejemplo su densidad) como de los
materiales con que está fabricado el dispositivo; ya que estas son las que tienen mayor
interacción con el fluido, lo anterior es lo que provoca el movimiento del líquido en los
microcanales de estos dispositivo; al estudio de estos fenómenos se le llama
microfluídica [3].
No obstante lo anterior para realizar las pruebas y calibración de dichos diseños, la
mayoría de las veces se utilizan bombas externas que son fabricadas con dimensiones
superiores a la de los dispositivos y, están diseñadas para realizar la caracterización de
los sistemas, no para formar parte de ellos.
En la actualidad las investigaciones a nivel mundial están enfocadas a la integración de
dispositivos fabricados con tecnología MEMS, que en su conjunto logren llevar a cabo
un análisis completo de las muestras que reciban procedentes del ser humano (en
especial la sangre), de tal forma que puedan diagnosticar algún tipo de enfermedad o
dar la mayor información posible del análisis realizado; a este tipo de dispositivos se
les ha dado el nombre de lab-on-a-chip (LOC). Aunque ya existen dispositivos
portátiles que detectan proteínas o virus, todavía no se les puede considerar LOC´s, por sus tareas específicas, además de requerir equipo externo para la interpretación de los
datos obtenidos [3].
En la búsqueda de la integración de estas partes, se han encontrado algunas
dificultades, por lo que para facilitar el estudio se han dividido diversas líneas de
investigación, una de ellas las microbombas y los microcanales por separado, en estas
últimas investigaciones se han encontrado problemas, como el daño a algunas células
de la sangre; principalmente por el uso de altos voltajes, cargas eléctricas, el calor en
contacto con las estructuras de microcanales, entre otras. Es por esto que se han
desarrollado diversos trabajos enfocados a la disminución del voltaje y la temperatura
usado en estos diseños.
Por otra parte se cuenta con el diseño de un sistema de microcanales basado en la
filtración cross flow [4], que disminuye el daño a las células de la sangre, se basa en el
principio de que las células de la sangre viajan en el centro y el plasma en las paredes
del canal que lleva la sangre. El rendimiento de la filtración de dicho dispositivo
depende de la tendencia que tenga el fluido a mantener sus células en el centro y el
plasma en el perímetro, una de las formas de conseguir esto, es impulsar la sangre con
la mayor velocidad posible inclusive mayor a la que se genera con las fuerzas capilares
o presión provocada por la propia densidad de la sangre.
La propuesta de esta tesis es diseñar un dispositivo basado en tecnología MEMS que
promueva el flujo de sangre en los microcanales, diseñando un actuador que resuelva el
tema de la integración (en este caso que se pueda unir a un sistema de microcanales),
hacer más autónomo dicho sistema, y que provoque el menor daño a las células de la
sangre; todo esto con un enfoque lab-on-a-chip; que facilite posteriores integraciones a
sistemas más complejos.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>OBJETIVO GENERAL
Diseñar y simular una microbomba basada en tecnología MEMS para promover el flujo
de sangre en un sistema de microcanales para la separación de elementos formes del
plasma de la sangre.

OBJETIVOS PARTICULARES
1. Aprender el funcionamiento de los diferentes dispositivos usados para promover
el flujo de líquidos, ya sea para dosificación o, filtración; en microcanales.
2. Estudiar la tecnología usada en los actuadores MEMS (en específico las
microbombas).
3. Proponer varios diseños de microbombas basadas en tecnología MEMS que
puedan acondicionarse al sistema de microcanales para la filtración de la
sangre; y elegir la mejor opción.
4. Reducir el voltaje de consumo del dispositivo en comparación con otros por lo
menos menor a 40 volts.
5. Proponer un diseño que sea integrable a un sistema de microcanales específico.
6. Simular el diseño para verificar su funcionamiento.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>A lo largo de esta tesis se plantean distintas formas de construir una microbomba con
las características antes mencionadas, todas ellas basadas en fenómenos físicos, algunos
de ellos, como lo es la mecánica, neumática, electrostática, entre otras, al igual que en
los artículos consultados se busca hacer una mejora para resolver la problemática
discutida anteriormente. Algunos de los planteamientos hechos son los siguientes.
 La reducción de voltaje disminuye el calor provocado por el dispositivo

 Evitar que las cargas eléctricas entren en contacto directo con el fluido, y así
reducir la posibilidad de cambiar las propiedades del mismo.
 No utilizar el voltaje para calentar alguna cámara de aire (principio de
termoneumática), con el objetivo de disminuir el contacto directo del calor
producido por el dispositivo con el fluido.
 La reducción del dispositivo provoca la reducción de la fuerza necesaria para
mover un fluido, y se manifiesta en una reducción del voltaje usado; además
que se facilita en el futuro la integración al cuerpo humano.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En la actualidad se publican artículos que presentan diseños de dispositivos MEMS en
particular de micromotores, microbombas y microcanales [5-8]; donde se presentan
avances, pruebas tanto a dispositivos mejorados, como a nuevos diseños; dichos
avances y pruebas pueden ser de resistencia, transferencia de calor, reducción del
voltaje de alimentación, solución a un problema en particular (como detección de
alguna proteína), entre otras. Lo anterior corrobora lo mencionado a que es una
investigación actual y, de relevancia para el futuro; cabe mencionar en este sentido que
es poca la investigación realizada y publicada en México acerca de los MEMS, sobre
todo en la línea de microfluidos; siendo una materia en desarrollo en nuestro país, pero
con mucho crecimiento y futuro.
En México ya hay laboratorios donde es posible fabricar algunos dispositivos basados
en tecnología MEMS, incluyendo algunos diseños de microcanales, pero aún son pocos
los diseños y trabajos realizados, que aprovechen dichas instalaciones; por lo cual es
necesario empezar ya con este tipo de investigación, que puede repercutir académica y
socialmente. Los beneficios que pudiera traer esto son a varios niveles, por un lado el
económico ya que aunque es una materia en crecimiento, hay dispositivos en el
mercado que están generando grandes ganancias [1]; de tal forma que resulta mejor
tener nuestros propios diseños para vender y no tener que importarlos.
Por otro lado el objetivo de estas investigaciones es alcanzar una mejor salud en las
personas, facilitando el diagnostico, proporcionando mayor información que pudiera
ser relevante al médico, prevención y detección de enfermedades, en fin los alcances
son amplios, inclusive por qué no, en un futuro curar enfermedades que hasta el día de
hoy no lo son. Por supuesto que aún falta más desarrollo tecnológico en los
dispositivos, por lo cual es necesario continuar trabajando en nuevos dispositivos que
contribuyan a alcanzar estos objetivos, buscar nuevas soluciones a las necesidades
existentes y a los problemas que de esta se deriven.
En base a lo anterior se ha decidido diseñar un dispositivo con tecnología MEMS,
porque existe una necesidad, en este caso, de promover el flujo de sangre a
microcanales para ayudar a la filtración de la misma; y que a su vez reduzca el daño a
las células de la sangre, con un enfoque hacia la integración de dispositivos (lab-on-achip);
y el diseño de una microbomba para un sistema de microcanales específico se
propone como una solución. En cuestión de aplicaciones, la microbomba que se diseña
en esta tesis no solo es para uso exclusivo de dicho sistema de microcanales, si no
también podría ser usada para la dosificación de medicamentos o promover el flujo de
algún otro fluido. Para corroborar dichos diseños se hará uso de herramientas
computacionales y simulaciones que sean necesarias, por ejemplo COMSOL
Multiphysics.
Un motivo más para la realización de este diseño es la posibilidad de que en un futuro
se pueda fabricar, en el Centro de Nanociencias y Micro y Nanotecnologías del IPN,
existe esta posibilidad; además de que en el laboratorio de MICROSE del CIC-IPN se
cuenta con recursos; como las herramientas de diseño y simulación para corroborar el
funcionamiento de este actuador.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con base en la información recaudada de diferentes medios, se consiguió entender y
conocer el funcionamiento de los diferentes dispositivos usados para promover el flujo
de fluidos, en específico las microbombas, incluso se realiza una tabla donde se
condensa la información recabada y, se describe de manera breve los métodos y,
tecnologías ocupadas en dichos dispositivos, esto puede ser una fuente de información
para futuras investigaciones; por lo que los dos primeros objetivos planteados se
alcanzaron.
Con la investigación realizada se propusieron varios diseños de microbombas basados
en tecnología MEMS, de estos se eligió la mejor opción; en este sentido se utilizaron
los criterios de elección en base a los materiales, tecnologías de fabricación,
herramientas de diseño y, que se pueda adaptar a un sistema de microcanales; en este
caso se concluye que las microbombas peristálticas basadas en PDMS son mejor
opción que las giratorias o las de válvulas tipo check; las primeras la transmisión para
generar el bombeo es muy compleja y, las segundas existen reportes de atascamiento de
las válvulas, además de los materiales de fabricación no facilitan el flujo de fluido.
Por la aplicación que se pretende tenga la microbomba, el calor generado por el de
voltaje de alimentación, debe de ser el menor posible, se propuso que estuviera por
debajo de los 40 v, que es el valor medio que se reporta en los artículos, por lo cual se
realizó un análisis para encontrar el mejor método de actuación, que ocupará un voltaje
de 20 v y, que además el calor que se generara no estuviera en contacto directo con el
fluido o se pudiera disipar con mayor facilidad, se encontró que el comb drive es el que
mejor cubre los criterios referidos aquí, esté trabaja bajo el principio electrostático.
Con base a los cálculos y simulaciones realizadas se obtuvo un dispositivo que
satisface la aplicación para la cual se diseñó, en este sentido fueron las dimensiones las
que se variaron con mayor frecuencia para conseguir el objetivo de integración a un
sistema de microcanales.
Tanto el diseño de la microbomba como las modificaciones realizadas al sistema de
microcanales fueron simulados, para verificar su funcionamiento, en este caso se
corrobora el comportamiento mecánico, como el desplazamiento y esfuerzos, la
velocidad del flujo, en este caso de la sangre. Con base en los resultados, se concluye
que los esfuerzos están por debajo del módulo de Young y, por lo tanto del esfuerzo de
ruptura, y que la velocidad del fluido realmente tiene un incremento en las partes donde
se esperaba que lo tuviera.
Con base a lo anterior se puede afirmar que el objetivo principal se cumple, con la
prerrogativa de que es posible mejorar el trabajo o buscar otras alternativas, de las
cuales se habla un poco en el siguiente apartado. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Computación
Diseño y simulación de una microbomba MEMS
para flujo de sangre en un sistema de
microcanales
T E S I S
QUE PARA OBTENER EL GRADO DE
MAESTRÍA EN CIENCIAS EN INGENIERÍA DE CÓMPUTO
CON OPCIÓN EN SISTEMAS DIGITALES
P R E S E N T A
ING. EBREL GONZÁLEZ ROSAS
Directores de Tesis:
DR. HÉCTOR BÁEZ MEDINA
DRA. MAYAHUEL ORTEGA AVILES
México, D.F., Diciembre de 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Aplicación de redes neuronales para la identificación de objetos en tiempo real en imágenes tomadas por un quadrotor.
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Como ya se mencionó, el reciente auge de los Quadrotor ofrece una gran variedad de alternativas
de investigación, debido a la complejidad que plantea el manejo y adquisición de datos por medio
de esta herramienta y por otro lado su versatilidad y bajo costo, nos surge la cuestión ¿Es posible
utilizar un robot como lo es el Quadrotor como herramienta de adquisición de información? y aún
más en específico, ¿Es posible realizar la detección de objetos mediante una cámara montada en
el Quadrotor?, ¿A qué altura es posible realizar la detección de objetos?, ¿Qué características debe de tener la cámara para poder procesar las imágenes?, ¿Cómo se realiza la clasificación y que
técnicas se recomienda utilizar?
Es debido a las preguntas anteriores que se motiva el desarrollo de la presente tesis, ya que como
se comentará en la sección del estado del arte, existen diversos estudios y simulaciones que utilizan
estas herramientas, la mayoría con un procesamiento de video post vuelo, pero hasta el momento
del desarrollo de esta tesis no se encontró un estudio comparativo formal que nos sirviera en un
futuro como guía para el desarrollo de plataformas de investigación más robustas.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Se trata de que un Quadrotor sea un supervisor en el aire; el cual seguirá una ruta pre establecida
de vuelo y dentro de la ruta de vuelo existirán diferentes escenarios los cuales se deberán de
analizar. Cada escenario contendrá diferentes objetos, los cuales deberán ser identificados y
analizados en tiempo real.

Objetivos específicos
 Diseñar e implementar una interface que permita controlar al Ar. Drone Parrot.
 Implementar una interface que permita realizar el análisis de las imágenes tomadas por
el Quadrotor en tiempo real.
 Realizar un estudio comparativo entre diferentes redes neuronales y algunos
clasificadores de tipo estadístico.
 Del análisis anterior, implementar el clasificador que mejores resultados arroje.
 Los algoritmos desarrollados deberán de ser probados en una ruta de vuelo y los
resultados de la identificación en tiempo real de los objetos por escenario deberán de
ser reportados al usuario mediante una GUI.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Como parte del proyecto de investigación para la creación de un escuadrón de Quadrotores, cuya
principal finalidad es la de realizar vigilancia aérea, éstos deberán de ser capaces de analizar las
situaciones en piso mediante imágenes tomadas por una cámara montada en dichos dispositivos.
De esta forma en un lapso de 10 a 15 años, el objetivo es que un Quadrotor pueda identificar los
incidentes como lo son incendios, accidentes automovilísticos, entre otros, estos incidentes podrán
ser monitoreados y atendidos con mayor eficacia. Proporcionando una mejora en la calidad de vida
en lo general de la ciudadanía.
Sin embargo, el estado actual del arte aún se encuentra lejos de lograr lo anterior, por lo que una
primera aproximación para resolver la problemática es analizar la factibilidad en el uso de estas
herramientas, así como realizar un estudio formal sobre las técnicas las cuales nos ayuden a
analizar las situaciones en piso e identificación de objetos en tiempo real.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En el presente capítulo se presentan los procedimientos, técnicas y metodologías empleados para
realizar el control de forma adecuada del Quadrotor, la extracción y procesamiento del video en
tiempo real, la detección de puntos de interés, su filtrado, clasificación y la puesta en operación de
todo el conjunto de técnicas en una interface gráfica de usuario (GUI) como producto final.
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Es posible bajo ciertas condiciones, como se demuestra en este trabajo, el realizar la detección de
objetos en tiempo real desde una cámara de video montada en un Quadrotor (sin base
estabilizadora). Esto por su naturaleza es bastante complicado debido a la inherente inestabilidad
que poseen los Quadrotores, lo que resulta en imágenes o videos con distorsiones tales como
variaciones en la distancia de foco, distorsiones en el plano de referencia y en nuestro caso
particular variaciones de iluminación, ya que todas las pruebas se realizaron a luz de día.
Como se demostró, es posible realizar la extracción de descriptores de puntos de interés,
clasificarlos mediante una red neuronal MLP previamente entrenada, realizar un filtrado espacial
mediante desviación estándar y un filtrado temporal histórico en aproximadamente 100 a 150 ms.
con imágenes de 360 x 240 pixeles en formato RGB y con las características de un equipo de cómputo como se mencionó en la sección 4.1.1. Claro apoyándonos en técnicas de programación
que mejoran los tiempos de procesamiento y garantizan el flujo ordenado de imágenes.
Con base al estudio comparativo entre las diferentes redes neuronales y clasificadores de tipo
estadístico, se demuestra y sustenta el uso de la red neuronal MLP como mejor opción para nuestro
caso en particular.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITECNICO NACIONAL
CENTRO DE INVESTIGACION EN COMPUTACION
Aplicación de redes neuronales para la identificación de objetos en tiempo real en imágenes tomadas por un quadrotor.
TESIS
Para obtener el grado de :
MAESTRIA EN CIENCIAS EN INGENIERIA DE COMPUTO CON OPCION EN SISTEMAS DIGITALES
Presenta:
Ing. Gerardo Hernández Hernández
Directores:
Dr. Juan Humberto Sosa Azuela
Dr. Carlos Aguilar Ibáñez
México, D.F. Noviembre 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Enrutamiento compacto usando etiquetas basadas en prefijos</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>El problema de enrutamiento en una red de comunicaciones, que es modelada con un grafo
G = (V, E) donde los nodos v ∈ V representan computadoras o enrutadores y las aristas
e ∈ E representan enlaces de comunicación bidireccionales entre dos computadoras, consiste en
encontrar un conjunto de nodos que formen un camino desde un nodo origen arbitrario a otro
nodo destino, también arbitrario. Un camino C válido del nodo s al nodo d es una secuencia
de nodos C = s, n1, n2, ..., ni
, d tal que para cualquier par de nodos nj
, nj+1 consecutivos en el
camino C se cumple que (nj
, nj+1) ∈ E. Adicionalmente, se busca que el camino C que conecte
a los nodos origen y destino minimice alguna métrica de costo como puede ser la longitud del
camino medido en términos del número de nodos que lo componen, el retardo que experimenta
un paquete al transitar por dicha ruta, el consumo total de energía requerida para transportar
el paquete desde el origen al destino, etc.
Como hemos mencionado, los esquemas de enrutamiento tradicionales basados tanto en
vectores de distancia [1], [22], [33] como en el estado de los enlaces [26], [10] tienen la desventaja
de que el tamaño de sus tablas de enrutamiento es proporcional al número de nodos en la
red, es decir, su complejidad espacial es Θ(|V |). Lo anterior limita de manera considerable la
escalabilidad de los protocolos ya que las consultas a estas tablas pueden ser muy tardadas
pero sobre todo por la sobrecarga de control necesaria para mantener actualizadas todas las
entradas.
Para subsanar el problema de escalabilidad de los esquemas de enrutamiento tradicional,
se han desarrollado esquemas de enrutamiento compacto cuyo objetivo es que tanto las tablas
de enrutamiento como la sobrecarga de control crezca de manera sublineal con respecto al
tamaño de la red, es decir, su complejidad tanto espacial como de mensajes sea o(|V |). En este
trabajo abordamos el problema de diseñar, verificar y caracterizar un esquema de enrutamiento
compacto que minimice la longitud de las rutas y que balancee la carga a lo largo de la red.
El balanceo de carga es importante porque disminuye los problemas de congestion generados
cuando múltiples rutas tienden a sobreponerse en ciertas regiones de la red.
En el presente trabajo se propone utilizar un esquema de enrutamiento basado en etiquetas
que están compuestas por prefijos. Estos etiquetados inducen un árbol sobre la red que es
utilizado para enrutar paquetes desde cualquier origen hasta cualquier destino. A pesar de
sus ventajas, esta forma de enrutamiento tiene tres problemas fundamentales que se listan a
continuación.
1. Longitud de rutas.
Aun cuando el stretch es algo característico del enrutamiento compacto, al realizarse el
enrutamiento sobre el árbol inducido por el etiquetado basado en prefijos, las longitudes de
los caminos establecidos entre dos hojas del árbol puede llegar a ser de orden O(log(|V |))
lo cual es relativamente alto. Es importante mencionar que la elección del nodo raíz del
árbol de etiquetado es importante, debido a que una mala elección podría generar rutas
de tamaño considerable. En la Figura 1.3, se muestra el caso en el que un camino va desde
el nodo 11 al nodo 1 en un árbol de etiquetados aun y cuando en el gafo original la ruta
más corta entre ese par de nodos es de unos cuantos saltos.
2. Congestión en los nodos que se encuentran cerca de la raíz del árbol inducido por los
etiquetados de prefijos.
Debido a que los caminos encontrados por el algoritmo de enrutamiento compacto están
sobre el árbol inducido por el etiquetado, éstos tienden a concentrarse en regiones cercanas
a la raíz. Lo anterior puede propiciar que las regiones de la red que se encuentran cerca
del nodo raíz se congestionen con facilidad porque muchos de los caminos calculados por
el algoritmo pasarán por ahí. En la Figura 1.4, se muestra que al comunicarse los nodos
de las áreas amarillas, los nodos superiores marcados con color verde, serán los nodos que
concentrarán ese flujo.
3. Poca tolerancia a fallas.
Uno de los problemas principales de los esquemas de enrutamiento basados en etiquetados
de prefijos es el que se conoce como tormenta de re-etiquetado. Una tormenta de reetiquetado
se presenta cuando un nodo cercano a la raíz desaparece de la red, lo que
provoca que todos sus descendientes en el árbol tengan que cambiar de etiqueta. En el
caso extremo, cuando el nodo raíz desaparece, la red completa tiene que ser re-etiquetada
lo cual es costoso en términos de la sobre carga de red. La Figura 1.5 ilustra este problema.
En la presente tesis se propone desarrollar un nuevo esquema de enrutamiento compacto
que sea capaz de reducir el impacto de los problemas antes mencionados, pero manteniendo las
características deseables de utilizar tablas de enrutamiento que crezcan de manera sublineal con
respecto al tamaño de la red y además que tengan un stretch menor que el esquema planteado
como base. 
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo General
Diseñar, verificar y caracterizar experimentalmente un algoritmo de enrutamiento compacto
usando etiquetas basadas en prefijos.

Objetivos específicos
Diseñar, verificar y caracterizar una serie de algoritmos de elección de centros de un grafo
para los casos de un único centro y dos centros.
Diseñar, verificar y caracterizar un algoritmo de enrutamiento basado en dos etiquetados
de prefijos.
Realizar un análisis experimental del impacto de la selección de centros en el desempeño de
algoritmos de enrutamiento basados en un único etiquetado. Las métricas de desempeño
utilizadas son el estiramiento (stretch), promedio de las rutas, y el número promedio y
máximo de caminos que pasan por un nodo.
Realizar un análisis experimental del desempeño de los algoritmos de enrutamiento que
utilizan múltiples etiquetados.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La presente investigación es relevante desde el punto de vista teórico debido a que se busca experimentar con un esquema novedoso de enrutamiento para redes de computadora de gran escala que está basado en resultados de teoría de grafos y de teoría de algoritmos. Como se ha mencionado, las redes de computadoras han tomado gran importancia gracias a que han cambiado la manera en la cual las personas obtienen información y la manera en cómo se comunican. Lo anterior ha fomentado que cada vez más dispositivos estén conectados a la red global y por lo tanto, que su tamaño crezca aceleradamente por lo que se requieren de nuevos esquemas de enrutamiento específicamente diseñados para redes dinámicas de cientos de miles de nodos.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>A lo largo de este trabajo se ha mostrado el desarrollo del diseño del algoritmo de enrutamiento
compacto usando etiquetas basadas en prefijos. Dicho algoritmo se aplica en redes de
computadoras sin cambios topológicos. Posteriormente se verificó su funcionamiento mediante
una serie de experimentos los cuales ayudaron a caracterizarlo. Parte del trabajo realizado
consistió en el diseño de una serie de algoritmos para la selección tanto de un nodo raíz como
de dos nodos raíz. Posteriormente se procedió a verificar su correcto funcionamiento, esto
mediante una serie de demostraciones realizadas sobre los mismos algoritmos. Por medio de la
realización de una serie de experimentos se logró caracterizar el comportamiento de los algoritmos
propuestos. Los algoritmos para la selección de centros corresponden a los criterios con los
cuales se pensó se obtendrían mejores resultados. Para la elección de un nodo raíz se diseñaron
algoritmos basados en los siguientes criterios.
Selección del nodo con el grado mayor.
Selección del nodo considerado el centro de la red. Dicho nodo es aquel cuya distancia a
su nodos más alejado es mínima.
Selección del nodo considerado el centro promedio de la red. Dicho nodo es aquel cuya
distancia promedio a todos los demás nodos en la red es mínima.
Para el algoritmo propuesto, se requieren de la selección de dos nodos raíz, por lo tanto los
algoritmos realizados para elegir a dos nodos dentro de la red como raíces estuvieron basados
en los siguientes criterios.
Selección del nodo considerado el centro promedio de la red y el nodo más alejado a él.
Selección de los nodos más alejados en toda la red.
Selección arbitraria de dos nodos.
Una vez que se eligió uno o dos nodos raíces, se continuó con un etiquetamiento o dos,
dependiendo del caso, de todos los nodos de la red, ésto mediante un algoritmo descrito en la
Sección 4 de este trabajo. El algoritmo de etiquetado respetó las relaciones que existen entre
los nodos al inducirse el árbol lógico, es decir, las etiquetas asignadas a los nodos mostraban
una relación padre e hijo, siendo la etiqueta del padre el prefijo de la etiqueta del hijo. Una
vez que todo está etiquetado, el enrutatmiento se realiza de manera sencilla dentro de la red,
simplemente buscando la mayor coincidencia de las etiquetas de los vecinos con la etiqueta
destino. Dado este esquema de enrutamiento se puede apreciar que el tamaño de las tablas
de enrutamiento es del orden de O(d), donde d es el grado máximo dentro de la red. Gracias
al diseño del algoritmo propuesto es que se logró contar con las características mencionadas anteriormente. En el capítulo 4 es en donde se verificó que el algoritmo funciona de manera
correcta y posteriormente en el Capítulo 5 se muestran los resultados a los experimentos para
poder caracterizar su funcionamiento.
Basados en los experimentos realizados y de acuerdo con la información recabada y
mostrada en el capítulo 5, se puede concluir que en definitivo es mejor utilizar los recursos con
los que cuenta la red para poder hacer una elección correcta del nodo que fungirá como raíz,
ésto debido a que como se observó, una elección correcta del nodo raíz genera rutas de longitud
más corta que un nodo elegido arbitrariamente. Para el caso de un sólo nodo raíz, se concluyó
que el criterio que ayuda a reducir el stretch es el del nodo centro promedio. Por lo tanto una
mejora al esquema considerado como base es que elija a su nodo raíz con base en este criterio.
Con respecto al esquema de doble etiquetado, el mejor criterio de selección de nodos raíz
es en donde se elige el nodo centro promedio de la red en conjunto con el nodo más alejado
a él. En el Capítulo 5 se observó mediante los datos obtenidos que al elegir a ese par de
nodos se obtienen rutas de longitud menores, que en cierta medida se reduce la cantidad
de caminos que atraviesan a la raíz y a los nodos cercanos a ella y que fue el criterio que
más se acercó en las métricas seleccionadas al enrutamiento de camino más corto. Por lo
tanto se concluyó que esa es la mejor manera de hacer la selección de los dos nodos raíz de la red. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN
COMPUTACIÓN
Enrutamiento compacto usando
etiquetas basadas en prefijos
Tesis que presenta
Ing. Ismael Quintana Avalos
Para Obtener el Grado de
Maestro en Ciencias de la Computación
Directores
Dr. Rolando Menchaca Méndez
Dr. Ricardo Menchaca Méndez
México D.F. Julio 2014
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Análisis, diseño y construcción de un sistema de reconocimiento de estructuras geométricas por medio de sonares.
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un sistema multicanal que permita el reconocimiento del entorno en
espacios interiores mediante sonares ultrasónicos con aplicación en la navegación de
robots móviles autónomos utilizando secuencias binarias pseudo-aleatorias y
correlación.

Objetivos Específicos
1. Estudio y comprensión del sonar y la teoría relacionada
2. Construcción del arreglo de sonares, así como el diseño y construcción de una
plataforma de procesamiento embebida y su instalación sobre el robot móvil.
3. Implementación y validación del algoritmo de detección del entorno en la
plataforma móvil. Desarrollar los algoritmos de interpretación geométrica para la
generación de información pertinente a la navegación del robot móvil autónomo. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El desplazamiento y control de robots móviles autónomos es una tarea sumamente
complicada debido a la gran cantidad de factores que pueden afectar e influir en el
movimiento de estas máquinas. En la actualidad, con el desarrollo de la inteligencia
artificial, el nivel de autonomía que es posible implantarle a un robot móvil es muy alto,
por lo que es de suma importancia proveerle de medios, a estos algoritmos inteligentes,
para que sean capaces de detectar lo que hay a su alrededor con el fin de orientar
correctamente al robot y permitirle llevar a cabo sus tareas, independientemente de
cuáles sean éstas.
El análisis del entorno requiere tiempo para ser procesado, por lo que esto influye de
manera directa en la velocidad de respuesta del robot móvil. La decisión de utilizar el
sonar ultrasónico se debe a que el tipo de señales que genera son unidimensionales, por
tanto, de bajo costo computacional y no requiere de condiciones específicas de
iluminación como los medios ópticos o de dispositivos costosos como los sistema de
RADAR o LIDAR.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología empleada fue la siguiente:
 Caracterización de sensores de ultrasonido.
 Diseño del sistema de sonares y de la etapa de generación de las secuencias
binarias pseudo-aleatorias.
 Pruebas y validación del sistema en ambientes cerrados como: paredes,
corredores, esquinas y puertas.
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En este trabajo se diseñó y construyó un sistema de sonares para la detección de
obstáculos para un robot móvil de bajo costo el cual se utilizará en la implementación de
diferentes métodos y algoritmos de seguimiento de caminos (path planning) en
ambientes estructurados. Se realizó el diseño del sistema desde nivel componente, lo
cual requirió un completo entendimiento del funcionamiento de las ondas acústicas y el
control necesario sobre las mismas para permitir el uso de sus características físicas
para la detección de obstáculos mediante el concepto de sonar.
Se realizaron pruebas de detección de distintas estructuras, y se concluye que es posible,
en la mayoría de los casos, el reconocimiento de las características de éstas (paredes,
aberturas, ángulos, etc.), además de la obtención de una representación gráfica de las
mismas, con buenos resultados. Debido a la forma del arreglo de sonares implementado,
se observaron dificultades para detectar algunas estructuras como lo fueron los ángulos
obtusos, ya que estos dispersan la señal ultrasónica en dirección opuesta a la apertura
del arreglo.
Una contribución de este trabajo es el uso de un arreglo de sonares que permite la
ubicación de la fuente de reflexión en función del patrón de la onda reflejada, cuyo
análisis está basado en la modulación de las señales por medio de secuencias binarias
pseudo-aleatorias y la aplicación de un proceso de correlación cruzada para la detección
de la señal captada, de la posición del obstáculo y de la distancia, lo cual además
permitió la implementación del manejo concurrente de las señales en todas sus etapas:
transmisión, captura y procesamiento. El tipo de arreglo de sonares utilizado influyó
directamente en posibilitar la aplicación de este tipo de procesamiento, ya que la forma
del mismo, como se observa en los resultados obtenidos, ayuda a reducir la interferencia intercanal (crosstalk), por tanto incrementando la relación señal a ruido de manera
fortuita.
Otra de las contribuciones del proyecto, consiste en el análisis realizado a los tipos de
modulación aplicables a las señales acústicas empleadas en un sistema de sonar, ya que
no se encontró referencia alguna donde directamente se abordara el tema.
Debido a la naturaleza de las señales aplicadas en el sistema de sonares implementado,
fue posible el desarrollo y utilización de una versión modificada de la correlación
cruzada, más eficiente computacionalmente y aplicable solamente a señales binarias.
Esto permitió la implementación de procesamiento de las señales en una plataforma
económica y de prestaciones modestas, como lo es el FPGA utilizado.
En base a los resultados presentados y al análisis realizado a los mismos, se puede
concluir que se alcanzó con éxito el objetivo planteado al inicio de este trabajo de tesis.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación y Desarrollo de Tecnología Digital
Maestría en Ciencias en Sistemas Digitales
“Análisis, Diseño y Construcción de un Sistema de
Reconocimiento de Estructuras Geométricas por Medio de
Sonares”
TESIS
Que para obtener el grado de
Maestro en Ciencias en Sistemas Digitales
Presenta
Ing. Pedro Javier Adame Valdez
Bajo la dirección de
Dr. Luis Arturo González
Hernández
M.C. José Jaime Esqueda
Elizondo
Tijuana B.C., México Junio de 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>simulación de un computador cuántico usando computo de alto rendimiento</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En la última década se ha producido un gran avance en el desarrollo experimental de
una computadora cuántica, la cual pueda explotar toda la complejidad de la física cuántica
[4]. Hasta la fecha, la empresa D-Wave Systems Inc., afirma ser la primera empresa
comercial de computadoras cuánticas, y que ha desarrollado la primera del mundo [5].
Si bien hay opiniones controvertidas sobre la veracidad de la información [6], los investigadores
continúan trabajando en el desarrollo de nuevos algoritmos. En general, utilizan
plataformas de simulación para probar los algoritmos cuánticos en una computadora clásica,
siendo el principal problema los tiempos de ejecución y alto uso de memoria, los cuales
aumentan exponencialmente con el número de qubits usados, esto hace que la simulación
exija un alto poder computacional, afrontando este problema con el uso de una unidad
de procesamiento grafico (GPU) NVIDIA para la simulación de operaciones cuánticas y
mediciones de un registro cuántico, siendo una GPU considerada como una plataforma
computacional de alto rendimiento y accesible para la mayoría de investigadores.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Implementar y probar un registro cuántico de n qubits en una GPU.

Objetivos específicos
Investigar sobre la unidad mínima y constitutiva de la computación cuántica (qubit).
Investigar las distintas compuertas cuánticas y medición de un registro cuántico.
Implementar y diseñar circuitos cuánticos.
Resolver el algoritmo de la transformada cuántica de Fourier.
Diseñar e implementar un algoritmo gen ético cuántico. 
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>La máquina de Turing cuántica es la base de una computadora cuántica, aunque
de manera teórica es una propuesta prometedora, la construcción de una computadora
cuántica con suficientes qubits que la hagan verdaderamente práctica, físicamente aún no
es posible por los problemas de error en la manipulación de un sistema cuántico, pero
existen avances muy prometedores en la detección y corrección de estos errores [8]. El
uso de una plataforma de alto rendimiento como las GPUs, pueden ayudar a acelerar
la simulación de una computadora cuántica, diseñando una biblioteca de funciones para
cómputo cuántico con el objetivo de aprovechar la arquitectura de las GPUs.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Esta investigación se realiza con el propósito de obtener una plataforma de simulación
para cómputo cuántico con resultados en rendimiento mejores en comparación con las
plataformas ya existentes [7], y pueda utilizarse para el fomento y el desarrollo de nuevos
algoritmos cuánticos y la prueba y estudio de los ya existentes.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>En este capítulo se describe el modelo de una computadora cuántica física, que
consiste en una red de compuertas cuánticas aplicadas a un registro cuántico con un
estado inicial clásico, siguiendo el modelo para el desarrollo de una biblioteca de funciones
que simulen las características descritas de una computadora cuántica.
Se detalla la implementación de un registro cuántico |ψi =P2 n−1
i=0 ci
|ii codificado en
un objeto C++, para englobar todas las propiedades de este para un mejor manejo. El
objeto es creado en el CPU y puede ser copiado a GPU donde se realizan operaciones
sobre el, una vez realizado esto, el registro puede copiarse a CPU para depurar resultados,
leer amplitudes y estados.
Se explica la aplicación de compuertas cuánticas al registro en GPU, estas operaciones
se descomponen de su forma matricial con una técnica de búsqueda de patrones logrando
la adaptación del producto tensorial a la arquitectura del GPU. Se describen los tipos de
medición parcial y completa, que se pueden aplicar al registro cuántico, por la dependencia
de datos que se tiene en la medición completa esta sólo se puede realizar en el CPU.
La medición parcial se realiza en GPU usando la técnica de reducción, la biblioteca de
funciones esta diseñada para que su uso sea fácil e intuitivo imitando el modelo descrito
de una computadora cuántica; el usuario no se tiene que preocupar por copia de memoria,
que técnica en la medición aplicar para un mejor rendimiento, o si se realiza en CPU o
GPU, entre otras cosas, todo esto lo realiza la biblioteca de funciones; se describe con un
diagrama de clases la biblioteca de simulación de una computadora cuántica mostrando
la principales funciones y características. También se detalla la propuesta de un algoritmo
gen ético cuántico basado en la reproducción de insectos sociales. 
</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con una unidad de procesamiento gráfico con arquitectura SIMD (Single Instruction,
Multiple Data) se logró simular las operaciones de una computadora cuántica, creando
una biblioteca de funciones para la simulación de cómputo cuántico estableciendo
un entorno similar al de una computadora cuántica, pero no intentando reproducir
exactamente el hardware del dispositivo real, logrando simular un registro de 28 qubits
en un GPU GeForce GTX 980.
Se ha demostrado que las GPU pueden acelerar la simulación de una computadora
cuántica después de un número de optimizaciones que van desde manipulaciones algebraicas
reduciendo la complejidad de cálculo, hasta el diseño de agrupación de hilos en el
GPU, logrando un speedup promedio de 70 en la comparación con la versión más reciente
de la biblioteca de cómputo cuántico llamada libquantum 1.1.1, donde el proceso lo realiza
100 % en CPU utilizando en algunos de sus procesos cómputo paralelo usando openMP.
Se diseñó un algoritmo gen ético cuántico tomando como base el comportamiento de
reproducción de insectos sociales logrando reducir los parámetros de configuración a sólo
dos donde normalmente en los algoritmos gen éticos cuánticos ya existentes se tienen ocho
parámetros de configuración los cuales son elegidos empíricamente. Los resultados han
demostrado de acuerdo a indicadores usados en el experimento de comparación entre
los dos algoritmos genéticos cuánticos y el clásico, que el algoritmo gen ético cuántico desarrollado es superior a los usados en el experimento, el cual consiste en la aproximación al optimo global de funciones utilizados en la literatura para pruebas de algoritmos de optimización.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL 
CENTRO DE INVESTIGACIÓN Y DESARROLLO 
DE TECNOLOGÍA DIGITAL
MAESTRÍA EN CIENCIAS EN SISTEMAS DIGITALES
“SIMULACIÓN DE UN COMPUTADOR CUÁNTICO 
USANDO COMPUTO DE ALTO RENDIMIENTO” 
TESIS
QUE PARA OBTENER EL GRADO DE
MAESTRO EN CIENCIAS EN SISTEMAS DIGITALES
PRESENTA
ING. AJELET SAHAR BONIFACIO RIVERA HOYOS
BAJO LA DIRECCIÓN DE 
DR. OSCAR H. MONTIEL ROSS DR. ROBERTO SEPÚLVEDA CRUZ 
JUNIO 2015 TIJUANA, B.C., MÉXICO
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>DESARROLLO DE SOFTWARE PARA EL ANÁLISIS DE ESPECTROSCOPIA DE RUPTURA INDUCIDO POR LÁSER
</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivo general
Desarrollar un software para el procesamiento y análisis de los datos emitidos por la técnica
de Espectroscopia de Ruptura Inducido por Láser (LIBS), de distribución libre, multi plataforma
y de interfaz gráfica, amigable e intuitiva.
Objetivos específicos
1. Desarrollo de los módulos de tratamiento del espectro (graficado, suavizado, corrección del
línea base e identificación de intensidades pico).
2. Desarrollo y llenado de bases de datos, con líneas de emisión reportadas.
3. Desarrollo del módulo de identificación espectral.
4. Desarrollo de la interfaz gráfica.
5. Implementación de los módulos y la interfaz gráfica.
6. Realizar pruebas de identificación con patrones y muestras certificadas
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Actualmente se carece en el mercado de un software especializado en la identificación de elementos químicos basado en la técnica Espectroscopia de Ruptura Inducido por Láser, de distribución libre y multi plataforma.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Se decidió utilizar el lenguaje de programación Java, para el desarrollo de los módulos de
programación debido a sus características.
1. Es de distribución libre
2. Es multiplataforma
3. Velocidad de operación competitiva
4. Es un lenguaje interpretado
Para le gestión de las bases de datos se decidió utilizar el programa MySQL, debido a sus
características.
1. Es de distribución libre
2. Es multiplataforma
3. Alta velocidad de operación y búsqueda
El módulo de suavizado está basado en dos filtros digitales, el primer filtro elegido es el de
Ventana de Promedio Móvil o MovingAverage, debido a su sencillez, facilidad de implementación
y rapidez de convergencia. El segundo filtro llamado Savitzky-Golay, además de su sencillez y
facilidad de implementación tiene la ventaja de no perder intensidad relativa durante su proceso
de convergencia.
En el módulo que realiza la gráfica del espectro, se utiliza una librería llamada JFreeChart,
esta librería es fácil de implementar y contiene un gran número de rutinas con argumentos para
personalizar las gráficas, además de ser de distribución libre. 
Para el módulo de corrección de línea base, se decidió calcular todas las intensidades mínimas
del espectro mediante la derivada discreta que es el método maten ático más sencillo para esta
tarea. La interpolación lineal de las intensidades mínimas es una buena aproximada de la línea
base, dicha interpolación se logró mediante la ecuación de la recta que es el modelo matemático
correcto para este cálculo. Al suavizar esta línea base mediante el filtro MovingAverage, y restarla
del espectro original, se logra la corrección de la línea base.
El módulo de detección de intensidades pico está basado en la derivada discreta, debido
a la sencillez y facilidad de implementación de este modelo maten ático que refleja resultado
satisfactorios. Debido a que el espectro puede contener o no fondo electrónico, se mejora la
detección de las intensidades pico utilizando nuevamente el filtro Moving Average, y se aplica la
derivada discreta únicamente a los picos que sobresalgan por encima de esta nueva serie de datos
suavizada.
La búsqueda e identificación de elementos se realiza mediante el lenguaje de consulta estructurado
SQL (por sus siglas en ingles Structured Query Language). Es un lenguaje declarativo de
acceso a bases de datos relacionales que permite especificar diversos tipos de operaciones en ellas.
Una de sus características es el manejo del algebra y el cálculo relacional que permiten efectuar
consultas con el fin de recuperar de forma sencilla información de interés de las bases de datos,
así como hacer cambios en ellas. </Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se desarrolló un software especializado en la técnica de Espectroscopia de Ruptura Inducido
por Láser, multiplataforma, de interfaz gráfica y de distribución libre.
Se desarrollaron los módulos de tratamiento del espectro para graficar y suavizar los espectros
así como los de corrección de línea base y asociación de intensidades pico con su respectiva longitud
de onda. El módulo de suavizado está basado en los dos filtros digitales, Moving-Average y
Savitzky-Golay, con la interfaz gráfica que permite ajustar el tipo de filtro y el ancho de filtro.
El módulo corrección de línea base elimina el fondo electrónico progresivamente a criterio del
usuario.
Se desarrolló la base de datos con una tabla para cada elemento, conteniendo cada tabla
ionización sencilla y múltiple además de líneas persistentes.
Se desarrolló la base de datos y tablas que almacenan las líneas de emisión características
de los elementos químicos. Se implementó la interfaz gráfica con los módulos de tratamiento del
espectro. Finalmente se realizaron pruebas de identificación con patrones y muestras certificadas. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
CENTRO DE INVESTIGACIÓN EN CIENCIA APLICADA
Y TECNOLOGÍA AVANZADA UNIDAD ALTAMIRA
DESARROLLO DE SOFTWARE PARA EL
ANÁLISIS DE ESPECTROSCOPIA DE RUPTURA INDUCIDO POR LASER
TESIS PARA OBTENER EL GRADO DE: MAESTRÍA EN TECNOLOGÍA AVANZADA
PRESENTA:
ING. ZEFERINO PEREZ BÁEZ 
DIRECTOR DE TESIS:
DRA. TERESA FLORES REYES
NOVIEMBRE-2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Modelación y simulación de movimiento celular en actividad neuronal usando autómatas celulares</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Los modelos continuos de pulsos neuronales actualmente tienen cierta complejidad
en su análisis matemático [Strogatz (39)], pues son modelados básicamente por medio
de sistemas ecuaciones diferenciales, muchas variables y funciones matemáticas (un
ejemplo claro es el propuesto por [Huxley (18)]) para los cuales buscar una solución no
es una tarea sencilla derivado de ello, las observaciones y la experimentación pudiera
no ser algo sumamente viable al momento de cambiar variables o modificar parámetros
para estudiar algún otro fenómeno.
Así mismo, la complejidad computacional para la implementación de métodos numéricos
podría ser un tema destacable cuando se trata de listar los problemas que se dan
al momento de solucionar ecuaciones para modelos continuos. De manera concreta y en
resumen la complejidad es un punto a resaltar en el listado de problemas al momento
de hacer el modelado y la simulación computacional, esto debido a las limitaciones en
hardware y aquellas con ciertos métodos maten áticos para resolución de ecuaciones.
Por otro lado, esto tiene como consecuencia que la experimentación o la implementación
de nuevas variables (incluso el cambio de las existentes) o las condiciones de
frontera sean algo complicado de implementar tanto matemáticamente como computacionalmente,
lo que podría ciertamente dar como resultado dificultades al momento
de realizar experimentación bajo nuevas condiciones.
De forma concreta el problema se reduce a crear un algoritmo basado en autómatas
celulares el cual mediante un conjunto pequeño de reglas bien precisadas sea capaz
de presentar una dinámica biológicamente plausible, apegada a aquella que presentan
las células en el proceso del crecimiento de gliomas dando como resultado información
relativa a la dispersión de las células, así como de la densidad que se presenta en el
tumor para diferentes radios del mismo. Por otro lado, se pretende hacer uso de la
esencia del mismo algoritmo, una vez que se ha comprobado el correcto funcionamiento
del mismo ante el modelado del crecimiento tumoral mencionado anteriormente, para
implementar cambios necesarios y comprobar si mediante el mismo algoritmo es posible
hacer un modelo de las moléculas de neurotransmisores liberados en una sinapsis.
Para sustentar el funcionamiento de los modelos es necesario tomar referencias de
previas investigaciones en el área y apegar la dinámica de los autómatas a aquellas
presentadas en cada uno de los fenómenos que se desean modelar, esto conlleva a
realizar comparaciones entre los resultados teóricos y/o experimentos in vitro que usan
otros autores, un acercamiento a dichos valores llevaría a la plausibilidad biológica de
los modelos lo cual es importante puesto que esto acercaría a las implementaciones a
tener un comportamiento cercano a la realidad.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>El objetivo general es mostrar que los autómatas celulares son una herramienta
maten ática y computacional útil para modelar fenómenos biológicos en donde el movimiento
aleatorio este presente a lo largo de la dinámica de los mismos; así mismo,
plantear esta herramienta como que puede ser adaptado fácilmente a diversas experimentaciones
teóricas y brindar resultados biológicamente plausibles que señalen a este
tipo de modelos como una buena herramienta para realizar simulación y modelado de
diversos fenómenos (particularmente biológicos); de igual forma, realizar la modelación
de dos fenómenos biológicos distintos partiendo de un mismo modelo de caminatas
aleatorias. </Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>A lo largo de la elaboración de esta investigación fue necesario abordar principalmente
temas médico-biológicos para lo cual se hicieron algunas citas con expertos en la materia (alumnos y maestros de nivel superior y posgrado) en especialidades como
medicina, farmacología, bioquímica neuronal, etc., Además de hacer diversas consultas
bibliográficas en el área de interés, esto principalmente con la finalidad de poder
reconocer brechas en donde el campo de investigación fuese viable y útil, además de
adecuado para el tiempo que dura el programa de posgrado.
La parte teórica para este trabajo se concentró en diversas áreas de estudio principalmente
de la neurología, matemáticas, biofísica y computación, siendo todas igual de
importantes en el desarrollo de este trabajo y, en algunos tópicos, siendo incluso necesario
recurrir a pautas históricas sobre desarrollos (como [Hart (16)]), investigaciones,
experimentación, observaciones, de diversos autores para poder guiar esta investigación
de la mejor manera posible.
Particularmente en el área de las matemáticas se concentró una exploración muy
constante sobre modelos tanto continuos como discretos principalmente de la respuesta
eléctrica derivada de una sinapsis; así mismo de la liberación de neurotransmisores y
la relación de la dinámica de este proceso con la respuesta eléctrica. Aunque bien cabe
mencionar que los datos en algunas investigaciones de otros autores no son del todo
realistas, si marcan una brecha para poder dirigir correctamente no solo este trabajo si
no muchos otros que seguramente están siendo desarrollados actualmente.
Diversos tipos de experimentación fueron hechos con el modelo propuesto a manera
de obtener resultados previos, prevenir fallas en el código, estructurar el mismo y
verificar una relación entre el funcionamiento del programa y la dinámica de un movimiento
aleatorio con la finalidad de poder corroborar algunas hipótesis y así mismo
estar seguros de poder ofrecer una relación entre los resultados de las experimentaciones
y aquellos teóricos con los que se compararan más adelante. </Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>De los desarrollos y análisis maten áticos sobre los fenómenos de difusión y caminatas
aleatorias se puede concluir que ambos tienen como solución a los problemas planteados
una distribución de tipo gaussiano, por lo que resulta viable hacer una analogía
entre un fenómeno y otro, esto es destacable puesto que permite modelar fenómenos
partiendo del enfoque continuo (difusión) por medio de propuestas discretas (caminatas
aleatorias) y viceversa. Esto suma importancia cuando se toma en cuenta que
partiendo de un análisis individual de los entes que conforman un fenómeno (en este
caso células en un tumor o moléculas en la hendidura sináptica), se puede describir con
cierto éxito la dinámica colectiva de un fenómeno.
La replicación de los datos obtenidos en otros trabajos propuestos, los cuales sean
referencias principales para nuevos siempre forma parte importante debido a que, conocer
a fondo el funcionamiento de las propuestas planteadas por otros autores es de
suma importancia para dar pie a nuevos desarrollos e implementaciones, en el caso particular
de este trabajo, el análisis de la publicación [Hindsmarsh and Rose (17)] dirigió
los estudios a una nueva propuesta para los valores de las constantes de las funciones
reportadas en dicho trabajo. Como resultado de un estudio al modelo de Hindmarsh y
Rose en el capítulo 3 se propone una corrección a las constantes respectivas para las
funciones f(x) y g(x), es importante destacar que estas constantes obtenidas durante
el estudio del modelo de Hindmarsh y Rose, aseguran resultados en forma y unidades
como las reportadas en ese mismo trabajo. A manera de conclusión, se puede insistir
en la replicación de trabajos anteriormente propuestos que figuren como referencias
principales a otros nuevos, así como un estudio detallado de los mismos con la finalidad
de evitar contemplar consideraciones erróneas y/o mal propuestas.
El glioblastoma figura como uno de los tumores más malignos dada su reincidencia
después de las quimioterapias y/o extracción quirúrgica, la explicación para esto se
puede estudiar más a detalle en el análisis de los resultados obtenidos por [Andrew
M. Stein (1)] dados estos hechos, buscar respuestas en torno a la dinámica celular que
presenta el crecimiento de gliomas mediante la implementación de modelos como el que
se presenta en esta investigación resulta en análisis de datos interesantes, puesto que
por medio de dichas herramientas se obtienen no solo resultados semejantes (o a veces
mejores) que aquellos que se plantean mediante propuestas matemáticas continuas y/o
datos experimentales, si no que además se brinda la posibilidad de analizar el comportamiento
individual de células lo cual podría abrir nuevos senderos para el estudio de
este tipo de fenómenos donde el objetivo sea conocer el comportamiento individual para
dar explicaciones respecto al colectivo.
A partir de los resultados obtenidos mediante del desarrollo de un modelo discreto,
conformado por un autómata celular de caminantes aleatorios, se puede concluir que las
células en un tumor de tipo glioblastoma siguen la dinámica del movimiento aleatorio
planteado en el capítulo 2. Ya que el modelo permite modelar un fenómeno con dicha
dinámica, entonces mediante la readaptación de parámetros y reglas es posible modelar
otros más que estén relacionados directamente con la difusividad y el movimiento aleatorio.
Tomando en cuenta que los neurotransmisores, al instante de ser liberados para
generar una conexión sin ‘aptica se difunden entre el espacio formado por la etapa presináptica
y la post-sin ‘aptica, presentando un movimiento aleatorio y partiendo de la
hipótesis de que se puede usar el modelo para el crecimiento de gliomas, mediante
ciertos ajustes, se desarrolló un modelo para la dinámica de neurotransmisores, el cual
fue comparado con el modelo de Savtchenko-Rusakov.
Entonces, el modelo usado para describir la dinámica de los neurotransmisores en
el espacio sináptico, se apegó a los resultados de [Savtchenko and Rusakov (35)], esto
se concluye al observar la figura 5.4 y a los resultados obtenidos por el modelo en las
figuras 5.8 y 5.9. Como conclusión, esto indica que el modelo propuesto puede modelar
el comportamiento de los neurotransmisores en el espacio sináptico, además de que
cuenta con posibilidades de ser modificado para ofrecer otros tipos de experimentación.
De forma general se concluye que los fenómenos presentados en este trabajo pudieron
ser modelados por medio de un mismo modelo, lo que indica que este bien podría
ajustarse al comportamiento de otros fenómenos en los cuales el movimiento aleatorio
sea la principal característica de los mismos; para ello basta únicamente realizar un
estudio de las consideraciones generales de estos para que a través de estas se puedan
hacer las adaptaciones pertinentes. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL 
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
MODELACIÓN Y SIMULACIÓN DE MOVIMIENTO CELULAR EN ACTIVIDAD NEURONAL USANDO AUTÓMATAS CELULARES
T E S I S
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN CIENCIAS DE LA COMPUTACIÓN
PRESENTA:
ING. JORGE ALBERTO HERRERA MAGAÑA
TUTORES:
DR. JUAN CARLOS CHIMAL EGUÍA.
DRA. NORMA SÁNCHEZ SALAS
MÉXICO, CIUDAD DE MÉXICO, 2016
</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema de recomendación basado en realidad aumentada para el análisis de datos urbanos</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Hoy en día, al buscar una cartelera cinematográfica no hay una opción
que permita clasificar las películas acorde al género de película. Lo anterior
es relevante, porque el interés de cada persona está descrito por una serie de
preferencias de cada individuo.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar una metodología enfocada a la recuperación semántica de información,
con base en un perfil de usuario y tomando como base, información
del usuario, con la finalidad de generar recomendaciones precisas y
siendo éstas presentadas mediante realidad aumentada.
Objetivos específicos
Implementar una ontología de aplicación para el dominio cinematográfico.
Implementar un método de realidad aumentada para visualizar las salas
cinematográficas más cercanas.
Diseñar e implementar un método para procesar el per l de usuario
mediante consultas semánticas sobre la ontología.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Este trabajo brinda una opción al fortalecimiento del área de e-commerce,
ofreciendo al usuario una herramienta de cómputo que permite elegir una película
de una cartelera a partir del análisis de sus preferencias. Asimismo, este
sistema admite la difusión de películas no tan taquilleras que otros usuarios
puedan preferir y que son considerados de interés, más allá de las películas comerciales.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>La metodología se divide en tres secciones principales; la personalización, en la cual se incluye un perfil de usuario y un vector característico, en el perfil de usuario se tienen todas las preferencias del usuario las cuales sirven de base para poder comparar y generar las medidas de similitud, el vector característico será utilizado para almacenar las preferencias del usuario, su longitud será la cantidad de elementos que conformen la categoría ă C1, C2, . . . , Cn ą, las preferencias serán denotadas con 1 cuando sea de la preferencia del usuario y con un 0 cuando no sea de su preferencia. La segunda sección se conforma del procesamiento semántico en el cual, por medio de una ontología que es la que incluye toda la información y por medio del filtrado que se da por medio del vector característico que se construye en el proceso de personalización, utilizando técnicas o algoritmos para poder medir la similitud semántica como por ejemplo la distancia entre los nodos de la ontología resultando más cercanos los que tengan el significado más similar. La tercera sección se conforma de la visualización, esta es la representación de los resultados obtenidos después de la medición de la similitud semántica, podemos representar los resultados en forma de tablas por medio de una base de datos espacial, en la cual por medio de tablas tendremos los resultados de las similitudes y las ubicaciones de donde se originaron o de donde serán utilizadas, por medio de consultas, podemos recuperar la lista de los resultados ranquedos los cuales pueden ser representados tanto en dispositivos móviles (Mobile GIS) o en contenido web (Web GIS), para una representación gráfica de los mismos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>En el presente trabajo de tesis, se desarrolló una metodología enfocada a
la recuperación semántica de información con base en un perfil de usuario y
representada con ayuda de la realidad aumentada.
La metodología se divide en tres secciones, en la etapa de personalización
por medio de un perfil de usuario se eligen las preferencias del usuario, las
cuales son guardadas en un vector característicos, el cual es utilizado para
su posterior filtrado.
La segunda etapa es el procesamiento semántico en el cual se utiliza la
ontología de WordNet la cual es una base de datos léxica grande del idioma
Inglés, esta se divide en sustantivos, verbos, adjetivos y adverbios, cada uno
de estos cuenta con una definición, la cual se utiliza en este trabajo junto con
un filtrado por contexto (puede ser descrito como una función), esta toma
datos parciales de las preferencias del usuario como parámetro de entrada
y produce una lista de recomendaciones para cada usuario como salida, esta
recomendación está basada en la similitud de los géneros cinematográficos.
La tercera etapa de visualización, en esta etapa se visualizan dos cosas,
la primera son los puntos de interés con ayuda de la realidad aumentada y
por medio de una base de datos espacial la cual cuenta con las posiciones
geográficas en el caso de este trabajo el punto el cual a su vez cuenta con una latitud y longitud, cada punto en el caso de ser representado en dispositivo
móvil renderiza el icono o imagen por medio de la realidad aumentada. La
segunda visualización es la lista de resultados ya ranqueados, estos pueden
ser representados tanto en un dispositivo móvil (este puede ser mostrado con
ayuda de la realidad aumentada), en una página web, o guardar los datos en
una base de datos o en una base de datos espacial si se requiere las ubicaciones
geográficas.
La aplicación móvil para probar el funcionamiento sistema recomendador
brinda al usuario la facilidad de encontrar dentro de un radio configurable
a las necesidades del usuario, esta genera una consulta geoespacial la cual
regresa los puntos de interés, en el caso de la aplicación las salas cinematográficas,
al acceder a la imagen que nos muestra la aplicación despliega
diez resultados de la cartelera cinematográfica por medio del filtrado que se
género, mostrándose en las primeras posiciones las películas de mayor interés
para el usuario y en las últimas posiciones las que son de menor interés.
La metodología de enfocada a la recuperación semántica de información
con base en un perfil de usuario y representada con ayuda de la realidad
aumentada es resultado de un arduo trabajo de investigación en análisis y
transformación de datos geoespaciales, en el procesamiento semántico, en la
investigación de la creación y uso de las ontologías, el funcionamiento y uso
de la realidad aumentada. 
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>

INSTITUTO POLITÉCNICO NACIONAL 
CENTRO DE INVESTIGACIÓN EN COMPUTACIÓN
LABORATORIO DE PROCESAMIENTO INTELIGENTE DE INFORMACIÓN GEO-ESPACIAL 
SISTEMA DE RECOMENDACIÓN BASADO EN REALIDAD AUMENTADA
PARA EL ANÁLISIS DE DATOS URBANOS
TESIS
QUE PARA OBTENER EL GRADO DE:
MAESTRÍA EN CIENCIAS DE LA COMPUTACIÓN
P R E S E N T A:
LIC. FERNANDO JESÚS GONZÁLEZ ALONSO
DIRECTORES DE TESIS:
DR. MIGUEL JESÚS TORRES RUIZ 
DR. JOSÉ GIOVANNI GUZMÁN LUGO 
CIUDAD DE MÉXICO DICIEMBRE 2016</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Uso de la Tecnología de Reconocimiento de Voz para Apoyar la Práctica del Idioma Inglés en un Entorno Virtual – ENGLISH VOICE</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Actualmente la globalización ha traído la tendencia a nivel mundial de convergencia en la
educación y otros sectores, la cual está cambiando la forma en que se aprende el idioma
Inglés como lengua extranjera o segunda lengua. El idioma Inglés se ha convertido en la
lengua central de la comunicación de los negocios, la política, la administración, la ciencia
y la academia, además de ser la lengua dominante de la publicidad globalizada y la cultura
popular. [1]
Debido a lo anterior, la demanda para aprender y practicar el idioma Inglés crece día con
día, no así el espacio y tiempo para tal fin, lo cual genera la búsqueda de nuevas
herramientas y metodologías que apoyen y faciliten el proceso de aprendizaje y la práctica
del mismo a un menor costo y con mayor accesibilidad.
Los laboratorios virtuales de inglés surgen como una alternativa para resolver dichas
problemáticas, su objetivo principal es apoyar la práctica del idioma Inglés de manera que
cada estudiante alcance sus objetivos personales y, se minimice el tiempo y costos que ello
conlleva, utilizando para ello el paradigma de Educación Basada en Web. Dichos
laboratorios integran módulos con diferentes actividades para apoyar la práctica de cada
una de las habilidades a desarrollar en el idioma las cuales son: la comprensión de lectura,
comunicación oral o habla, gramática y escritura. Sin embargo, no integran actividades que
fomenten o apoyen directamente la práctica de la comunicación oral del idioma Inglés,
siendo ésta una herramienta primordial para un mejor desenvolvimiento en cualquier
ámbito que se desee incursionar en el que se requiera del dominio de una segunda lengua
distinto a la lengua madre. Hay laboratorios virtuales que tienen voz pregrabada, pero
no hay laboratorios virtuales que utilicen interacción con voz en tiempo de ejecución.
En concreto, el problema que estaremos tratando tiene que ver con la falta de tiempo,
espacio y dinero para practicar la habilidad oral en el idioma Inglés por lo cual enfocaremos
toda nuestra atención en brindar una herramienta que integre el uso de la tecnología de
reconocimiento de voz en un sistema que esté disponible en la Web como una alternativa
viable para minimizar tiempo y costos en el cumplimiento del objetivo que es practicar el
idioma Inglés además de posibilitar avanzar al alumno a su propio ritmo, y tener acceso
desde cualquier lugar donde tenga una conexión a la Web, y a la hora que quiera.
Investigamos los antecedentes que pudiera tener nuestro sistema, y encontramos que no hay
algún Trabajo Terminal dentro de la Escuela Superior de Cómputo que tenga algún
contenido o propósito similar. Por otro lado, encontramos un Trabajo de Titulación para
obtener el grado de Maestría desarrollado por un alumno del Centro de Investigación de
Cómputo del Instituto Politécnico Nacional, el cual integra un ejemplo de reconocimiento
de voz en su sistema; aunque es un ejemplo de lo que haremos no presenta las mismas
características con las que contará nuestro sistema.
En la Tabla 1.1 presentamos la comparación del sistema mencionado anteriormente con
nuestra propuesta.
Como podremos ver, los laboratorios virtuales para la enseñanza del idioma inglés
existentes no integran el reconocimiento de voz para interactuar con el sistema, y se
enfocan únicamente en contenidos como: libros en formato pdf, diccionario, traductores,
chat, audiolibros, voz pregrabada entre otros. Dicho lo cual confiamos en que nuestro
trabajo propone una alternativa innovadora que atenderá además las necesidades de las
personas que requieren de practicar su habilidad de comunicación oral del idioma Inglés.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Desarrollar un sistema que implemente la tecnología de reconocimiento de voz para apoyar
la práctica de la comunicación oral del idioma Inglés.

Objetivos específicos.
1. Desarrollar una aplicación Web usando el lenguaje de programación Java, en un
Entorno de Desarrollo Integrado Avanzado (Integrated Development Environment,
IDE por sus siglas en inglés).
2. Implementar la herramienta utilizando los patrones de diseño: Modelo - Vista -
Controlador y Composición.
3. Implementar los módulos siguientes:
 Módulo de Administrador,
 Módulo de Profesor y
 Módulo de Alumno.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El idioma Inglés es uno de los más demandados para aprender como segunda lengua debido
a la globalización del mundo actual [1]. Por otro lado, la disposición de tiempo y dinero es
de gran importancia para poder llevar a cabo el aprendizaje del mismo lo que crea la
necesidad de buscar nuevas herramientas y metodologías que brinden la comodidad y
practicidad para lograr el cometido que no es tan solo estudiar un segundo idioma, en este
caso el idioma Inglés, sino que además dominarlo en todos los sentidos, especialmente en
las habilidades del habla y el oído pues es la fluidez en éstas lo que nos lleva a obtener un
mejor desenvolvimiento en el ámbito que deseemos.
Atendiendo lo anterior, enfocaremos nuestra atención en crear un sistema que apoye la
práctica de la comunicación oral del idioma Inglés, con la cual el usuario podrá practicar de
forma interactiva con la computadora haciendo uso en todo momento de la tecnología de
reconocimiento de voz en su modalidad de comandos de voz y en un entorno basado en la
Web, haciéndolo accesible desde cualquier lugar y a cualquier hora donde se cuente con
una conexión a Internet. De esta manera el usuario ya no tendrá las limitaciones de espacio
y tiempo para llevar a cabo la práctica del idioma; así también se ahorrará en recursos
económicos ya que las herramientas que utilizaremos en el desarrollo serán todas gratuitas
por lo que el costo del mismo será muy accesible.
El sistema también dará lugar al reúso de las partes desarrolladas debido al hecho de que lo
implementaremos basándonos en el paradigma orientado a objetos, además, emplearemos
patrones de diseño automatizando así la producción de materiales didácticos de este tipo y
reduciendo la elevada complejidad que conlleva su elaboración.
El uso de la tecnología de reconocimiento de voz nos permitirá llevar a cabo una
interacción más natural con la computadora lo que propiciará mayor familiaridad y fácil
interacción con el sistema. El profesor tendrá una herramienta muy útil a la hora de crear
material multimedia pues ésta simplificará el proceso de desarrollo de este tipo de
materiales educativos.
El proceso Profesor-Alumno, permite analizar las métricas para dar forma y estructura para
el curso de una manera coherente, adecuada, y lógica, con la finalidad de dar
retroalimentación personalizada al estudiante, posibilitando una secuencia dinámica de los
materiales educativos en Tiempo de Ejecución en función de sus resultados para facilitar el
apoyo al aprendizaje, en línea. En este trabajo usaremos el modelo de Uskov[26] para la
construcción de los materiales educativos, sin embargo posibles siguientes proyectos
podrán agregar nuevas técnicas.
El sistema de XHTML+Voice reduce la complejidad de integración de voz en el contenido
y evaluaciones, lo que permite una interacción innovadora tanto en evaluaciones y
navegación y permite el desarrollo de un sistema atractivo, desafiante, de adaptación y a la
medida de los usuarios, además de permitir a las personas con capacidades diferentes
interactuar con ENGLISH VOICE por medio de comandos de voz sin necesidad de
trasladarse a algún lugar.
Por último, consideramos que el sistema que desarrollaremos complementará el trabajo
realizado hasta ahora por los creadores de laboratorios virtuales del idioma Inglés ya que
podría integrarse a éstos si así se deseara debido a que nuestro sistema será flexible y
tolerante a cambios, conservando siempre la esencia de su propósito.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Con la culminación del presente trabajo se logró obtener a ENGLISH-VOICE, un sistema
que funge como herramienta de apoyo en la práctica oral del idioma Inglés, mediante la
planeación, desarrollo y evaluación de actividades integrales.
ENGLISH-VOICE resultó ser una herramienta muy útil para los profesores del idioma
Inglés principalmente, quienes gracias a los resultados obtenidos del sistema tendrán la
posibilidad de crear contenidos interactivos para el alumno mediante la voz para apoyar la
práctica de la habilidad oral en el idioma Inglés, esto sin la necesidad de poseer
conocimientos técnicos avanzados.
Por otro lado, los alumnos podrán interactuar con los contenidos creados por el profesor
mediante la voz totalmente, sin la necesidad de usar otros dispositivos como el ratón o el
teclado y así practicar su habilidad oral en el idioma Inglés. Además, tanto el alumno como
el profesor podrán acceder al sistema a cualquier hora y en cualquier lugar siempre que se
cuente con conexión a la Internet.
Finalmente, concluimos que el sistema ENGLISH-VOICE representa una herramienta
totalmente innovadora al no haber otro sistema con iguales características, muy útil a la
hora de crear contenidos interactivos pues no requiere mayores conocimientos técnicos de
parte del profesor y a la hora de resolver dichos contenidos ya que permite un canal de
comunicación bidireccional entre el alumno y el sistema. Además, consideramos que es
muy importante la movilidad que permite el sistema al usuario por el simple hecho de ser
un entorno Web y por lo tanto estar disponible en todo momento y lugar.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLTÉCNICO NACIONAL
 ESCUELA SUPERIOR DE CÓMPUTO
 ESCOM
Trabajo de Titulación
“Uso de la Tecnología de Reconocimiento de Voz para
Apoyar la Práctica del Idioma Inglés en un Entorno
Virtual – ENGLISH VOICE”.
Que para cumplir con la opción de titulación curricular en la carrera de
“Ingeniería en Sistemas Computacionales”
Presentan
Alonso Contreras José Manuel
Briones Rosas Leticia
Esparza Reyes Alberto Efraín
Valdés González José Uriel
Directores
 M. en C. Rubén Peredo Valderrama
 M. en C. Macario Hernández Cruz.
 México, D.F. a 22 Mayo de 2015.</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Aplicación web de apoyo al psicólogo para la valoración del nivel de estrés</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Tomando el concepto de la Organización Mundial de la Salud, la cual menciona que
el estrés es el "conjunto de reacciones fisiológicas que preparan el organismo para la
acción". Aun cuando en su concepto, parece abocarse solamente a cuestiones fisiológicas, es
en realidad un asunto complejo que incluye a todo el organismo y los aspectos psicológicos
y sociales que intervienen en una persona.
Si observamos, el ser humano desde tiempos muy lejanos hasta la actualidad ha
vivido con estrés, el poder sobrevivir en las primeras eras, con los acontecimientos que les
surgían generaban estrés, como pelear con bestias y animales salvajes así como con rivales
que luchaban por defender o expandir sus territorios. Y en la actualidad donde día a día se
tiene que lidiar con diversas actividades como lo son: trabajo, labores domésticas, deportes,
escuela, actividades que pueden presentar una serie de dificultades las cuales pueden generar
estrés.
La consecuencia de un estrés excesivo, es que todo el organismo se descompensa,
bajan las defensas y surgen las enfermedades, tanto de mente como de cuerpo. Por lo tanto
esto nos genera más angustia –más sensación de peligro- y así nos vemos envueltos en un
círculo vicioso que hace que mucha gente termine en la sala de urgencias de algún hospital,
o que de plano pierda la vida, “gracias” a un mecanismo de supervivencia que se salió de
control [1].
En una publicación del periódico la Jornada en internet [2] realizada a el Dr. Felipe
Uribe, académico de la Facultad de Sicología de la Universidad Nacional Autónoma de
México (UNAM).
El experto señaló que -El Estrés- es un problema de salud público que debe ser atendido
también por autoridades laborales, pues a diferencia de otros países, en México no existen
leyes ni normas que fomenten su disminución y es por ello que 40% de la población en
México sufre estrés
Argentina, Chile y Brasil cuentan con leyes para tratar de controlarlo, además de que
empresarios, trabajadores, sindicatos, médicos y psicólogos intervienen para prevenir
problemas de salud derivados de esta reacción fisiológica.
En ese sentido, consideró que el Estado mexicano debe procurar el bienestar de sus
trabajadores, estudiantes y familias, impulsar una cultura de prevención a través de la
realización de diagnósticos que determinen sus niveles, y promover estudios para que se
conozca el estado de salud en función de la patología.
Recomiendan, asimismo, fomentar programas de rehabilitación y generar facilidades
para que la población cuente con espacios de esparcimiento, culturales o deportivos.
Detalló que si bien el estrés no es el causante de las enfermedades, sí representa una
variable constante relacionada con diversas patologías físicas y mentales.
Algunas personas manifiestan la alteración con llanto, violencia, gritos o evasiones;
otros son aprehensivos y no expresan su malestar, lo que puede provocar dolores musculares
en cuello, espalda, piernas, articulaciones y, en el caso de las mujeres, se incrementan los
cólicos menstruales.
También puede derivar en problemas cardiovasculares, que van desde cefaleas e
hipertensión, hasta embolias, arritmias o infartos. Desencadenan malestares
gastrointestinales como colitis, hernias hiatales, estreñimiento o gastritis, al igual que
dificultades reumáticas como fibromialgia [2]”.
Hasta este punto podemos observar el gran interés que debemos darle al tema del
estrés, ya que es un verdadero peligro tanto para la salud, así como para la integridad de las
personas si no se tiene un buen control del mismo, es un tema muy interesante ya que con un
estrés controlado podemos llevar una vida saludable y mucho mejor.
Debido a que la magnitud de esta problemática es muy inmensa, y debido a que no somos
expertos en el tema, nos damos a la tarea de tener que acotar el ambiente con el que
trabajaremos para con esto poder cubrir un área en específico y se puedan tener mejores
resultados, no sin antes dar a conocer toda esta información sobre la importancia que se debe
de tomar sobre este tema tan importante.
Trataremos al estrés académico como el elemento de cambio en los componentes del
Proceso de Enseñanza y Aprendizaje, a través de un conjunto de mecanismos adaptativos
individuales e institucionales, producto de la demanda desbordante y exigida en las
experiencias de enseñanza y aprendizaje que se desarrollan en las Instituciones de Educación
Superior, con el propósito de mantener el estado estacionario (estado dinámico del sistema
donde se producen entradas y salidas sin modificar de manera considerable las
características y propiedades de éste) en el proceso educativo.
Para una institución educativa es importante conocer los niveles de estrés académico
en sus estudiantes, ya que el estrés se ha asociado a la depresión, enfermedades crónicas,
enfermedades cardiacas y fallas en el sistema inmune, y complementariamente al fracaso
escolar y a un desempeño académico pobre.
Diversas investigaciones han mostrado que el estrés académico ocurre en los
estudiantes de primaria, secundaria y preparatoria; también se ha demostrado que dicho
estrés aumenta conforme el estudiante progresa en sus estudios, y que llega a sus grados más
altos cuando cursa sus estudios universitarios. Los estudios superiores representan, pues, el
punto culminante del estrés académico por las altas cargas de trabajo, pero también porque
coinciden con una etapa de la vida en la que el estudiante debe enfrentarse a muchos
cambios en su vida. Específicamente, el ingreso a la universidad coincide con el proceso de
separación de la familia, la incorporación al mercado laboral y por lo tanto la adaptación a
un medio poco habitual. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar una aplicación web, la cual por medio de una serie de preguntas ayude a
saber el nivel de estrés que presenta el usuario en ese momento, para con esto facilitar este
proceso al psicólogo y mostrar los resultados de una forma fácil, rápida y precisa.

Objetivo Específico
Diseñar una aplicación web que servirá de apoyo para el psicólogo, la cual,
conforme a la realización de un Test indique el nivel de estrés, además de proporcionar
información referente al estrés, como lo son hábitos de salud, estilo de afrontamiento de los
problemas, redes de apoyo social. Estas áreas le ayudaran al psicólogo a dar una mejor
valoración al paciente, además mostrará información adicional referente al estrés como su
Significado, Enfermedades, Libros, Alternativas y Lugares de interés.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>La propuesta que se presenta es una forma fácil y sencilla de conocer el nivel de
estrés que presenta una persona. En la actualidad existen varios dispositivos que ya realizan
ese cálculo, pero en realidad mucha gente no los conoce o no los adquiere, ya que no tienen
una idea clara de lo que es el estrés y no conocen las consecuencias que les puede provocar
y por lo tanto no le dan la importancia que tiene que darse.
También nos basamos en el “Código ético del psicólogo” de la “Sociedad Mexicana
de Psicología” [4] el cual nos habla sobre Calidad de la valoración y/o evaluación
Psicológica. Puntos que son muy importantes y tenemos considerados para que se cumplan
y se realice un buen trabajo fundamentado en estos artículos:
Artículo 15
El psicólogo que construye, desarrolla, adapta, administra, o usa técnicas de
valoración psicológica, entrevista, prueba, cuestionarios, u otros instrumentos, y/o los
califica o interpreta, con fines clínicos, educativos, de selección de personal,
organizaciones, forenses, de investigación, u otros, lo hace en forma y con propósitos
apropiados a la luz de los datos de investigación o acerca de la utilidad y la aplicación
apropiadas a las técnicas. Es decir, las valoraciones, recomendaciones, informes, y
diagnósticos psicológicos o enunciados evaluativos del psicólogo se basan estrictamente en
información y técnicas (incluyendo entrevistas personales al individuo cuando es
apropiado) suficientes y actuales para proporcionar sustento a sus interpretaciones y
recomendaciones.
Artículo 16
El psicólogo que desarrolla y conduce investigación con pruebas y otras técnicas de
valoración utiliza procedimientos científicos y conocimiento profesional actualizando para
su diseño, estandarización, validación, reducción o eliminación de sesgos y
recomendaciones de uso.
Artículo 17
El psicólogo que administra, califica, interpreta o usa técnicas de valoración se
cerciora de que éstas se basan en datos sólidos que garanticen la confiabilidad, validez y
normas, así como la aplicación apropiada y usos de las técnicas y/o instrumentos que
emplea. Asimismo, toma decisiones, reconociendo los límites de la certidumbre con la que
es posible diagnosticar, emitir juicio, o hacer predicciones acerca de individuos o grupos.
Artículo 18
El psicólogo identifica situaciones en las que ciertas técnicas de valoración o
normas no son aplicables o requieren de ajustes para su administración o interpretación.
Debido a factores tales como el género, la edad, la raza, el origen étnico, la nacionalidad, la
orientación sexual, la minusválida, el idioma o el nivel socioeconómico de los individuos o
grupos, no emplea técnicas o instrumentos solamente traducidos de otro idioma y/o cuyos
datos acerca de su construcción, adaptación, validez, confiabilidad, estandarización y/o
investigación de resultados se hayan determinado con base en estudios realizados en poblaciones diferentes a la que pertenezcan los individuos o grupos que pretenda valorar.
Estos artículos también especifican que el Psicólogo debe de seguir con estos
lineamientos, puesto que nosotros no somos especialistas en Psicología, en este trabajo
estamos apoyados y coordinados por nuestra directora la M. en C. Gisela González
Albarrán, ya que ella es Lic. en Psicología y es quien nos guía y orienta para desarrollar un
buen trabajo, siguiendo todos los lineamientos que se requieren.
Esta aplicación web también le permitirá al usuario adquirir más información sobre
el estrés pues, mostrará una breve explicación de cada una de las alternativas que se
recomienden, por ejemplo si una de las alternativas es sobre tomar masaje o yoga, se le
mostrara los posibles lugares a donde se puede canalizar al paciente.
Este trabajo será una aplicación web ya que es un medio por el cual hoy en día la
mayoría de gente está interesada en acceder. Los Psicólogos en su consultorio pueden
contar con una computadora e internet, nos referimos a ellos ya que serán ellos, quienes
podrán utilizar el producto de este trabajo y puesto que nuestra área de trabajo será la
ESCOM, sabemos que los alumnos están más familiarizados con una computadora, podrán
contestarlo de manera más fácil y sencilla.
A continuación se muestran las gráficas de un cuestionario que fue aplicado a 22
Psicólogos del Centro Interdisciplinario de Ciencias de la Salud – Unidad Santo Tomás y a
dos de la ESCOM, con el objetivo de recabar información respecto a la utilidad que tendría
para los psicólogos, el contar con una herramienta para detectar los niveles de estrés
académico, ante los elevados niveles que actualmente afectan a las personas, también nos
ayudó para saber la importancia del estrés y su control.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>Para realizar este trabajo se utilizará RUP [7] el cual es un proceso de desarrollo de
software y junto con el Lenguaje Unificado de Modelado UML, constituye la metodología
estándar más utilizada para el análisis, implementación y documentación de sistemas
orientados a objetos, el objetivo de esta metodología es asegurar la producción de software
de alta calidad, que satisfaga las necesidades del usuario final dentro de un tiempo y
presupuesto predecible.
En la Figura siguiente se presenta de manera general y gráfica la secuencia de etapas
que se desarrollan en el Modelo RUP.
En cada una de la secuencias haremos uso de herramientas y técnicas que se
apeguen al paradigma de programación orientado a objetos.
Fases:
1. Inicio:
Alcanzar un acuerdo entre todos los interesados respecto a los objetivos del ciclo de
vida para el proyecto, generando el ámbito del proyecto, el caso de negocio, síntesis de
arquitectura posible y el alcance del proyecto.
2. Elaboración:
Establecimiento de la línea base para la Arquitectura del sistema y proporcionar una
base estable para el diseño y el esfuerzo de implementación de la siguiente fase, mitigando
la mayoría de los riesgos tecnológicos.
3. Construcción:
Completar el desarrollo del sistema basado en la línea base de la arquitectura.
4. Transición:
Garantizar que el software está listo para entregarlo a los usuarios.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Durante la realización de esta trabajo, nos dimos cuenta que nos falta mucho por
entender de la verdadera importancia que le debemos de dar al estrés, es por ello que se
optó por realizar una aplicación web, ya que es de fácil acceso y entendimiento para la
gente.
En las encuetas realizadas en el CICS Santo Tomas, a los docentes del área de
psicología, nos indicaron que el problema del estrés no es algo que se tenga controlado en
México, ya que no se considera una enfermedad, también nos puntualizaron que es
importante la realización de trabajos como el de este tipo, ya que con esto les facilita la
valoración y el diagnóstico del nivel de estrés.
También se mencionó que es más fácil la realización de los test en computadora ya
que no se tiene la presión de la cantidad de preguntas y respuestas, también que es amigable
la vista y no se vuelve tedioso.
El costo se reduce, ya que dicho test no se utilizaba por su alto costo en la
implementación del mismo.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Escuela Superior de Cómputo
ESCOM
Trabajo de titulación
"Aplicación web de apoyo al psicólogo para la valoración
del nivel de estrés"
No. Registro: ISCCR046-2012-A044/2015
Que para cumplir con la opción de titulación curricular en la carrera de:
“Ingeniería en Sistemas Computacionales”
Presentan
Saúl García Albarrán
Alejandra Martínez Morales
Directores
M. en C. Gisela González Albarrán 
M. en C. Tanibet Pérez de los Santos Mondragón 
ABRIL 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Fabricación de nanobiosensores tipo FET basados en grafeno</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Al día de hoy, se han elaborado una gran variedad de FETs con canal de grafeno, en su
mayoría el grafeno usado se sintetiza dentro del mismo grupo de investigación, al hacer esto
no se aprovecha que una gran variedad de compañías sintetizan grafeno a precios accesibles.
Se debe tomar en cuenta además, que la adsorción de biomoléculas sobre el grafeno puede
ocasionar una desnaturalización de las mismas, con lo cual se debe modificar la forma en la
que se adsorben dichas proteínas y así preservar sus funciones. En un afán por reducir costos
de fabricación, puede aprovecharse la ventaja de usar grafeno comercial.
Una forma de evitar que las proteínas pierdan sus funciones al interactuar con el grafeno
es mediante el decorado del grafeno, este proceso consiste en depositar sobre toda la hoja de
grafeno nanopartículas de metal (puede ser oro, níquel, platino, etc.) y después mediante
grupos funcionales o químicas de superficie las biomoléculas se adsorben sobre el metal, sin
embargo, la mayoría de los trabajos de decorado que existen en la actualidad no cubren de
manera homogénea la superficie de la hoja de grafeno, por lo cual para este trabajo de
investigación se procederá a utilizar grafeno comercial y mediante un proceso de decorado por
depósito químico en fase vapor desarrollado en ESIQIE-IPN, el cual ha probado decorar de
una manera homogénea, poner nanopartículas de platino sobre el grafeno y de este modo tener
sitios específicos (el platino) en donde adsorber las proteínas.</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo> Objetivo General.
Fabricar un nanobiosensor a base de grafeno no decorado y decorado en configuración
transistor de efecto de campo (FET) y validar su respuesta eléctrica al adsorber proteínas.

Objetivos Particulares.
 Estudiar la morfología del grafeno no decorado y decorado con nanopartículas de
platino (Pt) usando AFM y MEB.
 Realizar los microelectrodos con fotolitografía en el CNMN-IPN.
 Depositar localmente el grafeno entre los microelectrodos utilizando un robot de
depósito localizado (Microplotter marca SonoPlot GIX II).
 Caracterizar eléctricamente el GFET con biomoléculas y sin biomoléculas, utilizando
una estación de pruebas ubicada en el CNMN-IPN.
 Validar los cambios en la conductividad eléctrica del grafeno y los cambios en su
superficie, utilizando grafeno decorado y sin decorar.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>H0 El grafeno se adhiere a una superficie funcionalizada con 3-
aminopropiltrietoxisilano (APTES).
H1 La conductividad en el grafeno se incrementa cuando es decorado con
nanopartículas metálicas.
H2 Las biomoléculas como la proteína G con grupos amino se fijan a las nanopartículas
de platino del grafeno cuando se usa una química de superficie con disulfuros.
H3 La conductividad del grafeno cambia conforme se depositan nanopartículas de
metal en su superficie.
H3 La conductividad del grafeno se decrementa conforme se adhieren biomoléculas en
su superficie.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El presente trabajo de tesis busca crear un dispositivo tipo FET, en el cual las regiones
de fuente y drenado estén conectadas mediante un nanomaterial, es decir el canal de dispositivo
es un nanomaterial, en este caso grafeno, para poder conocer y utilizar dicho material en
dispositivos como son los nanobiosensores, ya que es un material que puede utilizarse en
aplicaciones biomédicas, y que por sus prometedoras características eléctricas, se cree que es
de gran utilidad para el área de biosensado. Este proyecto permitirá generar tecnología propia
en el IPN mediante técnicas de micro y nanofabricación, que será de beneficio para la sociedad
mexicana, ya que con esta tecnología se pretenden generar nanobiosensores para detectar
biomarcadores de enfermedades específicas de la población mexicana y con ello ayudar a médicos y especialistas en el diagnóstico de este tipo de enfermedades. Otra ventaja que se
contempla es que el hecho de utilizar grafeno abarata costos, ya que este material es bastante
barato y la cantidad utilizada en cada dispositivo es mínima.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Hemos podido constatar, mediante imágenes de microscopia de fuerza atómica y
microscopia electrónica de barrido, que la superficie funcionalizada con grupos silanos adsorbe
un mayor número de hojuelas de grafeno y que además, con el paso del tiempo, su resistencia
no varía, como sucede cuando solo se polariza la superficie con cargas o se limpia con
soluciones dejando grupos silanol.
La funcionalización con grupos silanos permite una superposición de hojuelas entre sí,
con lo cual se consigue contactar un electrodo con otro, sin necesidad de recurrir a fenómenos
cuánticos como el tunelamiento en el canal de grafeno.
El mejor solvente para realizar una dispersión de óxido de grafeno reducido (rGO) es
el dimetilformamida (DMF) comparado contra el dimetilsulfóxido (DMSO) ya que la
dispersión permanece estable por 2 a 3 semanas más que con DMSO.
Conseguimos construir microelectrodos, los cuales poseen un ancho de 10 μm, y un
largo de 3140 μm; como pudo observarse en el capítulo anterior, las separación entre los
electrodos fue de 10, 5, 4, 3, 2, 1 μm y 500 nm, este último en realidad fue imposible de obtener
debido a las limitantes de la técnica de fotolitografía, nuestra máxima resolución fue de 1 μm.
Para aumentar esta resolución se pretende utilizar litografía electrónica (EBL) en el centro de
investigación en computación (CIC) del IPN.
Utilizar el microplotter Sonoplot para realizar el deposito tanto de APTES como de los
nanomateriales nos permitió depositar una cantidad mínima de nuestras soluciones (picolitros),
esto nos permite optimizar costos, ya que de por si es mínima la cantidad de grafeno que dispersamos en DMF (0.1 mg) y que además podamos utilizar cantidades tan pequeñas es de
gran ayuda. Otra gran ventaja es que pudimos utilizar compuestos que de no haber sido en
cantidades tan reducidas, hubieran destruido los patrones en la resina, con lo cual hubiera sido
imposible realizar la caracterización eléctrica.
Al realizar la caracterización eléctrica de los dispositivos nos dimos cuenta que
necesitamos un voltaje muy alto en la compuerta si queremos obtener curvas parecidas a las
que se presentan en el apartado 2.3, sin embargo, esto solo nos sirve para poder ver el cambio
de comportamiento (de p a n y viceversa) en el grafeno, pero para nuestra investigación no es
necesario.
Al comparar el grafeno decorado y sin decorar, existe realmente una mejora en la
conductancia, en el caso del rGO/Pt. 
El hecho de que la resistencia del rGO/Pt aumente cuando se funcionaliza la superficie
con APTES, puede deberse o a la química de superficie o también al número de capas
adsorbidas sobre el grafeno.
La conductancia si varia (16.80%) cuando se adhiere una biomolecula al grafeno
decorado, o simplemente se depositan soluciones sobre este, es importante destacar que la
detección de proteínas se consigue en cuanto se depositan estas sobre el grafeno, sin necesidad
de esperar un tiempo de incubación. Pero también constatamos que después del tiempo de
incubación la corriente cambia aún más.</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico Nacional
Centro de Investigación en Computación
Fabricación de nanobiosensores tipo FET basados en grafeno
T E S I S
Para obtener el grado de:
Maestría en Ciencias en Ingeniería de Computo con opción en
Sistemas Digitales
P R E S E N T A:
Ing. Sergio Proa Coronado
Directores de Tesis:
Dr. Adrián Martínez Rivas
Dr. Salvador Mendoza Acevedo
MAYO 2015</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Sistema analizador de micrografías de hipocampos murinos
para la cuantificación neuronal</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En este trabajo se presenta la propuesta de un sistema computacional que analizará micrografías
(fotos obtenidas a partir de un microscopio) de hipocampos (sección del cerebro) de organismos
murinos, con la finalidad de cuantificar las neuronas presentes en cada una de dichas
micrografías. Con este trabajo se pretende agilizar una parte esencial del trabajo que llevan a
cabo los investigadores del área de enfermedades crónico-degenerativas de la Universidad
Autónoma del Estado de Morelos, en la cual se estudian las patologías asociadas a la obesidad
como la neuroinflamación y el daño cognitivo que esta ocasiona. </Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar e implementar un sistema computacional capaz de cuantificar las neuronas presentes en
micrografías de hipocampos murinos, que sirva para agilizar en tiempo y esfuerzo el trabajo que
llevan a cabo los investigadores del laboratorio de neuroinflamación de la Universidad
Autónoma del Estado de Morelos.
Objetivos específicos:
1. Agilizar el proceso de conteo de neuronas en micrografías que realizan visualmente los
investigadores de neuroinflamación de la Universidad Autónoma del Estado de Morelos.
2. Brindar un resultado útil para los investigadores de neuroinflamación de la Universidad
Autónoma del Estado de Morelos.
3. Realizar un manual de usuario para el sistema.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible realizar la sincronización de dos sistemas de caos utilizando un sistema
de lógica tipo 1 o tipo 2 con un error de sincronización reducido.
Tomando en cuenta las características propias de la lógica difusa y en especial de la
lógica difusa tipo-2, donde el manejo de la incertidumbre y su mejor rendimiento para
el trato de sistemas complejos, permitirá llevar a cabo una mejor sincronización de dos
sistemas caóticos en un sistema del tipo maestro-esclavo.
</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>En las investigaciones desarrolladas en el área de las ciencias médico-biológicas, los
investigadores se enfrentan a procesos que muchas veces pueden tornarse extensos en cuanto a
tiempo, lo que puede llevar a que los resultados de dichas investigaciones sean obtenidos en
tiempos no muy factibles para sus propósitos, como es el caso de los investigadores que se
dedican al estudio de las enfermedades crónico degenerativas, los cuales en alguna parte de sus
trabajos de investigación se ven en la necesidad de analizar micrografías de hipocampos y llevar
a cabo interpretaciones de éstas. La densidad neuronal es una de las más importantes
características y consiste en cuantificar el número de neuronas presentes en cada una de las
micrografías.
El análisis anteriormente mencionado puede tomar varios minutos por micrografía, el
inconveniente es que muchas veces la cantidad de micrografías a analizar por experimento llega
a superar las 300 micrografías, es por esta razón que se propone solucionar este problema a
través de un software que realice este trabajo de una forma automatizada, rápida y sobre todo,
eficaz. Realizando la búsqueda del estado del arte, nos percatamos de que no existe una
herramienta computacional como la propuesta en este trabajo, otra de las razones que se
consideraron para la realización de este trabajo es que además de beneficiar en primera instancia
a los investigadores de la Universidad Autónoma del Estado de Morelos, se beneficiaría también
a un gran sector de la comunidad científica, hablamos del área de las enfermedades crónicodegenerativas.
</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se llevó a cabo el desarrollo de un sistema capaz de gestionar los distintos grupos de micrografías
de un experimento de tal manera que los usuarios finales (investigadores), puedan llevar una
mejor organización de las imágenes requeridas en sus experimentos, pero sobretodo, es una
herramienta que automatiza el conteo de células en las micrografías que los investigadores
requieren y entrega un análisis estadístico de utilidad para los usuarios.
Se utilizaron diversos algoritmos para el tratamiento de imágenes, algunos de los cuales sirvieron
para eliminar el ruido en las imágenes, otros sirvieron para definir de mejor manera ciertas
características de las células que se pretendían contar, pero el algoritmo más importante para
poder realizar el conteo de los núcleos celulares en las micrografías fue el algoritmo de
etiquetado para asignar etiquetas a las componentes conexas, para posteriormente obtener el área de dichas
etiquetas.
Se logró tratar las imágenes dentro del sistema de tal manera que los usuarios no tienen que
adentrarse en las características de las imágenes, solo basta con que le indiquen al sistema cuáles
son las imágenes que desean incluir en el análisis.
Se obtuvo al final de este trabajo terminal un sistema que ofrece una interfaz que facilita su
utilización para los usuarios finales, con un diseño agradable y serio y, con información y diálogos
claros para establecer la comunicación con los usuarios.
Con el fin de tener un producto confiable se realizaron diversas pruebas con varias micrografías,
las cuales fueron proporcionadas por los usuarios finales (investigadores), con el fin de corroborar
los resultados entregados por el sistema con los resultados que ellos calculaban de la forma
tradicional.
De acuerdo a las pruebas realizadas con el sistema y en comparación con los resultados obtenidos
por los investigadores, podemos concluir que el “Sistema analizador de micrografías de
hipocampos murinos para la cuantificación neuronal” resulta una herramienta que agiliza los
experimentos realizados por los investigadores de área de enfermedades crónico-degenerativas, ya
que reduce en tiempo la tarea de contabilizar las células contenidas en las micrografías.
Se concluye también que al final de este trabajo terminal se cumplió el objetivo propuesto al inicio
de éste:
Diseñar e implementar un sistema computacional capaz de cuantificar las neuronas presentes en
micrografías de hipocampos murinos, que sirva para agilizar en tiempo y esfuerzo el trabajo que
llevan a cabo los investigadores del laboratorio de neuroinflamación de la Universidad
Autónoma del Estado de Morelos. </Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE CÓMPUTO
ESCOM
Trabajo Terminal
“Sistema analizador de micrografías de hipocampos murinos
para la cuantificación neuronal”
2015-A045
Presentan
Delgado Díaz de León Raúl Emmanuel
Morales Rojas Griselda Guadalupe
Pérez Ventura Salomón 
JUNIO 2016</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>API PARA EL DESARROLLO
DE SISTEMAS DE RECOMENDACIÓN</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>Actualmente, la cantidad de información que existe en Internet es inmensa, esto nos
muestra el creciente problema para los usuarios al encontrar algo que les sea relevante; entre
mayor es la cantidad de información, más díficil se vuelve la búsqueda. Es aquí donde los
sistemas de recomendación tienen su razón de ser. Tan solo ejemplos claros como Google o
Facebook hacen uso de este tipo de sistemas para mostrar los resultados de las búsquedas,
o del contenido que se muestra en el feed de noticias. El uso de los sistemas de recomendación
ha permitido el auge de sistemas como los son Amazon, Spotify y Netflix. En general,
todo el sistema de comercio electrónico se ha visto beneficiado con el uso de los sistemas de
recomendación.
Es por esto, que cada vez más sistemas utilizan este tipo de herramientas para brindar
a sus usuarios servicios añadidos que los destaquen entre otras plataformas. Sin embargo,
la construcción de un sistema de recomendación no es una tarea sencilla. Requiere de un
proceso consciente de análisis para la manera en que los artículos y usuarios se verán relacionados.
Así, supone tener un conocimiento de las características propias de un sistema de
recomendación, los algoritmos que resuelvan la obtención de recomendaciones y al mismo
tiempo, resolver el problema en particular para los artículos y usuarios, lo cual recae en una
marcada curva de aprendizaje así como en un mayor tiempo de desarrollo para este tipo de
sistemas.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Diseñar e implementar una API que brinde funciones de abstracción, operación de datos
y análisis de los mismos a través de las características de la información para obtener, por
medio de evaluaciones y predicciones, un conjunto de datos que represente una recomendación.
</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>El uso de sistemas de recomendación se ha extendido popularmente en los últimos años,
teniendo ejemplos en casi cualquier parte de la web, siendo más común su uso en sistemas
de comercio electrónico. Sin embargo, el desarrollo de aplicaciones o sistemas que contengan
dentro de sus características generar una recomendación requiere un conocimiento especializado
en este rubro por parte de los desarrolladores para construir el sistema desde cero,
o bien conlleva el uso de componentes comerciales que terminan formando parte del sistema
completo. Esto implica que al momento de desarrollar un sistema que resuelva una
necesidad a través de un sistema de recomendaciones, el programador debe adquirir los conocimientos
necesarios para desarrollar el sistema de recomendaciones al mismo tiempo que
intenta resolver el problema de dominio al que se está enfrentando. La API a desarrollar
permitirá al programador obtener las herramientas necesarias para la implementación de
un sistema de recomendación utilizando conocimientos de la estadística para implementar
sistemas de recomendación basados en contenido, colaborativos o híbridos a través de un
modelo de datos adecuado que permita realizar dichos procedimientos y así reducir la curva
de aprendizaje obligatoria durante la incursión en un dominio de aplicación no conocido,
permitiendo al desarrollador concentrar sus esfuerzos en resolver el problema particular de
su caso de estudio.
La API brindará funcionalidades propias de los sistemas de recomendación mediante
llamadas a bibliotecas que ofrecerán el acceso a las funcionalidades dichas. El desarrollo de
esta API que permita proveer la infraestructura mínima necesaria para crear un sistema
de recomendación,funge como proyecto integrador de los conocimientos adquiridos durante
el estudio de la carrera de Ingeniería en Sistemas Computacionales, para el desarrollo del
citado proyecto se requiere hacer uso de los saberes adquiridos por los participantes durante
su trayectoria escolar tales como los conocimientos en materia de inteligencia artificial,
ingeniería de software, matemáticas discretas, reconocimiento de patrones, programación
orientada a objetos, entre otras. Al final el uso conjunto de los conocimientos mencionados
terminará en dicha API cuyos beneficios pueden ser vistos de manera inmediata al término
de su desarrollo con su verificación y validación en un caso de estudio particular.</Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Para la aplicación cliente correspondiente al caso de estudio de recomendación de platillos,
el sistema tendrá un modelo de datos que parte del modelo general mínimo necesario
para trabajar la información añadiendo información relevante sobre el caso de estudio que
recae en platillos como artículos a recomendar. Considerando al final las siguientes entidades.
Platillos
Usuarios
Restaurantes
Categorías
La figura 6.2 muestra las entidades utilizadas en el sistema de recomendación de platillos
así como las relaciones existentes entre las mismas. En éste los nodos del grafo corresponden
a las entidades Usuario, Platillo, Restaurante y Categoría. Donde las relaciones entre ellos
se describen de la siguiente manera:
Un platillo puede ser de una o más categorías.
Un platillo puede ser servido en uno o más restaurantes.
Un usuario puede agregar un platillo
Un usuario puede interactuar con un platillo a través de un conteo de clics.
Un usuario puede evaluar qué tanto le gustó un platillo a través de un rating cuantitativo</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>INSTITUTO POLITÉCNICO NACIONAL
ESCUELA SUPERIOR DE CÓMPUTO
ESCOM
Trabajo Terminal
“API PARA EL DESARROLLO
DE SISTEMAS DE RECOMENDACIÓN”
2015-A056
Presentan
López Garduño Blanca Azucena
Bautista de Jesús Héctor Gerardo
Castro Esparza José Antonio
 Directores
 M. en E. Carlos Silva Sánchez M. en C. Rocío Reséndiz Muñoz
Mayo 2016</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
<div class='tablas_corpus'>
	<table>
		<Tesis>
			<tr>
				<td width='180' id='campos' >Grado_de_Tesis</td>
				<td width='1000'>
					<Grado_de_Tesis>Maestria</Grado_de_Tesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Titulo</td>
				<td width='1000'>
					<Titulo>Implementación de un modelo numérico y diseño de una interfaz para la interacción de microondas con emulsiones de petróleo crudo, aplicado al deshidratado y desalado.</Titulo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Problema</td>
				<td width='1000'>
					<Problema>En México los recursos derivados de las actividades en la industria del petróleo
son de vital importancia, el funcionamiento de la economía nacional depende en
buena medida de ello. Aún por encima de esto, se puede decir que en la
actualidad un país que no tenga independencia energética verá altamente
comprometidos sus intereses y dependerá en gran medida de naciones
extranjeras para su correcto funcionamiento.
Por lo anterior, encontrar alternativas para tratar con la mayor eficacia posible el
crudo obtenido del subsuelo es un asunto de gran importancia. Bajo esta óptica
las alternativas que provee el calentamiento volumétrico que generan las
microondas cobran importancia dado que el sistema de agua-aceite-sales, en el
que se encuentra incluido como componente el petróleo crudo, provoca muchos
problemas en su extracción, transporte por ductos y refinación. Esto porque que
estas emulsiones generan corrosión y provocan que las tecnologías de separación
existentes sean cada vez menos eficaces.
Vale la pena mencionar también que los parámetros de interés reológicos y de
composición del crudo que se extrae cambian con el tiempo, la etapa de
extracción en la que se encuentre el pozo y la zona de ubicación del mismo. Por lo
tanto, aunque se considere un modelo a la medida para desarrollar nuevos tipos
de deshidratadores y desaladores, éstos deben de cambiar sus parámetros de
funcionamiento de acuerdo a las condiciones antes mencionadas del crudo
extraído.
</Problema>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Objetivo</td>
				<td width='1000'>
					<Objetivo>Objetivos generales.
 Implementación de un modelo matemático de la interacción de las
microondas con un sistema de crudo-agua.
 Concepción y diseño de un dispositivo aplicador.
 Concepción y diseño de una interfaz que se pueda implementar en planta
para interactuar con el modelo.

 Objetivos particulares:
 Analizar e implementar una representación de las ecuaciones de Maxwell
que permita resolver la intensidad del campo eléctrico generado por las
microondas, tomando en cuenta los parámetros de las dimensiones del
dispositivo, la potencia de entrada y los materiales involucrados.
 Analizar e implementar las ecuaciones que representen la transferencia del
calor volumétrico generado por las microondas y la velocidad del flujo.
 Diseñar de acuerdo a los modelos anteriores las geometrías más
apropiadas para el aplicador.
 Implementar una interfaz para interactuar con el modelo en cuestión,
permitiendo su implementación en un dispositivo de planta para la
visualización de los datos y la definición de parámetros de operación.</Objetivo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Hipotesis</td>
				<td width='1000'>
					<Hipotesis>Es posible utilizar la implementación de un modelo matemático para demostrar la
viabilidad de implementar a escala industrial un dispositivo deshidratador de crudo
que tome como base de funcionamiento el calentamiento volumétrico generado
por las microondas, así como desarrollar dispositivos de control más eficientes que
dentro de su implementación incluyan el modelo en cuestión, así como una
interfaz de control en tiempo real del proceso.</Hipotesis>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Justificacion</td>
				<td width='1000'>
					<Justificacion>Hasta el momento de escribir esta tesis aparentemente no existe ningún trabajo de
modelación realizado en México que involucre calentamiento volumétrico vía
microondas y emulsiones de petróleo crudo, así como ningún trabajo registrado a
nivel mundial que demuestre la inviabilidad de utilizar esta tecnología (microondas)
para el fin buscado en este trabajo. Por el contrario, en la literatura se registra
como un área de oportunidad importante la interacción de microondas con fluidos
viscosos en el sector industrial.
Entonces es una necesidad el desarrollo de dispositivos deshidratadores más
eficientes que tengan la capacidad de proveer facilidades para que los ingenieros
de planta ajusten los parámetros de funcionamiento de los mismos. Esto con el fin de incrementar la productividad, hacer más eficiente el consumo energético y por
medio de la visualización dar una idea más precisa de lo que ocurre en el proceso.
Las microondas proveen una alternativa para desarrollar este tipo de dispositivos
deshidratadores, pero para este tipo de tecnología la relación entre los parámetros
de frecuencia, la geometría y constitución de los materiales es un aspecto vital
para el correcto funcionamiento de los mismos. La forma en que trabajan las
microondas sobre los materiales de interés es compleja si lo comparamos con los
parámetros a considerar cuando se diseñan dispositivos que utilizan otras fuentes
de calor.
Adicionalmente, poder contar con un modelo que represente el calentamiento
volumétrico vía microondas de las emulsiones en cuestión, tomando en cuenta
aspectos como la velocidad del flujo del fluido y las dimensiones del dispositivo,
cobra mucha importancia para el diseño del dispositivo y la implementación de
algún tipo de control sobre el proceso. También cobra importancia que este mismo
modelo tenga la capacidad de adaptarse a la información en tiempo real, por parte
del usuario o de algún sensor cuando este se encuentre operando en planta. </Justificacion>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Metodologia</td>
				<td width='1000'>
					<Metodologia>4.1 Modelación del fenómeno electromagnético.
A fin de representar el calentamiento volumétrico en el proceso de desalado y
deshidratado de crudo se partió de las ecuaciones de Maxwell, con lo que se
obtuvo un modelo que representa la interacción de las microondas con el sistema
crudo-agua-sales. Tomando en cuenta la naturaleza armónica de los campos
electromagnéticos (que varían sus intensidades en forma senosoidal en el
tiempo), es posible usar una conversión fasorial para obtener las ecuaciones de
Maxwell, conocidas como ecuaciones fasoriales de Maxwell o en el dominio de la
frecuencia [14], [17] y [18].
Las ecuaciones de Maxwell en el dominio de la frecuencia forman un sistema
subdeterminado. Este problema se aborda por medio de establecer las relaciones
constitutivas que aportan una forma de relacionar también al material a ser
irradiado en la determinación de las magnitudes de los campos involucrados [17].
Se determinó una ecuación que se pueda resolver para el campo eléctrico y que
incluya los parámetros de interés del sistema involucrado, para de esta manera
poder simular en primera instancia el comportamiento de los campos
electromagnéticos en guías de onda y cavidades resonantes.
Posteriormente se procedió a determinar la frecuencia de trabajo, a diseñar una
guía de onda en una geometría en 3D para probar la ecuación por medio de una
simulación de la misma. Esto incluyó determinar las medidas de la misma el modo
de transmisión y la frecuencia de corte.
4.2 Selección de la frecuencia de operación.
Todo el equipo desarrollado para generar calentamiento volumétrico por lo regular
se diseña para trabajar en modo TE10 y en la frecuencia de 915 MHz o 2.45 GHz.
Dado que gran parte del espectro electromagnético que se encuentra en la
frecuencia de microondas, estas se utilizan para otras aplicaciones como
comunicaciones u operaciones con radar [24]. Se optó por seleccionar la
frecuencia de 2.45 GHz en el modo TE10 para simular el calentamiento volumétrico
en el material de interés, que en este caso es el sistema de crudo agua y sales.
4.3 Diseño de una guía de onda.
Cualquier diseño de equipo de calentamiento volumétrico hace indispensable el
diseño de una guía de onda que se adapte a las necesidades del problema. Para
fines de desarrollar una simulación más apegada a la realidad, se hizo necesario
diseñar y simular la ecuación obtenida en una guía de onda rectangular. Ésta será
la encargada de llevar (guiar) las ondas electromagnéticas desde el lugar en
donde se generan al lugar en donde han de aplicarse. Esto es necesario ya que,
inclusive para procesos tan sencillos como un horno de microondas doméstico, se
necesita una guía de onda para llevar las microondas a la cavidad en donde se
calientan los alimentos.
Dado que determinar las dimensiones de una guía de onda es un trabajo muy
complejo, se procedió a utilizar algunos parámetros sobre las guías de onda
disponibles comercialmente [2] [29] y con base en estos diseñar una que se
adapte a las necesidades del problema.
Se procedió a calcular la frecuencia de corte de ambas estructuras y la longitud de
onda esperada para la frecuencia propuesta. Se seleccionó la guía de onda WR-
430, por su frecuencia de corte inferior y por conveniencia de su tamaño, ya que
una frecuencia de corte inferior funciona como un filtro. Esto para evitar que las
ondas electromagnéticas que no se encuentran en la frecuencia apropiada
participen en el proceso.
Así se procedió a crear en un software para diseño asistido por computadora un
modelo tridimensional de la guía de onda basado en las dimensiones propuestas.
Se utilizó un software basado en el elemento finito para simular el comportamiento
de las ondas electromagnéticas en la guía de onda, basado en la ecuación
obtenida y tomando en cuenta las condiciones de frontera impuestas por la física
del problema.
4.4 Modelación del transporte de calor generado por el calentamiento volumétrico.
Partiendo del hecho de que se quiere aprovechar al máximo la interacción
de las microondas con el sistema en cuestión, se partió de un primer prototipo, en
donde el contenedor aprovecha al máximo los factores como el flujo de la
emulsión, considera la profundidad de penetración, y otros factores de interés.
Dada la naturaleza de las ondas electromagnéticas, no resulta de ninguna utilidad
modelar un fenómeno si no se toma en cuenta las dimensiones del dominio
implicado. Considerando todos estos factores se determinó que el prototipo más
apropiado sea el de un tubo de diámetro un poco menor a la dimensión horizontal
de la guía de onda rectangular. Esto con la finalidad de eliminar la necesidad de
otra cavidad resonante, el cual en la implementación del prototipo, en la sección del mismo que atraviese por la guía de onda estará hecho de cuarzo. El cuarzo es
un material casi transparente para las microondas, y este tubo de cuarzo a su vez
estará recubierto por fuera de un metal altamente conductor (cobre) para evitar
que las microondas escapen hacia el exterior o a zonas no deseadas.
De acuerdo con la teoría de los medios continuos que considera los objetos de
estudios como sistemas continuos finitos con fronteras bien definidas (se puede
contrastar contra la teoría cuántica) y a los antecedentes de trabajos directamente
relacionados con este, en uno de los cuales se está trabajando en un medidor de
la constante dieléctrica, se decidió considerar al sistema de crudo-agua-sales
como un medio isotrópico, esto también tomando en cuenta la alta estabilidad de
las diferentes fases del sistema y con el fin de poder simplificar la implementación
del modelo, esto se debe a que en los primeros intentos, el modelo saturaba la
capacidad de los equipos de cómputo disponibles, aún después de 7 horas de
procesamiento de un solo problema para reflejar el calentamiento volumétrico de 1
cm3
de agua, por lo tanto desde muy temprano en el trabajo se hizo necesario
considerar todas las optimizaciones posibles aplicables a la simulación.
En los problemas que tratan con fenómenos electromagnéticos es necesario
considerar la matriz de dispersión, que en resumen podemos describirla como una
cuantificación del efecto que pueden tener las diferentes fuentes de energía
electromagnética sobre otras fuentes que participen dentro del mismo sistema. Por
tanto procedimos a establecer la matriz de dispersión.
Dada la naturaleza de los problemas electromagnéticos y sus condiciones de
frontera se pudo dividir los dominios de estudios en forma simétrica, esto con el fin
de disminuir la cantidad de cálculos que realizo el equipo de cómputo que
teníamos disponible.
Partiendo desde una ecuación de balance de energía, se utilizó la ley de Fourier
que relaciona el flujo de energía desconocido con el gradiente de temperatura.
Para la simulación de la parte electromagnética solo se utilizaron las propiedades
de dos materiales el Cobre y el Aire.
Después se procedió a alimentar las propiedades del agua al modelo, cabe aclarar
que en este no se incluyeron fenómenos propios del agua como la ebullición o
evaporación, concentrándonos exclusivamente en el incremento de temperatura.
4.5 Acoplamiento del movimiento del fluido al transporte de calor.
Al igual que en otras partes del modelo, la parte que considera el movimiento del
fluido podría modelarse por separado y resolverse para considerarla dentro del
mismo, para esto podría utilizarse las ecuaciones de Navier-Stokes en caso de
que el fluido requiera de una consideración más exacta, pero en nuestro caso
dadas las investigaciones preliminares de otros trabajos relacionados con este y el
considerar que las cuestiones de transporte del sistema de crudo-agua-sales no
requieren de geometrías muy complejas, se consideró un movimiento del fluido en
la dirección de la coordenada Z del modelo
Al ser prácticamente dos modelos que se resuelven uno en el dominio de la
frecuencia y otro en el dominio del tiempo, se vuelven un poco complejas las
manipulaciones, pero se incluyeron otras condiciones de frontera para que el
modelo reflejara más las condiciones del proceso.
Se procedió a simular nuevamente el proceso con las nuevas condiciones de
frontera.
4.6 Validación del modelo.
Cabe mencionar que en [12], dadas las condiciones de los fenómenos
electromagnéticos el diseño de la simulación es también el diseño de la validación
y que aún sin la realización del prototipo físico, la simulación en [12] fue aceptada
como una publicación, y que el diseño a detalle del dispositivo validador también
fue aceptado como publicación por separado, esto sin que existan evidencias de
publicaciones posteriores de que estos prototipos hayan sido llevados a cabo.
Como un método de validación adicional se procederá a utilizar el mismo método
para reproducir el experimento que se llevó a cabo en [5], dado que este
experimento reporta la información necesaria para poder reproducirlo en una
simulación así como tablas de los resultados censados durante la ejecución del
experimento.
4.7 Experimento de Nour, Nour y Yunus en [5].
Para poder reproducir el experimento en [5] se tiene que primero modelar las
geometrías del microondas, en este experimento se reporta la utilización de un
microondas EMO 880 SS, pero no reportan las medidas del mismo, sobre todo las
medidas de su cavidad resonante ni de su guía de onda, dado que el diseño de cavidades para calentamiento volumétrico y su acoplamiento con las guías de
onda y el diseño de los respectivos iris es una labor extremadamente compleja, se
optó por utilizar las medidas de una cavidad conocida, como la del microondas de
laboratorio Perkin Elmer modelo PaarPhysicaMultiwave, se eligió este modelo en
particular porque las dimensiones de su guía de onda coinciden al menos en un
95% con las empleadas en la simulación numérica de nuestro trabajo de
investigación.
En cuanto al vaso de precipitado que contiene el fluido, este experimento no
menciona las dimensiones de este vaso, pero menciona la capacidad del mismo,
que es de 800 ml y hace referencia al vaso utilizado en el experimento publicado
en [10] el cuál si menciona las dimensiones.
A partir de estos datos duros se procedió a construir el modelo que representará
esta configuración. Se procedió a simular el calentamiento volumétrico para los
materiales implicados en el trabajo que son el agua y el crudo iraní a 20, 40, 60 y
80 segundos.</Metodologia>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Resultados</td>
				<td width='1000'>
					<Resultados>Se pudieron implementar los siguientes modelos numéricos e interfaz:
 Se pudo deducir a partir de ecuaciones generales conocidas y
ampliamente estudiadas, un modelo que representará en una medida
aceptable el comportamiento de las ondas electromagnéticas con los
fluidos de interés.
 En base al modelo obtenido, se pudo determinar la intensidad del campo
eléctrico dentro de la geometría de interés, y de esta manera obtener el
calor volumétrico que puede ser generado por este.
 Con la ayuda del modelo se pudo diseñar tanto de una guía de onda como
el de un aplicador de calor volumétrico al proceso de deshidratado y
desalado de crudo.
 La geometría diseñada para la aplicación del calentamiento volumétrico, es
la interfaz para que el modelo interactúe con el fluido de interés. Este
modelo demostró la validez de esta interfaz, mostrando que es posible su
funcionamiento e implementación en el campo industrial, haciendo notar el
hecho de que faltaría aún el correspondiente trabajo interdisciplinario
industrial para poder llevar a la realidad este dispositivo.
 Se pudieron validar los modelos desarrollados en este trabajo por medio de
la comparación de los resultados de la simulación contra datos
experimentales obtenidos de publicaciones, que fueron revisadas por
cuerpos colegiados, es decir con arbitraje estricto internacional.
 Se logró que el modelo .
Conclusión general.
Como conclusión general se puede mencionar que de acuerdo a lo demostrado
por los trabajos de simulación se puede determinar que la forma en que es posible
encarar el reto del escalamiento industrial de la tecnología del calentamiento
volumétrico al proceso de deshidratado y desalado del crudo es por medio de
aplicar este en un proceso en flujo, corroborando de esta manera las ideas que
surgieron dentro del equipo de trabajo, de esta forma se puede inferir que de
acuerdo a las tecnologías ya implementadas dentro de la industria petrolera de
nuestro país, esta tecnología se puede implementar como complemento a las ya
instaladas, y que de esta manera con los adecuados estudios y reconfiguraciones,
hacer más eficiente el proceso de refinación del crudo extraído en nuestro país.
También y de acuerdo a los estudios preliminares y a las políticas operativas
imperantes en las refinerías de nuestro país respecto a las temperaturas a las que
se debe manejar el crudo, se puede concluir que será necesario implementar
también pequeñas modificaciones en los puntos posteriores al sitio de irradiación
en los ductos que portan el fluido para propiciar una mejor distribución de la
temperatura en el fluido, lo que a su vez pudiera favorecer el proceso de
separación de emulsiones de crudo.
</Resultados>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Tipo</td>
				<td width='1000'>
					<Tipo>positivo</Tipo>
				</td>
			</tr >
			<tr>
				<td width='180' id='campos' >Otros</td>
				<td width='1000'>
					<Otros>Instituto Politécnico nacional
Centro de Investigación en Ciencia Aplicada y
Tecnología Avanzada – Unidad Altamira.
Implementación de un modelo numérico y diseño de
una interfaz para la interacción de microondas con
emulsiones de petróleo crudo, aplicado al
deshidratado y desalado.
TESIS
Para obtener el grado de
MAESTRÍA EN TECNOLOGÍA AVANZADA
Presenta:
Luis Javier Andrade Cruz
Director de tesis:
Dr. Arturo López Marure
Altamira Tamaulipas....................Agosto 2014</Otros>
				</td>
			</tr >
		</Tesis>
	</table>
</div>
</Informacion>
</body>
